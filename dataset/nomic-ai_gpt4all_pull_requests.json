[
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-22T00:53:44Z",
        "closed_at": "2023-10-22T04:22:37Z",
        "merged_at": "2023-10-22T04:22:37Z",
        "body": "Follow-up to #1532",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-20T14:32:48Z",
        "closed_at": "2023-10-21T18:56:07Z",
        "merged_at": "2023-10-21T18:56:07Z",
        "body": "corrected model download directory\r\n\r\n## Describe your changes\r\nthe model download directory was wrong\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [X ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 58,
        "deletions": 63,
        "changed_files": 9,
        "created_at": "2023-10-19T00:37:42Z",
        "closed_at": "2023-10-21T14:38:46Z",
        "merged_at": "2023-10-21T14:38:46Z",
        "body": "This will make the speech output less verbose for screen reader users.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 11,
        "changed_files": 6,
        "created_at": "2023-10-18T21:14:38Z",
        "closed_at": "2023-10-19T00:24:55Z",
        "merged_at": "2023-10-19T00:24:55Z",
        "body": "This PR bumps the version of the python bindings to 2.0.0rc1, and fixes the CircleCI Windows build.\r\n\r\n### Wish list\r\n- [x] Be more robust against interrupted model retrieval due to flaky connections\r\n- [x] Display a meaningful error message when trying to load a GGML file, instead of a generic one\r\n- [x] Test macOS build",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 1038,
        "changed_files": 10,
        "created_at": "2023-10-16T02:06:25Z",
        "closed_at": "2023-10-19T19:25:18Z",
        "merged_at": "2023-10-19T19:25:18Z",
        "body": "Add replit-code-v1.5 support by fixing upstream first and then removing our custom MPT implementation in favor of llama.cpp.\r\n\r\nThis has been tested on CPU and CUDA, and via the python bindings. It does not currently support Vulkan due to not having an Alibi kernel.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-13T14:52:02Z",
        "closed_at": "2023-10-18T16:23:01Z",
        "merged_at": "2023-10-18T16:23:01Z",
        "body": "I tried to install from pip on OS X 10.13 High Sierra and got \"load command 0x80000034 is unknown\" while trying to load a .dylib. This corresponds to the LC_DYLD_CHAINED_FIXUPS command, which is only available as of OS X 10.15 (see [10.14 SDK](https://github.com/phracker/MacOSX-SDKs/blob/041600eda65c6a668f66cb7d56b7d1da3e8bcc93/MacOSX10.14.sdk/usr/include/mach-o/loader.h#L313), [10.15 SDK](https://github.com/phracker/MacOSX-SDKs/blob/041600eda65c6a668f66cb7d56b7d1da3e8bcc93/MacOSX10.15.sdk/usr/include/mach-o/loader.h#L324)).\r\n\r\nrelated to #70 (although that issue mainly refers to the chat client)",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-13T05:09:44Z",
        "closed_at": "2023-10-13T18:28:03Z",
        "merged_at": null,
        "body": "testing / debugging\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-12T03:54:15Z",
        "closed_at": "2023-10-12T18:56:55Z",
        "merged_at": "2023-10-12T18:56:54Z",
        "body": "- [x] reenable gpu prompt processing\r\n- [x] q6k mat*mat works\r\n- [x] q4_1 mat*mat works\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-11T21:35:45Z",
        "closed_at": "2023-10-12T11:52:56Z",
        "merged_at": "2023-10-12T11:52:56Z",
        "body": "file in models2.json has no `ggml-` prefix\r\nhttps://discordapp.com/channels/1076964370942267462/1093558720690143283/1161778216462192692\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 74,
        "deletions": 156,
        "changed_files": 9,
        "created_at": "2023-10-11T16:15:16Z",
        "closed_at": "2023-10-12T11:52:12Z",
        "merged_at": "2023-10-12T11:52:12Z",
        "body": "This also changes the UI behavior to always open a 'New Chat' and setting it as current instead of setting a restored chat as current. This improves usability by not requiring the user to wait if they want to immediately start chatting.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 192,
        "deletions": 31,
        "changed_files": 5,
        "created_at": "2023-10-10T20:44:36Z",
        "closed_at": "2023-10-11T13:16:02Z",
        "merged_at": "2023-10-11T13:16:02Z",
        "body": "This restores the state from a file even if the model is missing for instance. This is necessary for the new release since all the models will have changed. This will also enable a new feature in an upcoming commit where we have a new setting for always restoring state from text instead of via kvcache so as to limit disk usage.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-10T18:12:09Z",
        "closed_at": "2023-10-11T21:14:37Z",
        "merged_at": "2023-10-11T21:14:37Z",
        "body": "* disable llama.cpp logging unless GPT4ALL_VERBOSE_LLAMACPP envvar is nonempty\r\n* make verbose flag for retrieve_model default false (but also be overridable via gpt4all constructor)\r\n\r\nshould be able to run a basic test:\r\n\r\n```python\r\nimport gpt4all\r\nmodel = gpt4all.GPT4All('/Users/aaron/Downloads/rift-coder-v0-7b-q4_0.gguf')\r\nprint(model.generate('def fib(n):'))\r\n```\r\n\r\nand see no non-model output when successful\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-10T15:41:21Z",
        "closed_at": "2023-10-11T15:30:49Z",
        "merged_at": "2023-10-11T15:30:49Z",
        "body": "Fixes #1479",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-10T14:49:06Z",
        "closed_at": "2023-10-10T19:50:03Z",
        "merged_at": "2023-10-10T19:50:03Z",
        "body": "I forgot to uncomment these lines when I finished GPT-J support.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-10T14:18:36Z",
        "closed_at": "2023-10-11T15:31:34Z",
        "merged_at": "2023-10-11T15:31:34Z",
        "body": "I saw this while building with Visual Studio. It seems like a good idea to initialize responseLogits before serializing it, even if its value isn't used anywhere.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-09T17:40:06Z",
        "closed_at": "2023-10-10T15:44:43Z",
        "merged_at": "2023-10-10T15:44:43Z",
        "body": "## Describe your changes\r\n\r\nAdded the just-released EM German Mistral Model to `models2.json`.\r\nYou can find more information, example outputs and so on in [the models repository](https://github.com/jphme/EM_German/tree/main). The model comes with the Apache-2.0 license of the base model.\r\n\r\nTBH, I still don't understand 100% how GPT4ALL`s model list works (I suppose its still based on a proprietary hosted model list?); the model wasn't available for download in the model list when I rebuild GPT4all, but I confirmed that the [model works falwlessly](https://drive.google.com/file/d/1T6AIhtew4K3fQMDKiOBT6YIeM2m7rpVa/view) within the most recent app.\r\n\r\nI asked in Discord and they said I should just prepare a PR.\r\n\r\nReson for this is I am the author of the most-downloaded llama-2 model finetuned on German texts and people kept asking me how to use it in a GUI; would be good to have a user-friendly alternative to LM Studio, thus I would appreciate support within GPT4ALL. \r\n\r\n## Checklist before requesting a review\r\n- [X ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 60,
        "deletions": 18,
        "changed_files": 9,
        "created_at": "2023-10-02T14:07:30Z",
        "closed_at": "2023-10-05T14:13:21Z",
        "merged_at": "2023-10-05T14:13:21Z",
        "body": "## Describe your changes\r\n\r\nShow a specific warning in the GUI when a GPU device was chosen but was not used.\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1291,
        "deletions": 0,
        "changed_files": 17,
        "created_at": "2023-10-01T19:16:25Z",
        "closed_at": "2023-10-03T02:58:30Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-09-30T23:22:51Z",
        "closed_at": "2023-10-11T18:12:40Z",
        "merged_at": "2023-10-11T18:12:40Z",
        "body": "This makes it easier to use the GPT4All interface from Python with arbitrary paths, which may be of type pathlib.Path instead of a string.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 187,
        "deletions": 85,
        "changed_files": 5,
        "created_at": "2023-09-30T01:12:58Z",
        "closed_at": "2023-10-11T17:56:43Z",
        "merged_at": null,
        "body": "bert+vulkan wip\r\n\r\n- \u274c removed memory estimation & buffer resizing code - currently allocating a worst-case size - small embeddings currently use more memory than needed\r\n * [x] (f16) bert loads onto gpu\r\n * [x] still works on cpu\r\n * [ ] gpu impl doesn't work - encounters unimplemented ggml ops\r\n    *  [x] f16 mat*mat\r\n ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-21T13:56:31Z",
        "closed_at": "2023-10-12T11:52:36Z",
        "merged_at": "2023-10-12T11:52:36Z",
        "body": "## Describe your changes\r\n\r\nAdded star-history to README.md\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 196,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2023-09-19T18:00:37Z",
        "closed_at": "2023-09-27T18:24:21Z",
        "merged_at": "2023-09-27T18:24:21Z",
        "body": "working now - uploads to the circleci job as artifacts\r\n- [x] windows\r\n- [x] linux\r\n- [x] macos\r\n- [x] make update button link to website instead of launching maintenance tool",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 48,
        "deletions": 4,
        "changed_files": 9,
        "created_at": "2023-09-18T23:00:15Z",
        "closed_at": "2023-10-12T15:57:55Z",
        "merged_at": null,
        "body": "Allows disabling (most) logging in the backend - unless it is enabled by another option explicitly requesting it, or is fatal - and turns this option on for the Python binding builds in CircleCI\r\n\r\nMostly ifdefs around logging statements, except in `llama.cpp` (the file) (see submodule change and PR https://github.com/nomic-ai/llama.cpp/pull/6 to update master) where I instead ifdef away `fprintf` to avoid introducing more merge conflicts than absolutely necessary.\r\n\r\nThis notably does not prevent the \r\n\r\n```\r\nobjc[66083]: Class GGMLMetalClass is implemented in both /Users/aaron/proj/gpt4all/gpt4all-bindings/python/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x110744210) and /Users/aaron/proj/gpt4all/gpt4all-bindings/python/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x111e6c210). One of the two will be used. Which one is undefined.\r\n```\r\n\r\nmessage that occurs on MacOS - this comes from the Objective-C runtime and is printed because more than one library that is statically linked against llama.cpp has a `GGMLMetalClass` defined in it. This is difficult to prevent because Obj-C classes are not namespaced and cannot be hidden like other symbols in statically linked libraries, and we currently load multiple model backend libraries that may link this code into the same app simultaneously.\r\n\r\nThis is at least *harmless*, as the class [contains no code](https://github.com/ggerganov/llama.cpp/blob/8781013ef654270cbead3e0011e33a6d690fb168/ggml-metal.m#L118-L121) and is only used to leverage Objective-C functionality on MacOS to [find the library path](https://github.com/ggerganov/llama.cpp/blob/8781013ef654270cbead3e0011e33a6d690fb168/ggml-metal.m#L158-L159) - all of our backend files live in the same directory, so it does not matter which implementation is actually used here. In order to *prevent* this message from being output we would need to either prevent multiple backend libraries from being resident at once (which would mean making them able to be *unloaded* and *reloaded* safely, likely not currently the case), *OR* somehow generate different classnames for each library that links llama.cpp (a change not likely to be upstreamable as its only needed accommodate loading multiple distinct instances of the llama.cpp library into the same application)\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 119,
        "deletions": 4,
        "changed_files": 8,
        "created_at": "2023-09-15T16:59:16Z",
        "closed_at": "2023-09-18T20:30:13Z",
        "merged_at": null,
        "body": null,
        "comments": 3
    },
    {
        "merged": true,
        "additions": 223,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-09-15T08:52:10Z",
        "closed_at": "2023-10-05T14:12:44Z",
        "merged_at": "2023-10-05T14:12:44Z",
        "body": "## Describe your changes\r\nAdd flatpak manifest for building flatpaks on Linux\r\n\r\nRefer: #698 \r\n\r\n# TODOs:\r\n- [x] Create flatpak manifests\r\n- [ ] Add circleci tests\r\n- [ ] submit to flathub\r\n\r\nTesters and reviews welcome! You will need  org.kde.Sdk 6.5 and org.freedesktop.Sdk.Extension.node14 for it to build. Please do install it before running flatpak-builder.\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2023-09-14T12:26:15Z",
        "closed_at": "2023-09-14T13:59:38Z",
        "merged_at": "2023-09-14T13:59:38Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 265,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2023-09-13T09:57:23Z",
        "closed_at": "2023-10-12T11:53:12Z",
        "merged_at": "2023-10-12T11:53:12Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ x] I have performed a self-review of my code.\r\n- [ x] If it is a core feature, I have added thorough tests.\r\n- [ x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-09-12T19:40:09Z",
        "closed_at": "2023-09-12T21:16:02Z",
        "merged_at": "2023-09-12T21:16:02Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-12T18:36:16Z",
        "closed_at": "2023-09-13T22:16:25Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-12T18:01:18Z",
        "closed_at": "2023-10-12T11:55:47Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-05T20:54:44Z",
        "closed_at": "2023-09-11T15:44:40Z",
        "merged_at": "2023-09-11T15:44:40Z",
        "body": "Removes extra transitive deps (except for libvulkan) when building model backends with Vulkan support (libkompute, libfmt)\r\nto avoid some headaches with dynamic linking.\r\n\r\nchanges are in llama.cpp, related pr: https://github.com/nomic-ai/llama.cpp/pull/2",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-05T14:13:07Z",
        "closed_at": "2023-10-12T11:55:23Z",
        "merged_at": null,
        "body": "Fix for executable `glscl not found` on win11 when running build_win-mingw.ps1 in gpt4all-binding/csharp directory\r\n\r\nFull error message:\r\n`-- Found Vulkan: C:/VulkanSDK/1.3.261.1/Lib/vulkan-1.lib (found version \"1.3.261\") found components: glslc glslangValidator                                                   CMake Error at llama.cpp.cmake:160 (message):                                                                                                                                   glslc not found                                                                                                                                                             \r\nCall Stack (most recent call first):                                                                                                                                            \r\nCMakeLists.txt:46 (include)`\r\non win11, from PowerShell run with admin rights\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 594,
        "deletions": 86,
        "changed_files": 21,
        "created_at": "2023-08-31T17:02:17Z",
        "closed_at": "2023-08-31T19:21:00Z",
        "merged_at": null,
        "body": "- starcoder: use ggml_graph_plan\n- bump llama.cpp version + needed fixes for that\n- Update to newer llama.cpp and disable older forks.\n- Nomic vulkan backend licensed under the Software for Open Models License (SOM), version 1.0.\n- Add SOM to codespell ignore list.\n- Lower case the som.\n- Add a comment indicating future work.\n- Fix for windows.\n- Get VulkanSDK installed on linux circleci.\n- Make it work on gpt4all-backend linux circleci too.\n- Fix yaml parsing\n- Fix missing run in circleci\n- More circleci\n- More circleci\n- More circleci\n- More circleci\n- More circleci\n- More circleci\n- More circleci\n- More circleci\n- More circleci\n- More circleci\n- More circleci\n- more Circleci\n- circleci tweaks\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 616,
        "deletions": 87,
        "changed_files": 21,
        "created_at": "2023-08-30T14:40:37Z",
        "closed_at": "2023-08-31T19:32:08Z",
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-08-25T02:08:20Z",
        "closed_at": "2023-10-19T19:05:21Z",
        "merged_at": "2023-10-19T19:05:21Z",
        "body": "\u2026json#publishconfig\r\n\r\n## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ x ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-08-16T10:49:58Z",
        "closed_at": "2023-10-12T18:01:44Z",
        "merged_at": "2023-10-12T18:01:44Z",
        "body": "Solves #788",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-08-15T12:49:17Z",
        "closed_at": "2023-10-12T18:24:48Z",
        "merged_at": null,
        "body": "## Describe your changes\r\nSimply write some messages to `stderr` so it's easier to identify the problem when the `create`/`create2` call fails.\r\n\r\n## Issue ticket number and link\r\n- #1256 \r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n_None_\r\n\r\n### Steps to Reproduce\r\n_None_\r\n\r\n## Notes\r\nNot sure if this is the right way to go about that, but it's better than not having anything and be left guessing when trying to troubleshoot an issue.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 34,
        "deletions": 11,
        "changed_files": 3,
        "created_at": "2023-08-14T20:03:24Z",
        "closed_at": "2023-08-17T14:32:09Z",
        "merged_at": "2023-08-17T14:32:09Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-08-10T16:35:45Z",
        "closed_at": "2023-08-10T21:56:54Z",
        "merged_at": "2023-08-10T21:56:54Z",
        "body": "## Describe your changes\r\n- minor oversight: there are now six supported architectures\r\n- LLAMA -> LLaMA (for v1)\r\n- note about Llama 2 and link to license\r\n- limit some of the paragraphs to 150 chars\r\n\r\n## Issue ticket number and link\r\n_None_\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n_None_\r\n\r\n### Steps to Reproduce\r\n_None_\r\n\r\n## Notes\r\nMade the PR on my own repository by accident. The note on Llama 2 is what I was talking about on Discord.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5860,
        "deletions": 6922,
        "changed_files": 15,
        "created_at": "2023-08-08T15:30:32Z",
        "closed_at": "2023-08-14T16:45:46Z",
        "merged_at": "2023-08-14T16:45:46Z",
        "body": "* remove packaged yarn\r\n\r\n* prompt templates update wip\r\n\r\n* prompt template update\r\n\r\n* system prompt template, update types, remove embed promises, cleanup\r\n\r\n* support both snakecased and camelcased prompt context\r\n\r\n* fix #1277 libbert, libfalcon and libreplit libs not being moved into the right folder after build\r\n\r\n* added support for modelConfigFile param, allowing the user to specify a local file instead of downloading the remote models.json. added a warning message if code fails to load a model config. included prompt context docs by amogus.\r\n\r\n* snakecase warning, put logic for loading local models.json into listModels, added constant for the default remote model list url, test improvements, simpler hasOwnProperty call\r\n\r\n* add DEFAULT_PROMPT_CONTEXT, export new constants\r\n\r\n* add md5sum testcase and fix constants export\r\n\r\n* update types\r\n\r\n* throw if attempting to list models without a source\r\n\r\n* rebuild docs\r\n\r\n* fix download logging undefined url, toFixed typo, pass config filesize in for future progress report\r\n\r\n* added overload with union types\r\n\r\n* bump to 2.2.0, remove alpha\r\n\r\n## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-08-07T22:31:38Z",
        "closed_at": "2023-08-09T07:27:44Z",
        "merged_at": "2023-08-09T07:27:43Z",
        "body": "## Describe your changes\r\n- Replace high-level `IsProcessorFeaturePresent`\r\n- Reintroduce low-level compiler intrinsics implementation\r\n\r\n## Issue ticket number and link\r\n- #1288\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n_None_\r\n\r\n### Steps to Reproduce\r\n- Try to run GPT4All on an outdated Windows version. It'll fail with the catch-all model load error despite having AVX/AVX2.\r\n\r\n- See the comments in linked issue (from [here](https://github.com/nomic-ai/gpt4all/issues/1288#issuecomment-1667241169)); the [test script](https://gist.github.com/cosmic-snow/6c93526ea4c8b5428def2bf63d6be390) returns `False` for all AVX checks even if present.\r\n\r\n## Notes\r\nIt's quite unfortunate how this all came to be. The previous low-level code was changed in an attempt to fix the Windows AVX-only problem, but that ultimately turned out be caused by an incomplete pre-processor macro. That in turn masked this problem.\r\n\r\nThe (potential) fix here reintroduces the old low-level code. But because of the bug, this was never really tested to work as it should, although I think several people have looked at it, compared with the documentation, and found no obvious fault: <https://learn.microsoft.com/en-us/cpp/intrinsics/cpuid-cpuidex>\r\n\r\nIt does work on my machine, but I don't have an AVX-only system to test, nor do I have an older build of Windows 10.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 174,
        "deletions": 21,
        "changed_files": 20,
        "created_at": "2023-08-07T16:53:54Z",
        "closed_at": "2023-08-07T17:54:13Z",
        "merged_at": "2023-08-07T17:54:13Z",
        "body": "## Describe your changes\r\nadded a dropdown in application settings to allow of transition of fontsize from small, medium, and large.\r\n\r\n## Issue ticket number and link\r\n#986 #647 #488 \r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n\r\n## Demo\r\n\r\nhttps://github.com/nomic-ai/gpt4all/assets/58596666/a5d31239-f304-4fe7-ae2e-430ab031858a\r\n\r\n\r\n\r\n### Steps to Reproduce\r\nchange font size in application settings\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-08-04T01:16:24Z",
        "closed_at": "2023-08-10T15:27:09Z",
        "merged_at": "2023-08-10T15:27:09Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 79,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-08-03T14:41:44Z",
        "closed_at": "2023-08-10T14:43:08Z",
        "merged_at": "2023-08-10T14:43:07Z",
        "body": "## Describe your changes\r\n\r\nAdded an endpoint similar to the completion endpoints that matches OpenAI OpenAPI spec and loads Embed4All into memory\r\n\r\n## Issue ticket number and link\r\n\r\nref: https://twitter.com/andriy_mulyar/status/1682204779357896704?s=20\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 68,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-07-31T15:05:09Z",
        "closed_at": "2023-07-31T16:18:20Z",
        "merged_at": "2023-07-31T16:18:20Z",
        "body": "## Describe your changes\r\nfixed issue of code blocks in light mode would have dark text.\r\n\r\n## Issue ticket number and link\r\nIssue #1291\r\n[https://github.com/nomic-ai/gpt4all/issues/1291](url)\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n\r\n## Demo\r\n<img width=\"1270\" alt=\"image\" src=\"https://github.com/nomic-ai/gpt4all/assets/58596666/195a215e-fbe4-403e-8ee6-7b7148aed9fc\">\r\n\r\n\r\n### Steps to Reproduce\r\nask for code while in light mode\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-07-31T03:52:56Z",
        "closed_at": "2023-08-02T14:58:14Z",
        "merged_at": "2023-08-02T14:58:14Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 5860,
        "deletions": 6922,
        "changed_files": 15,
        "created_at": "2023-07-28T04:28:49Z",
        "closed_at": "2023-08-08T15:29:56Z",
        "merged_at": "2023-08-08T15:29:56Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n- InferenceModel & EmbeddingModel wrappers to hold some state in JS\r\n- simplified downloadModel a little\r\n- retrieveModel now returns the model config from the downloaded json\r\n- createPrompt -> formatChatPrompt and updated it to use promptTemplate & systemPrompt\r\n- added py defaults as defaults to createCompletion\r\n- removed ModelFile types\r\n- removed the extra yarn install as the issues it fixed seems to be solved\r\n\r\nsome work still needed:\r\n- kv cache reuse / proper cache session (and context reset if its not a chat session)\r\n- better types for `loadModel`\r\n- regarding `downloadModel`: prompt templates not getting used when `allowDownload=false` might be a gotcha and make models suddenly perform worse when used offline. we could cache models.json in the `modelsPath` maybe?",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2023-07-27T01:55:57Z",
        "closed_at": "2023-07-27T03:06:17Z",
        "merged_at": "2023-07-27T03:06:17Z",
        "body": "## Describe your changes\r\nBetter error handling and display for nodejs users\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 45,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-07-26T21:33:26Z",
        "closed_at": "2023-07-30T18:29:51Z",
        "merged_at": "2023-07-30T18:29:51Z",
        "body": "This is a follow-up to the PR https://github.com/nomic-ai/gpt4all/pull/867 which was left unfinished. I've implemented a buffer of bytes that gets filled until it can be decoded into a string. Most of the credit goes to the previous PR. \r\n\r\nHowever, there is an issue that can be seen in the pastebin from the demo section. Something goes wrong with the bytes of an emoji and the rest of the messages doesn't get decoded. This can be fixed by \"cutting out\" the bad emoji leaving a \"?\" in its place and decoding the rest of the message without it. But why does a bad emoji happen in the first place? It's just the model being dumb or I'm missing something?\r\n\r\n**Update 27.07:** In the new commit, the buffer is split into space-separated chunks. If the generation of a chunk was finished, but it wasn't decoded, we replace it by the <?> unicode symbol and keep going. You can see an example of the new behavior in the demo.\r\n\r\n**Update 28.07:** Rewrote the the unicode decoding process by using the format of the multi-byte unicode symbols:\r\n![image](https://github.com/nomic-ai/gpt4all/assets/15333776/7640c6fd-3864-49e5-afa6-06542f15694a)\r\n[click for details](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)\r\n\r\n## [Previous PR](https://github.com/nomic-ai/gpt4all/pull/867) \r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n\r\n## Demo\r\nhttps://pastebin.com/jivmECqT\r\n\r\n**Update 27.07:** https://pastebin.com/qTTJPRiv\r\n**Update 28.07:** https://pastebin.com/4RT3ajRf\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 89,
        "deletions": 31,
        "changed_files": 4,
        "created_at": "2023-07-26T19:44:54Z",
        "closed_at": "2023-07-27T13:31:56Z",
        "merged_at": "2023-07-27T13:31:56Z",
        "body": "## Describe your changes\r\nadded aa button to switch color modes and check functionality between light mode and dark mode\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n\r\n## Demo\r\nhttps://github.com/nomic-ai/gpt4all/assets/58596666/5668fd2a-dd65-4b7b-b250-1d610f194de7\r\n\r\n### Steps to Reproduce\r\nclick on settings, application settings, change theme\r\n\r\n## Notes\r\npossible future features are more color based mode like a blue based mode or adding detection of system color as default color scheme\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-07-25T15:51:31Z",
        "closed_at": "2023-07-25T21:24:20Z",
        "merged_at": "2023-07-25T21:24:20Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-07-24T12:19:31Z",
        "closed_at": "2023-08-11T18:14:53Z",
        "merged_at": "2023-08-11T18:14:53Z",
        "body": "## Describe your changes\r\nDocumentation\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 441,
        "deletions": 59,
        "changed_files": 9,
        "created_at": "2023-07-23T01:14:02Z",
        "closed_at": "2023-07-30T23:34:06Z",
        "merged_at": "2023-07-30T23:34:06Z",
        "body": "## Describe your changes\r\n- Add Release build hint to Readme.\r\n- Documentation overhaul: extend Python docs and update with information on the last big PR.\r\n- black & isort linting in Python bindings.\r\n- Move Chat GUI out of the Bindings group in the docs navigation.\r\n\r\n## Issue ticket number and link\r\n_None_\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n_None_\r\n\r\n### Steps to Reproduce\r\nmainly: `make documentation` \ud83d\ude09\r\n\r\n## Notes\r\nComments welcome. Especially if something can be rearranged or made more clear & concise. I've been at it for a while.\r\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-07-21T19:43:16Z",
        "closed_at": "2023-07-27T15:32:09Z",
        "merged_at": null,
        "body": "`llmodel_model_create` calls `llmodel_model_create2` and ignore it's error reporting. So any error spit by `llmodel_model_create2` would not be reflected to the user. this fix it. there are plenty of users online who wonder why their setup doesn't open models. now they will know exactly why =)",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-07-18T02:19:17Z",
        "closed_at": "2023-07-19T14:36:23Z",
        "merged_at": "2023-07-19T14:36:23Z",
        "body": "## Describe your changes\r\nTopP 0.1 was found to be somewhat too aggressive, so a more moderate default of 0.4 would be better suited for general use.\r\n## Checklist before requesting a review\r\n- [*] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [*] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2023-07-16T16:46:08Z",
        "closed_at": "2023-07-17T20:21:03Z",
        "merged_at": "2023-07-17T20:21:03Z",
        "body": "## Describe your changes\r\n- Change to `llmodel_embedding()` to behave more gracefully when receiving unexpected input.\r\n- Instead of crashing, receiving `NULL` for the model or text, or getting an empty string returns `NULL`.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\nOn Windows:\r\n```py\r\n\\> py\r\nPython 3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from gpt4all import GPT4All, Embed4All\r\n>>> text = ''\r\n>>> embedder = Embed4All()\r\nFound model file at  <...>\\\\ggml-all-MiniLM-L6-v2-f16.bin\r\n>>> output = embedder.embed(text)\r\n>>> print(output)\r\n[]\r\n```\r\n\r\n### Steps to Reproduce\r\nTry the above without the patch. It doesn't like it very much.\r\n\r\n## Notes\r\nI could do some error codes and messages instead, if desired (might be more appropriate if `model == nullptr`.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-07-15T15:06:54Z",
        "closed_at": "2023-07-19T11:05:43Z",
        "merged_at": "2023-07-19T11:05:43Z",
        "body": "## Describe your changes\r\nThe main readme has installer links, too. So it makes sense to make the AVX/AVX2 requirement visible here.\r\n\r\n## Issue ticket number and link\r\n_None_\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n_None_\r\n\r\n### Steps to Reproduce\r\n_None_\r\n\r\n## Notes\r\n_None_\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2023-07-14T16:42:03Z",
        "closed_at": "2023-07-15T22:07:43Z",
        "merged_at": "2023-07-15T22:07:43Z",
        "body": "## Describe your changes\r\nMake sure model file is both found and readable to java  before letting backend load it.\r\n\r\n\r\n## Issue ticket number and link\r\nTo solve issue https://github.com/nomic-ai/gpt4all/issues/1167\r\n\r\n## Checklist before requesting a review\r\n- [x ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Notes\r\n\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 36,
        "changed_files": 7,
        "created_at": "2023-07-14T15:17:59Z",
        "closed_at": "2023-09-07T17:02:34Z",
        "merged_at": null,
        "body": "major difference here is the change from `ggml_graph_compute` to require doing\na `ggml_graph_plan` first and how scratch memory that is in addition to the dst\ntensors built during normal graph construction is no longer allocated during\ngraph execution but figured out by `ggml_graph_plan` - this could change size\non each run and we write to the `eval_buf` during graph construction (copying\nthe input token IDs, for one) so this needs to be a new buffer, but it need not\nretain information between calls\n\nthis is a submodule change - don't forget to `git submodule update`!\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2023-07-14T14:09:23Z",
        "closed_at": "2023-07-14T16:38:08Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1436,
        "deletions": 77,
        "changed_files": 25,
        "created_at": "2023-07-13T22:25:28Z",
        "closed_at": "2023-07-14T13:47:35Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 153,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-07-13T21:11:33Z",
        "closed_at": "2023-07-14T15:36:01Z",
        "merged_at": "2023-07-14T15:36:01Z",
        "body": "## Describe your changes\r\nCreated highlighting rules to make is easier for users to intake code in html, php, and latex\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n\r\n## Demo\r\nHTML:\r\n<img width=\"1254\" alt=\"image\" src=\"https://github.com/nomic-ai/gpt4all/assets/58596666/4c625509-83ef-439b-9e54-2183c1e2c19e\">\r\nPHP:\r\n<img width=\"1248\" alt=\"image\" src=\"https://github.com/nomic-ai/gpt4all/assets/58596666/911a00e9-1948-4b3e-9e53-897ae5fd7009\">\r\nLaTeX:\r\n<img width=\"1239\" alt=\"image\" src=\"https://github.com/nomic-ai/gpt4all/assets/58596666/0367ea2c-5b28-4412-8a28-09b8aaad5696\">\r\n\r\n\r\n\r\n### Steps to Reproduce\r\ngroovy model, write some [language] code\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-07-13T11:28:57Z",
        "closed_at": "2023-07-13T18:44:18Z",
        "merged_at": "2023-07-13T18:44:18Z",
        "body": "## Describe your changes\r\n- fix Windows MSVC arch detection in llmodel.cpp\r\n- to fix AVX-only handling\r\n\r\n## Issue ticket number and link\r\nthere are several now. probably all of these are about the same bug:\r\n- #965\r\n- #1009 \r\n- #1060 \r\n- #1084\r\n- #1097\r\n- #1175 \r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n\r\n\r\n### Steps to Reproduce\r\nWindows MSVC compiler doesn't have `__x86_64__` for x86-64, but instead: `_M_X64`\r\n\r\n## Notes\r\nHope this finally fixes that.  \ud83d\ude05",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-07-12T15:45:05Z",
        "closed_at": "2023-07-12T16:46:47Z",
        "merged_at": "2023-07-12T16:46:47Z",
        "body": "see https://learn.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-isprocessorfeaturepresent\n\n(uploading to check it builds on CI, don't push if it doesn't!)\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-07-12T14:26:53Z",
        "closed_at": "2023-07-12T19:19:27Z",
        "merged_at": "2023-07-12T19:19:27Z",
        "body": "## Describe your changes\r\n- Add information about the AVX/AVX2 requirement.\r\n- Update supported architectures.\r\n\r\n## Issue ticket number and link\r\nNo issue, but had a case on Discord and figured it'd be a good idea to mention it in the docs and on the website.\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n_None_\r\n\r\n### Steps to Reproduce\r\n_None_\r\n\r\n## Notes\r\nBasically, I created this PR to add the AVX/AVX2 information. Not sure if there are better links for the added foundation models, I just searched them on Hugging Face.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-07-12T11:24:09Z",
        "closed_at": "2023-07-12T14:49:25Z",
        "merged_at": "2023-07-12T14:49:25Z",
        "body": "always write `build_commands.json` to cmake build dir friendly for using editors with clangd integration that don't also manage the build themselves - this is currently already enabled in llama.cpp cmakelists but the exported `build_commands` are missing the chat and backend sources since it is not set early enough\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 100,
        "deletions": 13,
        "changed_files": 7,
        "created_at": "2023-07-11T05:35:42Z",
        "closed_at": "2023-07-11T16:43:45Z",
        "merged_at": "2023-07-11T16:43:45Z",
        "body": "\r\n## Describe your changes\r\nThe bindings version changed to 1.1.4\r\nUpdate Java bindings to be compatible with gpt4all 2.4.11\r\nfalcon model support.\r\nDeveloper docs included for Java bindings with instructions on how to build Java bindings.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 245,
        "deletions": 29,
        "changed_files": 9,
        "created_at": "2023-07-10T17:54:17Z",
        "closed_at": "2023-07-12T19:18:25Z",
        "merged_at": "2023-07-12T19:18:24Z",
        "body": "## Describe your changes\r\nUpdated training scripts with configs + attention masking correclty \r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-07-10T17:01:24Z",
        "closed_at": "2023-07-10T20:23:33Z",
        "merged_at": "2023-07-10T20:23:33Z",
        "body": "## Describe your changes\r\ncreated highlighting rules for json formatted file and c# code\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n\r\n## Demo\r\njson:\r\n<img width=\"834\" alt=\"image\" src=\"https://github.com/nomic-ai/gpt4all/assets/58596666/77ccf3d9-11e9-4d23-a8c2-c411076901c0\">\r\n<img width=\"1152\" alt=\"image\" src=\"https://github.com/nomic-ai/gpt4all/assets/58596666/fa41dec1-2e22-44be-b5c6-58ba15f1ad98\">\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 435,
        "deletions": 138,
        "changed_files": 23,
        "created_at": "2023-07-10T15:05:43Z",
        "closed_at": "2023-07-12T18:48:38Z",
        "merged_at": null,
        "body": "Allows taking a reference to or copy of the current kv cache and changing the current kv cache for a loaded model - unties kv cache lifetime from model lifetime and allows quick switching between different contexts, also useful for implementing something like CFG\r\n\r\ntodo:\r\n\r\n- [ ] support replit/metal generally (needs to recreate metal context on each `eval` call)\r\n- [ ] llama is not supported currently - the [WIP CFG implementation](https://github.com/ggerganov/llama.cpp/pull/2135) for llama.cpp just creates two `struct llama_context`s, (llama.cpp now has the ability to [create a duplicate context from an existing one](llama_new_context_with_model) that I believe shares the weight data) - figure out what to do here",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 69,
        "deletions": 65,
        "changed_files": 6,
        "created_at": "2023-07-08T14:06:13Z",
        "closed_at": "2023-10-12T11:54:01Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 212,
        "deletions": 145,
        "changed_files": 3,
        "created_at": "2023-07-06T15:35:04Z",
        "closed_at": "2023-07-19T22:36:50Z",
        "merged_at": "2023-07-19T22:36:49Z",
        "body": "## `pyllmodel`\r\n\r\nThe main idea of the PR is to use the callback functionality more heavily which fits with the style of the backend. The `prompt_model` function now requires an additional argument: `callback`, and uses it to return the generated tokens. This means that we get rid of `DualStreamProcessor` and the code is simplified. The `prompt_model_streaming` function used to duplicate a lot of code from `prompt_model`. Now, since `prompt_model` accepts a callback, the function `prompt_model_streaming` delegates the prompt generation to `prompt_model` and only manages the queue and the threading. I also added the `verbose` argument to `prompt_model` which prints out the prompt before it is passed to the model.\r\n\r\n## `pygpt4all`\r\n\r\nI added the `callback` and `verbose` arguments to `generate`. Due to callbacks, the current chat session is updated properly even when `streaming = True` (before, the user had to add assistant's response to `current_chat_session` him/herself). Also, chat session now accepts a `header`, i.e. an instruction that comes before the transcript of the conversation. This can be used to give the assistant some character.\r\n\r\n## Update\r\n\r\nThe `verbose` argument of `prompt_model` was removed and replaced by a `logging.info` call. The `header` argument of the chat session was renamed to `system_prompt` and a new argument `prompt_template` was added. The `retrieve_model` method returns the model config received from [models.json](https://github.com/nomic-ai/gpt4all/blob/c8d761a0040c19108b61443cbb1b3a13d393d9fb/gpt4all-chat/metadata/models.json) instead of the model path. The `systemPrompt` and `promptTemplate` from the config (if given) are used as default values for the `system_prompt` and `prompt_template` of the chat sessions.\r\n\r\n~~Update 2~~\r\n\r\n~~Added the possibility to set the prompt template using set_prompt_template. Now everything which is sent to the generate function is formatted even if the chat session is not activated. The users can use prompt templates with multiple placeholders which can be filled by passing a dictionary to the generate method. See an example in the showcase section.~~\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Showcase\r\n\r\nUsing chat session header, `verbose = True`, and showing that the chat session is updated properly with `streaming=True`: https://pastebin.com/UAz8KC2x\r\n\r\nUsing a custom callback to make a your mama joke: https://pastebin.com/s4NgHzqD\r\n\r\nUpdate: default system prompts and prompt templates are model-specific now. We can also change both if we want: https://pastebin.com/zqKHp5aN\r\n\r\n~~Update 2: using prompt template with and without multiple placeholders outside of a chat session: https://pastebin.com/AShu4BcD~~\r\n",
        "comments": 31
    },
    {
        "merged": true,
        "additions": 137,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-07-05T13:57:21Z",
        "closed_at": "2023-07-05T15:04:13Z",
        "merged_at": "2023-07-05T15:04:13Z",
        "body": "## Describe your changes\r\nimplemented highlighting in the chat client for bash and go\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\\\r\n\r\n## Demo\r\nBash:\r\n<img width=\"1184\" alt=\"Screenshot 2023-07-05 at 9 54 48 AM\" src=\"https://github.com/nomic-ai/gpt4all/assets/58596666/918aee17-8e89-4063-b3f3-d2bef71ee749\">\r\nGo:\r\n<img width=\"1182\" alt=\"Screenshot 2023-07-05 at 9 56 40 AM\" src=\"https://github.com/nomic-ai/gpt4all/assets/58596666/5ed2b83e-ba57-4934-b5d8-9d0479a24a60\">\r\n\r\n### Steps to Reproduce\r\nask gpt to write a script in go or bash\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2023-07-05T12:26:42Z",
        "closed_at": "2023-07-05T13:35:13Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\nhttps://github.com/nomic-ai/gpt4all/issues/1026\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\nAfter:\r\n<img width=\"629\" alt=\"image\" src=\"https://github.com/nomic-ai/gpt4all/assets/31350541/39d200a3-2f66-4ff6-a4c3-d4a9241767d3\">\r\n\r\nBefore:\r\n![image](https://github.com/nomic-ai/gpt4all/assets/31350541/abdd0acc-b90d-455b-b682-e9e059aa451c)\r\n\r\n### Steps to Reproduce\r\nPrompt: `\ufeffWrite a simple hello world in python`\r\n\r\n## Notes\r\nNothing of note\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 20,
        "changed_files": 3,
        "created_at": "2023-07-03T23:50:07Z",
        "closed_at": "2023-07-04T01:30:25Z",
        "merged_at": "2023-07-04T01:30:25Z",
        "body": "Fixes some (not all) of the typechecking complaints I get editing these files\nwith a linter enabled.\n\n- python: do not mutate locals()\n- python: fix (some) typing complaints\n- python: queue sentinel need not be a str\n- python: make long inference tests opt in\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 95,
        "deletions": 37,
        "changed_files": 3,
        "created_at": "2023-07-03T19:52:01Z",
        "closed_at": "2023-07-06T03:17:31Z",
        "merged_at": "2023-07-06T03:17:31Z",
        "body": "## Describe your changes\r\nAdding the ability to do streaming requests to the \"/completions\" endpoint of the API. Added in a test to showcase using the openai python client to stream the response from the api.\r\n\r\nThis addresses the NotImplementedError in the completions route of the api\r\nhttps://github.com/nomic-ai/gpt4all/blob/main/gpt4all-api/gpt4all_api/app/api_v1/routes/completions.py#L54\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n```python\r\n    model = \"gpt4all-j-v1.3-groovy\"\r\n    prompt = \"Who is Michael Jordan?\"\r\n    for resp in openai.Completion.create(\r\n            model=model,\r\n            prompt=prompt,\r\n            max_tokens=50,\r\n            temperature=0.28,\r\n            top_p=0.95,\r\n            n=1,\r\n            echo=True,\r\n            stream=True):\r\n        print(resp.choices[0].text)\r\n```\r\nThe above results in the model output being streamed to your console, directly from the API\r\n\r\n\r\n## Notes\r\nThere is a great issue thread over on the openai-node library page, discussing a lot of this streaming work.\r\nhttps://github.com/openai/openai-node/issues/18\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 59,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-07-01T22:20:49Z",
        "closed_at": "2023-07-06T02:42:15Z",
        "merged_at": "2023-07-06T02:42:15Z",
        "body": "## Describe your changes\r\n- Adapted to Python bindings API changes\r\n- Version selection based on package information\r\n- Does not currently work with `1.0.1` however, as it's not fully implemented:\r\n  ```\r\n  NotImplementedError: Streaming tokens in a chat session is not currently supported.\r\n  ```\r\n  - Does, however, work with `main`.\r\n\r\n## Issue ticket number and link\r\n- in the meantime, someone reported it as: #1123 \r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n![image](https://github.com/nomic-ai/gpt4all/assets/134004613/24f96ef7-5167-4f02-bb46-bbc15913a57b)\r\n(Commited version doesn't include the message `version 1.0.1, using new loop`)\r\n\r\n### Steps to Reproduce\r\n- CLI is currently broken with Python bindings v1.0.0 & v1.0.1; just try to run it\r\n- ~~This PR won't fix that yet, but it should once streaming with chat session is supported.~~ It is and it works.\r\n\r\n## Notes\r\n- Alternative ideas to fix it? Maybe I didn't understand the new API completely?\r\n- It's mostly a hack for now in any case. Fixing it is first priority, we can talk about improvements later.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-07-01T12:15:06Z",
        "closed_at": "2023-07-06T16:50:05Z",
        "merged_at": "2023-07-06T16:50:05Z",
        "body": "## Describe your changes\r\nAdds [Qt6::WaylandCompositor](https://doc.qt.io/qt-6/qwaylandcompositor.html) to cmakelist to make the GPT4ALL chat interface runnable natively on a Linux Wayland session.\r\nThis time only it only adds for the Linux Builds.\r\n\r\n## Notes\r\nTo make this PR work, we need to add [Qt6::WaylandCompositor](https://doc.qt.io/qt-6/qwaylandcompositor.html) in circleci config. Unless that is done, this PR will result in build failure.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 269,
        "deletions": 110,
        "changed_files": 14,
        "created_at": "2023-06-30T20:48:50Z",
        "closed_at": "2023-07-21T19:13:30Z",
        "merged_at": "2023-07-21T19:13:30Z",
        "body": "## Describe your changes\r\nAdds capability to run GPU accelerated inference server using GPT4All API \r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\nFollow instructions in `gpt4all-api/README.md`\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 24,
        "changed_files": 2,
        "created_at": "2023-06-29T19:26:56Z",
        "closed_at": "2023-06-30T17:53:45Z",
        "merged_at": "2023-06-30T17:53:45Z",
        "body": "should reduce required eval memory for these model types and makes all of our models reserve eval memory in the same way (almost - some still do the mem_per_token thing and some don't - next task is to unify that behavior)\r\n\r\n- backend/gptj: use scratch buffers\r\n- backend/mpt: use scratch bufs\r\n- fix format-related compile warnings\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 228,
        "deletions": 188,
        "changed_files": 15,
        "created_at": "2023-06-29T00:43:12Z",
        "closed_at": "2023-06-29T03:44:48Z",
        "merged_at": "2023-06-29T03:44:48Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 442,
        "deletions": 412,
        "changed_files": 15,
        "created_at": "2023-06-28T22:20:22Z",
        "closed_at": "2023-06-30T20:02:03Z",
        "merged_at": "2023-06-30T20:02:03Z",
        "body": "Testing, documenting and refactoring the python bindings to:\r\n- isort and black\r\n- include inference unit tests for orca model\r\n- expose writable c api parameters\r\n- change streaming to return an iterable, remove top level generator method in favor is just generate\r\n- introduces a chat session context manager for fast inference during chats with prompt templates.\r\n- bug fix to top_k and top_p to make them *actually work*\r\n\r\n<img width=\"1510\" alt=\"image\" src=\"https://github.com/nomic-ai/gpt4all/assets/13879686/4d788342-e941-45ed-9042-d0dafd313b4e\">\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 81,
        "deletions": 202,
        "changed_files": 6,
        "created_at": "2023-06-28T21:13:16Z",
        "closed_at": "2023-06-29T00:35:08Z",
        "merged_at": "2023-06-29T00:35:08Z",
        "body": "prepping to hack on these by hopefully making there be fewer places to fix the same bug\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 459,
        "deletions": 158,
        "changed_files": 5,
        "created_at": "2023-06-28T17:50:43Z",
        "closed_at": "2023-06-28T23:11:24Z",
        "merged_at": "2023-06-28T23:11:24Z",
        "body": "is necessary to get rid of technical debt before we drastically increase the complexity of settings by adding per model settings and mirostat and other fun things. Right now the settings are divided between QML and C++ and some convenience methods to deal with settings sync and so on that are in other singletons. This change consolidates all the logic for settings into a single class with a single API for both C++ and QML.\r\n\r\n\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-28T00:43:52Z",
        "closed_at": "2023-06-28T18:39:29Z",
        "merged_at": null,
        "body": "falcon has a larger than usual number of `ggml_tensor*`s so the `eval` buffer (which is used for all tensors, scratch buffers only contain *data*) needs to be bigger especially for large inputs (large prompt w/ large prompt batch size)\r\n\r\nempirically this size avoid crashing up to ~ prompt batch size 128 - it should not need to be this big for smaller 'prompt batch size's but making it dynamically sized would need to be undone later if we wanted to support falcon on the metal backend which does not allow resizing buffers after allocation\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 100,
        "deletions": 7,
        "changed_files": 9,
        "created_at": "2023-06-27T15:55:38Z",
        "closed_at": "2023-06-27T17:23:56Z",
        "merged_at": "2023-06-27T17:23:56Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1230,
        "deletions": 6,
        "changed_files": 8,
        "created_at": "2023-06-27T02:57:10Z",
        "closed_at": "2023-06-27T17:06:39Z",
        "merged_at": "2023-06-27T17:06:39Z",
        "body": "Tested with https://huggingface.co/TheBloke/falcon-7b-instruct-GGML/blob/main/falcon7b-instruct.ggmlv3.q4_0.bin\r\n \r\ndoes not support 40B models (requires changes to ggml that do not yet exist upstream) but otherwise uses the same format as currently distributed ggml falcon models",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 61,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-26T20:27:12Z",
        "closed_at": "2023-06-27T13:58:08Z",
        "merged_at": null,
        "body": "## Describe your changes\r\nIntegration of Codacy: By incorporating Codacy into our workflow, we gain access to powerful code analysis tools that can automatically identify potential issues, vulnerabilities, and maintainability concerns within our codebase. Codacy will help us ensure compliance with best coding practices and enhance the overall security of the software.\r\n\r\nThe changes in this pull request include:\r\n\r\n    Adding Codacy configuration files to the repository\r\n    Updating the CI workflow to trigger code analysis and security checks using Codacy\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\nNothing to demo, just added the Workflow\r\n\r\n### Steps to Reproduce\r\nNothing to reproduce, just added the Workflow\r\n\r\n## Notes\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 154,
        "deletions": 8,
        "changed_files": 14,
        "created_at": "2023-06-26T19:19:20Z",
        "closed_at": "2023-06-26T21:27:59Z",
        "merged_at": "2023-06-26T21:27:59Z",
        "body": "most of these can just shortcut out of the model loading logic \r\nllama is a bit worse to deal with because we submodule it so I have to at least parse the hparams, and then I just use the size on disk as an estimate for the mem size (which seems reasonable since we mmap() the llama files anyway)\r\n\r\ntested via added method in python binding (`.gpt4all` is a symlink to my actual model dir)\r\n\r\n```python\r\nfrom pathlib import Path\r\nfrom gpt4all import pyllmodel\r\n\r\nmodels = list((Path.home() / '.gpt4all').glob('*.bin'))\r\nllm = pyllmodel.LLModel()\r\n\r\nallsizes = {}\r\nfor model in models:\r\n    print(f'get size for {model}: ', end='')\r\n    sz = llm.memory_needed(str(model))\r\n    print(f'{sz}')\r\n    allsizes[model.name] = sz\r\n\r\nprint(allsizes)\r\n```\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-06-26T15:33:15Z",
        "closed_at": "2023-06-26T17:40:53Z",
        "merged_at": "2023-06-26T17:40:53Z",
        "body": "add max_size param to ggml_metal_add_buffer - introduced in https://github.com/ggerganov/llama.cpp/pull/1826\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-25T13:53:39Z",
        "closed_at": "2023-06-27T13:40:12Z",
        "merged_at": null,
        "body": "This commit adds all 3 orca-mini models to the `models.json`. I have chosen 5_0 quantization for the 3B version since 4_0 hurts its performance way too much.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-25T12:55:25Z",
        "closed_at": "2023-06-26T17:58:24Z",
        "merged_at": "2023-06-26T17:58:24Z",
        "body": "## Describe your changes\r\nThe patch include support for running natively on a Linux Wayland display server/compositor which is successor to old Xorg. Cmakelist was missing WaylandClient so added it back.\r\n\r\n## Issue ticket number and link\r\nThis will fix #1047 .\r\n\r\nAlthough I have tested it at my end, this PR should be reviewed properly in all configurations before merging.\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 355,
        "deletions": 116,
        "changed_files": 9,
        "created_at": "2023-06-24T21:44:50Z",
        "closed_at": "2023-07-05T20:33:12Z",
        "merged_at": "2023-07-05T20:33:12Z",
        "body": "## Describe your changes\r\n- The main change is to support gpt4all 2.4.8 (previous 2.4.6). Addition of Replit and Metal support in this Java binding.\r\n- A few minor additions and changes as described in Readme.md version history\r\n- Add working Unit tests in BasicTests.java (The test does not depend on loading the backend and tests all the Java code in isolation). Previous disabled Tests.java where depending on the backend removed. Will be covered by more advanced integration tests in the future.\r\n- Docs update\r\n\r\n- Tested on Windows 11, Linux WSL, and MacBook M1 hardware. Replit with metal support specifically tested on Macbook.\r\n- Version 1.1.3 published to maven central.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x ] I have performed a self-review of my code.\r\n- [x ] If it is a core feature, I have added thorough tests.\r\n- [x ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2023-06-23T21:14:44Z",
        "closed_at": "2023-06-27T14:49:46Z",
        "merged_at": "2023-06-27T14:49:46Z",
        "body": "## Describe your changes\r\nCleaned up and made the sideloading part more readable, also moved Replit architecture to supported ones. (+ renamed all \"ggML\" to \"GGML\" because who calls it \"ggML\"??)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 17,
        "changed_files": 1,
        "created_at": "2023-06-22T20:55:52Z",
        "closed_at": "2023-06-22T22:49:58Z",
        "merged_at": "2023-06-22T22:49:58Z",
        "body": "## Describe your changes\r\n\r\nThere were a number of grammatical errors in the README for the Java bindings. This corrects them.\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-22T08:48:08Z",
        "closed_at": "2023-06-23T08:16:36Z",
        "merged_at": "2023-06-23T08:16:36Z",
        "body": "## Describe your changes\r\nChange **python bindings**\r\nAdded possibility to set number of cpu threads `n_threads` directly in class GPT4All, when instantiating.\r\n\r\n`mpt = gpt4all.GPT4All(model_name = \"ggml-mpt-7b-chat\",  model_path = \"D:/00613/nomic.ai/GPT4all\", allow_download=False, n_threads=6)`\r\n\r\nIt seems to me, that the class GPT4All works as an interface to the python user. So the python user should be able to set the number of threads directly with a method of GPT4All.\r\n\r\n## Issue ticket number and link\r\n#1029 Add the possibility to set the number of CPU threads (n_threads) with the python bindings\r\nwhen instantiating.\r\n\r\n\r\n## Checklist before requesting a review\r\n- [X] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n![grafik](https://github.com/nomic-ai/gpt4all/assets/55889795/9a5b0fc8-c08c-40f7-8ca3-9d27f1a7e380)\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-21T13:36:05Z",
        "closed_at": "2023-06-23T20:28:53Z",
        "merged_at": "2023-06-23T20:28:53Z",
        "body": "## Describe your changes\r\nChanged \"Enable web server:\" to \"Enable API server:\"\r\nFound a few of new people confusing Web server with a WebUI, with this change it should be clearer as to what this setting does.\r\n## Checklist before requesting a review\r\n- [*] I have performed a self-review of my code.\r\n- [*] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n![image](https://github.com/nomic-ai/gpt4all/assets/96608645/64d91e6e-e19e-4d55-ae37-b0fac94b7092)\r\n\r\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-06-21T12:20:39Z",
        "closed_at": "2023-06-22T16:09:39Z",
        "merged_at": "2023-06-22T16:09:39Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Notes\r\nAfais, it should be sufficient to just add `.org` to the list of supported extensions.\r\nAs Org Mode is a clear text format, it should give usable results.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 66,
        "deletions": 80,
        "changed_files": 6,
        "created_at": "2023-06-20T20:15:01Z",
        "closed_at": "2023-06-20T21:18:11Z",
        "merged_at": "2023-06-20T21:18:11Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 254,
        "deletions": 89,
        "changed_files": 8,
        "created_at": "2023-06-20T08:46:15Z",
        "closed_at": "2023-06-22T00:34:33Z",
        "merged_at": null,
        "body": "Adds circleci workflow step to run Java unit tests for Java binding if there is a chance within gpt4all-bindings/java/ directory.\r\nFor now, simply run Junit tests that are active in the project. In the future should add a build for the binding jar and integration test against the backend. \r\n\r\nIncluded unit tests of Java code of the binging project. Unit tests do not attempt to test integration between Java code and the backend.\r\n\r\nA few Javadoc fixes to avoid errors reported during maven build.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 116,
        "deletions": 56,
        "changed_files": 6,
        "created_at": "2023-06-19T23:51:53Z",
        "closed_at": "2023-06-20T17:39:23Z",
        "merged_at": "2023-06-20T17:39:23Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 181,
        "deletions": 4,
        "changed_files": 6,
        "created_at": "2023-06-19T18:35:46Z",
        "closed_at": "2023-06-19T20:12:37Z",
        "merged_at": "2023-06-19T20:12:37Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 282,
        "deletions": 3,
        "changed_files": 5,
        "created_at": "2023-06-19T16:29:00Z",
        "closed_at": "2023-06-23T19:09:31Z",
        "merged_at": "2023-06-23T19:09:31Z",
        "body": "## Describe your changes\r\nMostly additions to the docs.\r\n\r\n- Add gpt4all-bindings/cli/README.md\r\n- Unify version information\r\n- Add gpt4all-bindings/cli/developer_notes.md\r\n- Add gpt4all-bindings/python/docs/gpt4all_cli.md\r\n- Update gpt4all-bindings/python/docs/index.md\r\n- Bump version to 0.3.5, as that is the latest `gpt4all` package on PyPI\r\n\r\n## Issue ticket number and link\r\n- Closes <https://github.com/nomic-ai/gpt4all/issues/646>\r\n- It basically addresses (unless there are other plans), as well: #682 \r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n_None yet_\r\n\r\n### Steps to Reproduce\r\n_None/mostly documentation_\r\n\r\n## Notes\r\nSuggestions welcome.\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 84,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2023-06-19T14:39:48Z",
        "closed_at": "2023-06-19T18:34:53Z",
        "merged_at": "2023-06-19T18:34:53Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-19T10:25:01Z",
        "closed_at": "2023-06-22T15:48:12Z",
        "merged_at": "2023-06-22T15:48:12Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [X] I have performed a self-review of my code.\r\n- [X] If it is a core feature, I have added thorough tests.\r\n- [X] I have added thorough documentation for my code.\r\n- [X] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [X] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-06-18T15:02:23Z",
        "closed_at": "2023-06-18T18:10:30Z",
        "merged_at": "2023-06-18T18:10:30Z",
        "body": "## Describe your changes\r\n- 2 typos\r\n- 1 formatting adjustment to match the others\r\n\r\n## Issue ticket number and link\r\n_None._\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n_None_\r\n\r\n### Steps to Reproduce\r\nIt's mostly a cosmetic change.\r\n\r\n## Notes\r\n_None_\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-18T13:00:26Z",
        "closed_at": "2023-06-18T18:08:43Z",
        "merged_at": "2023-06-18T18:08:43Z",
        "body": "## Describe your changes\r\n- Add some notes about common Windows problems when trying to make a local build (MinGW and MSVC).\r\n\r\n## Issue ticket number and link\r\nNo direct issue for this docs PR, but I could point to a few where people encountered these problems.\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n_None_\r\n\r\n### Steps to Reproduce\r\n- Build on Windows without prior knowledge; current `README.md` doesn't provide this information.\r\n\r\n## Notes\r\nIf you have feedback, I'd gladly amend/expand on this.\r\n\r\nNote: There are several approaches to try and build/get it working with both MinGW and MSVC. I tried to keep the instructions compact and general.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-17T11:28:52Z",
        "closed_at": "2023-06-22T16:19:50Z",
        "merged_at": "2023-06-22T16:19:50Z",
        "body": "## Describe your changes\r\nAdd the `Access-Control-Allow-Origin` header to the chat GUI's web API to enable [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS).\r\n\r\n## Issue ticket number and link\r\nIt was mentioned in <https://github.com/nomic-ai/gpt4all/issues/941> _\"Introduce a button in the UI Settings to enable CORS for the Web Server Mode of GPT4ALL UI.\"_\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\nUsing the following example HTML document:\r\n```html\r\n<html><head><title>CORS test</title></head>\r\n<body>\r\n    <script type=\"text/javascript\">\r\n        function foo() {\r\n            console.log(\"start click handler\");\r\n            function xhrHandler(e) {\r\n                console.log(\"handled.\")\r\n                console.log(e);\r\n            };\r\n            var xhr = new XMLHttpRequest();\r\n            xhr.addEventListener(\"load\", xhrHandler);\r\n            xhr.open(\"GET\", \"http://localhost:4891/v1/models\");\r\n            xhr.send();\r\n            console.log(\"end click handler\");\r\n        }\r\n    </script>\r\n    <input type=\"button\" onclick=\"foo()\" value=\"click me!\"></input>\r\n</body>\r\n</html>\r\n```\r\n\r\n#### Without the CORS header:\r\n![image](https://github.com/nomic-ai/gpt4all/assets/134004613/4c77b30d-b394-4f80-b240-63c48d817f5e)\r\n\r\n#### With the CORS header:\r\n![image](https://github.com/nomic-ai/gpt4all/assets/134004613/e38a5370-4009-485a-9136-2c9347e362bc)\r\n\r\n\r\n### Steps to Reproduce\r\n- Use the example HTML\r\n- Start the web server in the chat GUI\r\n- The request is not allowed without a CORS header\r\n\r\n## Notes\r\nC++ is not my strength, but I followed the example in the docs and it seemed straight-forward.\r\nPlease have a close look, though.\r\n\r\nThe linked issue requests a button to add to the UI. I simply enabled the header for everyone. I could make the PR more sophisticated, if necessary/desired.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-17T05:45:53Z",
        "closed_at": "2023-06-18T18:07:46Z",
        "merged_at": "2023-06-18T18:07:46Z",
        "body": "## Describe your changes\r\nFix 2 spelling typos in gpt4all.py \r\nNo function change. \r\n\r\n## Issue ticket number and link\r\nhttps://github.com/nomic-ai/gpt4all/issues/832\r\n\r\n## Checklist before requesting a review\r\n- [ x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 74,
        "deletions": 33,
        "changed_files": 6,
        "created_at": "2023-06-16T15:10:07Z",
        "closed_at": "2023-06-16T18:13:37Z",
        "merged_at": null,
        "body": "    Deprecates the following models so they won't show up in next release:\r\n    \r\n    * Snoozy\r\n    * Vicuna 7b\r\n    * Vicuna 13b\r\n    * Wizard 7b\r\n    * Stable vicuna 13b\r\n    * MPT base\r\n    * Nous vicuna 13b\r\n    * MPT instruct\r\n    \r\n    Converts the following to Q4_0\r\n    \r\n    * Nous Hermes\r\n    * Wizard 13b uncensored\r\n    \r\n    Promotes Nous Hermes to best llama model\r\n    \r\n    This leaves the following models for download in next version:\r\n    \r\n    * Groovy (best GPT-J model)\r\n    * Hermes (best LLAMA model)\r\n    * MPT chat (best MPT model)\r\n    * Wizard 13b uncensored\r\n    * Replit\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-06-16T04:46:16Z",
        "closed_at": "2023-07-03T12:53:47Z",
        "merged_at": null,
        "body": "## Describe your changes\r\nUpdated the emojis in the readme doc to\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n \r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 521,
        "deletions": 19,
        "changed_files": 14,
        "created_at": "2023-06-15T20:29:52Z",
        "closed_at": "2023-07-11T22:09:40Z",
        "merged_at": "2023-07-11T22:09:40Z",
        "body": "## Describe your changes\r\nThis is an ongoing attempt to integrate the C# bindings build, test and deployment to the CircleCI pipeline.\r\n\r\n### Progress:\r\n - [x] `build-csharp-deploy` workflow:\r\n   - [x] `build-csharp-linux` \r\n   - [x] `build-csharp-windows` \r\n   - [x] `build-csharp-macos` \r\n   - [x] `store-and-upload-nupkgs` (pack and deploy NuGet packages eventually) \r\n   \r\nLet me know if I'm off track here.\r\n\r\n## Open issues\r\n - How to actually build the native libs, some options:\r\n   - The build scripts (this draft, can be switched once the workflow is in place)\r\n   - #984\r\n   - @drasticactions runtimes\r\n - The deployment method that we assumed requires to have the native libraries **for every support platform** in place before creting the Gpt4All nuget package. As far as I understand this can be solved with circleci *workspaces* (see **Update1**)\r\n   - we could also consider other ways of doing this, e.g a separated package for each platform native assets.  \r\n\r\n#### UPDATE  1 \r\n[Share the Data between jobs](https://circleci.com/docs/workspaces/)\r\n<details>\r\n<summary>The workspace bit seem to work as expected</summary>\r\n\r\n![image](https://github.com/nomic-ai/gpt4all/assets/14858640/7592c156-cc41-4c0e-bdbf-38fa86802bb8)\r\n\r\n</details>\r\n\r\n#### UPDATE  2 \r\n[Test integration / collection](https://circleci.com/docs/collect-test-data)\r\n<details>\r\n<summary>Got the test integration working</summary>\r\n\r\n![image](https://github.com/nomic-ai/gpt4all/assets/14858640/84b624ab-f3f3-471c-b546-aef2e36737fe)\r\n\r\n</details>\r\n\r\n#### UPDATE 3\r\n<details>\r\n<summary>Experimented with the build-bindings-backend-* jobs</summary>\r\n\r\n```\r\n/tmp/workspace:\r\nlinux-x64  osx-x64  win-x64  win-x64_msvc\r\n\r\n/tmp/workspace/linux-x64:\r\nlibgptj-avxonly.so\t\t   libllamamodel-mainline-default.so\r\nlibgptj-default.so\t\t   libllmodel.so\r\nlibllamamodel-230511-avxonly.so    libmpt-avxonly.so\r\nlibllamamodel-230511-default.so    libmpt-default.so\r\nlibllamamodel-230519-avxonly.so    libreplit-mainline-avxonly.so\r\nlibllamamodel-230519-default.so    libreplit-mainline-default.so\r\nlibllamamodel-mainline-avxonly.so\r\n\r\n/tmp/workspace/osx-x64:\r\nlibgptj-avxonly.dylib\t\t      libllmodel.0.2.0.dylib\r\nlibgptj-default.dylib\t\t      libllmodel.0.dylib\r\nlibllamamodel-230511-avxonly.dylib    libllmodel.dylib\r\nlibllamamodel-230511-default.dylib    libmpt-avxonly.dylib\r\nlibllamamodel-230519-avxonly.dylib    libmpt-default.dylib\r\nlibllamamodel-230519-default.dylib    libreplit-mainline-avxonly.dylib\r\nlibllamamodel-mainline-avxonly.dylib  libreplit-mainline-default.dylib\r\nlibllamamodel-mainline-default.dylib  libreplit-mainline-metal.dylib\r\nlibllamamodel-mainline-metal.dylib\r\n\r\n/tmp/workspace/win-x64:\r\nlibatomic-1.dll     libgfortran-5.dll  \r\nlibquadmath-0.dll  libstdc++-6.dll\r\nlibgcc_s_seh-1.dll  libgomp-1.dll      \r\nlibssp-0.dll\t  libwinpthread-1.dll\r\ngptj-avxonly.dll\t\t llamamodel-mainline-default.dll\r\ngptj-default.dll\t\t llmodel.dll\r\nllamamodel-230511-avxonly.dll\t mpt-avxonly.dll\r\nllamamodel-230511-default.dll\t mpt-default.dll\r\nllamamodel-230519-avxonly.dll\t replit-mainline-avxonly.dll\r\nllamamodel-230519-default.dll\t replit-mainline-default.dll\r\nllamamodel-mainline-avxonly.dll\r\n\r\n/tmp/workspace/win-x64_msvc:\r\ngptj-avxonly.dll\t\t llamamodel-mainline-default.dll\r\ngptj-default.dll\t\t llmodel.dll\r\nllamamodel-230511-avxonly.dll\t mpt-avxonly.dll\r\nllamamodel-230511-default.dll\t mpt-default.dll\r\nllamamodel-230519-avxonly.dll\t replit-mainline-avxonly.dll\r\nllamamodel-230519-default.dll\t replit-mainline-default.dll\r\nllamamodel-mainline-avxonly.dll\r\n```\r\n\r\n</details>\r\n\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n",
        "comments": 23
    },
    {
        "merged": false,
        "additions": 55,
        "deletions": 55,
        "changed_files": 5,
        "created_at": "2023-06-15T11:52:31Z",
        "closed_at": "2023-06-27T14:00:38Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\nDO NOT MERGE. This is for testing only and will not be merged into main\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2023-06-15T09:33:25Z",
        "closed_at": "2023-06-16T19:56:23Z",
        "merged_at": "2023-06-16T19:56:23Z",
        "body": "This cleans up the code a very tiny bit by removing the `MB` variable from each implementation and instead having an `operator \"\"_MiB` in `utils.h`.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 152,
        "deletions": 161,
        "changed_files": 23,
        "created_at": "2023-06-15T09:03:30Z",
        "closed_at": "2023-06-16T19:57:25Z",
        "merged_at": null,
        "body": "This PR aims to unify the C++ code style of backend/chat.\r\n\r\nThe backend follows a different code style than the chat, because the code style I've primarily observed in them is different.\r\n\r\nWe really need our own clang-format config applied before each commit...",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-06-15T08:28:40Z",
        "closed_at": "2023-06-26T17:49:58Z",
        "merged_at": "2023-06-26T17:49:58Z",
        "body": "Fixed several warnings and UB in the Replit implementation",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-14T19:40:08Z",
        "closed_at": "2023-06-15T21:06:15Z",
        "merged_at": "2023-06-15T21:06:15Z",
        "body": "- [x] fix loading and running llama model files using the new \"k\" quantizations - the change here makes them work on non-Metal builds now instead of simply failing to load\r\n- [ ] they load but still generate garbage output on a Metal build, even though a build of the `llama.cpp` we're using's `main` binary can run them on Metal without problems. \r\n    - it turns out this bug exists upstream if you do what we do and set `n_ctx` to 2048: https://github.com/ggerganov/llama.cpp/issues/1881\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2023-06-14T14:14:14Z",
        "closed_at": "2023-06-22T07:29:15Z",
        "merged_at": "2023-06-22T07:29:15Z",
        "body": "To not show every little tiny network spike to the user",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-14T12:42:20Z",
        "closed_at": "2023-06-22T07:28:40Z",
        "merged_at": "2023-06-22T07:28:40Z",
        "body": "Fixes #983",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2023-06-14T10:02:35Z",
        "closed_at": "2023-06-14T14:27:19Z",
        "merged_at": "2023-06-14T14:27:19Z",
        "body": "This is used to identify the path where all the various implementations are\r\n\r\n## Describe your changes\r\n\r\nThis adds `SetLibrarySearchPath` as model option to search lib path in the specified directory instead of the current work directory.\r\n\r\n## Issue ticket number and link\r\n\r\n#826 \r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\nBefore:\r\n\r\n```\r\n11:59AM DBG Loading model in memory from file: /home/mudler/_git/LocalAI/models/ggml-gpt4all-j                                                                                                                       \r\nDebug: New GPT4ALL                                                                                                                                                                                                          \r\nDebug: Library search path: /tmp/localai/backend_data                                                                                                                                                                       \r\nload_gpt4all_model: error 'Success'                                                                                                                                                                                  \r\n11:59AM DBG [gpt4all] Fails: failed loading mode\r\n```\r\n\r\nAfter:\r\n```\r\ngptj_model_load: loading model from '/home/mudler/_git/LocalAI/models/ggml-gpt4all-j' - please wait ...                                                                                                              \r\ngptj_model_load: n_vocab = 50400                                                                                                                                                                                     \r\ngptj_model_load: n_ctx   = 2048                                                                                                                                                                                      \r\ngptj_model_load: n_embd  = 4096\r\ngptj_model_load: n_head  = 16\r\ngptj_model_load: n_layer = 28\r\ngptj_model_load: n_rot   = 64\r\ngptj_model_load: f16     = 2\r\ngptj_model_load: ggml ctx size = 5401.45 MB\r\ngptj_model_load: kv self size  =  896.00 MB\r\ngptj_model_load: ................................... done\r\ngptj_model_load: model size =  3609.38 MB / num tensors = 285\r\n12:01PM DBG [gpt4all] Loads OK\r\n12:01PM DBG Response: {\"object\":\"chat.completion\",\"model\":\"ggml-gpt4all-j\",\"choices\":[{\"message\":{\"role\":\"assistant\",\"content\":\"I'm doing well, thank you. How about yourself?\"}}],\"usage\":{\"prompt_tokens\":0,\"comple\r\ntion_tokens\":0,\"total_tokens\":0}}\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\nThis affect only binaries that are not in the same directory within the implementations\r\n\r\n## Notes\r\n\r\nWithout this it's impossible to specify a search path to find the various implementations libs.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-13T15:30:14Z",
        "closed_at": "2023-06-20T14:21:51Z",
        "merged_at": "2023-06-20T14:21:51Z",
        "body": "## Describe your changes\r\n\r\n0.3.5 for metal libraries\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 31,
        "changed_files": 2,
        "created_at": "2023-06-13T02:49:01Z",
        "closed_at": "2023-06-13T13:07:08Z",
        "merged_at": "2023-06-13T13:07:08Z",
        "body": "## Describe your changes\r\n\r\nWas not correctly using prompt context. Now it's preserved in class. Fixes, BOS issue.\r\n\r\n![Screenshot 2023-06-12 at 10 36 12 PM](https://github.com/nomic-ai/gpt4all/assets/22121416/efe7d099-d9a7-4e91-b3c1-6912efc4685c)\r\n\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 11,
        "changed_files": 11,
        "created_at": "2023-06-13T01:57:39Z",
        "closed_at": "2023-06-13T11:14:02Z",
        "merged_at": "2023-06-13T11:14:02Z",
        "body": "fixes a definite use-after-free and likely avoids some other\r\npotential ones - std::string will convert to a std::string_view\r\nautomatically but as soon as the std::string in question goes out of\r\nscope it is already freed and the string_view is pointing at freed\r\nmemory - this is *mostly* fine when its returning a reference to the\r\ntokenizer's internal vocab table but it's, imo, too easy to return a\r\nreference to a dynamically constructed string with this as replit is\r\ndoing (and unfortunately needs to do to convert the internal whitespace\r\nreplacement symbol back to a space)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-06-13T01:53:26Z",
        "closed_at": "2023-06-13T07:53:27Z",
        "merged_at": "2023-06-13T07:53:27Z",
        "body": "Fix typo in javadoc,\r\nAdd word to ignore list for codespellrc\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1281,
        "deletions": 1,
        "changed_files": 15,
        "created_at": "2023-06-13T01:05:51Z",
        "closed_at": "2023-07-21T01:36:21Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\nRust bindings!\r\n\r\nI've added some rust bindings to the project that call into the c api. Following rust ecosystem conventions there is a `gpt4all-sys` crate, which is just generated code from the c api and then a `gpt4all` crate which provides a safe wrapper around the `sys` crate.\r\n\r\nCurrently it only supports creating a model from a local file, and doing inference. There is some major ergonomics issues around the callback based API for prompts, if there is sufficient interest in rust bindings I can make some changes to the c api to make it better.\r\n\r\nThis PR is mostly to figure out if this is wanted - I'll likely be using this in our own projects regardless but having it in-tree seems like a win for everyone.\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n\r\n```rust\r\n#[no_mangle]\r\nextern \"C\" fn prompt_callback(_: i32) -> bool {\r\n    true\r\n}\r\n\r\n#[no_mangle]\r\nextern \"C\" fn response_callback(_: i32, response: *const std::os::raw::c_char) -> bool {\r\n    let response = unsafe { std::ffi::CStr::from_ptr(response) };\r\n    let response = response.to_str().unwrap();\r\n    print!(\"{response}\");\r\n    true\r\n}\r\n\r\n#[no_mangle]\r\nextern \"C\" fn recalculate_callback(is_recalculating: bool) -> bool {\r\n    is_recalculating\r\n}\r\n\r\n#[test]\r\nfn prompt() {\r\n    let model = Model::new(\"ggml-gpt4all-j-v1.3-groovy.bin\", \"auto\")\r\n        .expect(\"Failed to load model\");\r\n    let model = model\r\n        .initialize()\r\n        .expect(\"failed to initialize model\");\r\n    let mut context = Context::default();\r\n    model\r\n        .prompt(\r\n            \"What is blue and smells like red paint?\", // blue paint!\r\n            prompt_callback,\r\n            response_callback,\r\n            recalculate_callback,\r\n            &mut context,\r\n        )\r\n        .expect(\"Failed to prompt\");\r\n}\r\n```\r\n\r\nResults:\r\n\r\n> gptj_model_load: loading model from 'ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\r\n> gptj_model_load: n_vocab = 50400\r\n> gptj_model_load: n_ctx   = 2048\r\n> gptj_model_load: n_embd  = 4096\r\n> gptj_model_load: n_head  = 16\r\n> gptj_model_load: n_layer = 28\r\n> gptj_model_load: n_rot   = 64\r\n> gptj_model_load: f16     = 2\r\n> gptj_model_load: ggml ctx size = 5401.45 MB\r\n> gptj_model_load: kv self size  =  896.00 MB\r\n> gptj_model_load: ................................... done\r\n> gptj_model_load: model size =  3609.38 MB / num tensors = 285\r\n> \r\n> Blue and smells like red paint is a common description of the color and odor of turpentine. Turpentine is a solvent used in painting and woodworking, but it can also be used as a solvent for cleaning and degreasing. The odor of turpentine is caused by the breakdown products that are formed when it is exposed to air.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-13T00:11:56Z",
        "closed_at": "2023-06-13T12:40:38Z",
        "merged_at": "2023-06-13T12:40:38Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8091,
        "deletions": 3205,
        "changed_files": 30,
        "created_at": "2023-06-12T20:11:27Z",
        "closed_at": "2023-07-25T15:46:41Z",
        "merged_at": "2023-07-25T15:46:41Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n\r\nFeatures: \r\n- fixes improper named exports \r\n    - ESM needs named exports, this addresses fixes so users can use our library in esm and cjs.\r\n- Empty strings would occur very often due to an improperly formed prompt with `createCompletion` thank you @iimez for discovering the bug\r\n- adds basic testing with jest\r\n- add circle ci integration to build documentation and package\r\n- downloading models come with better safety and md5sum checking thank you @iimez \r\n- resumable / partial downloads\r\n- Completes #913 \r\n- embeddings api!\r\n- documentation",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 32,
        "changed_files": 1,
        "created_at": "2023-06-12T19:33:56Z",
        "closed_at": "2023-06-28T12:50:06Z",
        "merged_at": null,
        "body": "## Describe your changes\r\nNOT FOR REVIEW. Opening PR to enable running CI.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-12T13:22:55Z",
        "closed_at": "2023-06-12T15:43:21Z",
        "merged_at": "2023-06-12T15:43:21Z",
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 115,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-06-12T00:00:05Z",
        "closed_at": "2023-06-12T12:08:18Z",
        "merged_at": "2023-06-12T12:08:18Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 102,
        "deletions": 32,
        "changed_files": 3,
        "created_at": "2023-06-09T22:17:21Z",
        "closed_at": "2023-06-13T14:29:15Z",
        "merged_at": "2023-06-13T14:29:15Z",
        "body": "works now\r\n\r\nTODOs:\r\n* [x] figure out why debug build generates garbage output (Edit: not a new issue, also exists on main)\r\n    * will be fixed by https://github.com/nomic-ai/gpt4all/pull/968\r\n* [x] unimplemented metal kernels for ggml ops\r\n    * [x] `ggml_norm`\r\n    * [x] `ggml_gelu`\r\n    * [x] `ggml_alibi`\r\n    * [x] `ggml_cpy` for f16 src (f16 kv cache)\r\n    * [x] PR new metal impls to upstream\r\n* [x] add scratch buffers to save memory use\r\n   * can't easily dynamically resize buffers when using `metal`, once a buffer has been `ggml_metal_add_buffer`'d to the `ggml_metal_context` it is there for the remaining life of the `ggml_metal_context`, so scrap the `mem_per_token` stuff and just lower the memory requirement altogether by using scratch buffers\r\n* [x] fall back to normal implementation for unsupported quant types\r\n* [x] fix for finding `ggml-metal.metal` next to the .dylib landed upstream, re-sync our fork\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 25,
        "changed_files": 1,
        "created_at": "2023-06-09T15:04:25Z",
        "closed_at": "2023-06-13T03:11:54Z",
        "merged_at": "2023-06-13T03:11:54Z",
        "body": "## Describe your changes\r\n\r\nUpdated python workflows to follow @manyoso's circleci patterns with hold approvals and easier pypi deployments.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-06-09T14:51:17Z",
        "closed_at": "2023-06-12T15:55:56Z",
        "merged_at": "2023-06-12T15:55:56Z",
        "body": "- A bug introduced in 0cb2b86730a56cee7460b0582418228e8049d3ef breaks AVX on Windows with MSVC\r\n- currently getting: `warning C5102: ignoring invalid command-line macro definition '/arch:AVX2'`\r\n\r\n## Describe your changes\r\n- The solution is to use `_options(...)` not `_definitions(...)`\r\n\r\n## Issue ticket number and link\r\n- https://github.com/nomic-ai/gpt4all/issues/923\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\nDescription of the problem is in the accompanying issue: #923 \r\n\r\n### Steps to Reproduce\r\nBuild on Windows without the fix produces warnings like:\r\n```\r\ncommand line : warning C5102: ignoring invalid command-line macro definition '/arch:AVX2' ...\r\n```",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 39,
        "changed_files": 2,
        "created_at": "2023-06-09T12:45:58Z",
        "closed_at": "2023-06-09T14:17:09Z",
        "merged_at": "2023-06-09T14:17:09Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-06-09T08:20:07Z",
        "closed_at": "2023-06-09T12:55:15Z",
        "merged_at": "2023-06-09T12:55:15Z",
        "body": "The anonymous namespace already makes it static.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4057,
        "deletions": 470,
        "changed_files": 25,
        "created_at": "2023-06-08T20:41:15Z",
        "closed_at": "2023-06-12T19:00:21Z",
        "merged_at": "2023-06-12T19:00:21Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-08T19:26:21Z",
        "closed_at": "2023-06-09T12:44:47Z",
        "merged_at": "2023-06-09T12:44:47Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2023-06-08T18:04:02Z",
        "closed_at": "2023-06-16T20:06:22Z",
        "merged_at": "2023-06-16T20:06:22Z",
        "body": "The CLI app.py is currently broken. See discussion starting in this issue here: <https://github.com/nomic-ai/gpt4all/issues/820#issuecomment-1582955946>\r\n\r\n## Describe your changes\r\n- the bindings API changed in 057b9, but the CLI was not updated\r\n- change 'std_passthrough' param to the renamed 'streaming'\r\n- remove '_cli_override_response_callback' as it breaks and is no longer needed\r\n\r\n## Issue ticket number and link\r\nThe bug was discovered as part of a separate/wider issue: https://github.com/nomic-ai/gpt4all/issues/820#issuecomment-1582955946\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n- bug traceback see: <https://github.com/nomic-ai/gpt4all/issues/820#issuecomment-1582955946>\r\n\r\n- fixed version (Windows cmd):\r\n  ![image](https://github.com/nomic-ai/gpt4all/assets/134004613/7b8a5f84-7250-43b4-966e-ca12f2962523)\r\n\r\n### Steps to Reproduce\r\ntry to run, it'll break:\r\n```shell\r\npython -m pip install --user -U gpt4all typer  # installs current gpt4all==0.3.0\r\npython gpt4all-bindings/cli/app.py repl\r\n```\r\n\r\n## Notes\r\nI did not yet increase the version number, but that would probably be appropriate. Should it follow the PyPI package versioning?\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 370,
        "deletions": 189,
        "changed_files": 2,
        "created_at": "2023-06-08T17:28:23Z",
        "closed_at": "2023-06-08T22:02:45Z",
        "merged_at": "2023-06-08T22:02:44Z",
        "body": "This isn't quite ready because it triggers on every push to the repo. Need to add filters/actions so that this only triggers when a maintainer / owner requests it.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-08T17:09:50Z",
        "closed_at": "2023-06-09T08:13:35Z",
        "merged_at": "2023-06-09T08:13:35Z",
        "body": "Have own section for short usage example, as it is not specific to local build\r\n\r\n## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-08T03:57:54Z",
        "closed_at": "2023-06-08T18:08:31Z",
        "merged_at": "2023-06-08T18:08:31Z",
        "body": "copied directly from llama.cpp - without this temp=0.0 will just\r\nscale all the logits to infinity and give bad output\r\n\r\nset temp=0, before:\r\n![Screenshot 2023-06-07 205718](https://github.com/nomic-ai/gpt4all/assets/169252/5640e5cf-970b-4c76-bba0-71d962773c10)\r\nafter:\r\n![Screenshot 2023-06-07 205535](https://github.com/nomic-ai/gpt4all/assets/169252/ac76b1de-7346-47ff-a871-de0dd3c0931c)\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-08T03:43:23Z",
        "closed_at": "2023-06-08T18:08:10Z",
        "merged_at": "2023-06-08T18:08:10Z",
        "body": "no effect, but avoids a *potential* bug later if we use\r\nactualVocabSize - which is for when a model has a larger\r\nembedding tensor/# of output logits than actually trained token\r\nto allow room for adding extras in finetuning - presently all of our\r\nmodels have had \"placeholder\" tokens in the vocab so this hasn't broken\r\nanything, but if the sizes did differ we want the equivalent of\r\n`logits[actualVocabSize:]` (the start point is unchanged), not\r\n`logits[:-actualVocabSize]` (this.)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 130,
        "deletions": 23,
        "changed_files": 3,
        "created_at": "2023-06-07T17:20:31Z",
        "closed_at": "2023-06-09T14:17:44Z",
        "merged_at": "2023-06-09T14:17:44Z",
        "body": "## Describe your changes\r\nMethod to return a generator for model response. \r\nHow it works: we kick off the C library prompt call in a separate thread so we don't block return of generator. The prompt response callback puts tokens into queue instead of printing them. Generator then dequeues and yields tokens as they are added.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code. (I manually made sure all other model prompt methods are working)\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<img width=\"910\" alt=\"Screenshot 2023-06-07 at 1 11 27 PM\" src=\"https://github.com/nomic-ai/gpt4all/assets/22121416/007dc933-bc7c-4fdb-a1a2-ab6d1467669b\">\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 200,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-06-07T13:19:12Z",
        "closed_at": "2023-06-07T17:18:43Z",
        "merged_at": null,
        "body": "This PR adds some test suites targeting the llmodel C and C++ interfaces using google test.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-07T12:57:26Z",
        "closed_at": "2023-06-26T17:47:28Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\nReleased over 2 weeks ago, [Manticore 13B Chat](https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg) (as opposed to the original Manticore 13B without chat) is a SOTA model tuned on instruct, instruct augmented (datasets similar to Microsoft's Orca), and chat (sharegpt, pygmalion) datasets.\r\n\r\nManticore 13B Chat has top marks on ARC-C, ARC-E, PIQA, and Winogrande. (overall average score of 69.53)\r\n\r\neleutherai benchmarks:\r\n\r\nhf-causal-experimental (pretrained=openaccess-ai-collective/manticore-13b-chat-pyg),\r\n    limit: None, provide_description: False, num_fewshot: 0, batch_size: None\r\n|    Task     |Version| Metric |Value |   |Stderr|\r\n|-------------|------:|--------|-----:|---|-----:|\r\n|arc_challenge|      0|acc     |0.4898|_  |0.0146|\r\n|             |       |acc_norm|0.5128|_  |0.0146|\r\n|arc_easy     |      0|acc     |0.7803|_  |0.0085|\r\n|             |       |acc_norm|0.7584|_  |0.0088|\r\n|boolq        |      1|acc     |0.8012|_  |0.0070|\r\n|hellaswag    |      0|acc     |0.6122|_  |0.0049|\r\n|             |       |acc_norm|0.7978|_  |0.0040|\r\n|openbookqa   |      0|acc     |0.3580|_  |0.0215|\r\n|             |       |acc_norm|0.4600|_  |0.0223|\r\n|piqa         |      0|acc     |0.7998|_  |0.0093|\r\n|             |       |acc_norm|0.8090|_  |0.0092|\r\n|winogrande   |      0|acc     |0.7285|_  |0.0125|\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [X] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [X] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 118,
        "deletions": 55,
        "changed_files": 6,
        "created_at": "2023-06-07T09:40:45Z",
        "closed_at": "2023-06-26T17:55:31Z",
        "merged_at": null,
        "body": "Using the model with the prompt it was finetuned on often (if not always) leads to better response quality than just using one generic prompt for all models.\r\n\r\nThis PR accomplishes this by:\r\n 1. Adding a `prompt_format` JSON key to each model in `models.json`\r\n 2. Downloading each models object from `models.json` locally when downloading the model (laying groundwork for allowing the user to view currently downloaded models description etc. without an internet connection)\r\n 3. Preferring the prompt given in `prompt_format` over the configured prompt\r\n 4. Letting the user change the currently selected models prompt format in the settings, above the default prompt template\r\n\r\nAdditionally, this change allows for an offline model \"download\" list and aims to implement further per-model configurability (need feedback from @manyoso)",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 149,
        "deletions": 79,
        "changed_files": 2,
        "created_at": "2023-06-07T07:58:48Z",
        "closed_at": "2023-06-07T16:18:22Z",
        "merged_at": "2023-06-07T16:18:22Z",
        "body": "This change syncs `llama.cpp.cmake` with upstream llama.cpp `CMakeLists.txt`.\r\n\r\nIt makes room for:\r\n - metal support\r\n - CUDA/OpenCL support\r\n\r\nAlthough both are untested.\r\n\r\n - Will simplify #746; important changes picked up from there\r\n - Will lay some groundwork for #885\r\n\r\nAnd fixes `avxonly` builds still using AVX2.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 141,
        "deletions": 67,
        "changed_files": 7,
        "created_at": "2023-06-07T02:34:00Z",
        "closed_at": "2023-06-09T18:58:12Z",
        "merged_at": "2023-06-09T18:58:12Z",
        "body": "todos:\r\n\r\n* [x] Currently need to ask for a Metal build with cmake -DLLAMA_METAL=ON - this should probably be just the default on MacOS/arm, and probably *off* for MacOS/Intel where I don't think it's supported (Metal API is supported, but I believe llama.cpp currently expects [unified memory](https://developer.apple.com/documentation/metal/mtldevice/3229025-hasunifiedmemory))\r\n   * built by default on mac now\r\n* [x] Metal currently *only* works with q4_0, any other files will fail to load on a Metal build - need to somehow, ugh, detect beforehand if the file is anything other than q4_0 to pass n_gpu_layers=0 to disable Metal\r\n    * detected and now falling back to default impl\r\n* [x] `ggml-metal.metal` needs to be included in `Contents/Resources` in the  app bundle and the documented way to do that in CMake isn't doing the trick, but it works after manually copying it in (the code in llama.cpp looks for it using[`pathForResource:`](https://developer.apple.com/documentation/foundation/nsbundle/1410989-pathforresource))\r\n    * [x] figured this out, also needed to be a source file on the relevant target - still need to figure out how to distribute it for the bindings though - technically it doesn't need to be a file, it's [passed to the relevant metal API ](https://github.com/ggerganov/llama.cpp/blob/0bf7cf1b296fc9fca05411b37afdf08a531487d2/ggml-metal.m#L109-L115) just as `NSString`, so could be embedded in the binary, but right now the upstream code is hardcoded to search for it with `pathForResource` - changing this behavior would mean forking llama.cpp.. again.\r\n\r\nthis only curently does anything for llama ggmlv3 models. Metal (and ggml GPU offload generally) requires some additional support on the model code side to move stuff in and out of GPU memory (well, Metal on M1/M2 does unified memory so its simpler than say, CUDA), and in addition to the current q4_0 quant limitation, only a subset (the subset needed to run llama) of the ggml ops are currently implemented in Metal. Notably missing for supporting our other models are\r\n\r\n * GELU activation (most models use this, but llama uses swish)\r\n * ALiBi positional bias (MPT, Replit)\r\n * regular old layernorm (llama uses RMSNorm)\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 64,
        "deletions": 5,
        "changed_files": 5,
        "created_at": "2023-06-06T20:07:24Z",
        "closed_at": "2023-06-10T14:07:42Z",
        "merged_at": null,
        "body": "## Describe your changes\r\nAs mentioned in #864 and https://github.com/nomic-ai/gpt4all/issues/647#issuecomment-1565852945 Markdown rendering can make some content invisible.\r\n\r\nTo address that, I've added a setting to toggle the state of Markdown rendering.\r\n\r\nNote: It's been years since I've touched C++ and Qt. Although the changes didn't seem very complicated, there's no guarantee I got everything right.\r\n\r\n## Issue ticket number and link\r\nhttps://github.com/nomic-ai/gpt4all/issues/864\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\nHere is what it looks like with the fixes:\r\n\r\nhttps://github.com/nomic-ai/gpt4all/assets/134004613/2cefa5c2-2825-458b-854d-d2aabcebc328\r\n\r\n### Steps to Reproduce\r\nSimply try entering something like `<this could be interpreted as HTML>` as prompt, or get the model to respond with something like that to trigger the issue.\r\n\r\n## Notes\r\nCreated as a draft because it's adding a setting. There might be alternative ways to address it.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 50,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2023-06-06T00:47:30Z",
        "closed_at": "2023-06-26T17:50:32Z",
        "merged_at": null,
        "body": "## My 2nd attempt at creating the ability to switch between 'LocalHost' and 'Any' and having the ability of switching ports from 4891\r\n\r\n## Hopefully I've done it in a way where it's default is still LocalHost, 4891. I'm still very new to this... I won't be offened by any feedback. Hopefully I added Qsettings in the right place as suggested on my last pull request.\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-05T23:54:22Z",
        "closed_at": "2023-07-28T15:31:43Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\nI combined the bytes of the emojis to be encoded accurately.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n\r\n\r\n## Demo\r\nBefore:\r\n<img width=\"1095\" alt=\"Screenshot 2023-06-06 at 2 51 13 AM\" src=\"https://github.com/nomic-ai/gpt4all/assets/38670228/742173b1-0fb3-4acd-910d-d5e2b30fc15e\">\r\n\r\nAfter:\r\n<img width=\"950\" alt=\"Screenshot 2023-06-06 at 2 49 57 AM\" src=\"https://github.com/nomic-ai/gpt4all/assets/38670228/621ac824-4af7-4653-877d-2c39cac6bf8b\">\r\n\r\n\r\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2023-06-05T19:45:06Z",
        "closed_at": "2023-06-12T16:41:22Z",
        "merged_at": "2023-06-12T16:41:22Z",
        "body": "There seems to be some memory cleanup missing from the bindings code.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-05T16:48:12Z",
        "closed_at": "2023-06-08T17:44:17Z",
        "merged_at": "2023-06-08T17:44:17Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2023-06-05T11:46:11Z",
        "closed_at": "2023-06-05T13:13:13Z",
        "merged_at": null,
        "body": "## Attempt to add the ability for server mode to talk externally and add changeable ports. I'm very new to this and won't be offened if it rejected.\r\n\r\n## I couldn't figure out how to initialize it to start on port 4891\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-06-05T01:44:47Z",
        "closed_at": "2023-06-09T12:51:10Z",
        "merged_at": "2023-06-09T12:51:10Z",
        "body": "the defaults that we have right now aren't ideal, i tried many models(nous-vicuna, snoozy, groovy....) with the values given here: https://www.reddit.com/r/LocalLLaMA/wiki/index/#wiki_prompting , i used the \"Precise\" preset, and all of the models gave much better results.\r\nAmount of times the models got stuck in a loop writing the same thing over and over significantly decreased using the new values, that allowed to make them write more longer responses, and follow the conversation for longer, without going schitzo.\r\nAnd i believe the newcomers will appreciate not having to figure out why the models are constantly getting stuck in loops whenever the conversation gets a little long.\r\n\r\nAlso changed the `batch size` from `9` to `128`, 9 is too conservative imo.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 603,
        "deletions": 2,
        "changed_files": 21,
        "created_at": "2023-06-05T01:32:57Z",
        "closed_at": "2023-06-28T18:28:52Z",
        "merged_at": "2023-06-28T18:28:52Z",
        "body": "## Describe your changes\r\nScaffolding for the GPT4All API that runs in a docker container with the OpenAI OpenAPI specification found here:\r\nThis should follow https://github.com/openai/openai-openapi/blob/master/openapi.yaml\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n\r\n## Demo\r\nSee README.md of gpt4all-api\r\n\r\nhttps://localhost:4891/docs\r\n<img width=\"1512\" alt=\"image\" src=\"https://github.com/nomic-ai/gpt4all/assets/13879686/a8ea91cf-dc62-496c-a608-22a611a5c0f6\">\r\n\r\n\r\n\r\n## Notes\r\nIntended to fully match the OpenAI OpenAI spec for engines, completions, chat completions and embeddings.\r\n\r\nThis can eventually support GPU inference by side-caring a triton inference server.\r\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 32,
        "changed_files": 1,
        "created_at": "2023-06-04T23:07:42Z",
        "closed_at": "2023-06-05T00:26:43Z",
        "merged_at": "2023-06-05T00:26:43Z",
        "body": "This PR is more about asking for help curating the models.json. I think my descriptions are awful and I need to do a much better job of surfacing relevant information to the end user about what models will meet their needs. The audience for this download dialog should not be an ML researcher, a developer, or someone with a lot of knowledge about open source LLM's, but rather average users who are looking for help deciding which model to download and try: which is quite an investment given the size of these models.\r\n\r\nComments and ideas welcome...\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-04T22:00:07Z",
        "closed_at": "2023-06-05T19:35:40Z",
        "merged_at": "2023-06-05T19:35:40Z",
        "body": "## Describe your changes\r\n\r\nOtherwise threads count doesn't take effect\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-04T02:36:45Z",
        "closed_at": "2023-06-04T12:46:38Z",
        "merged_at": "2023-06-04T12:46:38Z",
        "body": "\r\n\r\n## Describe your changes\r\nhuggingface -> Hugging Face",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 281,
        "deletions": 452,
        "changed_files": 10,
        "created_at": "2023-06-03T15:27:31Z",
        "closed_at": "2023-06-04T12:59:25Z",
        "merged_at": "2023-06-04T12:59:25Z",
        "body": "Fixed up version of the tuxifan backend_prompt_dedup\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-03T10:42:54Z",
        "closed_at": "2023-06-03T11:51:18Z",
        "merged_at": "2023-06-03T11:51:18Z",
        "body": "## Describe your changes\r\nRepeating the change that once was done in https://github.com/nomic-ai/gpt4all/pull/663 but then was overriden by https://github.com/nomic-ai/gpt4all/commit/48275d0dcc0cbd23b7f6469e65b86112a6169dec\r\n\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-02T18:41:52Z",
        "closed_at": "2023-06-02T19:46:33Z",
        "merged_at": "2023-06-02T19:46:33Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 82,
        "deletions": 146,
        "changed_files": 6,
        "created_at": "2023-06-02T15:15:54Z",
        "closed_at": "2023-06-02T16:32:26Z",
        "merged_at": "2023-06-02T16:32:26Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<img width=\"877\" alt=\"Screenshot 2023-06-02 at 11 14 37 AM\" src=\"https://github.com/nomic-ai/gpt4all/assets/22121416/1089995d-5bd9-45c0-b60b-ee174ace3194\">\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 25,
        "changed_files": 12,
        "created_at": "2023-06-02T14:50:28Z",
        "closed_at": "2023-06-02T19:46:41Z",
        "merged_at": "2023-06-02T19:46:41Z",
        "body": "Nothing more to say \ud83d\ude04\r\nMostly trivial changes.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2023-06-02T14:00:57Z",
        "closed_at": "2023-06-02T15:21:14Z",
        "merged_at": null,
        "body": "The `GPT4ALL_IMPLEMENTATIONS_PATH` environment variable is ignored now\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 266,
        "deletions": 447,
        "changed_files": 9,
        "created_at": "2023-06-02T12:43:51Z",
        "closed_at": "2023-06-03T15:26:51Z",
        "merged_at": null,
        "body": "This PR unifies the code of the `LLModel::prompt()` function.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 32,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2023-06-02T10:06:54Z",
        "closed_at": "2023-06-04T19:29:17Z",
        "merged_at": null,
        "body": "This change adds models that have failed to load to an ignore list and makes `ChatLLM::loadDefaultModel` iterate over all models found until one has succeeded to load.\r\n\r\nIt also makes sure to release the modelInfo on load failure in `ChatLLM::loadModel`.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1090,
        "deletions": 0,
        "changed_files": 12,
        "created_at": "2023-06-02T05:54:42Z",
        "closed_at": "2023-06-12T18:58:07Z",
        "merged_at": "2023-06-12T18:58:06Z",
        "body": "## Describe your changes\r\nThis is a complete implementation of Java bindings for Gpt4all.\r\nJava developers will be able to load llmodel shared library into Java JVM and make requests to generate text similar to python, c#, go bindings. The shared libraries will have to be present on the machine of the JVM and found via the standard library loading process like setting LD_LIBRARY_PATH. Developers simply have to add a dependency for the Java binding library to their maven or gradle project as described in the README file. The library jar has already been published to maven central so is immediately available to any Java developer. Published to maven central under the PR submitter org id.\r\n\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x ] I have performed a self-review of my code.\r\n- [x ] If it is a core feature, I have added thorough tests.\r\n- [x ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\nI plan to use the Java bindings for some internal projects at my company but I think others may be interested. Java is a very common deployment vehicle for microservice at many corporations and there are many developers that are generally familiar with the Java ecosystem as it is one of the top 3 programming languages around.\r\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 96,
        "changed_files": 9,
        "created_at": "2023-06-02T03:16:42Z",
        "closed_at": "2023-06-02T11:20:59Z",
        "merged_at": "2023-06-02T11:20:59Z",
        "body": "\u2026 backend model impl.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 90,
        "deletions": 90,
        "changed_files": 9,
        "created_at": "2023-06-01T18:15:52Z",
        "closed_at": "2023-06-01T21:14:17Z",
        "merged_at": "2023-06-01T21:14:17Z",
        "body": "Makes localdocs work with server mode.\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 56,
        "deletions": 43,
        "changed_files": 2,
        "created_at": "2023-06-01T16:04:20Z",
        "closed_at": "2023-07-11T12:36:28Z",
        "merged_at": null,
        "body": "Hi! I cleaned up the code of pyllmodel and added response_callback and reverse_prompts features to GPT4All's generate class.\r\n\r\n## Describe your changes\r\n\r\nI added the **response_callback** and **reverse_prompts** arguments to the **generate** function of **GPT4All**, so now users have access to these features. The method **chat_completion** now calls the **generate** method of **GPT4All** and accepts these arguments too. \r\n\r\nI also cleaned up the pyllmodel file. Its' only purpose now is to be the Python interface for the corresponding CPP class. The method **generate** of **LLModel** requires a **response_callback** function now  as an argument and returns None, which corresponds to the behavior of the **prompt** method from CPP.\r\n\r\nOn the user side, these changes don't break anything. \r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n\r\n## Notes\r\nThe reverse prompt feature is already implemented in CPP, in llmodel::prompt method, however, users have no access to it. I didn't want to touch CPP code without discussing first, so I re-implemented the feature in Python.\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-01T15:55:33Z",
        "closed_at": "2023-06-01T17:03:44Z",
        "merged_at": "2023-06-01T17:03:44Z",
        "body": "## Describe your changes\r\n\r\nThis is a leftover which otherwise breaks compilation outside of the source\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-06-01T15:29:42Z",
        "closed_at": "2023-06-01T18:24:24Z",
        "merged_at": "2023-06-01T18:24:24Z",
        "body": "## Describe your changes\r\nThere is one file that was missing a `#define NOMINMAX` when including the windows.h header. This was causing the msvc build to fail, since because the order of operations of how, I believe, it was being compiled, it would include the min/max headers that would break \"std::min\" and \"std::max\"\r\n\r\nNo, I don't get why that is the case.\r\n\r\nAdding the definition fixed it.\r\n\r\nWhile I was here, I updated the build scripts to also pull in the new DLLs being generated.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-01T14:10:09Z",
        "closed_at": "2023-06-01T15:41:05Z",
        "merged_at": "2023-06-01T15:41:05Z",
        "body": "This makes life easier for bindings devs :-)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 43,
        "changed_files": 2,
        "created_at": "2023-06-01T12:47:06Z",
        "closed_at": "2023-06-01T14:51:46Z",
        "merged_at": "2023-06-01T14:51:46Z",
        "body": "This change fixes/explains all current FIXMEs",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 103,
        "changed_files": 8,
        "created_at": "2023-06-01T12:45:25Z",
        "closed_at": "2023-06-01T14:36:22Z",
        "merged_at": "2023-06-01T14:36:22Z",
        "body": "## Describe your changes\r\nNote: you need to have *.so files nearby the binary produced by the binding in order to make this to work\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n```\r\n$ make example/main\r\n$ mv example/main buildllm\r\n$ cd buildllm\r\n$ ./main -m ~/_git/LocalAI/models/ggml-gpt4all-j -t 4\r\ngptj_model_load: loading model from '/home/mudler/_git/LocalAI/models/ggml-gpt4all-j' - please wait ...\r\ngptj_model_load: n_vocab = 50400\r\ngptj_model_load: n_ctx   = 2048\r\ngptj_model_load: n_embd  = 4096\r\ngptj_model_load: n_head  = 16\r\ngptj_model_load: n_layer = 28\r\ngptj_model_load: n_rot   = 64\r\ngptj_model_load: f16     = 2\r\ngptj_model_load: ggml ctx size = 5401.45 MB\r\ngptj_model_load: kv self size  =  896.00 MB\r\ngptj_model_load: ................................... done\r\ngptj_model_load: model size =  3609.38 MB / num tensors = 285\r\nModel loaded successfully.\r\n>>> what's up?\r\n\r\nI'm sorry, as an AI language model, I don't have the capability to answer a greeting or question. Is there anything else\r\n```\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 83,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-06-01T12:17:18Z",
        "closed_at": "2023-06-01T14:50:43Z",
        "merged_at": "2023-06-01T14:50:43Z",
        "body": "This PR implements a simple message handler that:\r\n 1. Keeps only the previous (and ofc current) log file\r\n 2. Logs to stdout on log file creation error",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 225,
        "deletions": 117,
        "changed_files": 12,
        "created_at": "2023-06-01T09:43:52Z",
        "closed_at": "2023-06-04T19:29:33Z",
        "merged_at": null,
        "body": "Originally, I only wanted to prompt evaluation progress reporting, however on the way there I noticed the whole callback thing isn't currently implemented particularly well.\r\n\r\nI have:\r\n - Reworked the `prompt()` functions to accept a struct of callbacks instead\r\n - Made the callbacks return `void` instead of `bool` and provide a `stop()` function\r\n - Reimplemented the old `prompt()` function with the same signature to \"redirect\" to the new one.\r\n   - Exactly the same behaviour as the original old one\r\n   - Marked as deprecated\r\n - Added a prompt evaluation progress reporting callback\r\n\r\nFrom the API user facing side, this isn't a breaking change. However, from the implementation facing side, it is.\r\n\r\nI believe this change is important to make sure future additions of callbacks *don't* break the API. Also, it decreases code complexity for the API user by allowing that Callback struct to be constructed once and to then be just passed around.\r\n\r\nBefore, returning `false` from a callback didn't necessarily cause `prompt()` to stop. Calling `stop()` however *always* does, earlier or later.\r\n\r\nTodo:\r\n - [x] Update all calls made from `llmodel_c` to the old `prompt()` function to the new one",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 92,
        "deletions": 55,
        "changed_files": 7,
        "created_at": "2023-05-31T08:16:44Z",
        "closed_at": "2023-06-01T11:59:45Z",
        "merged_at": null,
        "body": "After GitHub has decided to mess up my PR once, and then to just close it for no reason another time, here comes the third attempt.\r\n\r\nThis is a follow-up to the previous implementation management improvement PR that was reverted due to serious issues in it. I took the second chance to make even more fundamental changes (not API breaking tho!).\r\n\r\nChanges:\r\n\r\n -  Fixes the compile error\r\n -  Allows the API user to have more control and insights into the different implementations\r\n -  Specially useful for gpt4all-chat\r\n -  Makes the code (much!) simpler\r\n\r\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 850,
        "deletions": 671,
        "changed_files": 21,
        "created_at": "2023-05-30T15:35:02Z",
        "closed_at": "2023-06-13T12:05:34Z",
        "merged_at": "2023-06-13T12:05:34Z",
        "body": "## Describe your changes\r\n\r\nThe current .NET Binding includes build scripts for building and bundling locally built gpt4all libraries with cmake, and only for Windows and Linux x64 builds. I have been working on build scripts to build the gpt4all libraries for [many more platforms](https://github.com/drasticactions/gpt4all.net.Runtime), including Mac, iOS, and Android, and with some tweaks to this binding, those libraries can all work here. I have a proof of concept of it [here](https://github.com/drasticactions/drastic.gpt4all.net), and now I'm trying to bring that code back into this repo.\r\n\r\n<img width=\"576\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2023-05-30 21 17 07\" src=\"https://github.com/nomic-ai/gpt4all/assets/898335/37952d3b-b515-42cb-adbc-0a93bc4a3485\">\r\n\r\n<img width=\"412\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2023-06-01 0 24 54\" src=\"https://github.com/nomic-ai/gpt4all/assets/898335/9e6395bb-8c33-4f64-9d8b-5631047d095c\">\r\n\r\n<img width=\"473\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2023-05-31 20 45 30\" src=\"https://github.com/nomic-ai/gpt4all/assets/898335/9bec8bab-85f3-4298-9564-fe1aa8e78026\">\r\n\r\nThis PR removes the hardcoded runtime bundle and introduces a library loader system, initially introduced in [whisper.net](https://github.com/sandrohanea/whisper.net/tree/main/Whisper.net/LibraryLoader). For libraries that are linked in as part of the build (iOS, Android, etc) the loader returns true. If it accepts dynamically loaded libraries (macOS, Windows, Linux) then you can load your libraries as part of your program by invoking `NativeLibraryLoader.LoadNativeLibrary();`\r\n\r\nCC @mvenditto \r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 34,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2023-05-30T14:16:27Z",
        "closed_at": "2023-05-30T15:26:35Z",
        "merged_at": "2023-05-30T15:26:35Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-05-29T14:27:59Z",
        "closed_at": "2023-05-30T20:34:23Z",
        "merged_at": "2023-05-30T20:34:23Z",
        "body": "## Describe your changes\r\n\r\nThe json response body of the web server's\r\nendpoint \"POST /v1/chat/completions\" is adhering to the openAi api schema\r\n\r\n## Issue ticket number and link\r\nissue #753 \r\n\r\n## Checklist before requesting a review\r\n- [X] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n  **not a core feature**\r\n- [x] I have added thorough documentation for my code.\r\n  **The change makes the application behave consistenlty with the current documentation.**\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n  **i think i do not have permission to attach labels on the PR. github does not provide me the option to do so**\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n  **no changes in GUI.**\r\n\r\n### Steps to Reproduce\r\ndescribed in the relevant issue\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 53,
        "deletions": 19,
        "changed_files": 2,
        "created_at": "2023-05-28T13:31:26Z",
        "closed_at": "2023-05-30T12:09:51Z",
        "merged_at": "2023-05-30T12:09:51Z",
        "body": "This change improves the way llmodel handles the different available build variants by:\r\n * Moving static functions into `LLModel` class\r\n * Seperating implementation list from `get_implementation()` (now `LLModel::getImplementation()`)\r\n * Allowing to `getBuildVariantList()` (for GUI)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-05-28T12:18:26Z",
        "closed_at": "2023-06-01T01:26:19Z",
        "merged_at": "2023-06-01T01:26:19Z",
        "body": "This MR attempt to implement detection for avxonly being required on the following platforms: Linux/GCC/x86_64, Windows/GCC/x86_64, Windows/MSVC/x86_64.\r\n\r\nI know the Linux implementation works, but I need people to test it under Windows!",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-27T22:10:35Z",
        "closed_at": "2023-05-28T23:50:45Z",
        "merged_at": "2023-05-28T23:50:45Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-05-27T18:28:54Z",
        "closed_at": "2023-05-28T23:51:11Z",
        "merged_at": "2023-05-28T23:51:11Z",
        "body": "fix golang gpt4all import path\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-26T19:32:53Z",
        "closed_at": "2023-05-28T23:56:24Z",
        "merged_at": "2023-05-28T23:56:24Z",
        "body": "## Describe your changes\r\nNoticed that the docs in the training directory are targeting the wrong repository. Small update to fix the indicated repo.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 84,
        "deletions": 75,
        "changed_files": 2,
        "created_at": "2023-05-25T15:58:00Z",
        "closed_at": "2023-05-26T13:24:04Z",
        "merged_at": "2023-05-26T13:24:04Z",
        "body": "## Describe your changes\r\n\r\nMany improvements to the Python bindings:\r\n\r\n- fix ignoring `--threads=4` (popular default, usually doesn't affect the users)\r\n- cleanup half-downloaded models\r\n- descriptive exceptions\r\n- make code simpler and more pythonic. \r\n\r\n## Issue ticket number and link\r\n-\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-25T07:20:34Z",
        "closed_at": "2023-05-25T15:35:48Z",
        "merged_at": "2023-05-25T15:35:48Z",
        "body": "This is an initial attempt to auto-detect what build variant should be loaded at runtime.\r\n\r\n - [x] avxonly detection (on x86_64)\r\n - [ ] avxonly detection (on aarch64)\r\n - [ ] CUDA detection and implementation",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 122,
        "deletions": 29,
        "changed_files": 8,
        "created_at": "2023-05-24T21:54:33Z",
        "closed_at": "2023-06-01T20:01:28Z",
        "merged_at": "2023-06-01T20:01:28Z",
        "body": "## Describe your changes\r\nAdded support to (optionally) integrate with the .NET logging infrastructure, ready to be used with DI.\r\n\r\n## Checklist before requesting a review\r\n- [ x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x ] I have added thorough documentation for my code.\r\n- [x ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\nExample using Serilog (without DI):\r\n```csharp\r\nusing Gpt4All;\r\nusing Serilog;\r\nusing Serilog.Core;\r\nusing Microsoft.Extensions.Logging;\r\n\r\nvar logger = new LoggerConfiguration()\r\n    .WriteTo.Console()\r\n    .MinimumLevel.Debug()\r\n    .CreateLogger();\r\n\r\nvar loggerFactory = new LoggerFactory().AddSerilog(logger);\r\n\r\nvar modelFactory = new Gpt4AllModelFactory(loggerFactory: loggerFactory);\r\n\r\n// ...\r\n```\r\n```\r\n[22:51:00 INF] Creating model path=C:\\models\\ggml-gpt4all-j-v1.3-groovy.bin type=GPTJ\r\n[22:51:00 DBG] Model created handle=0x1762702781760\r\n[22:51:00 INF] Model loading started\r\n[22:51:02 INF] Model loading completed success=True\r\n[22:51:02 INF] Start streaming prediction task\r\n[22:51:02 INF] Prompt input='list 3 colors' ctx=\r\n        {\r\n            logits_size = 0\r\n            tokens_size = 0\r\n            n_past = 0\r\n            n_ctx = 1024\r\n            n_predict = 128\r\n            [...truncated for brevity..]\r\n        }\r\n[22:51:10 INF] Prediction task completed elapsed=7.8376754s\r\n```\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1140,
        "deletions": 6,
        "changed_files": 8,
        "created_at": "2023-05-24T20:23:52Z",
        "closed_at": "2023-06-06T21:09:00Z",
        "merged_at": "2023-06-06T21:09:00Z",
        "body": "## Describe your changes\r\n1. script to convert hugging face replit model to ggml\r\n2. ggml replit model backend  + llmodel_c library integration\r\n3. Python bindings for replit model (currently this does not work due to whitespace parsing)\r\n\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n\r\n<img width=\"913\" alt=\"Screenshot 2023-06-02 at 2 50 05 PM\" src=\"https://github.com/nomic-ai/gpt4all/assets/22121416/c7547564-7ebc-4973-8669-c672d13dca64\">\r\n\r\n",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 3,
        "changed_files": 5,
        "created_at": "2023-05-24T20:04:42Z",
        "closed_at": "2023-05-28T23:57:01Z",
        "merged_at": "2023-05-28T23:57:01Z",
        "body": "## Describe your changes\r\nIntroduced the possibility to provide a prompt formatter (`IPromptFormatter`) to transform the prompt before passing it to the actual model. \r\nA default implementation (`DefaultPromptFormatter`), based on the template used in the python bindings, is provided.\r\n## Issue ticket number and link\r\n#707 \r\n\r\n## Checklist before requesting a review\r\n- [x ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2210,
        "deletions": 29,
        "changed_files": 25,
        "created_at": "2023-05-24T19:16:08Z",
        "closed_at": "2023-05-25T15:28:06Z",
        "merged_at": "2023-05-25T15:28:06Z",
        "body": "## Describe your changes\r\nLocaldocs feature\r\n\r\n## Checklist before requesting a review\r\n- [X ] I have performed a self-review of my code.\r\n- [X] I have added thorough documentation for my code.",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-24T08:37:25Z",
        "closed_at": "2023-05-24T19:15:13Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 110,
        "changed_files": 4,
        "created_at": "2023-05-23T13:55:47Z",
        "closed_at": "2023-05-23T15:03:32Z",
        "merged_at": "2023-05-23T15:03:32Z",
        "body": "## Describe your changes\r\nDeduplicated button code\r\n## Issue ticket number and link\r\nContinuing work and discussed on Discord\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 81,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2023-05-22T23:19:30Z",
        "closed_at": "2023-05-25T15:34:07Z",
        "merged_at": "2023-05-25T15:34:07Z",
        "body": "Create a `Gpt4All.Tests` project to hold unit tests for the C# bindings.\r\nSince the API is rapidly evolving right now, it only tests loading the three different types of models.\r\nHelp is welcome.\r\nThe plan is to eventually test the whole C# bindings library.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 21,
        "changed_files": 1,
        "created_at": "2023-05-22T21:45:42Z",
        "closed_at": "2023-05-25T15:34:22Z",
        "merged_at": "2023-05-25T15:34:22Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 78,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-05-22T19:09:32Z",
        "closed_at": "2023-05-22T21:37:53Z",
        "merged_at": null,
        "body": "## Describe your changes\r\nAllow developer to install QT with well known package managers. \r\n\r\nThe following steps can be executed on OSX to build the chat client. Please let me know if a script that runs these commands would be helpful! However this helped me build the app locally as a dev\r\n\r\n```bash\r\nbash ./getqt.sh\r\n# Installing Qt 6...\r\n# Using brew package manager...\r\n# Already up-to-date.\r\n# Qt 6 has been successfully installed.\r\n\r\n# build from cli\r\nmkdir build\r\ncd build\r\ncmake ..\r\ncmake --build . --parallel\r\nopen bin/gpt4all.app\r\n```\r\n\r\n## Checklist before requesting a review\r\n- [X] I have performed a self-review of my code.\r\n\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 78,
        "changed_files": 1,
        "created_at": "2023-05-22T18:24:40Z",
        "closed_at": "2023-05-22T21:22:06Z",
        "merged_at": "2023-05-22T21:22:06Z",
        "body": "## Describe your changes\r\nReduced duplicate button code in ChatDrawer.qml, on top of the MyButton and MyComboBox changes\r\n## Issue ticket number and link\r\nDiscussed on Discord\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 100,
        "deletions": 155,
        "changed_files": 5,
        "created_at": "2023-05-22T13:08:19Z",
        "closed_at": "2023-05-22T18:25:43Z",
        "merged_at": "2023-05-22T18:25:43Z",
        "body": "Start deduplicating the copy+pasta that is our current qml\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-05-21T20:27:53Z",
        "closed_at": "2023-05-22T13:04:26Z",
        "merged_at": "2023-05-22T13:04:26Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-05-21T19:30:25Z",
        "closed_at": "2023-05-22T13:04:37Z",
        "merged_at": "2023-05-22T13:04:37Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-05-21T16:20:08Z",
        "closed_at": "2023-05-22T03:14:18Z",
        "merged_at": "2023-05-22T03:14:18Z",
        "body": "## Describe your changes\r\nimproved documentation landing page\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [x] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [x] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 47173,
        "deletions": 236,
        "changed_files": 14,
        "created_at": "2023-05-21T13:06:26Z",
        "closed_at": "2023-05-30T16:05:57Z",
        "merged_at": "2023-05-30T16:05:57Z",
        "body": "## Adds a more complete and correct C++implementation of the tokenizer used in MPT and GPT-J\r\n\r\nnamely: \r\n* proper BPE tokenization - differs from the current implementation by using the `merges` list from the tokenizer config to combine tokens in the correct order - this noticeably improves output quality as without this many words (such as \"assistant\" as found in MPT-chat's ChatML prompts) are encoded *differently* than they were when the model was trained\r\n* complete additional vocab handling - several tokens are added to the tokenizer *after* the BPE vocabulary is learned, these have to be handled in a preprocessing step instead of simply added to the tokenizer vocabulary, as otherwise they could get split apart and incorrectly tokenized - this previously existed but only handled \"special\" tokens (like endoftext, and ChatML's `<|im_start|>` etc), but is necessary for *all* added tokens - notably MPT adds several lengths of just runs of spaces (`\"  \"`, `\"    \"`, `\"        \"`), presumably to allow more efficient tokenization of code indentation\r\n\r\nAnd the parts that required introducing a dependency on ICU:\r\n* [Unicode normalization](https://en.wikipedia.org/wiki/Unicode_equivalence) - both MPT and GPT-J expect \"NFC\" (fully composed, accented characters are always represented as a single codepoint where possible) normalization and it is the only type implemented here. This makes sure different ways of representing the same character are always tokenized the same way.\r\n* Handling the GPT2 BPE pretokenization *in unicode* - that is, the regex needs to apply to *Unicode codepoints*, and word-splitting on \"alphanumeric\" codepoints requires knowing *which* codepoints represent alphanumeric characters. `std::regex` is unfortunately incomplete here and will improperly split non-English text, causing incorrect tokenization.\r\n* UTF8 encoding and decoding generally - especially for the byte-codepoint encoding which is necessary for proper encoding and decoding of multibyte characters\r\n\r\nThe ICU dependency also adds some annoyance to the build process - on Ubuntu I just needed to install `libicu-dev`, on windows it worked after installing MSYS2 and running `pacman -S icu-devel`, and I don't have a Mac to test with.\r\n\r\nI prototyped this at https://github.com/apage43/bpe.cpp which also has a small standalone test \r\n\r\n## Demo\r\n\r\n### Before:\r\n[Tok-Before-1.webm](https://github.com/nomic-ai/gpt4all/assets/169252/af28558b-147b-43a0-b545-2b0e72e30315)\r\n\r\n### After:\r\n\r\n[Tok-After-1.webm](https://github.com/nomic-ai/gpt4all/assets/169252/6c49deaa-0a00-4f94-a56e-655184c8b17f)\r\n\r\n## TODO\r\n\r\n - [x] Bake tokenizer configs into the library to sidestep the distribution problem and avoid introducing extra complexity to bindings users\r\n - [x] Use fixed tokenizers for *decoding* as well as encoding - currently this calls the responseCallback with `std::string`s that may *not* be valid utf8 \r\n\r\n",
        "comments": 22
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-21T11:21:53Z",
        "closed_at": "2023-05-22T21:23:06Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\nAn update to the FAQ documentation regarding the Prompt Templates field in settings.\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n\r\nI chose to add an invitation to the current more relevant issue about Prompt Engineering as a way to boost the effort towards new findings.  \r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2023-05-20T23:26:38Z",
        "closed_at": "2023-07-12T06:01:23Z",
        "merged_at": null,
        "body": "## Describe your changes\r\nI moved the `### Prompt: ` message to all user prompts and made newline/space usage consistent.\r\n\r\n## Issue ticket number and link\r\n#653 \r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\nThis is an example of the outputs I was getting from `ggm-gpt4all-j-v1.3-groovy` before the changes: (output from verbose)\r\n```\r\n### Instruction:\r\n            The prompt below is a question to answer, a task to complete, or a conversation\r\n            to respond to; decide which and write an appropriate response.\r\n\r\n### Prompt:\r\nwhat is the mass of aluminium? is it strong?\r\n### Response:  The mass of aluminium is approximately 2.8 x 10^-3 kilograms, and it is considered strong.\r\nit is used in\r\n### Response:\r\n  The mass of aluminium is approximately 2.8 x 10^-3 kilograms, and it is considered strong.It's used in a variety of applications, such as construction materials and alloys.\r\n ```\r\nAfter the formatting changes I got these responses:\r\n```\r\n### Instruction:\r\n            The prompt below is a question to answer, a task to complete, or a conversation\r\n            to respond to; decide which and write an appropriate response.\r\n\r\n### Prompt:\r\nwhat is the mass of aluminium? is it strong?\r\n### Response:\r\n The mass of aluminium is approximately 2.8 x 10^-3 kilograms, and it is considered a strong metal.\r\n### Prompt:\r\nit is used in\r\n### Response:\r\n Yes, aluminum is used in many applications such as construction materials, cookware, and even in the production of aluminum cans.\r\n```\r\n\r\nBy prepending `Prompt: ` to user prompts, it seems to be must less likely to interpret the prompts as parts of its last response. It also repeats less of previous responses in some cases.\r\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1949,
        "deletions": 0,
        "changed_files": 33,
        "created_at": "2023-05-20T18:22:04Z",
        "closed_at": "2023-05-22T19:56:49Z",
        "merged_at": "2023-05-22T19:56:49Z",
        "body": "## Describe your changes\r\nFirst working version of the C# binding. \r\n\r\n## Issue ticket number and link\r\n#649 \r\n\r\n## Notes\r\n  - Tested the build on Windows and Linux(Ubuntu). \r\n    - OSX support can be easily added but I do not own a Mac to test it.\r\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-05-19T20:55:09Z",
        "closed_at": "2023-05-20T20:36:30Z",
        "merged_at": "2023-05-20T20:36:30Z",
        "body": "## Describe your changes\r\nWhen using the golang bindings, cgo includes in `gpt4all.go` are relative, this makes any use of the bindings outside of the gpt4all repository fail.\r\n\r\nThis PR adds `$(SRCDIR)` to the includes so they are absolute.\r\n\r\n## Issue ticket number and link\r\nThis fixes issue: #614 \r\n\r\n## Checklist before requesting a review\r\n- [x ] I have performed a self-review of my code.\r\n- [ x] If it is a core feature, I have added thorough tests.\r\n- [ x] I have added thorough documentation for my code.\r\n- [x ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [X ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n### Steps to Reproduce\r\n\r\nInclude the bindings in a new golang project.\r\n\r\nTry to build:\r\n```\r\n$ make main \r\n        C_INCLUDE_PATH=/Users/XXX/code/src/github.com/XXX/intelligens/include \\\r\n        LIBRARY_PATH=/Users/XXX/code/src/github.com/XXX/intelligens/lib \\\r\n        go build\r\n# github.com/nomic-ai/gpt4all/gpt4all-bindings/golang\r\nbinding.cpp:1:10: fatal error: '../../gpt4all-backend/llmodel_c.h' file not found\r\nmake: *** [main] Error 1\r\n```\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 114,
        "deletions": 39,
        "changed_files": 10,
        "created_at": "2023-05-19T13:30:21Z",
        "closed_at": "2023-05-21T14:49:42Z",
        "merged_at": "2023-05-21T14:49:42Z",
        "body": "## Describe your changes\r\n\r\nThis change introduces full compatibility with new ggml quanitzation *without* killing the old one (which is renamed to `{llama,ggml}-old`).\r\nThe API is be kept unchanged and these changes completely invisible to it.\r\n\r\n## Issue ticket number and link\r\n\r\nEvery single one that complains about new llama models not working :-)\r\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2023-05-19T01:26:43Z",
        "closed_at": "2023-05-19T14:20:37Z",
        "merged_at": null,
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-18T22:36:44Z",
        "closed_at": "2023-05-19T01:40:18Z",
        "merged_at": "2023-05-19T01:40:18Z",
        "body": "install gpt4all, not gpt4all-chat in qt set up example.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1110,
        "deletions": 294,
        "changed_files": 32,
        "created_at": "2023-05-18T20:26:16Z",
        "closed_at": "2023-05-19T12:54:54Z",
        "merged_at": "2023-05-19T12:54:54Z",
        "body": "We've discussed it all on Discord.\r\n\r\nBasically, this change:\r\n - Improves:\r\n   - API\r\n - Adds:\r\n   - Support for dynamically loading model implementations\r\n   - Ability to build stuff like avxonly, cuda, opencl, ... in the same build\r\n     - allows developer/user to select those dynamically\r\n\r\nEach model implementation may use a different fork of ggml, name collisions are taken care of.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-05-17T12:43:21Z",
        "closed_at": "2023-05-17T13:56:21Z",
        "merged_at": "2023-05-17T13:56:21Z",
        "body": "## Describe your changes\n\nFix typo in README\n\n## Issue ticket number and link\n\nNA\n\n## Checklist before requesting a review\n- [x] I have performed a self-review of my code.\n- [x] If it is a core feature, I have added thorough tests.\n- [x] I have added thorough documentation for my code.\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\n- [x] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\n\n## Demo\n<!-- Screenshots or video of new or updated code changes !-->\nNA\n\n### Steps to Reproduce\n<!-- Steps to reproduce demo !-->\nNA\n\n## Notes\n<!-- Any other relevant information to include about PR !-->\nNA",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2023-05-17T06:26:19Z",
        "closed_at": "2023-05-17T11:54:10Z",
        "merged_at": "2023-05-17T11:54:10Z",
        "body": "Caught with AddressSanitizer running a basic prompt test against llmodel standalone. This fix allows ASan builds to complete a simple prompt without illegal accesses but there are still notably several leaks.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2023-05-17T01:16:53Z",
        "closed_at": "2023-05-17T11:55:06Z",
        "merged_at": null,
        "body": "220230420\r\n\r\n## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [ ] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 757,
        "deletions": 1,
        "changed_files": 14,
        "created_at": "2023-05-16T18:10:26Z",
        "closed_at": "2023-05-22T19:55:22Z",
        "merged_at": "2023-05-22T19:55:22Z",
        "body": "## Describe your changes\r\n\r\n## Issue ticket number and link\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [x] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n\r\n## Demo\r\n<!-- Screenshots or video of new or updated code changes !-->\r\n\r\n### Steps to Reproduce\r\n<!-- Steps to reproduce demo !-->\r\n\r\n## Notes\r\n<!-- Any other relevant information to include about PR !-->\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 72,
        "deletions": 233,
        "changed_files": 4,
        "created_at": "2023-05-16T00:57:52Z",
        "closed_at": "2023-05-16T14:30:19Z",
        "merged_at": "2023-05-16T14:30:19Z",
        "body": "* Removes non-threadsafe use of static inference buffer in mpt code and actually uses model struct buffer\r\n* Deduplicates tokenizing and sampling code in gptj and mpt models\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 6,
        "changed_files": 7,
        "created_at": "2023-05-15T20:46:08Z",
        "closed_at": "2023-05-16T15:33:59Z",
        "merged_at": "2023-05-16T15:33:59Z",
        "body": "## Describe your changes\r\n\r\nMaking gpt4all typo free from now and there on\r\n\r\n## Checklist before requesting a review\r\n- [x] I have performed a self-review of my code.\r\n- [ ] If it is a core feature, I have added thorough tests.\r\n- [ ] I have added thorough documentation for my code.\r\n- [ ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.\r\n- [ ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 163,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2023-05-13T23:21:32Z",
        "closed_at": "2023-05-16T20:47:54Z",
        "merged_at": "2023-05-16T20:47:54Z",
        "body": "This PR implements a small CLI chat repl. There are small changes made to support this inside of the `pyllmodel.py` but most logic is contained to a new file.\r\n\r\nPlease let me know if I should make any changes! \r\n\r\nhttps://github.com/nomic-ai/gpt4all/assets/9896130/5e37bb0e-b774-4261-88b1-6ce214048a69\r\n\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-05-13T16:00:07Z",
        "closed_at": "2023-05-13T17:52:37Z",
        "merged_at": "2023-05-13T17:52:37Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-05-13T11:17:20Z",
        "closed_at": "2023-05-16T15:36:46Z",
        "merged_at": "2023-05-16T15:36:46Z",
        "body": "### Summary\r\n\r\nThis pull request adds more general create and destroy functions:\r\n- `llmodel_model_create`: Creates an LLmodel instance from a model file.\r\n- `llmodel_model_destroy`: Destroys an LLmodel instance.\r\n\r\n```c++\r\n/**\r\n * Create a LLmodel instance.\r\n * Recognises correct model type from file at model_path\r\n * @param model_path A string representing the path to the model file. \r\n * @return A pointer to the LLmodel instance.\r\n */\r\nllmodel_model llmodel_model_create(const char *model_path);\r\n\r\n/**\r\n * Destroy a LLmodel instance.\r\n * Recognises correct model type using type info\r\n * @param model a pointer to a LLmodel instance.\r\n */\r\nvoid llmodel_model_destroy(llmodel_model model);\r\n```\r\n### Details\r\nThe `llmodel_model_create` function recognises the correct model type `using uint32_t magic;` from `model_path` and creates `llmodel_xxxx_create` pointers accordingly.\r\n\r\nThe destroy function gets the `modelTypeInfo == typeid()` from the `llmodel_model` and calls `llmodel_xxxx_destroy` functions accordingly.\r\n\r\n### Why it would be helpful?\r\nThis way the user does not have to implement similar checks in their applications. Especially when new model types are added.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-05-12T18:13:57Z",
        "closed_at": "2023-05-12T21:11:53Z",
        "merged_at": "2023-05-12T21:11:52Z",
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 76,
        "deletions": 16,
        "changed_files": 7,
        "created_at": "2023-05-12T17:11:20Z",
        "closed_at": "2023-05-12T19:21:56Z",
        "merged_at": "2023-05-12T19:21:56Z",
        "body": "Python bindings for MPT models\r\n\r\n- Updated bindings\r\n- Updated docs\r\n- Updated tests\r\n- Updated PyPI package\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-05-12T15:19:53Z",
        "closed_at": "2023-05-12T16:27:48Z",
        "merged_at": "2023-05-12T16:27:48Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-05-11T19:31:26Z",
        "closed_at": "2023-05-11T20:49:15Z",
        "merged_at": "2023-05-11T20:49:15Z",
        "body": "Adds mpt-7b-instruct to the model list.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-05-11T18:30:19Z",
        "closed_at": "2023-05-12T14:21:14Z",
        "merged_at": "2023-05-12T14:21:14Z",
        "body": "Forgot a few small things:\r\n\r\n1. Rename PR template folder so github will recognize\r\n2. python bindings readme now includes pip package install. ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-05-11T17:53:45Z",
        "closed_at": "2023-05-20T11:00:36Z",
        "merged_at": null,
        "body": "Title: gpt4all-backend: Add compatibility with new sampling algorithms in llama.cpp\r\n\r\nDescription: This pull request allows compilation with newest llama.cpp as a submodule.\r\n\r\nChanges:\r\n\r\nImplemented temperature sampling with repetition penalty as an alternative to the previous `llama_sample_top_p_top_k`  sampling method.\r\n\r\n```c++\r\n        // Temperature sampling with repetition penalty\r\n        llama_sample_repetition_penalty(\r\n            d_ptr->ctx, &candidates_data,\r\n            promptCtx.tokens.data() + promptCtx.n_ctx - promptCtx.repeat_last_n, promptCtx.repeat_last_n,\r\n            promptCtx.repeat_penalty);\r\n        llama_sample_top_k(d_ptr->ctx, &candidates_data, promptCtx.top_k, 1);\r\n        llama_sample_top_p(d_ptr->ctx, &candidates_data, promptCtx.top_p, 1);\r\n        llama_sample_temperature(d_ptr->ctx, &candidates_data, promptCtx.temp);\r\n        llama_token id = llama_sample_token(d_ptr->ctx, &candidates_data);\r\n```\r\n\r\nThis is aimed just to have the minimal changes needed for llama.cpp to compile. In the future one needs to add the other sampling methods and expand the context to include the new sampling parameters.\r\n\r\n> **Note**\r\n> Compared to examples/main.cpp this omits [Tail Free](https://www.trentonbricken.com/Tail-Free-Sampling/) and  [Locally Typical](https://arxiv.org/abs/2202.00666) samplings. This is to avoid changing the context with extra parameters.  Those were not used or included in the old `llama_sample_top_p_top_k` either.\r\n\r\n```c++\r\n        llama_sample_tail_free(ctx, &candidates_p, tfs_z);\r\n        llama_sample_typical(ctx, &candidates_p, typical_p);\r\n```\r\n\r\n> **Note**\r\n> Llama.cpp needs to be included as a submodule as it now generates version info from git automatically using  Cmakelists.txt. See https://github.com/ggerganov/llama.cpp/pull/1232 and https://github.com/ggerganov/llama.cpp/pull/1289\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 134,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2023-05-11T16:33:30Z",
        "closed_at": "2023-05-11T18:18:11Z",
        "merged_at": "2023-05-11T18:18:11Z",
        "body": "- Contributing.md\r\n- Pull request template\r\n- Some updates to README",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 79,
        "changed_files": 1,
        "created_at": "2023-05-11T15:21:07Z",
        "closed_at": "2023-05-11T16:36:42Z",
        "merged_at": "2023-05-11T16:36:42Z",
        "body": "We uploaded an earlier version of the script that didn't add the right things to the ggml file. This is the correct one to use ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 801,
        "deletions": 0,
        "changed_files": 11,
        "created_at": "2023-05-11T10:48:32Z",
        "closed_at": "2023-05-15T16:45:57Z",
        "merged_at": "2023-05-15T16:45:56Z",
        "body": "This is a wrap up of what's currently used by https://github.com/go-skynet/LocalAI, I think this is far from perfect, but a good stab for a first implementation, and polish it in a second round. It is WIP yet as I'm still testing it and need to tidy it a bit, feedback is welcome ",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 263,
        "deletions": 1,
        "changed_files": 7,
        "created_at": "2023-05-11T03:52:38Z",
        "closed_at": "2023-05-11T16:33:59Z",
        "merged_at": null,
        "body": "Amazing project, heres a small contribution\r\n\r\nPython bindings and chat demo\r\n\r\n![demo](https://raw.githubusercontent.com/nomic-ai/gpt4all/9d8c5afbe340b00bae82f5eb3aa872c31672fdc8/gpt4all-bindings/python/gif/demo.gif)",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1229,
        "deletions": 0,
        "changed_files": 19,
        "created_at": "2023-05-10T20:34:49Z",
        "closed_at": "2023-05-11T16:14:45Z",
        "merged_at": "2023-05-11T16:14:45Z",
        "body": "This PR introduces Python bindings around the model backends found in `gpt4all-backend`.\r\n\r\nThe PR contains:\r\n\r\n- A Python package with \r\n  - ctype bindings around `llmodel` C API\r\n  - GPT4All universal model API with download model and chat completion functionality\r\n  - tests\r\n  - setup.py that handles building the package with the C library\r\n\r\n - mkdocs with docstrings\r\n - circleci pipeline for building python bindings for Windows (64), Linux, OSX (universal). This build is currently stopped since I only need to run it when deploying to pypi. A more robust CI/CD pipeline with tests and for monorepo still needs to be introduced.\r\n \r\n [Restricted Access] Tutorial colab: https://colab.research.google.com/drive/1QRFHV5lj1Kb7_tGZZGZ-E6BfX6izpeMI?usp=sharing\r\n ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 370,
        "deletions": 95,
        "changed_files": 2,
        "created_at": "2023-05-08T16:39:56Z",
        "closed_at": "2023-05-11T16:32:41Z",
        "merged_at": null,
        "body": "- Removed outdated download/installation instructions\r\n- Added gpt4all.io model links",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-05-02T09:58:32Z",
        "closed_at": "2023-05-05T11:34:39Z",
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-29T22:07:38Z",
        "closed_at": "2023-05-02T14:15:40Z",
        "merged_at": "2023-05-02T14:15:40Z",
        "body": "README.md typo fix.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-28T06:54:47Z",
        "closed_at": "2023-05-11T16:32:22Z",
        "merged_at": null,
        "body": "Fixed setup",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2023-04-27T18:42:51Z",
        "closed_at": "2023-05-11T16:31:58Z",
        "merged_at": null,
        "body": "added nice table for edge models",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 297,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2023-04-26T22:32:58Z",
        "closed_at": "2023-04-27T13:59:59Z",
        "merged_at": null,
        "body": "How do you pass parameters to speed up gpt4all",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5168,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-24T06:31:43Z",
        "closed_at": "2023-05-11T16:32:07Z",
        "merged_at": null,
        "body": null,
        "comments": 2
    },
    {
        "merged": false,
        "additions": 484,
        "deletions": 78,
        "changed_files": 4,
        "created_at": "2023-04-23T13:32:55Z",
        "closed_at": "2023-05-11T16:36:52Z",
        "merged_at": null,
        "body": "* Adds automatic checkpoint resuming logic to ``train.py``\r\n* Updated train progress bar to include epoch and train_loss\r\n* Added an ``__init__.py`` to make it easier to invoke gpt4all code from other Python wrappers; there is a larger discussion that should be had regarding whether python files should be moved into a subfolder and make the project a proper Python package.\r\n* Configuration files should be backwards compatible\r\n\r\nTo start a new training session use the following configuration:\r\n\r\n```yaml\r\ncheckpoint: ~\r\n```\r\n\r\nAs before, to force a specific checkpoint:\r\n\r\n```yaml\r\ncheckpoint: \"foo/gpt4all-7b-hf-lora/step_135000\"\r\ntrain_args:\r\n  resume_from_checkpoint: \"step_135000\"\r\n```",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 118,
        "deletions": 121,
        "changed_files": 4,
        "created_at": "2023-04-20T01:46:24Z",
        "closed_at": "2023-05-11T16:37:05Z",
        "merged_at": null,
        "body": "I cleaned up the markdown documentation, not nothing significant, just some basic syntax fixes and formatting applied.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2023-04-19T08:34:05Z",
        "closed_at": "2023-05-11T16:33:00Z",
        "merged_at": null,
        "body": null,
        "comments": 8
    },
    {
        "merged": false,
        "additions": 133,
        "deletions": 145,
        "changed_files": 10,
        "created_at": "2023-04-15T18:22:32Z",
        "closed_at": "2023-04-17T23:59:04Z",
        "merged_at": null,
        "body": "PyCharm gave me a lot of warnings about the current main branch so I fixed many of those (not all).\r\nThe vast majority of fixes are auto-fixes.\r\nThe inference works perfectly after my changes.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-04-14T11:34:25Z",
        "closed_at": "2023-04-15T16:33:35Z",
        "merged_at": null,
        "body": "Extremely minor change, \"Doulingo\" -> \"Duolingo\" in `eval_data/user_oriented_instructions.jsonl`",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-13T19:49:32Z",
        "closed_at": "2023-04-13T21:39:19Z",
        "merged_at": "2023-04-13T21:39:19Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-04-11T10:58:14Z",
        "closed_at": "2023-05-11T16:34:33Z",
        "merged_at": null,
        "body": "This is tested on both Windows 10 and Debian 10.\r\n\r\n**Untested on OSX and MAC. But should work.** \r\nNeed reports as I have no such hardware to test.  \r\n\r\n**What it does:** It will allow to simplify the **Try yourself** section in the README.md\r\nThe OS will be selected by a generic command `./gpt4all-lora-quantized` instead of manual selection by:\r\n```\r\nM1 Mac/OSX: cd chat;./gpt4all-lora-quantized-OSX-m1\r\nLinux: cd chat;./gpt4all-lora-quantized-linux-x86\r\nWindows (PowerShell): cd chat;./gpt4all-lora-quantized-win64.exe\r\nIntel Mac/OSX: cd chat;./gpt4all-lora-quantized-OSX-intel\r\n```\r\n### Old\r\n![image](https://user-images.githubusercontent.com/21064622/231138970-51faf88d-bea8-4285-a02e-fec16d6d78ac.png)\r\n\r\n### New\r\n1. Download the `gpt4all-lora-quantized.bin` file from [Direct Link](https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin) or [[Torrent-Magnet]](https://tinyurl.com/gpt4all-lora-quantized).\r\n2. Clone this repository, navigate to `chat`, and place the downloaded file there.\r\n3. Run the appropriate command for your OS: \r\n   * Windows \r\n      * (Batch) `cd chat && .\\gpt4all-lora-quantized`\r\n      * (PowerShell) `cd chat; ./gpt4all-lora-quantized`\r\n   * Linux/Mac/OSX `cd chat; chmod +x ./gpt4all-lora-quantized; ./gpt4all-lora-quantized`\r\n\r\n## All old arguments are supported and tested.\r\n `./gpt4all-lora-quantized --help`\r\n `./gpt4all-lora-quantized --model .\\gpt4all-lora-quantized.bin`\r\n...\r\n\r\n\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-10T09:08:05Z",
        "closed_at": "2023-04-10T14:55:03Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-10T03:28:48Z",
        "closed_at": "2023-05-11T16:34:51Z",
        "merged_at": null,
        "body": "When there one and only one model found in the chat dir, use that model\nautomatically without asking the user for selection.\n\nAnd show the get started link when no model found.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-08T17:05:44Z",
        "closed_at": "2023-04-13T21:39:43Z",
        "merged_at": null,
        "body": "If `init_trackers` isn't called, the training process will throw an error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/gpt4all/train.py\", line 213, in <module>\r\n    train(accelerator, config=config)\r\n  File \"/home/xxx/gpt4all/train.py\", line 189, in train\r\n    accelerator.end_training()\r\n  File \"/home/xxx/gpt4all/venv/lib/python3.9/site-packages/accelerate/accelerator.py\", line 548, in _inner\r\n    return PartialState().on_main_process(function)(*args, **kwargs)\r\n  File \"/home/xxx/gpt4all/venv/lib/python3.9/site-packages/accelerate/accelerator.py\", line 2118, in end_training\r\n    for tracker in self.trackers:\r\nAttributeError: 'Accelerator' object has no attribute 'trackers'\r\n```",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-08T03:57:01Z",
        "closed_at": "2023-05-11T16:35:09Z",
        "merged_at": null,
        "body": "referring this issue https://github.com/nomic-ai/gpt4all/issues/195 , i wasn't sure if it's RAM or VRAM, so i changed it to avoid confusion",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2023-04-07T20:07:45Z",
        "closed_at": "2023-05-11T16:34:58Z",
        "merged_at": null,
        "body": "Mostly removing excessive whitespace and adding some more spaces to comments.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 76,
        "deletions": 60,
        "changed_files": 1,
        "created_at": "2023-04-07T13:34:57Z",
        "closed_at": "2023-04-15T16:34:04Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-04-06T23:59:36Z",
        "closed_at": "2023-04-07T14:50:56Z",
        "merged_at": "2023-04-07T14:50:56Z",
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-06T22:12:26Z",
        "closed_at": "2023-04-07T14:50:28Z",
        "merged_at": "2023-04-07T14:50:28Z",
        "body": "Mismatch of MD5 between the `-ggml` and regular `gpt4all-lora-quantized.bin`. ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-06T16:59:51Z",
        "closed_at": "2023-04-06T18:14:21Z",
        "merged_at": null,
        "body": null,
        "comments": 3
    },
    {
        "merged": false,
        "additions": 186,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-04-04T19:00:59Z",
        "closed_at": "2023-05-07T12:17:45Z",
        "merged_at": null,
        "body": "I've upgraded the GUI a bit\r\n![image](https://user-images.githubusercontent.com/24637991/231870006-26fa35c4-17f0-40c6-a700-0458e7a0e604.png)\r\n\r\nNow QML files are used for a more modern design on the frontend, and python on the backend.\r\nWhen you ask a question, a daemon is launched that accesses the nomic.gpt4all library. Then he gives the answer to the front end. \r\n\r\nP.s. I don't understand at all why it gives such answers sometimes.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-04T11:55:40Z",
        "closed_at": "2023-04-05T18:10:57Z",
        "merged_at": null,
        "body": "Just a quick note to let folks downloading the checkpoint know how big it is.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-04-04T04:43:01Z",
        "closed_at": "2023-05-11T16:34:41Z",
        "merged_at": null,
        "body": "# Quickly Demo\r\n$ docker build -t nomic-ai/gpt4all:1.0.0 .\r\n$ docker run -it --rm -v ./gpt4all-lora-quantized.bin:/opt/gpt4all nomic-ai/gpt4all:1.0.0\r\n\r\n# Multi Arch\r\n$ docker buildx build --platform linux/amd64,linux/arm64 --push -t nomic-ai/gpt4all:1.0.0 .\r\n\r\n## Mac Intel\r\n$ docker buildx build --platform linux/amd64 --push -t nomic-ai/gpt4all-amd64:1.0.0 .\r\n$ docker run -it --rm -v ./gpt4all-lora-quantized.bin:/opt/gpt4all nomic-ai/gpt4all-amd64:1.0.0 /opt/gpt4all/gpt4all-lora-quantized-OSX-intel -m /opt/gpt4all/gpt4all-lora-quantized.bin\r\n## Mac m1\r\n$ docker buildx build --platform linux/arm64 --push -t nomic-ai/gpt4all-arm64:1.0.0 .\r\n$ docker run -it --rm -v ./gpt4all-lora-quantized.bin:/opt/gpt4all nomic-ai/gpt4all-arm64:1.0.0 /opt/gpt4all/gpt4all-lora-quantized-OSX-m1 -m /opt/gpt4all/gpt4all-lora-quantized.bin",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-04-04T00:13:49Z",
        "closed_at": "2023-04-06T18:15:57Z",
        "merged_at": "2023-04-06T18:15:57Z",
        "body": "Fixing specific issues with consistency in the formatting not covered by other README PRs.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2023-04-03T16:19:07Z",
        "closed_at": "2023-04-06T18:16:34Z",
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-04-02T16:20:23Z",
        "closed_at": "2023-04-03T21:21:51Z",
        "merged_at": "2023-04-03T21:21:51Z",
        "body": "`git submodule configure` isn't a valid command. I assume the intended command is `init`, which can be combined with `update` using the `--init` option.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 34,
        "deletions": 32,
        "changed_files": 1,
        "created_at": "2023-04-02T04:24:36Z",
        "closed_at": "2023-04-06T18:17:06Z",
        "merged_at": "2023-04-06T18:17:06Z",
        "body": "Type and formatting improvements.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-02T03:57:02Z",
        "closed_at": "2023-04-03T21:34:23Z",
        "merged_at": "2023-04-03T21:34:23Z",
        "body": "Fixing the following bug:\r\n```python\r\n  File \"/mnt/storage7T/gpt4all/data.py\", line 73, in load_data\r\n    dataset = dataset.train_test_split(test_size=.05, seed=config[\"seed\"])\r\nAttributeError: 'DatasetDict' object has no attribute 'train_test_split'\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 /mnt/storage7T/gpt4all/train.py:207 in <module>                                                  \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   204 \u2502   else:                                                                                  \u2502\r\n\u2502   205 \u2502   \u2502   accelerator = Accelerator()                                                        \u2502\r\n\u2502   206 \u2502                                                                                          \u2502\r\n\u2502 \u2771 207 \u2502   train(accelerator, config=config)                                                      \u2502\r\n\u2502   208                                                                                            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /mnt/storage7T/gpt4all/train.py:57 in train                                                      \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    54 \u2502                                                                                          \u2502\r\n\u2502    55 \u2502                                                                                          \u2502\r\n\u2502    56 \u2502   with accelerator.main_process_first():                                                 \u2502\r\n\u2502 \u2771  57 \u2502   \u2502   train_dataloader, val_dataloader = load_data(config, tokenizer)                    \u2502\r\n\u2502    58 \u2502                                                                                          \u2502\r\n\u2502    59 \u2502                                                                                          \u2502\r\n\u2502    60 \u2502   checkpoint = config[\"gradient_checkpointing\"]                                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /mnt/storage7T/gpt4all/data.py:73 in load_data                                                   \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    70 \u2502   else:                                                                                  \u2502\r\n\u2502    71 \u2502   \u2502   dataset = load_dataset(dataset_path)                                               \u2502\r\n\u2502    72 \u2502                                                                                          \u2502\r\n\u2502 \u2771  73 \u2502   dataset = dataset.train_test_split(test_size=.05, seed=config[\"seed\"])                 \u2502\r\n\u2502    74 \u2502                                                                                          \u2502\r\n\u2502    75 \u2502   train_dataset, val_dataset = dataset[\"train\"], dataset[\"test\"]                         \u2502\r\n\u2502    76                                                                                            \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nAttributeError: 'DatasetDict' object has no attribute 'train_test_split'\r\nwandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 58,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-01T19:47:28Z",
        "closed_at": "2023-04-03T21:15:59Z",
        "merged_at": null,
        "body": "Making a way to either include or use this project an easier way, still working on it a bit",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 88,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-01T15:30:54Z",
        "closed_at": "2023-04-03T21:20:29Z",
        "merged_at": "2023-04-03T21:20:29Z",
        "body": "The script detects the user's operating system, lists available .bin files and prompts the user to select a .bin file to run. Ensuring a more user-friendly experience.\r\n![image](https://user-images.githubusercontent.com/120996278/229298626-5baf7d35-af48-41ff-abd3-2347cf36c829.png)\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 205,
        "deletions": 4,
        "changed_files": 6,
        "created_at": "2023-04-01T09:07:53Z",
        "closed_at": "2023-04-03T21:21:20Z",
        "merged_at": null,
        "body": "I have added install.sh and install.bat which is a script that verifies the existance of python, pipy and venv, install them if they're not there (after getting user concent), then creates a virtual environment and install all required libraries. Users can then use the python code directly by activating the virtual environment.\r\n\r\nI also added uninstall.sh and uninstaoll.bat\r\nI hope it helps.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-04-01T01:11:14Z",
        "closed_at": "2023-04-03T21:19:06Z",
        "merged_at": null,
        "body": "Reordered 1 and 2 so people first download the repo then download the model. \r\nAdded commands that people can copy paste. \r\nAdded emoji to discord link.\r\nAdded return line for readability.\r\nThat's it",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-01T00:26:25Z",
        "closed_at": "2023-04-03T21:17:52Z",
        "merged_at": "2023-04-03T21:17:52Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-31T23:16:50Z",
        "closed_at": "2023-04-03T21:16:23Z",
        "merged_at": "2023-04-03T21:16:23Z",
        "body": null,
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-03-31T10:22:42Z",
        "closed_at": "2023-04-03T21:22:26Z",
        "merged_at": null,
        "body": "Resolves [131](https://github.com/nomic-ai/gpt4all/issues/131) by adding instructions to verify file integrity using the sha512sum command, making sure to include checksums for gpt4all-lora-quantized.bin and gpt4all-lora-unfiltered-quantized.bin",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-03-31T07:55:44Z",
        "closed_at": "2023-04-06T18:17:33Z",
        "merged_at": "2023-04-06T18:17:32Z",
        "body": "Added additional OS terminal commands to readme for the unfiltered bin:\r\n\r\nM1 Mac/OSX: cd chat;./gpt4all-lora-quantized-OSX-m1 -m gpt4all-lora-unfiltered-quantized.bin\r\nLinux: cd chat;./gpt4all-lora-quantized-linux-x86 -m gpt4all-lora-unfiltered-quantized.bin\r\nWindows (PowerShell): cd chat;./gpt4all-lora-quantized-win64.exe -m gpt4all-lora-unfiltered-quantized.bin\r\nIntel Mac/OSX: cd chat;./gpt4all-lora-quantized-OSX-intel -m gpt4all-lora-unfiltered-quantized.bin",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-03-31T01:06:31Z",
        "closed_at": "2023-04-13T21:40:11Z",
        "merged_at": null,
        "body": "Shorten prompts and responses if they are too long to fit within the specified max length. This ensures that the model does not exceed the input length limit during training.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 71,
        "deletions": 73,
        "changed_files": 2,
        "created_at": "2023-03-30T18:10:29Z",
        "closed_at": "2023-04-06T18:18:13Z",
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-30T15:55:02Z",
        "closed_at": "2023-04-03T21:18:11Z",
        "merged_at": "2023-04-03T21:18:11Z",
        "body": "Conditonal -> Conditional",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-03-30T04:02:16Z",
        "closed_at": "2023-03-31T01:40:14Z",
        "merged_at": null,
        "body": "Colab demo",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-29T12:50:18Z",
        "closed_at": "2023-03-29T14:38:26Z",
        "merged_at": "2023-03-29T14:38:26Z",
        "body": "The data downloads pretty slowly (and, direct from S3 I'd guess is costing you guys a fair chunk!)\r\n\r\nThis torrent link should fix both issues.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-03-29T10:50:00Z",
        "closed_at": "2023-03-29T14:35:38Z",
        "merged_at": "2023-03-29T14:35:38Z",
        "body": "Fixes #14 \r\nI compiled the binary [from the gpt4all.cpp fork](https://github.com/zanussbaum/gpt4all.cpp) using cmake\r\n![image](https://user-images.githubusercontent.com/59402820/228510974-3ead41fc-453d-4583-8912-4f4997a3ac90.png)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-03-29T08:24:08Z",
        "closed_at": "2023-03-29T14:34:52Z",
        "merged_at": "2023-03-29T14:34:52Z",
        "body": "### This PR adds a pre-compiled binary, like the existing ones for M1 Macs and Linux, for Intel Macs.\r\n\r\nI wanted to try this out on my Intel MacBook Pro. I saw in https://github.com/nomic-ai/gpt4all/issues/17 the link to the https://github.com/zanussbaum/gpt4all.cpp source.\r\n\r\nCompiled that, and renamed to match this project. Works great on my machine. Hope this helps others.\r\n\r\n<img width=\"1304\" alt=\"image\" src=\"https://user-images.githubusercontent.com/6340841/228472662-611f7d52-5cab-4ef2-8deb-e76a103a8229.png\">\r\n\r\n\r\nThanks",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2023-03-29T04:18:57Z",
        "closed_at": "2023-03-29T14:48:24Z",
        "merged_at": null,
        "body": "Hi, I am a beginner, just wanted to update docs, so its easy to copy commands mentioned in readme",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-03-28T18:49:03Z",
        "closed_at": "2023-03-29T17:50:27Z",
        "merged_at": "2023-03-29T17:50:27Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 89,
        "deletions": 1,
        "changed_files": 6,
        "created_at": "2023-03-28T18:48:40Z",
        "closed_at": "2023-03-28T20:23:50Z",
        "merged_at": "2023-03-28T20:23:49Z",
        "body": null,
        "comments": 0
    }
]