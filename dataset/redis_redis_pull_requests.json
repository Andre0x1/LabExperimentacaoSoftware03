[
    {
        "merged": true,
        "additions": 25,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-10-18T22:08:15Z",
        "closed_at": "2023-10-22T08:56:45Z",
        "merged_at": "2023-10-22T08:56:45Z",
        "body": "Fixing issues described in #12672, started after #11695 when the defrag tests are being executed in cluster mode too.\r\nFor some reason, it looks like the defragmentation is over too quickly, before the test is able to detect that it's running.\r\nso now instead of waiting to see that it's active, we wait to see that it did some work\r\n```\r\n[err]: Active defrag big list: cluster in tests/unit/memefficiency.tcl\r\ndefrag not started.\r\n[err]: Active defrag big keys: cluster in tests/unit/memefficiency.tcl\r\ndefrag didn't stop.\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-18T19:01:09Z",
        "closed_at": "2023-10-19T18:12:59Z",
        "merged_at": "2023-10-19T18:12:59Z",
        "body": "Temporarily disabling few of the defrag tests in cluster mode to make the daily run stable:\r\n\r\n1. Active defrag eval scripts\r\n2. Active defrag big keys\r\n3. Active defrag big list\r\n4. Active defrag edge case\r\n\r\nFew scenarios observed and I'm investigating:\r\n\r\n1. `defrag not started` - Either the defrag scan got completed real quick or never started.\r\n2. Actual fragmentation is lower than the expected fragmentation - Might require us to lower the fragmentation expectation or have separate threshold for cluster mode.\r\n3. Max latency exceeds from the current set threshold.\r\n4. `defrag didn't stop` - Sometimes allocator_frag_ratio reaches `1.06` and the test fails. Expectation is `1.05`.\r\n\r\nFailure run: https://github.com/redis/redis/actions/runs/6527277037/job/17721912648\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-17T11:39:41Z",
        "closed_at": "2023-10-18T10:25:52Z",
        "merged_at": "2023-10-18T10:25:52Z",
        "body": "As discussed in https://github.com/redis/redis/pull/12611#discussion_r1337097889\r\nAdd a build CI for macox 11 and 13 to avoid compatibility breakage introduced by future macos sdk versions.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 243,
        "deletions": 56,
        "changed_files": 14,
        "created_at": "2023-10-17T10:50:20Z",
        "closed_at": "2023-10-18T07:44:10Z",
        "merged_at": "2023-10-18T07:44:10Z",
        "body": "Upgrade urgency SECURITY: See security fixes below.\r\n\r\nSecurity fixes\r\n==============\r\n\r\n* (CVE-2023-45145) The wrong order of listen(2) and chmod(2) calls creates a\r\n  race condition that can be used by another process to bypass desired Unix\r\n  socket permissions on startup.\r\n\r\n\r\nPlatform / toolchain support related changes\r\n=================================================\r\n\r\n* Fix compilation error on MacOS 13 (#12611)\r\n\r\nBug fixes\r\n=========\r\n\r\n* WAITAOF could timeout in the absence of write traffic in case a new AOF is\r\n  created and an AOF rewrite can't immediately start (#12620)\r\n\r\nRedis cluster\r\n=============\r\n\r\n* Fix crash when running rebalance command in a mixed cluster of 7.0 and 7.2\r\n  nodes (#12604)\r\n* Fix the return type of the slot number in cluster shards to integer, which\r\n  makes it consistent with past behavior (#12561)\r\n* Fix CLUSTER commands are called from modules or scripts to return TLS info\r\n  appropriately (#12569)\r\n\r\nChanges in CLI tools\r\n====================\r\n\r\n* redis-cli, fix crash on reconnect when in SUBSCRIBE mode (#12571)\r\n\r\nModule API changes\r\n==================\r\n\r\n* Fix overflow calculation for next timer event (#12474)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 144,
        "deletions": 29,
        "changed_files": 8,
        "created_at": "2023-10-17T10:47:16Z",
        "closed_at": "2023-10-18T07:43:45Z",
        "merged_at": "2023-10-18T07:43:45Z",
        "body": "Upgrade urgency SECURITY: See security fixes below.\r\n\r\nSecurity fixes\r\n==============\r\n\r\n* (CVE-2023-45145) The wrong order of listen(2) and chmod(2) calls creates a\r\n  race condition that can be used by another process to bypass desired Unix\r\n  socket permissions on startup.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 34,
        "deletions": 13,
        "changed_files": 6,
        "created_at": "2023-10-17T10:44:47Z",
        "closed_at": "2023-10-18T07:43:10Z",
        "merged_at": "2023-10-18T07:43:10Z",
        "body": "Upgrade urgency SECURITY: See security fixes below.\r\n\r\nSecurity fixes\r\n==============\r\n\r\n* (CVE-2023-45145) The wrong order of listen(2) and chmod(2) calls creates a\r\n  race condition that can be used by another process to bypass desired Unix\r\n  socket permissions on startup.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 17,
        "changed_files": 3,
        "created_at": "2023-10-16T22:18:23Z",
        "closed_at": "2023-10-19T10:58:32Z",
        "merged_at": "2023-10-19T10:58:32Z",
        "body": "Dictionary iterator logic in the `tryResizeHashTables` method is picking the next (incorrect) dictionary while the cursor is at a given slot. This could lead to some dictionary/slot getting skipped from resizing.\r\n\r\nSaw failure in recent run: https://github.com/redis/redis/commit/d27c7413a95a0a271b94376f3ec64dae06326b29\r\n\r\n> Instead of affirming for resize message, rather validate for the final dictionary size.\r\n\r\nOn freebsd:\r\nhttps://github.com/redis/redis/actions/runs/6540680776/job/17761013478\r\nhttps://pipelinesghubeus22.actions.githubusercontent.com/2oDd4EuUudJqGKlOAB2KXZpKHTtseqbUa63unZUQGqUSgXNthI/_apis/pipelines/1/runs/40953/signedlogcontent/14?urlExpires=2023-10-17T02%3A07%3A01.6196538Z&urlSigningMethod=HMACV1&urlSignature=yOYvJogzR%2FbpT02XI2vr2Zc0DGBprgft3zKjtIIpfAk%3D\r\n\r\n```\r\n2023-10-17T00:11:35.6970430Z [err]: expire scan should skip dictionaries with lot's of empty buckets in tests/unit/expire.tcl\r\n2023-10-17T00:11:35.7072700Z Expected '102' to be equal to '2' (context: type eval line 17 cmd {assert_equal 102 [r dbsize]} proc ::test)\r\n```\r\n> Removed the no. of keys/dbsize assertion\r\n\r\nproblem introduced recently in #11695",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 61,
        "deletions": 76,
        "changed_files": 3,
        "created_at": "2023-10-15T15:10:31Z",
        "closed_at": "2023-10-16T14:21:50Z",
        "merged_at": "2023-10-16T14:21:50Z",
        "body": "Using heap allocation during signal handlers is unsafe.\r\nThis PR purpose is to replace all the heap allocations done within the signal handlers raised upon server crash and assertions.\r\nThese were added in #12453.\r\n\r\nwriteStacktraces(): allocates the stacktraces output array on the calling thread's stack and assigns the address to a global variable.\r\nIt calls `ThreadsManager_runOnThreads()` that invokes `collect_stacktrace_data()` by each thread: each thread writes to a different location in the above array to allow sync writes.\r\n\r\nget_ready_to_signal_threads_tids(): instead of allocating the `tids` array, it receives it as a fixed size array parameter, allocated on on the stack of the calling function, and returns the number of valid threads. The array size is hard-coded to 50.\r\n\r\n`ThreadsManager_runOnThreads():` To avoid the outputs array allocation, the **callback signature** was changed. Now it should return void. This function return type has also changed to int - returns 1 if successful, and 0 otherwise.\r\n\r\nOther unsafe calls will be handled in following PRs",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2023-10-11T08:00:45Z",
        "closed_at": "2023-10-12T12:34:08Z",
        "merged_at": "2023-10-12T12:34:08Z",
        "body": "The function was renamed, but the comments were outdated.\r\nFix for https://github.com/redis/redis/issues/12644",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-09T15:16:50Z",
        "closed_at": "2023-10-10T02:09:24Z",
        "merged_at": null,
        "body": "Bumps [codespell](https://github.com/codespell-project/codespell) from 2.2.5 to 2.2.6.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/codespell-project/codespell/releases\">codespell's releases</a>.</em></p>\n<blockquote>\n<h2>v2.2.6</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Test to check our string escape word triggers on its own by <a href=\"https://github.com/peternewman\"><code>@\u200bpeternewman</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2879\">codespell-project/codespell#2879</a></li>\n<li>feat(dictionary): add additiional additional entry by <a href=\"https://github.com/IndexSeek\"><code>@\u200bIndexSeek</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2882\">codespell-project/codespell#2882</a></li>\n<li>Add <code>currebtly</code> Misspelling by <a href=\"https://github.com/korverdev\"><code>@\u200bkorverdev</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2881\">codespell-project/codespell#2881</a></li>\n<li>Add more typos for &quot;approximate&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2884\">codespell-project/codespell#2884</a></li>\n<li>Add new typos for &quot;load&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2885\">codespell-project/codespell#2885</a></li>\n<li>Add typos for &quot;accommodate&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2886\">codespell-project/codespell#2886</a></li>\n<li>Add spelling correction for 'prodive' by <a href=\"https://github.com/adrien-berchet\"><code>@\u200badrien-berchet</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2889\">codespell-project/codespell#2889</a></li>\n<li>Add <code>annhilate</code> Mispelling by <a href=\"https://github.com/korverdev\"><code>@\u200bkorverdev</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2891\">codespell-project/codespell#2891</a></li>\n<li>Add alternative spelling corrections for &quot;merget&quot; by <a href=\"https://github.com/cfi-gb\"><code>@\u200bcfi-gb</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2890\">codespell-project/codespell#2890</a></li>\n<li>Add various typos by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2887\">codespell-project/codespell#2887</a></li>\n<li>Add suffixes to existing typos by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2895\">codespell-project/codespell#2895</a></li>\n<li>Add <code>twine</code> As Dev Dependency by <a href=\"https://github.com/korverdev\"><code>@\u200bkorverdev</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2901\">codespell-project/codespell#2901</a></li>\n<li>frustrum-&gt;frustum by <a href=\"https://github.com/tkoyama010\"><code>@\u200btkoyama010</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2899\">codespell-project/codespell#2899</a></li>\n<li>Add several spelling corrections by <a href=\"https://github.com/luzpaz\"><code>@\u200bluzpaz</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2900\">codespell-project/codespell#2900</a></li>\n<li>Add suffixes to existing typos, part 2 by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2902\">codespell-project/codespell#2902</a></li>\n<li>Add iamges-&gt;images and cannel variants by <a href=\"https://github.com/peternewman\"><code>@\u200bpeternewman</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2893\">codespell-project/codespell#2893</a></li>\n<li>Add all corrections in <a href=\"https://redirect.github.com/codespell-project/codespell/issues/2857\">#2857</a> by <a href=\"https://github.com/janosh\"><code>@\u200bjanosh</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2898\">codespell-project/codespell#2898</a></li>\n<li>Add suffixes to typos that start with &quot;a&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2905\">codespell-project/codespell#2905</a></li>\n<li>Add suffixes to typos from &quot;b&quot; to &quot;ch&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2906\">codespell-project/codespell#2906</a></li>\n<li>Add suffixes to typos from &quot;cop&quot; to &quot;cy&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2911\">codespell-project/codespell#2911</a></li>\n<li>Add spelling corrections for upperace and lowerace. by <a href=\"https://github.com/cfi-gb\"><code>@\u200bcfi-gb</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2909\">codespell-project/codespell#2909</a></li>\n<li>Add suffixes to typos from &quot;ci&quot; to &quot;con&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2907\">codespell-project/codespell#2907</a></li>\n<li>Add variations of 'check' by <a href=\"https://github.com/luzpaz\"><code>@\u200bluzpaz</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2904\">codespell-project/codespell#2904</a></li>\n<li>Add suffixes to typos that start with &quot;de&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2913\">codespell-project/codespell#2913</a></li>\n<li>Add suffixes to typos from &quot;di&quot; to &quot;dy&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2914\">codespell-project/codespell#2914</a></li>\n<li>Add suffixes to typos that start with &quot;e&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2915\">codespell-project/codespell#2915</a></li>\n<li>Add some spelling corrections by <a href=\"https://github.com/fxlb\"><code>@\u200bfxlb</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2918\">codespell-project/codespell#2918</a></li>\n<li>Add suffixes to typos that start with &quot;f&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2916\">codespell-project/codespell#2916</a></li>\n<li>Add typos for &quot;general&quot;, &quot;generate&quot;, &quot;generic&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2919\">codespell-project/codespell#2919</a></li>\n<li>Move <code>dateset</code> to code by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2917\">codespell-project/codespell#2917</a></li>\n<li>Add correction for &quot;distict&quot; by <a href=\"https://github.com/jdufresne\"><code>@\u200bjdufresne</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2910\">codespell-project/codespell#2910</a></li>\n<li>Add several spelling corrections by <a href=\"https://github.com/luzpaz\"><code>@\u200bluzpaz</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2908\">codespell-project/codespell#2908</a></li>\n<li>Add new suggestions for existing typos by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2921\">codespell-project/codespell#2921</a></li>\n<li>Add suffixes to typos that start with &quot;g&quot; or &quot;h&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2922\">codespell-project/codespell#2922</a></li>\n<li>Dev Container by <a href=\"https://github.com/korverdev\"><code>@\u200bkorverdev</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2903\">codespell-project/codespell#2903</a></li>\n<li>Add postifx-&gt;postfix to code dictionary by <a href=\"https://github.com/luzpaz\"><code>@\u200bluzpaz</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2927\">codespell-project/codespell#2927</a></li>\n<li>Add openes-&gt;opens, openness, to dictionary by <a href=\"https://github.com/luzpaz\"><code>@\u200bluzpaz</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2930\">codespell-project/codespell#2930</a></li>\n<li>Add suffixes to typos from &quot;ib&quot; to &quot;im&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2924\">codespell-project/codespell#2924</a></li>\n<li>Add indepentend-&gt;independent by <a href=\"https://github.com/alxgu\"><code>@\u200balxgu</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2892\">codespell-project/codespell#2892</a></li>\n<li>Add several spelling corrections by <a href=\"https://github.com/luzpaz\"><code>@\u200bluzpaz</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2926\">codespell-project/codespell#2926</a></li>\n<li>feat: Add collogue-&gt;colleague to dictionary by <a href=\"https://github.com/matthewfeickert\"><code>@\u200bmatthewfeickert</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2923\">codespell-project/codespell#2923</a></li>\n<li>Typos from OpenSSL 3.0 by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2883\">codespell-project/codespell#2883</a></li>\n<li>Add suffixes to typos from &quot;ip&quot; to &quot;k&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2931\">codespell-project/codespell#2931</a></li>\n<li>Add suffixes to typos that start with &quot;in&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2925\">codespell-project/codespell#2925</a></li>\n<li>Add typos discovered in django/deps repository by <a href=\"https://github.com/jdufresne\"><code>@\u200bjdufresne</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2932\">codespell-project/codespell#2932</a></li>\n<li>Add some archaic terms for Muslim to dictionary by <a href=\"https://github.com/skangas\"><code>@\u200bskangas</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2610\">codespell-project/codespell#2610</a></li>\n<li>Add words from misspell's dictionary (A-1) by <a href=\"https://github.com/skangas\"><code>@\u200bskangas</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2748\">codespell-project/codespell#2748</a></li>\n<li>Add suffixes to typos that start with &quot;l&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2936\">codespell-project/codespell#2936</a></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/6e41aba91fb32e9feb741a6258eefeb9c6e4a482\"><code>6e41aba</code></a> Use f-strings wherever possible (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/3102\">#3102</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/9898841e684e470c48784cfa7c04c8c9143caecf\"><code>9898841</code></a> More typos (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/3101\">#3101</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/50a6dba6ade8ca545f928adf38daea3beddbe231\"><code>50a6dba</code></a> fixing setuptool_scm dependency because the latest version is broken (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/3100\">#3100</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/41de2a39b85e2f69fa6da48243f5a17d708be5b9\"><code>41de2a3</code></a> many new typos from different repositories (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/3021\">#3021</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/b04780ee8b63ac2730c708bd801ef2b2a85b9552\"><code>b04780e</code></a> Add suffixes to typos from &quot;v&quot; to &quot;z&quot; (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/3096\">#3096</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/83970f1d49c3f0b60b5537b794d46415680dcba0\"><code>83970f1</code></a> Add suffixes to typos that start with &quot;u&quot; (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/3095\">#3095</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/7a3e8c4d249e605b150129d97c54fc8d95cc30dd\"><code>7a3e8c4</code></a> Add suffixes to typos from &quot;ti&quot; to &quot;ty&quot; (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/3094\">#3094</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/2c6b2958f8ca4fd8172023bfa206226253201cf8\"><code>2c6b295</code></a> [pre-commit.ci] pre-commit autoupdate</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/c4207d61d5ccef0358a6eb928e606ea5832243e7\"><code>c4207d6</code></a> Add suffixes to typos from &quot;ta&quot; to &quot;th&quot; (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/3092\">#3092</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/98080c1077d4d2f8a80578bfc6e9433a2ed45b24\"><code>98080c1</code></a> Add suffixes to typos from &quot;su&quot; to &quot;sy&quot; (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/3090\">#3090</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/codespell-project/codespell/compare/v2.2.5...v2.2.6\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=codespell&package-manager=pip&previous-version=2.2.5&new-version=2.2.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-08T15:23:34Z",
        "closed_at": "2023-10-13T13:28:53Z",
        "merged_at": "2023-10-13T13:28:53Z",
        "body": "when a server in the test suite crashes and is restarted by redstart_server, we didn't clean it's pid from the list.\r\nwe can see that when the corrupt-dump-fuzzer hangs, it has a long list of servers to lean, but in fact they're all already dead.\r\nhttps://github.com/redis/redis/actions/runs/6444207925/job/17496920213#step:7:4860",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-08T09:20:12Z",
        "closed_at": "2023-10-08T13:19:32Z",
        "merged_at": "2023-10-08T13:19:32Z",
        "body": "recently there are some incidents of hanged tests in the CI\r\nwhen we try to reproduce them, we get an assertion, not a hang.\r\nmaybe the server logs will reveal some info.\r\n\r\nexample of a recent report:\r\nhttps://github.com/redis/redis/actions/runs/6444207925/job/17496920213#step:7:4670",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 108,
        "deletions": 18,
        "changed_files": 3,
        "created_at": "2023-10-05T08:40:20Z",
        "closed_at": "2023-10-10T08:10:40Z",
        "merged_at": "2023-10-10T08:10:40Z",
        "body": "The current commands.json doesn't mention the special NO ONE arguments.\r\nThis change is also applied to SLAVEOF, this closes #12632.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-04T10:03:24Z",
        "closed_at": "2023-10-05T10:50:17Z",
        "merged_at": "2023-10-05T10:50:17Z",
        "body": "Recently we added a way for the module to declare that it wishes to receive nested KSN, by setting ALLOW_NESTED_KEYSPACE_NOTIFICATIONS. but it looks like this flow has a bug, clearing the `active` member when it was previously set. however, since nesting is permitted, this bug has no implications, since regardless of the active member, the notification is permitted.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 111,
        "deletions": 148,
        "changed_files": 5,
        "created_at": "2023-09-30T20:54:31Z",
        "closed_at": "2023-10-02T05:20:54Z",
        "merged_at": "2023-10-02T05:20:54Z",
        "body": "In some tests, the code manually searches for a log message, and it uses tail -1 with a delay of 1 second, which can miss the expected line.\r\n\r\nAlso, because the aof tests use start_server_aof and not start_server, the test name doesn't log into the server log.\r\n\r\nTo fix the above, I made the following changes:\r\n- Change the start_server_aof to wrap the start_server. This will add the created aof server to the servers list, and make srv() and wait_for_log_messages() available for the tests.\r\n\r\n- Introduce a new option for start_server.\r\n   'wait_ready' - an option to let the caller start the test code without waiting for the server to be ready.\r\n   useful for tests on a server that is expected to exit on startup.\r\n\r\n- Create a new start_server_aof_ex.\r\n   The new proc also accept options as argument and make use of the new 'short_life' option for tests that are expected to exit on startup because of some error in the aof file(s).\r\n\r\nBecause of the above, I had to change many lines and replace every local srv variable (a server config) usage with the srv().",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-30T08:06:42Z",
        "closed_at": "2023-10-03T07:57:32Z",
        "merged_at": null,
        "body": "This pull request includes the following updates:\r\n\r\n- adding two rules into `src/Makefile`  `build-with-deps` and `clean-build-with-deps` so that will be easy for them who wants to build everything in one go.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 110,
        "deletions": 35,
        "changed_files": 3,
        "created_at": "2023-09-28T14:31:47Z",
        "closed_at": "2023-10-02T17:02:03Z",
        "merged_at": "2023-10-02T17:02:02Z",
        "body": "## Crash fix\r\n### Current behavior\r\nWe might crash if we fail to collect some of the threads' output. If it exceeds timeout for example.\r\n\r\nThe threads mngr API guarantees that the output array length will be `tids_len`, however, some indices can be NULL, in case it fails to collect some of the threads' outputs.\r\n\r\nWhen we use the threads mngr to collect the threads' stacktraces, we rely on this and skip NULL entries. Since the output array was allocated with malloc, instead of NULL, it contained garbage, so we got a segmentation fault when trying to read this garbage. (in debug.c:writeStacktraces() )\r\n\r\n### fix\r\nAllocate the global output array with zcalloc.\r\n\r\n### To reproduce the bug, you'll have to change the code:\r\n**in threadsmngr:ThreadsManager_runOnThreads():**\r\nmake sure the g_output_array allocation is initialized with garbage and not 0s \r\n(add `memset(g_output_array, 2, sizeof(void*) * tids_len);` below the allocation).\r\n\r\nForce one of the threads to write to the array:\r\nadd a global var: `static redisAtomic size_t return_now = 0;` \r\nadd to `invoke_callback()` before writing to the output array:\r\n```\r\n    size_t i_return;\r\n    atomicGetIncr(return_now, i_return, 1);\r\n    if(i_return == 1) return;\r\n```\r\ncompile, start the server with `--enable-debug-command local` and run `redis-cli debug assert` The assertion triggers the the stacktrace collection. \r\nExpect to get 2 prints of the stack trace - since we get the segmentation fault after we return from the threads mngr, it can be safely triggered again.\r\n\r\n## Added global variables r/w lock in ThreadsManager\r\nTo avoid a situation where the main thread runs `ThreadsManager_cleanups` while threads are still invoking the signal handler, we use a r/w lock.\r\nFor cleanups, we will acquire the write lock.\r\nThe threads will acquire the read lock to enable them to write simultaneously.\r\nIf we fail to acquire the read lock, it means cleanups are in progress and we return immediately. After acquiring the lock we can safely check that the global output array wasn't nullified and proceed to write to it.\r\nThis way we ensure the threads are not modifying the global variables/ trying to write to the output array after they were zeroed/nullified/destroyed(the semaphore).\r\n\r\n## other minor logging change\r\n1. removed logging if the semaphore times out because the threads can still write to the output array after this check. Instead, we print the total number of printed stacktraces compared to the exacted number (len_tids).\r\n2. use noinline attribute to make sure the uplevel number of ignored stack trace entries stays correct.\r\n3. improve testing",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-09-28T12:28:43Z",
        "closed_at": "2023-09-28T14:19:21Z",
        "merged_at": "2023-09-28T14:19:21Z",
        "body": "The problem is that WAITAOF could have hang in case commands were propagated only to replicas.\r\nThis can happen if a module uses RM_Call with the REDISMODULE_ARGV_NO_AOF flag.\r\nIn that case, master_repl_offset would increase, but there would be nothing to fsync, so in the absence of other traffic, fsynced_reploff_pending would stay the static, and WAITAOF can hang.\r\n\r\nThis commit updates fsynced_reploff_pending to the latest offset in flushAppendOnlyFile in case there's nothing to fsync. i.e. in case it's behind because of the above mentions case it'll be refreshed and release the WAITAOF.\r\n\r\nOther changes:\r\nFix a race in wait.tcl (client getting blocked vs. the fsync thread)\r\n\r\nRelease notes:\r\n```\r\nWAITAOF could timeout or hang if used after a module command that propagated effects only to replicas and not to AOF.\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 47,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2023-09-27T14:06:21Z",
        "closed_at": "2023-09-28T14:05:54Z",
        "merged_at": "2023-09-28T14:05:53Z",
        "body": "If we set `fsynced_reploff_pending` in `startAppendOnly`, and the fork doesn't start immediately (e.g. there's another fork active at the time), any subsequent commands will increment `server.master_repl_offset`, but will not cause a fsync (given they were executed before the fork started, they just ended up in the RDB part of it)\r\nTherefore, any WAITAOF will wait on the new master_repl_offset, but it will time out because no fsync will be executed.\r\n\r\nRelease notes:\r\n```\r\nWAITAOF could timeout in the absence of write traffic in case a new AOF is created and an AOFRW can't immediately start.\r\nThis can happen by the appendonly config is changed at runtime, but also after FLUSHALL, and replica full sync.\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-09-26T09:47:20Z",
        "closed_at": "2023-09-27T06:08:19Z",
        "merged_at": "2023-09-27T06:08:19Z",
        "body": "Recently we found some signal crashes, but unable to reproduce them.\r\nIt is a good idea to dump the server logs when a failure happends.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-09-26T07:20:12Z",
        "closed_at": "2023-10-08T08:12:50Z",
        "merged_at": "2023-10-08T08:12:50Z",
        "body": "Another solution for PR #12601 \r\n\r\nUse the __MAC_OS_X_VERSION_MIN_REQUIRED macro to detect the macOS system version instead of using MAC_OS_X_VERSION_10_6.\r\n\r\nFrom MacOSX14.0.sdk, the default definitions of MAC_OS_X_VERSION_xxx have been removed in usr/include/AvailabilityMacros.h. It includes AvailabilityVersions.h, where the following condition must be met:\r\n`#if (!defined(_POSIX_C_SOURCE) && !defined(_XOPEN_SOURCE)) || defined(_DARWIN_C_SOURCE)`\r\nOnly then will MAC_OS_X_VERSION_xxx be defined.\r\nHowever, in the project, _DARWIN_C_SOURCE is not defined, which leads to the loss of the definition for MAC_OS_X_VERSION_10_6.\r\n\r\n\r\n<img width=\"1160\" alt=\"image\" src=\"https://github.com/redis/redis/assets/17684070/259539ab-f769-4ece-9a28-f26148391c56\">\r\n\r\n(left is 13.3, right is 14.0)\r\n\r\n\r\nMacOSX13.3.sdk:\r\n\r\nhttps://github.com/alexey-lysiuk/macos-sdk/blob/6c1513f5b0667b76e24aaadcad130e90c545f046/MacOSX13.3.sdk/usr/include/AvailabilityMacros.h#L83-L98C43\r\n\r\nMacOSX14.0.sdk:\r\n\r\nhttps://github.com/alexey-lysiuk/macos-sdk/blob/6c1513f5b0667b76e24aaadcad130e90c545f046/MacOSX14.0.sdk/usr/include/AvailabilityMacros.h#L85-L89\r\n\r\n\r\n",
        "comments": 25
    },
    {
        "merged": false,
        "additions": 514,
        "deletions": 36,
        "changed_files": 16,
        "created_at": "2023-09-26T03:49:32Z",
        "closed_at": "2023-09-26T07:07:16Z",
        "merged_at": null,
        "body": "The k8s cluster cannot work properly if the redis pod restarts and the ip address changes.\r\n\r\nRedis 7.2, in the k8s cluster, if the pod restart ip changes, the redis cluster will not work properly. The log shows that the \"port cport \"information passed by gossip is correct, but the\" ip:port@cport \"in redis-node.conf is\" 0@0 \". Using redi-cli --cluster check Cluster nodes are also abnormal. Redis didn't fix itself after a while.\r\n\r\nSO redis should send corret ip port when build send msg.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 251153,
        "deletions": 49485,
        "changed_files": 1382,
        "created_at": "2023-09-24T18:38:04Z",
        "closed_at": "2023-09-26T05:08:59Z",
        "merged_at": null,
        "body": "for redis 5.0, when ues redis-cluster in k8s cluster, users should set host in cluster-announce-ip (cluster-announce-ip 'hostname').\r\nOtherwise, when the pod restarts and the ip changes, the redis cluster will not work properly. When redis sends messages through gossip, the ip address cannot be pinged through.\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-09-23T08:36:22Z",
        "closed_at": "2023-10-12T05:15:26Z",
        "merged_at": "2023-10-12T05:15:26Z",
        "body": "In #10536, we introduced the assert, some older versions of servers\r\n(like 7.0) doesn't gossip shard_id, so we will not add the node to\r\ncluster->shards, and node->shard_id is filled in randomly and may not\r\nbe found here.\r\n\r\nIt causes that if we add a 7.2 node to a 7.0 cluster and allocate slots\r\nto the 7.2 node, the 7.2 node will crash when it hits this assert. Somehow\r\nlike #12538.\r\n\r\nIn this PR, we just remove the assert and the search, and just using\r\nremoveChannelsInSlot since it will do an return if there are no active\r\nsubscription for a given slot.\r\n\r\nFixes #12603.\r\n\r\n```\r\nRelease notes\r\nFix crash when running rebalance command in a mixed cluster of 7.0 and 7.2\r\n```",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-22T11:34:35Z",
        "closed_at": "2023-09-26T10:54:51Z",
        "merged_at": null,
        "body": "Fix #12585\r\nmacOS version definitions were removed from `AvailabilityMacros.h` since macOS SDK 13.5.2,\r\nwhich resulted in the compiler not knowing the macOS version and using `fstat64` instead of `fstat`.\r\nAdd `_DARWIN_C_SOURCE` define to make the version be recognized.\r\n\r\nCI with commit(fail) https://github.com/redis/redis/commit/aa40f1b01b749e9adc65ca10616733475e1ec5c5\r\nhttps://github.com/sundb/redis/actions/runs/6271451836\r\n\r\nCI with commit(success) https://github.com/redis/redis/commit/90f50d5ce97df34c68787609549af17fb395803f\r\nhttps://github.com/sundb/redis/actions/runs/6273719849",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 159,
        "deletions": 33,
        "changed_files": 3,
        "created_at": "2023-09-22T00:13:28Z",
        "closed_at": "2023-09-27T20:32:27Z",
        "merged_at": null,
        "body": "Draft PR for an alternative mechanism to contrast with https://github.com/redis/redis/pull/12257. \r\n\r\nThere is quite a bit of black preprocessor magic going on, but the general idea is as follow:\r\n1. This is a macro which will call two sub-macros on the provided arguments: compact_values and compact_fmt.\r\n1.1 Compact fmt will put all the string literals next to each other, and the compiler will concatenate them together.\r\n1.2 Compact values will put all of the arguments sequentially together.\r\n2. These functions will construct a single \"format string\" and then list out all of the arguments, still in order.\r\n\r\nBoth of these functions fundamentally work the same way. There is a macro that determines the number of arguments, which then calls a specific function for that number of arguments. Then it get's recursively called with 2 fewer arguments, slowly peeling away all of the arguments while concatenating the format together.\r\n\r\nThere is currently a hardcoded limit of 64 arguments, if that value is exceeded we need to expand the list, but I wrote a tool to do that so it should be easy to maintain. I haven't found any clear limitations of this yet. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 14,
        "changed_files": 4,
        "created_at": "2023-09-21T03:31:34Z",
        "closed_at": "2023-10-12T07:54:50Z",
        "merged_at": "2023-10-12T07:54:50Z",
        "body": "In #11568 we removed the NOSCRIPT flag from commands and keep the BLOCKING flag.\r\nAiming to allow them in scripts and let them implicitly behave in the non-blocking way.\r\n\r\nIn that sense, the old behavior was to allow LPOP and reject BLPOP, and the new behavior, is to allow BLPOP too, and fail it only in case it ends up blocking.\r\nSo likewise, so far we allowed XREAD and rejected XREAD BLOCK, and we will now allow that too, and only reject it if it ends up blocking.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-14T18:06:47Z",
        "closed_at": "2023-09-24T17:25:00Z",
        "merged_at": null,
        "body": "#11914  in k8s cluster,  use domain instand of ip. Fix  most of pod ip change  redis cluster do not work correctly.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 58,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2023-09-14T02:07:38Z",
        "closed_at": "2023-10-13T03:48:27Z",
        "merged_at": "2023-10-13T03:48:27Z",
        "body": "Fixes #12558\r\n\r\n```\r\nRelease notes\r\nFix a bug where a replica would continue to serve slots it doesn't have data for after switching masters which owns different slots. \r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-12T08:48:44Z",
        "closed_at": "2023-10-11T07:45:39Z",
        "merged_at": "2023-10-11T07:45:38Z",
        "body": "When entering pubsub mode and using the redis-cli only\r\nconnect command, we need to reset pubsub_mode because\r\nwe switch to a different connection.\r\n\r\nThis will affect the prompt when the connection is successful,\r\nand redis-cli will crash when the connect fails:\r\n```\r\n127.0.0.1:6379> subscribe ch\r\n1) \"subscribe\"\r\n2) \"ch\"\r\n3) (integer) 1\r\n127.0.0.1:6379(subscribed mode)> connect 127.0.0.1 6380\r\n127.0.0.1:6380(subscribed mode)> ping\r\nPONG\r\n127.0.0.1:6380(subscribed mode)> connect a b\r\nCould not connect to Redis at a:0: Name or service not known\r\nSegmentation fault\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-09-12T02:35:41Z",
        "closed_at": "2023-09-21T15:41:32Z",
        "merged_at": "2023-09-21T15:41:32Z",
        "body": "As discussed in https://github.com/redis/redis/pull/12233#discussion_r1318317324, starting a change in #12233 (released in 7.2), CLUSTER commands use client's connection to decide whether to return TLS port or non-TLS port, but commands called by Lua script and module's RM_Call don't have a real client with connection, and would currently be regarded as non-TLS connections.\r\n\r\nWe can use server.current_client instead when it is available. When it is not (module calls commands without a real client), we may see this as an undefined behavior, and return null or default port (currently in this PR it returns default port, judged by server.tls_cluster).\r\n\r\n@MeirShpilraien @madolson @oranagra @zuiderkwast",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 217,
        "deletions": 115,
        "changed_files": 2,
        "created_at": "2023-09-11T10:21:12Z",
        "closed_at": "2023-10-02T04:19:17Z",
        "merged_at": null,
        "body": "- As is disscussed in #12548: Certen call to function quicklistReplaceEntry(), quicklistInsertBefore() and quicklistInsertAfter() will cause a packed node violate size limit.\r\n- As is disscussed in #12563: A node will not be compressed if it is not compress small enough. So node's member recompress will stay 0 after calling function quicklistDecompressNodeForUse(). If that node's entry is changed later, call function quicklistRecompressOnly() will not make that node compressed. In this situation, we should call function quicklistCompress() instead, I will take this approach in this commit, obviously it's not efficient. We should redesign 'recompress' to fundamentally solve the problem.\r\n- struct quicklistNode's member dont_compress is removed.",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-09T05:51:27Z",
        "closed_at": "2023-09-26T21:03:28Z",
        "merged_at": "2023-09-26T21:03:28Z",
        "body": "Clear owner_not_claiming_slot bit for the slot in clusterDelSlot to keep it consistent with slot ownership information.\r\n\r\nThis was pointed out by @nandihalli in https://github.com/redis/redis/pull/12344\r\n\r\nWriting unit tests for this change is not straightforward with the current test framework as it handles an edge case. The change is straightforward to reason about, so I didn't add a test.\r\n\r\n```\r\nRelease notes\r\nFix an issue where slot ownership was not being properly handled when deleting a slot from a node.\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-08T06:45:32Z",
        "closed_at": "2023-09-10T06:33:01Z",
        "merged_at": "2023-09-10T06:33:01Z",
        "body": "An unintentional change was introduced in #10536, we used\r\nto use addReplyLongLong and now it is addReplyBulkLonglong,\r\nrevert it back the previous behavior.\r\n\r\nIt's already released in 7.2, so it might be a breaking change.\r\n\r\n```\r\nRelease notes\r\nFix the return type of the slot number in cluster shards to integer, which maxes it consistent with past behavior.\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 14,
        "changed_files": 10,
        "created_at": "2023-09-07T07:31:41Z",
        "closed_at": "2023-09-08T13:10:17Z",
        "merged_at": "2023-09-08T13:10:17Z",
        "body": "Intended to replace #12329, plus additional fixes.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 177,
        "deletions": 20,
        "changed_files": 14,
        "created_at": "2023-09-06T12:37:43Z",
        "closed_at": "2023-09-06T17:55:58Z",
        "merged_at": "2023-09-06T17:55:58Z",
        "body": "Upgrade urgency SECURITY: See security fixes below.\r\n\r\nSecurity Fixes\r\n==============\r\n\r\n* (CVE-2023-41053) Redis does not correctly identify keys accessed by SORT_RO and\r\n  as a result may grant users executing this command access to keys that are not\r\n  explicitly authorized by the ACL configuration.\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Cluster: fix a race condition where a slot migration may revert on a\r\n  subsequent failover or node joining (#12344)\r\n* Ensure that the function load timeout is disabled during loading from RDB/AOF\r\n  and on replicas. (#12451)\r\n* Fix the assertion when script timeout occurs after it signaled a blocked client (#12459)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 113,
        "deletions": 26,
        "changed_files": 16,
        "created_at": "2023-09-06T12:37:16Z",
        "closed_at": "2023-09-06T17:56:15Z",
        "merged_at": "2023-09-06T17:56:15Z",
        "body": "Upgrade urgency SECURITY: See security fixes below.\r\n\r\nSecurity Fixes\r\n==============\r\n\r\n* (CVE-2023-41053) Redis does not correctly identify keys accessed by SORT_RO and,\r\n  as a result, may grant users executing this command access to keys that are not\r\n  explicitly authorized by the ACL configuration.\r\n\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Fix crashes when joining a node to an existing 7.0 Redis Cluster (#12538)\r\n* Correct request_policy and response_policy command tips on for some admin /\r\n  configuration commands (#12545, #12530)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 47,
        "deletions": 12,
        "changed_files": 7,
        "created_at": "2023-09-03T10:30:19Z",
        "closed_at": "2023-09-04T18:30:43Z",
        "merged_at": "2023-09-04T18:30:43Z",
        "body": "Updated the command [tips](https://redis.io/topics/command-tips) for ACL SAVE / SETUSER / DELUSER, CLIENT SETNAME / SETINFO, and LATENCY RESET.\r\nThe tips now match CONFIG SET, since there's a similar behavior for all of these commands - the user expects to update the various configurations & states on all nodes, not only on a single, random node.\r\nFor LATENCY RESET the response tip is now agg_sum.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2023-09-01T22:40:15Z",
        "closed_at": "2023-10-02T23:44:10Z",
        "merged_at": "2023-10-02T23:44:10Z",
        "body": "Fixed some usages of tabs which caused weird indentation in the code. Tried to find all of the places so their was one PR. I ignored all of the usages of tabs which don't really affect readability.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 14,
        "changed_files": 9,
        "created_at": "2023-09-01T17:11:23Z",
        "closed_at": "2023-10-03T01:58:44Z",
        "merged_at": "2023-10-03T01:58:44Z",
        "body": "We use the C standard assert() in various places in the codebase, which requires `NDEBUG` to be undefined. We introduced the `redisassert.h` file in order to allow low level files to access the assert that maps to serverPanic, but this was only applied tactically and is not available broadly.\r\n\r\nThis PR removes all usage of the standard library asserts and replaces them with an assert that maps to serverPanic. It makes us immune to accidentally setting the NDEBUG flag preventing assertions. I also marked marked the server asserts as \"likely\" to not execute. I spot checked various points in the code, and it didn't change the code layout on my x86 mac, but it is more consistent with `redisassert.h` and seems more correct overall.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-09-01T11:56:51Z",
        "closed_at": "2023-09-03T03:14:48Z",
        "merged_at": "2023-09-03T03:14:48Z",
        "body": "Hello. While testing upgrade on sharded cluster from 7.0 to 7.2 we got the segmentation fault in updateShardId.\r\nIt seems that 7.0 nodes have no shard id in ping extensions.\r\nBacktrace with gdb:\r\n```\r\ngdb /tmp/redis-server-asan /var/cores/redis-server-asan-26343-S11.core\r\nReading symbols from /tmp/redis-server-asan...\r\n[New LWP 26343]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nCore was generated by `/tmp/redis-server-asan *:6379 [cluster]     '.\r\nProgram terminated with signal SIGSEGV, Segmentation fault.\r\n#0  0x00007f139f43f75b in kill () from /lib/x86_64-linux-gnu/libc.so.6\r\n(gdb) bt\r\n#0  0x00007f139f43f75b in kill () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x0000555c46cd6b4f in bugReportEnd (killViaSignal=1, sig=11) at /home/secwall/redis/src/debug.c:2224\r\n#2  <signal handler called>\r\n#3  0x0000555c46ce10eb in updateShardId (node=0x555c47df2b20, shard_id=0x0) at /home/secwall/redis/src/cluster.c:941\r\n#4  0x0000555c46cec076 in clusterProcessPacket (link=link@entry=0x555c47df0990) at /home/secwall/redis/src/cluster.c:3099\r\n#5  0x0000555c46ceca96 in clusterReadHandler (conn=0x555c47df09f0) at /home/secwall/redis/src/cluster.c:3389\r\n#6  0x0000555c46d77e6c in callHandler (handler=<optimized out>, conn=0x555c47df09f0) at /home/secwall/redis/src/connhelpers.h:79\r\n#7  connSocketEventHandler (el=<optimized out>, fd=<optimized out>, clientData=0x555c47df09f0, mask=<optimized out>) at /home/secwall/redis/src/socket.c:298\r\n#8  0x0000555c46c41469 in aeProcessEvents (flags=27, eventLoop=0x555c47d46670) at /home/secwall/redis/src/ae.c:436\r\n#9  aeMain (eventLoop=0x555c47d46670) at /home/secwall/redis/src/ae.c:496\r\n#10 0x0000555c46c35b9f in main (argc=2, argv=<optimized out>) at /home/secwall/redis/src/server.c:7378\r\n```\r\n\r\nThis should fix issue #12507",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-01T09:51:59Z",
        "closed_at": "2023-09-01T11:15:12Z",
        "merged_at": "2023-09-01T11:15:12Z",
        "body": "The new test added in #12476 causes reply-schemas-validator to fail.\r\nWhen doing `catch {r get key}`, the req-res output is:\r\n```\r\n3\r\nget\r\n3\r\nkey\r\n12\r\n__argv_end__\r\n$100000\r\naaaaaaaaaaaaaaaaaaaa...4\r\ninfo\r\n5\r\nstats\r\n12\r\n__argv_end__\r\n=1670\r\ntxt:# Stats\r\n...\r\n```\r\n\r\nAnd we can see the link after `$100000`, there is a 4 in the last,\r\nit break the req-res-log-validator script since the format is wrong.\r\n\r\nThe reason i guess is after the client reconnection (after the output\r\nbuf limit), we will not add newlines, but append args directly.\r\nSince obuf-limits.tcl is doing the same thing, and it had the logreqres:skip\r\nflag, so this PR is following it.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 24,
        "changed_files": 1,
        "created_at": "2023-08-30T22:40:27Z",
        "closed_at": "2023-08-31T21:08:05Z",
        "merged_at": "2023-08-31T21:08:05Z",
        "body": "While working on #12486. Found that in `moduleConfigValidityCheck` and `isModuleConfigNameRegistered`, `sds` is not required. This also allowed to remove unnecessary memcopy from some of the config registering APIs.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2023-08-30T10:59:46Z",
        "closed_at": "2023-08-30T18:49:02Z",
        "merged_at": "2023-08-30T18:49:02Z",
        "body": "Since the three commands have similar behavior (change config, return OK), the [tips](https://redis.io/topics/command-tips) that govern how they should behave should be similar.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-08-29T08:23:11Z",
        "closed_at": "2023-08-30T19:00:03Z",
        "merged_at": "2023-08-30T19:00:03Z",
        "body": "Before\uff1a\r\n```\r\n127.0.0.1:6379> command getkeys sort_ro key\r\n(empty array)\r\n127.0.0.1:6379>\r\n```\r\nAfter:\r\n```\r\n127.0.0.1:6379> command getkeys sort_ro key\r\n1) \"key\"\r\n127.0.0.1:6379>\r\n```\r\n\r\nThis change also affects ACL, Redis may grant users executing this command access to keys that are not\r\nexplicitly authorized by the ACL configuration. (CVE-2023-41053)",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 4,
        "changed_files": 5,
        "created_at": "2023-08-25T04:02:23Z",
        "closed_at": "2023-08-25T06:36:20Z",
        "merged_at": null,
        "body": "Sometimes clients are not only identified by lib name and lib version, but also by the name of the instance, imagine you have more than 1 using the same library and we want to be able to add a name to each instance",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-08-25T03:32:28Z",
        "closed_at": "2023-08-27T08:42:55Z",
        "merged_at": "2023-08-27T08:42:55Z",
        "body": "This test failed several times:\r\n```\r\n*** [err]: LATENCY GRAPH can output the event graph in tests/unit/latency-monitor.tcl\r\nExpected '478' to be more than or equal to '500' (context: type eval\r\nline 8 cmd {assert_morethan_equal $high 500} proc ::test)\r\n```\r\n\r\nNot sure why, adding some verbose printing that'll print the command\r\nresult on the next time.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-08-21T12:22:54Z",
        "closed_at": "2023-08-22T15:15:48Z",
        "merged_at": "2023-08-22T15:15:48Z",
        "body": "warning against editing the config file and restarting the server.\r\nwhich will attempt to load an AOF file and disregard the RDB.\r\n\r\nFixes #12484",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2023-08-15T10:18:54Z",
        "closed_at": "2023-08-16T05:36:41Z",
        "merged_at": "2023-08-16T05:36:41Z",
        "body": "Add 7.2, drop 6.0 as per https://redis.io/docs/about/releases/\r\nAlso replace a few concordances of the `\u2019` char, with standard `'`",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 342,
        "deletions": 83,
        "changed_files": 44,
        "created_at": "2023-08-15T07:46:47Z",
        "closed_at": "2023-08-15T09:38:37Z",
        "merged_at": "2023-08-15T09:38:37Z",
        "body": "Upgrade urgency LOW: This is the first stable Release for Redis 7.2.\r\n\r\nBug Fixes\r\n=========\r\n\r\n* redis-cli in cluster mode handles `unknown-endpoint` (#12273)\r\n* Update request / response policy hints for a few commands (#12417)\r\n* Ensure that the function load timeout is disabled during loading from RDB/AOF and on replicas. (#12451)\r\n* Fix false success and a memory leak for ACL selector with bad parenthesis combination (#12452)\r\n* Fix the assertion when script timeout occurs after it signaled a blocked client (#12459)\r\n\r\nFixes for issues in previous releases of Redis 7.2\r\n--------------------------------------------------\r\n\r\n* Update MONITOR client's memory correctly for INFO and client-eviction (#12420)\r\n* The response of cluster nodes was unnecessarily adding an extra comma when no\r\n  hostname was present. (#12411)\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 193,
        "deletions": 14,
        "changed_files": 6,
        "created_at": "2023-08-15T01:45:21Z",
        "closed_at": "2023-08-30T20:01:25Z",
        "merged_at": "2023-08-30T20:01:25Z",
        "body": "Related to issue #12428 \r\n\r\nRecently, we added support for modules to include commands to existing ACL categories https://github.com/redis/redis/pull/11708. However, it is still not possible for modules to register new ACL categories for commands. This PR implements the ability to allow modules to create new ACL categories.\r\n\r\nThis PR adds a new Module API `int RM_AddACLCategory(RedisModuleCtx *ctx, const char *category_name)` to add a new ACL command category.\r\n\r\nHere, we initialize the `ACLCommandCategories` array by allocating space for 64 categories and duplicate the 21 default categories from the predefined array 'ACLDefaultCommandCategories' into the `ACLCommandCategories` array while ACL initialization. Valid ACL category names can only contain alphanumeric characters, underscores, and dashes.\r\n\r\nThe API when called, checks for the onload flag, category name validity, and for duplicate category name if present. If the conditions are satisfied, the API adds the new category to the trailing end of the `ACLCommandCategories` array and assigns the `acl_categories` flag bit according to the index at which the category is added.\r\n\r\nIf any error is encountered the `errno` is set accordingly by the API.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-08-12T17:35:16Z",
        "closed_at": "2023-08-24T09:30:43Z",
        "merged_at": null,
        "body": "reuse RedisModuleCronLoopV1 add activeRehashing hasActiveChildProcess ,\r\nuse RedisModule_SubscribeToServerEvent sub RedisModuleEvent_CronLoop to do resize, rehash dict, so want check `server.activerehashing`,`hasActiveChildProcess()`",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-08-11T22:36:07Z",
        "closed_at": "2023-08-16T07:38:59Z",
        "merged_at": "2023-08-16T07:38:59Z",
        "body": "Currently rdbSaveMillisecondTime, rdbSaveDoubleValue api's return type is int but they return the value directly from rdbWriteRaw function which has the return type of ssize_t. As this may cause overflow to int so changed to ssize_t. \r\n\r\n@oranagra , These rdbSaveBinaryFloatValue, rdbSaveType also has similar type but used widely, please let me know if we can change.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2023-08-11T08:19:27Z",
        "closed_at": "2023-08-15T03:57:56Z",
        "merged_at": "2023-08-15T03:57:56Z",
        "body": "We iterate over all replicas to get the result, the time complexity\r\nshould be O(N), like CLUSTER NODES complexity is O(N).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-08-11T08:10:22Z",
        "closed_at": "2023-08-30T18:51:15Z",
        "merged_at": "2023-08-30T18:51:15Z",
        "body": "Add these INFO metrics:\r\n* client_query_buffer_limit_disconnections\r\n* client_output_buffer_limit_disconnections\r\n\r\nSometimes it is useful to monitor whether clients reaches size limit of\r\nquery buffer and output buffer, to decide whether we need to adjust the\r\nbuffer size limit or reduce client query payload.\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-08-10T23:53:11Z",
        "closed_at": "2023-09-24T10:31:12Z",
        "merged_at": "2023-09-24T10:31:12Z",
        "body": "Proposing a fix for an overflow bug when calculating the next event for a timer. \r\n\r\nPlease find a Redis module which recreates the issue here: https://github.com/nirrattner/RedisModuleTimerExample\r\n\r\nThe `retval` variable is defined as an `int`, so with 4 bytes, it cannot properly represent microsecond values greater than the equivalent of about 35 minutes. \r\n\r\nThis bug shouldn't impact standard Redis behavior because I don't believe Redis has timer events that are scheduled as far as 35 minutes out, but it may affect custom Redis modules which interact with the event timers via the [`RedisModule_CreateTimer`](https://redis.io/docs/reference/modules/modules-api-ref/#redismodule_createtimer) functionality.\r\n\r\nThe impact is that [`usUntilEarliestTimer` may return 0](https://github.com/redis/redis/blob/7c179f9bf4390512196b3a2b2ad6d0f4cb625c8a/src/ae.c#L276) for as long as `retval` is scaled to an overflowing value. While `usUntilEarliestTimer` continues to return `0`, `aeApiPoll` will have a zero timeout, and so Redis will use significantly more CPU iterating through its event loop without pause. For timers scheduled far enough into the future, Redis will cycle between ~35 minute periods of high CPU usage and ~35 minute periods of standard CPU usage.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-08-10T04:00:07Z",
        "closed_at": "2023-08-10T06:58:53Z",
        "merged_at": "2023-08-10T06:58:53Z",
        "body": "When adding a new ACL rule, an attempt was made to remove any \"overlapping\" rules. However, when a match was found, the search was not resumed at the right location, but instead after the original position of the original command.\r\n\r\nFor example, if the current rules were `-config +config|get` and a rule `+config` was added. It would identify that -config was matched, but it would skip over +config|get, leaving the compacted rule `-config +config`. This would be evaluated safely, but looks weird.\r\n\r\nThis bug can only be triggered with subcommands, since that is the only way to have sequential matching rules. Resolves https://github.com/redis/redis/issues/12470. This is also only present in 7.2. I think there was also a minor risk of removing another valid rule, since it would start the search of the next command at an arbitrary point. I couldn't find a valid offset that would have cause a match using any of the existing commands that have subcommands with another command. ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-08-09T20:31:07Z",
        "closed_at": "2023-10-15T20:27:37Z",
        "merged_at": null,
        "body": "No error for info command section was mispelled, added the check at the starting of the info command api for all the available options of the command. \r\n\r\nActual results:\r\n\r\n```\r\n127.0.0.1:6379> info keyspaceee\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> info keyspacer\r\n127.0.0.1:6379> info cpuuu\r\n\r\n```\r\nAfter the changes:\r\n```\r\n\r\n127.0.0.1:6379> info cpuuu\r\nERR Invalid section 'cpuuu' to INFO [section [section ...]]\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> info cpu\r\n# CPU\r\nused_cpu_sys:0.023133\r\nused_cpu_user:0.024970\r\nused_cpu_sys_children:0.000000\r\nused_cpu_user_children:0.000000\r\nused_cpu_sys_main_thread:0.021886\r\nused_cpu_user_main_thread:0.025534\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> info keyspacer\r\nERR Invalid section 'keyspacer' to INFO [section [section ...]]\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> info keyspace\r\n# Keyspace\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> info clusters\r\nERR Invalid section 'clusters' to INFO [section [section ...]]\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> info cluster\r\n# Cluster\r\ncluster_enabled:0\r\n127.0.0.1:6379>\r\n\r\n```",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-08-07T03:00:23Z",
        "closed_at": "2023-08-21T09:50:41Z",
        "merged_at": "2023-08-21T09:50:41Z",
        "body": "Limit the range of LREM count to -LONG_MAX ~ LONG_MAX.\r\nBefore the fix, passing -LONG_MAX would cause an overflow\r\nand would effectively be the same as passing 0. (Because\r\nthis condition `toremove && removed == toremove `can never\r\nbe satisfied).\r\n\r\nThis is a minor fix as it shouldn't really affect users,\r\nmore like a cleanup.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-08-03T09:10:44Z",
        "closed_at": "2023-08-05T06:52:04Z",
        "merged_at": "2023-08-05T06:52:04Z",
        "body": "Fix the assertion when a busy script (timeout) signal ready keys (like LPUSH), and then an arbitrary client's `allow-busy` command steps into `handleClientsBlockedOnKeys` try wake up clients blocked on keys (like BLPOP).\r\n\r\n```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n34266:M 03 Aug 2023 16:59:07.807 # === ASSERTION FAILED ===\r\n34266:M 03 Aug 2023 16:59:07.807 # ==> blocked.c:326 'server.also_propagate.numops == 0' is not true\r\n\r\n------ STACK TRACE ------\r\n\r\nBacktrace:\r\n./redis-server *:6379[0x4c624e]\r\n./redis-server *:6379(processCommand+0xa8f)[0x580e5f]\r\n./redis-server *:6379(processInputBuffer+0xe1)[0x569c21]\r\n./redis-server *:6379(readQueryFromClient+0x300)[0x55d910]\r\n./redis-server *:6379[0x46a9c8]\r\n./redis-server *:6379(aeProcessEvents+0xba)[0x58f09a]\r\n./redis-server *:6379(processEventsWhileBlocked+0xa1)[0x55b171]\r\n./redis-server *:6379(scriptInterrupt+0x133)[0x4640c3]\r\n./redis-server *:6379[0x4658fb]\r\n./redis-server *:6379[0x5a0825]\r\n./redis-server *:6379[0x5aa0aa]\r\n./redis-server *:6379[0x5a0fc5]\r\n./redis-server *:6379[0x5a0358]\r\n./redis-server *:6379[0x5a114d]\r\n./redis-server *:6379(lua_pcall+0x46)[0x59eb06]\r\n./redis-server *:6379(luaCallFunction+0x1c1)[0x4655d1]\r\n./redis-server *:6379(evalGenericCommand+0x16e)[0x4e327e]\r\n./redis-server *:6379(call+0x140)[0x57fd60]\r\n./redis-server *:6379(processCommand+0xa78)[0x580e48]\r\n./redis-server *:6379(processInputBuffer+0xe1)[0x569c21]\r\n./redis-server *:6379(readQueryFromClient+0x300)[0x55d910]\r\n./redis-server *:6379[0x46a9c8]\r\n./redis-server *:6379(aeMain+0xf9)[0x585b29]\r\n./redis-server *:6379(main+0x395)[0x4527f5]\r\n/lib64/libc.so.6(__libc_start_main+0xf5)[0x7f588d2e6555]\r\n./redis-server *:6379[0x452f39]\r\n\r\n------ CLIENT LIST OUTPUT ------\r\nid=5 addr=127.0.0.1:51834 laddr=127.0.0.1:6379 fd=8 name= age=8 idle=8 flags=b db=0 sub=0 psub=0 ssub=0 multi=-1 qbuf=0 qbuf-free=0 argv-mem=7 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=1959 events=r cmd=blpop user=default redir=-1 resp=2 lib-name= lib-ver=\r\nid=6 addr=127.0.0.1:51846 laddr=127.0.0.1:6379 fd=9 name= age=5 idle=5 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 qbuf=87 qbuf-free=20387 argv-mem=64 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=37848 events= cmd=eval user=default redir=-1 resp=2 lib-name= lib-ver=\r\nid=7 addr=127.0.0.1:51852 laddr=127.0.0.1:6379 fd=10 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 qbuf=21 qbuf-free=20453 argv-mem=5 multi-mem=0 rbs=16384 rbp=16384 obl=127 oll=0 omem=0 tot-mem=37781 events=r cmd=auth user=default redir=-1 resp=2 lib-name= lib-ver=\r\n\r\n------ CURRENT CLIENT INFO ------\r\nid=7 addr=127.0.0.1:51852 laddr=127.0.0.1:6379 fd=10 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 qbuf=21 qbuf-free=20453 argv-mem=5 multi-mem=0 rbs=16384 rbp=16384 obl=127 oll=0 omem=0 tot-mem=37781 events=r cmd=auth user=default redir=-1 resp=2 lib-name= lib-ver=\r\nargc: '2'\r\nargv[0]: '\"auth\"'\r\n34266:M 03 Aug 2023 16:59:07.809 # key 'a' found in DB containing the following object:\r\n34266:M 03 Aug 2023 16:59:07.809 # Object type: 1\r\n34266:M 03 Aug 2023 16:59:07.809 # Object encoding: 11\r\n34266:M 03 Aug 2023 16:59:07.809 # Object refcount: 1\r\n\r\n------ EXECUTING CLIENT INFO ------\r\nid=6 addr=127.0.0.1:51846 laddr=127.0.0.1:6379 fd=9 name= age=5 idle=5 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 qbuf=87 qbuf-free=20387 argv-mem=64 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=37848 events= cmd=eval user=default redir=-1 resp=2 lib-name= lib-ver=\r\nargc: '3'\r\nargv[0]: '\"eval\"'\r\nargv[1]: '\"redis.call('lpush','a','b') local a=1 while(1) do a=a+1 end\"'\r\nargv[2]: '\"0\"'\r\n```\r\n\r\nReproduction process:\r\n1. start a redis with aof\r\n    `./redis-server --appendonly yes`\r\n2. exec blpop\r\n    `127.0.0.1:6379> blpop a 0`\r\n3. use another client call a busy script and this script push the blocked key\r\n    `127.0.0.1:6379> eval \"redis.call('lpush','a','b') while(1) do end\" 0`\r\n4. user a new client call an allow-busy command like auth\r\n    `127.0.0.1:6379> auth a`\r\n\r\nBTW, this issue also break the atomicity of script.\r\n\r\nThis bug has been around for many years, the old versions only have the atomic problem, only 7.0/7.2 has the assertion problem.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-08-03T07:14:35Z",
        "closed_at": "2023-08-05T07:00:55Z",
        "merged_at": "2023-08-05T07:00:55Z",
        "body": "if there are no subscribers, we can ignore the operation",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2023-08-03T06:24:14Z",
        "closed_at": "2023-08-05T04:57:06Z",
        "merged_at": "2023-08-05T04:57:06Z",
        "body": "This PR mainly fixes a possible integer overflow in `json_append_string()`.\r\nWhen we use `cjson.encoding()` to encode a string larger than 2GB, at specific compilation flags, an integer overflow may occur leading to truncation, resulting in the part of the string larger than 2GB not being encoded.\r\nOn the other hand, this overflow doesn't cause any read or write out-of-range or segment fault.\r\n\r\n1) using -O0 for lua_cjson (`make LUA_DEBUG=yes`)\r\n    In this case, `i` will overflow and leads to truncation.\r\n    When `i` reaches `INT_MAX+1` and overflows to INT_MIN, when compared to len, `i`(1000000..00) is expanded to 64 bits signed integer (1111111.....000000) .\r\n    At this point i will be greater than len and jump out of the loop, so `for (i = 0; i < len; i++)` will loop up to 2^31 times, and the part of larger than 2GB will be truncated.\r\n\r\n```asm\r\n`i` => -0x24(%rbp)\r\n<+253>:   addl   $0x1,-0x24(%rbp)       ; overflow if i large than 2^31\r\n<+257>:   mov    -0x24(%rbp),%eax\r\n<+260>:   movslq %eax,%rdx\t            ; move a 32-bit value with sign extension into a 64-bit signed\r\n<+263>:   mov    -0x20(%rbp),%rax\r\n<+267>:   cmp    %rax,%rdx              ; check `i < len`\r\n<+270>:   jb     0x212600 <json_append_string+148>\r\n```\r\n   \r\n2) using -O2/-O3 for lua_cjson (`make LUA_DEBUG=no`, **the default**)\r\n    In this case, because singed integer overflow is an undefined behavior, `i` will not overflow.\r\n   `i` will be optimized by the compiler and use 64-bit registers for all subsequent instructions.\r\n\r\n```asm\r\n<+180>:   add    $0x1,%rbx           ; Using 64-bit register `rbx` for i++\r\n<+184>:   lea    0x1(%rdx),%rsi\r\n<+188>:   mov    %rsi,0x10(%rbp)\r\n<+192>:   mov    %al,(%rcx,%rdx,1)\r\n<+195>:   cmp    %rbx,(%rsp)         ; check `i < len`\r\n<+199>:   ja     0x20b63a <json_append_string+154>\r\n```\r\n\r\n3) using 32bit\r\n    Because `strbuf_ensure_empty_length()` preallocates memory of length (len * 6 + 2), in 32-bit `cjson.encode()` can only handle strings smaller than ((2 ^ 32) - 3 ) / 6.\r\n    So 32bit is not affected.\r\n\r\nAlso change `i` in `strbuf_append_string()` to `size_t`.\r\nSince its second argument `str` is taken from the `char2escape` string array which is never larger than 6, so `strbuf_append_string()` is not at risk of overflow (the bug was unreachable).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 561,
        "deletions": 30,
        "changed_files": 8,
        "created_at": "2023-08-01T11:44:23Z",
        "closed_at": "2023-09-24T06:47:24Z",
        "merged_at": "2023-09-24T06:47:24Z",
        "body": "In this PR we are adding the functionality to collect all the process's threads' backtraces.\r\n\r\n## Changes made in this PR\r\n\r\n### **introduce threads mngr API**\r\nThe **threads mngr API** which has 2 abilities:\r\n* `ThreadsManager_init() `- register to SIGUSR2. called on the server start-up.\r\n* ` ThreadsManager_runOnThreads()` - receives a list of a pid_t and a callback, tells every thread in the list to invoke the callback, and returns the output collected by each invocation.\r\n**Elaborating atomicvar API**\r\n* `atomicIncrGet(var,newvalue_var,count) `-- Increment and get the atomic counter new value\r\n* `atomicFlagGetSet` -- Get and set the atomic counter value to 1\r\n\r\n### **Always set SIGALRM handler**\r\nSIGALRM handler prints the process's stacktrace to the log file. Up until now, it was set only if the `server.watchdog_period` > 0. This can be also useful if debugging is needed. However, in situations where the server can't get requests, (a deadlock, for example) we weren't able to change the signal handler.\r\nTo make it available at run time we set SIGALRM handler on server startup. The signal handler name was changed to a more general `sigalrmSignalHandler`.\r\n\r\n### **Print all the process' threads' stacktraces**\r\n\r\n`logStackTrace()` now calls `writeStacktraces()`, instead of logging the current thread stacktrace.\r\n`writeStacktraces()`:\r\n* On Linux systems we use the threads manager API to collect the backtraces of all the process' threads.\r\nTo get the `tids` list (threads ids) we read the `/proc/<redis-server-pid>/tasks` file which includes a list of directories. Each directory name corresponds to one tid (including the main thread). For each thread, we also need to check if it can get the signal from the threads manager (meaning it is not blocking/ignoring that signal). We send the threads manager this tids list and `collect_stacktrace_data()` callback, which collects the thread's backtrace addresses, its name, and tid.\r\n* On other systems, the behavior remained as it was (writing only the current thread stacktrace to the log file).\r\n\r\n## compatibility notes\r\n1. **The threads mngr API is only supported in linux.** \r\n2. We use `syscall(SYS_gettid)` and `syscall(SYS_tgkill...)` because their dedicated alternatives (`gettid()` and `tgkill`) were added in glibc 2.3.\r\n\r\n## Output example\r\n\r\nEach thread backtrace will have the following format:\r\n`<tid> <thread_name> [additional_info]`\r\n* **tid**: as read from the `/proc/<redis-server-pid>/tasks` file\r\n* **thread_name**: the tread name as it is registered in the os/\r\n* **additional_info**: Sometimes we want to add specific information about one of the threads. currently. it is only used to mark the thread that handles the backtraces collection by adding \"*\". In case of crash - this also indicates which thread caused the crash. The handling thread in won't necessarily appear first.\r\n```\r\n------ STACK TRACE ------\r\nEIP:\r\n/lib/aarch64-linux-gnu/libc.so.6(epoll_pwait+0x9c)[0xffffb9295ebc]\r\n\r\n67089 redis-server *\r\nlinux-vdso.so.1(__kernel_rt_sigreturn+0x0)[0xffffb9437790]\r\n/lib/aarch64-linux-gnu/libc.so.6(epoll_pwait+0x9c)[0xffffb9295ebc]\r\nredis-server *:6379(+0x75e0c)[0xaaaac2fe5e0c]\r\nredis-server *:6379(aeProcessEvents+0x18c)[0xaaaac2fe6c00]\r\nredis-server *:6379(aeMain+0x24)[0xaaaac2fe7038]\r\nredis-server *:6379(main+0xe0c)[0xaaaac3001afc]\r\n/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xffffb91d73fc]\r\n/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xffffb91d74cc]\r\nredis-server *:6379(_start+0x30)[0xaaaac2fe0370]\r\n\r\n67093 bio_lazy_free\r\n/lib/aarch64-linux-gnu/libc.so.6(+0x79dfc)[0xffffb9229dfc]\r\n/lib/aarch64-linux-gnu/libc.so.6(pthread_cond_wait+0x208)[0xffffb922c8fc]\r\nredis-server *:6379(bioProcessBackgroundJobs+0x174)[0xaaaac30976e8]\r\n/lib/aarch64-linux-gnu/libc.so.6(+0x7d5c8)[0xffffb922d5c8]\r\n/lib/aarch64-linux-gnu/libc.so.6(+0xe5d1c)[0xffffb9295d1c]\r\n\r\n67091 bio_close_file\r\n/lib/aarch64-linux-gnu/libc.so.6(+0x79dfc)[0xffffb9229dfc]\r\n/lib/aarch64-linux-gnu/libc.so.6(pthread_cond_wait+0x208)[0xffffb922c8fc]\r\nredis-server *:6379(bioProcessBackgroundJobs+0x174)[0xaaaac30976e8]\r\n/lib/aarch64-linux-gnu/libc.so.6(+0x7d5c8)[0xffffb922d5c8]\r\n/lib/aarch64-linux-gnu/libc.so.6(+0xe5d1c)[0xffffb9295d1c]\r\n\r\n67092 bio_aof\r\n/lib/aarch64-linux-gnu/libc.so.6(+0x79dfc)[0xffffb9229dfc]\r\n/lib/aarch64-linux-gnu/libc.so.6(pthread_cond_wait+0x208)[0xffffb922c8fc]\r\nredis-server *:6379(bioProcessBackgroundJobs+0x174)[0xaaaac30976e8]\r\n/lib/aarch64-linux-gnu/libc.so.6(+0x7d5c8)[0xffffb922d5c8]\r\n/lib/aarch64-linux-gnu/libc.so.6(+0xe5d1c)[0xffffb9295d1c]\r\n67089:signal-handler (1693824528) --------\r\n```\r\n\r\n\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-08-01T03:28:49Z",
        "closed_at": "2023-08-02T07:46:06Z",
        "merged_at": "2023-08-02T07:46:06Z",
        "body": "When doing merge selector, we should check whether the merge has started (i.e., whether open_bracket_start is -1) every time. Otherwise, encountering an illegal selector pattern could succeed and also cause memory leaks, for example:\r\n\r\n```\r\nacl setuser test1 (+PING (+SELECT (+DEL )\r\n```\r\n\r\nThe above would leak memory and succeed with only DEL being applied, and would now error after the fix.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2023-07-31T10:16:11Z",
        "closed_at": "2023-08-02T08:43:31Z",
        "merged_at": "2023-08-02T08:43:31Z",
        "body": "When loading a function from either RDB/AOF or a replica, it is essential not to fail on timeout errors. The loading time may vary due to various factors, such as hardware specifications or the system's workload during the loading process. Once a function has been successfully loaded, it should be allowed to load from persistence or on replicas without encountering a timeout failure.\r\n\r\nTo maintain a clear separation between the engine and Redis internals, the implementation avoid from directly checking the state of Redis within the engine itself. Instead, the engine receives the desired timeout as part of the library creation and must respects this timeout value. If Redis wishes to disable any timeout, it can simply send a value of 0.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 198,
        "deletions": 116,
        "changed_files": 5,
        "created_at": "2023-07-31T09:24:39Z",
        "closed_at": "2023-08-31T11:42:08Z",
        "merged_at": "2023-08-31T11:42:08Z",
        "body": "ZRANGE BYSCORE/BYLEX with [LIMIT offset count] option was using every level in skiplist to jump to the first/last node in range, but only use level[0] in skiplist to locate the node at offset, resulting in sub-optimal performance using LIMIT:\r\n```\r\nwhile (ln && offset--) {\r\n    if (reverse) {\r\n        ln = ln->backward;\r\n    } else {\r\n        ln = ln->level[0].forward;\r\n    }\r\n}\r\n```\r\nIt could be slow when offset is very big. We can get the total rank of the offset location and use skiplist to jump to it. It is an improvement from O(offset) to O(log rank).\r\n\r\nBelow shows how this is implemented (if the offset is positve):\r\n\r\nUse the skiplist to seach for the first element in the range, record its rank `rank_0`, so we can have the rank of the target node `rank_t`. Meanwhile we record the last node we visited which has zsl->level-1 levels and its rank `rank_1`. Then we start from the zsl->level-1 node, use skiplist to go forward `rank_t-rank_1` nodes to reach the target node.\r\n\r\nIt is very similiar when the offset is reversed.\r\n\r\nNote that if `rank_t` is very close to `rank_0`, we just start from the first element in range and go node by node, this for the case when zsl->level-1 node is to far away and it is quicker to reach the target node by node.\r\n\r\nHere is a test using a random generated zset including 10000 elements (with different positive scores), doing a bench mark which compares how fast the `ZRANGE` command is exucuted before and after the optimization. \r\n\r\nThe start score is set to 0 and the count is set to 1 to make sure that most of the time is spent on locating the offset.\r\n```\r\nmemtier_benchmark -h 127.0.0.1 -p 6379 --command=\"zrange test 0 +inf byscore limit <offset> 1\"\r\n```\r\n| offset | QPS(unstable) | QPS(optimized) |\r\n|--------|--------|--------|\r\n| 10 | 73386.02 | 74819.82 |\r\n| 1000 | 48084.96 | 73177.73 |\r\n| 2000 | 31156.79 | 72805.83 |\r\n| 5000 | 10954.83 | 71218.21 |\r\n\r\nWith the result above, we can see that the original code is greatly slowed down when offset gets bigger, and with the optimization the speed is almost not affected.\r\n\r\nSimiliar results are generated when testing reversed offset:\r\n```\r\nmemtier_benchmark -h 127.0.0.1 -p 6379 --command=\"zrange test +inf 0 byscore rev limit <offset> 1\"\r\n```\r\n| offset | QPS(unstable) | QPS(optimized) |\r\n|--------|--------|--------|\r\n| 10 | 74505.14 | 71653.67 |\r\n| 1000 | 46829.25 | 72842.75 |\r\n| 2000 | 28985.48 | 73669.01 |\r\n| 5000 | 11066.22 | 73963.45 | \r\n\r\nAnd the same conclusion is drawn from the tests of ZRANGE BYLEX.\r\n\r\nWould close #3454",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-07-28T06:46:20Z",
        "closed_at": "2023-08-31T19:45:36Z",
        "merged_at": "2023-08-31T19:45:36Z",
        "body": " Check that its argument is not NULL before using SSL_CTX_set_options to avoid null dereference.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-07-28T06:39:14Z",
        "closed_at": "2023-07-30T05:48:30Z",
        "merged_at": "2023-07-30T05:48:30Z",
        "body": "In the tcl foreach loop, the function should compare line rather than the whole file.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2023-07-27T14:19:38Z",
        "closed_at": "2023-08-05T04:29:25Z",
        "merged_at": "2023-08-05T04:29:25Z",
        "body": "GEOHASH / GEODIST / GEOPOS use zsetScore to get the score, in skiplist encoding,\r\nwe use dictFind to get the score, which is O(1), same as ZSCORE command.\r\nIt is not clear why these commands had O(Log(N)), and O(N) until now.\r\n\r\nThis closes #12432",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-07-27T10:37:24Z",
        "closed_at": "2023-08-01T15:03:33Z",
        "merged_at": "2023-08-01T15:03:33Z",
        "body": "Changing the masterauth while turning into a replica is racy.\r\n\r\nTurn into replica after changing the masterauth instead.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-07-26T19:15:23Z",
        "closed_at": "2023-10-15T19:37:25Z",
        "merged_at": null,
        "body": "part of sentinelHandleRedisInstance we have a check for master or slave, don't have any operations in the block. this code is added very long back in 6b5daa2df2 . \r\n\r\n```\r\n     sentinelCheckSubjectivelyDown(ri);\r\n\r\n    /* Masters and slaves */\r\n    if (ri->flags & (SRI_MASTER|SRI_SLAVE)) {\r\n        /* Nothing so far. */\r\n    }\r\n\r\n     /* Only masters */\r\n\r\n```\r\n\r\nSince we dont have any oprations under the condition so removing the redundency code.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-07-25T07:33:45Z",
        "closed_at": "2023-08-10T05:58:52Z",
        "merged_at": "2023-08-10T05:58:52Z",
        "body": "After SENTINEL RESET, sometimes the sentinel can\r\nsense the master again, causing the test to fail.\r\nHere we give it a few more chances.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-07-24T21:34:55Z",
        "closed_at": "2023-07-25T23:43:31Z",
        "merged_at": "2023-07-25T23:43:31Z",
        "body": "Additional test coverage for incr/decr operation.\r\n\r\ninteger number could be present in `raw` encoding format due to operation like `append`. A `incr`/`decr` operation following it optimize the string to `int` encoding format. \r\nFor e.g.\r\n\r\n```\r\n127.0.0.1:6379> set foo 1\r\nOK\r\n127.0.0.1:6379> object encoding foo\r\n\"int\"\r\n127.0.0.1:6379> append foo 2\r\n(integer) 2\r\n127.0.0.1:6379> object encoding foo\r\n\"raw\"\r\n127.0.0.1:6379> incr foo\r\n(integer) 13\r\n127.0.0.1:6379> object encoding foo\r\n\"int\"\r\n```",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-07-21T00:57:00Z",
        "closed_at": "2023-08-10T17:52:57Z",
        "merged_at": null,
        "body": "Handy Makefile target to run test with valgrind to detect memory leaks\r\n\r\n```\r\nmake test-valgrind\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-07-21T00:48:31Z",
        "closed_at": "2023-07-25T01:25:51Z",
        "merged_at": "2023-07-25T01:25:51Z",
        "body": "Add test coverage to validate object encoding update on APPEND command usage on a integer value\r\n\r\n### Observations\r\n\r\n#### SET command behavior (same with INCR/DECR operation)\r\n```\r\n127.0.0.1:6379> set test 10\r\nOK\r\n127.0.0.1:6379> get test\r\n\"10\"\r\n127.0.0.1:6379> debug object test\r\nValue at:0x1d88c40 refcount:2147483647 encoding:int serializedlength:2 lru:12177111 lru_seconds_idle:7\r\n```\r\n\r\n#### APPEND command behavior \r\n```\r\n127.0.0.1:6379> set test1 10\r\nOK\r\n127.0.0.1:6379> debug object test1\r\nValue at:0x1d88c40 refcount:2147483647 encoding:int serializedlength:2 lru:12177111 lru_seconds_idle:29\r\n127.0.0.1:6379> append object test1\r\n(integer) 5\r\n127.0.0.1:6379> append test1 10\r\n(integer) 4\r\n127.0.0.1:6379> debug object test1\r\nValue at:0x1e2a9b0 refcount:1 encoding:raw serializedlength:3 lru:12177165 lru_seconds_idle:9\r\n```\r\nI presume no optimization (`int`/`embstr`) is made for SDS generated from append command as the SDS is expected to grow in size (used for timeseries use cases).",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-07-20T15:24:29Z",
        "closed_at": "2023-08-14T20:02:53Z",
        "merged_at": null,
        "body": "**Description**\r\nClient list command doesn't returns anything when the client list is empty for the particular type.\r\n```\r\n127.0.0.1:6379> client list type normal\r\nid=3 addr=127.0.0.1:58112 laddr=127.0.0.1:6379 fd=8 name= age=22 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 qbuf=48 qbuf-free=16336 argv-mem=20 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=18242 events=r cmd=client|list user=default redir=-1 resp=2 lib-name= lib-ver=\r\n127.0.0.1:6379> client list type master\r\n127.0.0.1:6379> client list type replica\r\n127.0.0.1:6379> client list type pubsub\r\n127.0.0.1:6379>\r\n```\r\n\r\nI was expecting empty string or (nil) when the client list is empty.\r\n_IMO printing empty string is better._\r\n\r\n```\r\n127.0.0.1:6379> client list type master\r\n\"\"\r\n```\r\n\r\nOR\r\n\r\n```\r\n127.0.0.1:6379> client list type master\r\n(nil)\r\n```\r\n\r\n**Current behavior after changes :**\r\n```\r\n127.0.0.1:6379> client list type pubsub\r\n\"\"\r\n127.0.0.1:6379> client list type replica\r\n\"\"\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 64,
        "deletions": 34,
        "changed_files": 4,
        "created_at": "2023-07-20T10:24:29Z",
        "closed_at": "2023-08-20T16:16:45Z",
        "merged_at": "2023-08-20T16:16:45Z",
        "body": "This PR purpose is to make the crash report process thread safe.\r\nmain changes include:\r\n\r\n1. `setupSigSegvHandler()` is introduced to initialize the signal handler. This function first initializes the signal handler mutex (if not initialized yet) and then registers the process to the signal handler. \r\n\r\n2. **sigsegvHandler** flags :\r\nSA_NODEFER - don't add the signal to the process signal mask. We use this flag because we want to be able to handle a second call to the signal manually.\r\nremoved SA_RESETHAND: this flag resets the signal handler function upon the first entrance to the registered function. The reason to use this flag is to protect from recursively entering the signal handler by the same thread. But, it also means that if a second thread crashes while handling a signal, the process will be terminated immediately and we won't get the crash report.\r\nIn this PR we discard this flag. The signal handler guard described below purpose is to solve the above issues.\r\n\r\n3. Add a **signal handler lock** with ERRORCHECK attributes. \r\nThe lock's purpose is to ensure that only one thread generates a crash report. Once a second thread enters the signal handler it will be blocked.\r\nWe use the ERRORCHECK lock in order to protect from possible deadlock in case the thread handling the crash gets a signal. In the latest scenario, we log what we have collected until the handler crashed.\r\n\r\nAt the end of the crash report we reset the signal handler SIG_DFL, with no flags, and rethrow the signal to generate a core dump (if enabled) and exit the process.\r\n\r\nDuring the work on this PR we wanted to understand the historical reasons for how crash is handled.\r\nWith respect to the choice of the flag, we believe the **SA_RESETHAND** was not added for any specific purpose.\r\n**SA_ONSTACK** which is removed here from bugReportEnd(), was originally also set in the initial registration to signal handler, but removed [in this commit](https://github.com/redis/redis/commit/3ada43e732678e1f1ed0830c7407eef99ad63c46). In addition, it was removed from another location [by this commit](https://github.com/redis/redis/commit/deee2c1ef2249120ae7db0e7523bfad4041b21a6) with the following description, which is also relevant to why it should be removed from bugReportEnd:\r\n\r\n> it seems to be some valgrind bug with SA_ONSTACK.\r\n> SA_ONSTACK seems unneeded since WD is not recursive (SA_NODEFER was removed),\r\n> also, not sure if it's even valid without a call to sigaltstack()\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-07-18T11:04:10Z",
        "closed_at": "2023-07-25T13:10:38Z",
        "merged_at": "2023-07-25T13:10:38Z",
        "body": "A bug introduced in #11657 (7.2 RC1), causes client-eviction (#8687) and INFO to have inaccurate memory usage metrics of MONITOR clients.\r\n\r\nBecause the type in `c->type` and the type in `getClientType()` are confusing\r\n(in the later, `CLIENT_TYPE_NORMAL` not `CLIENT_TYPE_SLAVE`), the comment\r\nwe wrote in `updateClientMemUsageAndBucket` was wrong, and in fact that function\r\ndidn't skip monitor clients.\r\nAnd since it doesn't skip monitor clients, it was wrong to delete the call for it from\r\n`replicationFeedMonitors` (it wasn't a NOP).\r\nThat deletion could mean that the monitor client memory usage is not always up to\r\ndate (updated less frequently, but still a candidate for client eviction).",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2023-07-17T12:51:35Z",
        "closed_at": "2023-07-25T07:21:23Z",
        "merged_at": "2023-07-25T07:21:23Z",
        "body": "changing the response and request policy of a few commands, see https://redis.io/docs/reference/command-tips\r\n\r\n1. RANDOMKEY used to have no response policy, which means that when sent to multiple shards, the responses should be aggregated. this normally applies to commands that return arrays, but since RANDOMKEY replies with a simple string, it actually requires a SPECIAL response policy (for the client to select just one)\r\n2. SCAN used to have no response policy, but although the key names part of the response can be aggregated, the cursor part certainly can't.\r\n3. MSETNX had a request policy of MULTI_SHARD and response policy of AGG_MIN, but in fact the contract with MSETNX is that when one key exists, it returns 0 and doesn't set any key, routing it to multiple shards would mean that if one failed and another succeeded, it's atomicity is broken and it's impossible to return a valid response to the caller.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-07-16T22:13:33Z",
        "closed_at": "2023-07-18T06:36:45Z",
        "merged_at": "2023-07-18T06:36:45Z",
        "body": "The data structures in the comment are not in sync and don't need to be. Referring to function that handles conversion.\r\n\r\nCloses issue #12378 ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-07-16T03:54:55Z",
        "closed_at": "2023-08-13T18:26:03Z",
        "merged_at": null,
        "body": "I noticed strange behavior when calling specifically the `HELLO` command from within a module using `RedisModule_Call`. Regardless of authentication, the first `HELLO` command called by `RedisModule_Call` after the Redis Server starts up will always succeed, and regardless of authentication, all subsequent `HELLO` commands called by `RedisModule_Call` will fail for that given Redis Server process.\r\n\r\nThere is more information and recreation steps [in this repository](https://github.com/nirrattner/RedisModuleHelloExample/tree/main).\r\n\r\nI believe the fix is for the `RM_Call` client to inherit the `authenticated` value from the module context client `authenticated` value, although I would appreciate scrutiny here because I'm not very familiar with this code.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 68,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2023-07-15T23:37:02Z",
        "closed_at": "2023-08-07T08:43:31Z",
        "merged_at": null,
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-07-13T09:36:30Z",
        "closed_at": "2023-07-16T03:31:42Z",
        "merged_at": "2023-07-16T03:31:42Z",
        "body": "According to the format shown in https://redis.io/commands/cluster-nodes/\r\n```\r\n<ip:port@cport[,hostname[,auxiliary_field=value]*]>\r\n```\r\nwhen there is no hostname, and the auxiliary fields are hidden, the cluster topology should be\r\n```\r\n<ip:port@cport>\r\n```\r\nHowever in the code we always print the hostname even when it is an empty string, leaving an unnecessary comma tailing after cport, which is weird and conflicts with the doc.\r\n```\r\n94ca2f6cf85228a49fde7b738ee1209de7bee325 127.0.0.1:6379@16379, myself,master - 0 0 0 connected 0-16383\r\n```\r\n\r\n```\r\nRelease notes:\r\nThe response of cluster nodes was unnecessarily adding an extra comma when no hostname was present. This impacts all RC candidates of 7.2.\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 106,
        "deletions": 24,
        "changed_files": 11,
        "created_at": "2023-07-12T12:17:05Z",
        "closed_at": "2023-07-13T14:45:39Z",
        "merged_at": "2023-07-13T14:45:39Z",
        "body": "supersedes #12406\r\n\r\nThis updates hiredis to the 1.2.0 release. Included are README updates on how to do so for future users, given the confusion.\r\n\r\nTo do:\r\n- [x] remember to merge without squash / rebase",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 733,
        "deletions": 749,
        "changed_files": 15,
        "created_at": "2023-07-12T07:40:23Z",
        "closed_at": "2023-07-12T12:17:11Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-07-10T22:14:32Z",
        "closed_at": "2023-07-13T05:56:33Z",
        "merged_at": null,
        "body": "This is reference of : https://github.com/redis/redis/issues/6309\r\n\r\nWe could see different size of memory when bitfiled are set in different order as stated in https://github.com/redis/redis/issues/6309, This happened due to objectComputeSize is retrieving the size by calling malloc_usable_size API. As we know malloc_usable_size returns the more bytes compared to requested because it is depends on the alignment. Bitfield falled in string and RAW encoding type looks equal to string length of the key and object size. Thought it would be more consistent for the client if we use stringObjectLen instead of sdsZmallocSize (sdsZmallocSize -> zmalloc_size -> malloc_usable_size or malloc_size).\r\n\r\nBefore changing the api:\r\n\r\n```\r\n127.0.0.1:6379> DEL abc\r\n(integer) 1\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> bitfield abc set u32 #0 1\r\n1) (integer) 0\r\n127.0.0.1:6379> bitfield abc set u32 #22 1\r\n1) (integer) 0\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> MEMORY USAGE abc\r\n(integer) 233\r\n127.0.0.1:6379> del abc\r\n(integer) 1\r\n127.0.0.1:6379> bitfield abc set u32 #22 1\r\n1) (integer) 0\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> bitfield abc set u32 #0 1\r\n1) (integer) 0\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> MEMORY USAGE abc\r\n(integer) 141\r\n\r\n```\r\nResult after api is changed:\r\n\r\n```\r\n127.0.0.1:6379> del abc\r\n(integer) 1\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> bitfield abc set u32 #0 1\r\n1) (integer) 0\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> bitfield abc set u32 #22 1\r\n1) (integer) 0\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> memory usage abc\r\n(integer) 137\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> del abc\r\n(integer) 1\r\n127.0.0.1:6379>\r\n127.0.0.1:6379>  bitfield abc set u32 #22 1\r\n1) (integer) 0\r\n127.0.0.1:6379> bitfield abc set u32 #0 1\r\n1) (integer) 0\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> memory usage abc\r\n(integer) 137\r\n127.0.0.1:6379>\r\n\r\n```",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 5794,
        "deletions": 1375,
        "changed_files": 153,
        "created_at": "2023-07-10T07:30:07Z",
        "closed_at": "2023-07-10T11:55:21Z",
        "merged_at": "2023-07-10T11:55:21Z",
        "body": "Upgrade urgency LOW: This is the third Release Candidate for Redis 7.2.\r\nUpgrade urgency SECURITY: If you're using a previous release candidate of 7.2.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-24834) A specially crafted Lua script executing in Redis can trigger\r\n  a heap overflow in the cjson and cmsgpack libraries, and result in heap\r\n  corruption and potentially remote code execution. The problem exists in all\r\n  versions of Redis with Lua scripting support, starting from 2.6, and affects\r\n  only authenticated and authorized users.\r\n* (CVE-2023-36824) Extracting key names from a command and a list of arguments\r\n  may, in some cases, trigger a heap overflow and result in reading random heap\r\n  memory, heap corruption and potentially remote code execution. Specifically:\r\n  using COMMAND GETKEYS* and validation of key names in ACL rules.\r\n\r\nNew Features\r\n============\r\n\r\nNew administrative and introspection commands and command arguments\r\n-------------------------------------------------------------------\r\n\r\n* Make SENTINEL CONFIG [SET|GET] variadic. (#10362)\r\n\r\nPotentially Breaking / Behavior Changes\r\n=======================================\r\n\r\n* Cluster SHARD IDs are no longer visible in the cluster nodes output,\r\n  introduced in 7.2-RC1. (#10536, #12166)\r\n* When calling PUBLISH with a RESP3 client that's also subscribed to the same channel,\r\n  the order is changed and the reply is sent before the published message (#12326)\r\n\r\nNew configuration options\r\n=========================\r\n\r\n* Add a new loglevel \"nothing\" to disable logging (#12133)\r\n* Add cluster-announce-human-nodename - a unique identifier for a node that is\r\n  be used in logs for debugging (#9564)\r\n\r\nOther General Improvements\r\n==========================\r\n\r\n* Allow CLUSTER SLOTS / SHARDS commands during loading (#12269)\r\n* Support TLS service when \"tls-cluster\" is not enabled and persist both plain\r\n  and TLS port in nodes.conf (#12233)\r\n* Update SPOP and RESTORE commands to replicate unlink commands to replicas\r\n  when the server is configured to use async server deletes (#12320)\r\n* Try lazyfree the temporary zset in ZUNION / ZINTER / ZDIFF (#12229)\r\n\r\nPerformance and resource utilization improvements\r\n=================================================\r\n\r\n* Optimize PSUBSCRIBE and PUNSUBSCRIBE from O(N*M) to O(N) (#12298)\r\n* Optimize SCAN, SSCAN, HSCAN, ZSCAN commands (#12209)\r\n* Set Jemalloc --disable-cache-oblivious to reduce memory overhead (#12315)\r\n* Optimize ZINTERCARD to avoid create a temporary zset (#12229)\r\n* Optimize HRANDFIELD and ZRANDMEMBER listpack encoded (#12205)\r\n* Numerous other optimizations (#12155, #12082, #11626, #11944, #12316, #12250,\r\n  #12177, #12185)\r\n\r\n\r\nChanges in CLI tools\r\n====================\r\n\r\n* redis-cli: Handle RESP3 double responses that contain a NaN (#12254)\r\n* redis-cli: Support URIs with IPv6 (#11834)\r\n\r\nModule API changes\r\n==================\r\n\r\n* Align semantics of the new (v7.2 RC2) RM_ReplyWithErrorFormat with RM_ReplyWithError.\r\n  This is a breaking change that affects the generated error code. (#12321)\r\n* Forbid RM_AddPostNotificationJob on loading and on read-only replicas (#12304)\r\n* Add ability for module command filter to know which client is being handled (#12219)\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Fix broken protocol when PUBLISH is used inside MULTI when the RESP3\r\n  publishing client is also subscribed for the channel (#12326)\r\n* Fix WAIT to be effective after a blocked module command being unblocked (#12220)\r\n* Re-enable downscale rehashing while there is a fork child (#12276)\r\n* Fix possible hang in HRANDFIELD, SRANDMEMBER, ZRANDMEMBER when used with `<count>` (#12276)\r\n* Improve fairness issue in RANDOMKEY, HRANDFIELD, SRANDMEMBER, ZRANDMEMBER, SPOP, and eviction (#12276)\r\n* Cluster: fix a race condition where a slot migration may revert on a subsequent failover or node joining (#12344)\r\n\r\nFixes for issues in previous releases of Redis 7.2\r\n--------------------------------------------------\r\n\r\n* Fix XREADGROUP BLOCK with \">\" from hanging (#12301)\r\n* Fix assertion when a blocked command is rejected when re-processed. (#12247)\r\n* Fix use after free on a blocking RM_Call. (#12342)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 582,
        "deletions": 178,
        "changed_files": 21,
        "created_at": "2023-07-10T07:21:10Z",
        "closed_at": "2023-07-10T11:39:42Z",
        "merged_at": "2023-07-10T11:39:42Z",
        "body": "Upgrade urgency SECURITY: See security fixes below.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-24834) A specially crafted Lua script executing in Redis can trigger\r\n  a heap overflow in the cjson and cmsgpack libraries, and result in heap\r\n  corruption and potentially remote code execution. The problem exists in all\r\n  versions of Redis with Lua scripting support, starting from 2.6, and affects\r\n  only authenticated and authorized users.\r\n* (CVE-2023-36824) Extracting key names from a command and a list of arguments\r\n  may, in some cases, trigger a heap overflow and result in reading random heap\r\n  memory, heap corruption and potentially remote code execution. Specifically:\r\n  using COMMAND GETKEYS* and validation of key names in ACL rules.\r\n\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Re-enable downscale rehashing while there is a fork child (#12276)\r\n* Fix possible hang in HRANDFIELD, SRANDMEMBER, ZRANDMEMBER when used with `<count>` (#12276)\r\n* Improve fairness issue in RANDOMKEY, HRANDFIELD, SRANDMEMBER, ZRANDMEMBER, SPOP, and eviction (#12276)\r\n* Fix WAIT to be effective after a blocked module command being unblocked (#12220)\r\n* Avoid unnecessary full sync after master restart in a rare case (#12088)\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 338,
        "deletions": 130,
        "changed_files": 11,
        "created_at": "2023-07-10T07:20:37Z",
        "closed_at": "2023-07-10T11:38:00Z",
        "merged_at": "2023-07-10T11:38:00Z",
        "body": "Upgrade urgency SECURITY: See security fixes below.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-24834) A specially crafted Lua script executing in Redis can trigger\r\n  a heap overflow in the cjson and cmsgpack libraries, and result in heap\r\n  corruption and potentially remote code execution. The problem exists in all\r\n  versions of Redis with Lua scripting support, starting from 2.6, and affects\r\n  only authenticated and authorized users.\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Re-enable downscale rehashing while there is a fork child (#12276)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 338,
        "deletions": 129,
        "changed_files": 11,
        "created_at": "2023-07-10T07:20:02Z",
        "closed_at": "2023-07-10T11:37:47Z",
        "merged_at": "2023-07-10T11:37:46Z",
        "body": "Upgrade urgency SECURITY: See security fixes below.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-24834) A specially crafted Lua script executing in Redis can trigger\r\n  a heap overflow in the cjson and cmsgpack libraries, and result in heap\r\n  corruption and potentially remote code execution. The problem exists in all\r\n  versions of Redis with Lua scripting support, starting from 2.6, and affects\r\n  only authenticated and authorized users.\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Re-enable downscale rehashing while there is a fork child (#12276)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 13,
        "changed_files": 2,
        "created_at": "2023-07-07T20:39:52Z",
        "closed_at": "2023-08-21T09:53:46Z",
        "merged_at": "2023-08-21T09:53:46Z",
        "body": "Generally, In any command we first check for  the argument and then check if key exist.\r\n\r\nSome of the examples are\r\n\r\n```\r\n127.0.0.1:6379> getrange no-key invalid1 invalid2\r\n(error) ERR value is not an integer or out of range\r\n127.0.0.1:6379> setbit no-key 1 invalid\r\n(error) ERR bit is not an integer or out of range\r\n127.0.0.1:6379> xrange no-key invalid1 invalid2\r\n(error) ERR Invalid stream ID specified as stream command argument\r\n```\r\n\r\n**Before change** \r\n```\r\nbitcount no-key invalid1 invalid2\r\n0\r\n```\r\n\r\n**After change**\r\n```\r\nbitcount no-key invalid1 invalid2\r\n(error) ERR value is not an integer or out of range\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-07-07T03:17:39Z",
        "closed_at": "2023-07-07T13:40:45Z",
        "merged_at": "2023-07-07T13:40:45Z",
        "body": "valgrind report a Uninitialised warning:\r\n```\r\n==25508==  Uninitialised value was created by a heap allocation\r\n==25508==    at 0x4848899: malloc (in\r\n/usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n==25508==    by 0x1A35A1: ztrymalloc_usable_internal (zmalloc.c:117)\r\n==25508==    by 0x1A368D: zmalloc (zmalloc.c:145)\r\n==25508==    by 0x21FDEA: clusterInit (cluster.c:973)\r\n==25508==    by 0x19DC09: main (server.c:7306)\r\n```\r\n\r\nIntroduced in #12344",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-07-06T03:16:40Z",
        "closed_at": "2023-07-06T05:17:26Z",
        "merged_at": "2023-07-06T05:17:26Z",
        "body": "This is an addition to #12380, to prevent potential bugs when collecting keys from multiple commands in the future.\r\nNote that this function also resets numkeys in some cases.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-07-05T07:20:52Z",
        "closed_at": "2023-08-16T07:45:27Z",
        "merged_at": "2023-08-16T07:45:26Z",
        "body": "If dict is rehashing, the  entries in the head of table[0] is moved to table[1] and all entries in `table[0][0:rehashidx]` is NULL.\r\n\r\n`dictNext` start looking for non-NULL entry from table 0 index 0, and the first call of `dictNext` on a rehashing dict will Iterate many times to skip those NULL entries.  We can easily skip those entries by setting `iter->index` as `iter->d->rehashidx` when dict is rehashing and it's the first call of dictNext (`iter->index == -1 && iter->table == 0`).\r\n\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-07-05T04:00:39Z",
        "closed_at": "2023-07-05T06:32:31Z",
        "merged_at": "2023-07-05T06:32:30Z",
        "body": "The test fails on freebsd CI:\r\n```\r\n*** [err]: stats: eventloop metrics in tests/unit/info.tcl\r\nExpected '31777' to be less than '16183' (context: type eval line 17 cmd\r\n{assert_lessthan $el_sum2 [expr $el_sum1+10000] } proc ::test)\r\n```\r\n\r\nThe test added in #11963, fails on freebsd CI which is slow,\r\nincrease tollerance and also add some verbose logs, now we can\r\nsee these logs in verbose mode (for better views):\r\n```\r\neventloop metrics cycle1: 12, cycle2: 15\r\neventloop metrics el_sum1: 315, el_sum2: 411\r\neventloop metrics cmd_sum1: 126, cmd_sum2: 137\r\n[ok]: stats: eventloop metrics (111 ms)\r\ninstantaneous metrics instantaneous_eventloop_cycles_per_sec: 8\r\ninstantaneous metrics instantaneous_eventloop_duration_usec: 55\r\n[ok]: stats: instantaneous metrics (1603 ms)\r\n[ok]: stats: debug metrics (112 ms)\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-07-03T06:09:51Z",
        "closed_at": "2023-07-03T09:45:18Z",
        "merged_at": "2023-07-03T09:45:18Z",
        "body": "When getKeysUsingKeySpecs processes a command with more than one key-spec, and called with a total of more than 256 keys, it'll call getKeysPrepareResult again, but since numkeys isn't updated, getKeysPrepareResult will not bother to copy key names from the old result (leaving these slots uninitialized). Furthermore, it did not consider the keys it already found when allocating more space.\r\n\r\nFor the record, this is the list of built in variadic commands with more than one key-spec:\r\nPFMERGE, ZUNIONSTORE, SUNIONSTORE, ZINTERSTORE, SINTERSTORE, ZDIFFSTORE, SDIFFSTORE, BITOP, MIGRATE.\r\nOther than that, there could be other commands registered by modules (if they declare key-specs, and don't use `getkeys-api`).\r\n\r\nCVE-2023-36824 was assigned to this issue",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-07-03T03:51:34Z",
        "closed_at": "2023-07-03T06:20:30Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 28,
        "changed_files": 2,
        "created_at": "2023-07-02T15:28:35Z",
        "closed_at": "2023-07-02T17:38:31Z",
        "merged_at": "2023-07-02T17:38:31Z",
        "body": "The negative offset check was added in #9052, we realized\r\nthat this is a non-mandatory breaking change and we would\r\nlike to add it only in 8.0.\r\n\r\nThis reverts PR #9052, will be re-introduced later in 8.0.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-07-02T06:48:36Z",
        "closed_at": "2023-07-05T07:27:14Z",
        "merged_at": "2023-07-05T07:27:14Z",
        "body": "In cluster mode, we need more clients than the number of nodes.\r\nFixes #10111",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-07-01T16:12:41Z",
        "closed_at": "2023-08-29T00:34:58Z",
        "merged_at": null,
        "body": "Background threads are unable to reliably wakeup blocked clients.\r\n\r\nA short description of the bug.\r\n\r\nThe module machinery to unblock clients was re-written in 7.0. The scheme uses a pipe to notify the mainthread that an unblock has been requested. However, when the function modulePipeReadable is invoked by the mainthread it fails to call moduleHandleBlockedClients to check the list of unblock requests. This appears to work because the beginSleep function does call that function. However, this is unreliable and can result in missed wakeups.\r\n\r\nTo repro the problem, you need a command that processes requests in the background, blocking and unblocking the associated client. Then if you flood the mainthread with those requests you will see that it occasionally goes idle because all of the applications requests remain blocked (their unblock is in the list, but the list didn't get processed).\r\n\r\nA description of what you expected to happen.\r\n\r\nI expect modulePipeReadable to call moduleHandleBlockedClients.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2023-07-01T08:45:11Z",
        "closed_at": "2023-07-04T09:14:07Z",
        "merged_at": null,
        "body": "The command `SUBSCRIBE` and `PSUBSCIRBE` described on the official website are inconsistent with those in the JSON file\r\n\r\nwebsite says:\r\nBehavior change history\r\n- SUBSCRIBE\r\n\t\\>= 6.2.0: RESET can be called to exit subscribed state.\r\n- PSUBSCRIBE\r\n\t\\>= 6.2.0: RESET can be called to exit subscribed state.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-29T07:14:04Z",
        "closed_at": "2023-07-01T14:35:35Z",
        "merged_at": "2023-07-01T14:35:35Z",
        "body": "Change to use dictTryExpand, return error on OOM.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-28T08:20:51Z",
        "closed_at": "2023-06-28T09:23:56Z",
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-27T03:58:50Z",
        "closed_at": "2023-06-27T07:21:58Z",
        "merged_at": "2023-06-27T07:21:58Z",
        "body": "aof-disable-auto-gc was created for testing purposes,\r\nto check if certain AOF files were actually generated\r\nand if they were deletedcorrectly during testing.\r\n\r\nSo hiding it, see #12249 for more discussion.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-06-26T15:57:30Z",
        "closed_at": "2023-06-27T06:17:34Z",
        "merged_at": "2023-06-27T06:17:34Z",
        "body": "Bumps [vmactions/freebsd-vm](https://github.com/vmactions/freebsd-vm) from 0.3.0 to 0.3.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vmactions/freebsd-vm/releases\">vmactions/freebsd-vm's releases</a>.</em></p>\n<blockquote>\n<p>Support 13.2</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/9991fc3550abdc19b61d50063c7eeb7713bc498f\"><code>9991fc3</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/ead618a236240253ec373dcaf47af5953cc5b2d3\"><code>ead618a</code></a> support 13.2</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/7d8195d91011f3bbfd87eeae91508b2db171a4a3\"><code>7d8195d</code></a> support 13.2</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/22aefae156db38ea5ab7563f89808dc82db8e325\"><code>22aefae</code></a> Update version to v0.3.0</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/b8f3845234b3b8d0d61d47fe821025a87426ccbb\"><code>b8f3845</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/c0ee7d75d72637085a876c0002b8080c5f10cddd\"><code>c0ee7d7</code></a> Sync from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/b0107f5fa849af71b7e4b2d02edfa6537fa7a61c\"><code>b0107f5</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/74d8b9631187b3e8548524c23bd4854cc080f8c2\"><code>74d8b96</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/0e75f75eeeb07540cb785035b0a307b487ee3aaf\"><code>0e75f75</code></a> Sync from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/6e722fb2d04672db787abd957a542d0eb9c8b51e\"><code>6e722fb</code></a> Update version to v0.3.0</li>\n<li>See full diff in <a href=\"https://github.com/vmactions/freebsd-vm/compare/v0.3.0...v0.3.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vmactions/freebsd-vm&package-manager=github_actions&previous-version=0.3.0&new-version=0.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 45,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-26T08:46:22Z",
        "closed_at": "2023-06-27T08:53:42Z",
        "merged_at": "2023-06-27T08:53:42Z",
        "body": "- Add support for `getAndSetMcontextEip`\r\n- Add support for `logRegisters`",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-26T06:48:11Z",
        "closed_at": "2023-06-28T09:13:38Z",
        "merged_at": null,
        "body": "reduce cpu use when io thread",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 73,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2023-06-25T17:55:29Z",
        "closed_at": "2023-07-06T00:46:23Z",
        "merged_at": "2023-07-06T00:46:23Z",
        "body": "When a node no longer owns a slot, it clears the bit corresponding to the slot in the cluster bus messages. The receiving nodes currently don't record the fact that the sender stopped claiming a slot until some other node in the cluster starts claiming the slot. This can cause a slot to go missing during slot migration when subjected to inopportune race with addition of new shards or a failover. This fix forces the receiving nodes to process the loss of ownership to avoid spreading wrong information.\r\n\r\nThis is a follow up PR from https://github.com/redis/redis/pull/12336#issuecomment-1604582374\r\n\r\n```\r\nRelease notes\r\nFix a race condition where a slot migration might get reverted when another failover happened or a node was added to the cluster.\r\n```",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-06-25T08:52:43Z",
        "closed_at": "2023-06-25T11:21:04Z",
        "merged_at": "2023-06-25T11:21:04Z",
        "body": "The new blockedBeforeSleep was added in #12337, it breaks the order in 2ecb5ed.\r\n\r\nThis may be related to #2288, quoted from comment in #2288:\r\n```\r\nMoreover the clusterBeforeSleep() call was misplaced at the end of the chain of the\r\nbeforeSleep() call in redis.c. It should be at the top, before processing un blocking\r\nclients. This is exactly the reason in the specific instance of the bug as reported,\r\nwhy the state was not updated in time before clients served.\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 95,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-06-25T08:24:33Z",
        "closed_at": "2023-06-25T11:12:27Z",
        "merged_at": "2023-06-25T11:12:27Z",
        "body": "blocking RM_Call was introduced on: https://github.com/redis/redis/pull/11568 (7.2 RC1), which allows a module to perform blocking commands and get the reply asynchronously.If the command gets block, a special promise CallReply is returned that allow to set the unblock handler. The unblock handler will be called when the command invocation finish and it gets, as input, the command real reply.\r\n\r\nThe issue was that the real CallReply was created using a stack allocated RedisModuleCtx which is no longer available after the unblock handler finishes. So if the module keeps the CallReply after the unblock handler finished, the CallReply holds a pointer to invalid memory and will try to access it when the CallReply will be released.\r\n\r\nThe solution is to create the CallReply with a NULL context to make it totally detached and can be freed freely when the module wants.\r\n\r\nTest was added to cover this case, running the test with valgrind before the fix shows the use after free error. With the fix, there are no valgrind errors.\r\n\r\nunrelated: adding a missing `$rd close` in many tests in that file.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-23T16:49:05Z",
        "closed_at": "2023-06-27T08:54:17Z",
        "merged_at": "2023-06-27T08:54:17Z",
        "body": "Added missing O3 flag to linking stage in default option \"-O3 -flto\". \r\nFlags doesn't lead to significant changes in performance:\r\n- +0.21% in geomean for all benchmarks on ICX bare-metal (256 cpus)\r\n- +0.33% in geomean for all benchmarks on m6i.2xlarge (16 cpus) \r\nChecked on redis from Mar'30 (commit 1f76bb17ddcb2adc484bf82f1b839c45e264524f ). Comparison file is attached. \r\n\r\n[O3_summary.xlsx](https://github.com/redis/redis/files/11850362/O3_summary.xlsx)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-06-22T06:02:12Z",
        "closed_at": "2023-06-22T15:10:42Z",
        "merged_at": "2023-06-22T15:10:42Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 162,
        "deletions": 25,
        "changed_files": 7,
        "created_at": "2023-06-21T15:26:46Z",
        "closed_at": "2023-06-22T20:15:17Z",
        "merged_at": "2023-06-22T20:15:17Z",
        "body": "Apart from adding the missing coverage, this PR also adds `blockedBeforeSleep` that gathers all block-related functions from `beforeSleep`\r\n\r\nThe order inside `blockedBeforeSleep` is different: now `handleClientsBlockedOnKeys` (which may unblock clients) is called before `processUnblockedClients` (which handles unblocked clients).\r\nIt makes sense to have this order.\r\n\r\nThere are no visible effects of the wrong ordering, except some cleanups of the now-unblocked client would have  happen in the next `beforeSleep` (will now happen in the current one)\r\n\r\nThe reason we even got into it is because i triggers an assertion in logresreq.c (breaking the assumption that `unblockClient` is called **before** actually flushing the reply to the socket):\r\n`handleClientsBlockedOnKeys` is called, then it calls `moduleUnblockClientOnKey`, which calls `moduleUnblockClient`, which adds the client to `moduleUnblockedClients`\r\nback to `beforeSleep`, we call `handleClientsWithPendingWritesUsingThreads`, it writes the data of buf to the client, so `client->bufpos` became 0\r\nOn the next `beforeSleep`, we call `moduleHandleBlockedClients`, which calls `unblockClient`, which calls `reqresAppendResponse`, triggering the assert. (because the `bufpos` is 0) - see https://github.com/redis/redis/pull/12301#discussion_r1226386716",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-06-21T14:44:55Z",
        "closed_at": "2023-07-07T23:23:07Z",
        "merged_at": null,
        "body": "### Background\r\n`Node A(Master)` update `clusterState` based on `clusterMsg` from another `Node B(Master)`. This is a chance to cause slot mapping conflict between `Node A` and `Node B`, When:\r\n\r\n- `Node C(Master)` was migrated(SETSLOT) `Slot S` from `Node B`\r\n- `Node B` received updating from `Node C`. It's configEpoch collision with another node `Node D(Master)`. `Node B`'s node id is smaller, so `Node B` bumps to new configEpoch, which is maximum in the cluster.\r\n- `Node A` received PONG from `Node B`. POV of `Node A`: **new configEpoch of `Node B` is updated but `Slot S` is not updated because it didn't belong to `Node A` anymore**\r\n- `Node C(Master)`, which was assigned this slot, POV of `Node A` failed to assign it to `Node C` because its config epoch(senderConfigEpoch) is smaller than `server.cluster->slots[j]->configEpoch`.\r\n\r\nAs shown in the figure:\r\n\r\n![image](https://github.com/cyningsun/redis-test/assets/5696973/9e8037c4-0761-4623-938b-d96a94cd6f98)\r\n\r\n### How to fix it\r\n\r\nSkip updating the sender's configEpoch and Slots before the migrated slot is claimed in POV. \r\n\r\nconfigEpoch acts as the guard of slots it owns in POV. If there is a migrated slot of the sender in POV pending claim by a new master, the sender's config epoch in POV cannot be updated separately. \r\n\r\nOtherwise, the slot will be guarded by a new config epoch of the previous master. This config epoch maybe has been bumped after this slot was assigned to another master. \r\n\r\nThe new master will fail to claim the slot because `senderConfigEpoch` is smaller than `server.cluster->slots[j]->configEpoch`. \r\n\r\n### Example\r\nHere is a cluster that has 5 shards, and each shard has 2 replicas, shown below before slot migrating. \r\nmaster nodeid | master node | config epoch | current epoch | slot 0 owner (configEpoch) | slot 16383 owner (configEpoch) | slave nodeid | slave node\r\n-- |-- | -- | -- | -- | -- | -- | --\r\n6ea4c2f8e7efe9180db83cf0adf1b055a557c74d | 10.53.52.144:6379 | 5 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | 626af417d334896000f5e23e6305f29242945948 | 10.53.52.144:6373\r\nd69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 | 10.53.52.144:6378 | 2 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | 95e75b159b3bcb7c33a8df9db93b5443413dfccb | 10.53.52.144:6377\r\n0c92e0b02f839260b508a6fdf258539a26e96903 | 10.53.52.144:6376 | 8 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | 900e489fcf28e0648272fc6c8b61f0bd998a01a0 | 10.53.52.144:6371\r\nf567e9d0fb1ccbdfbfc6a5c505e1d84d224de0ef | 10.53.52.144:6374 | 3 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | f65353c097d7f7e0f44644a28ddb980c21fc37bf | 10.53.52.144:6380\r\nc013fefabf931b77a5488e4e0fe1e81db1eaf4aa | 10.53.52.144:6375 | 1 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | 2ddaf93f230facc9893bc95bf9234a3d580d0291 | 10.53.52.144:6372\r\n\r\n#### Operations\r\n- Step 1: Slot 0 is migrated from `6ea4c2f8e7efe9180db83cf0adf1b055a557c74d 10.53.52.144:6379` to `d69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 10.53.52.144:6378`\r\n- Step 2: Manual failover is sent to `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372`\r\n- Step 3: Slot 16383 is migrated from `c013fefabf931b77a5488e4e0fe1e81db1eaf4aa 10.53.52.144:6375` to  `0c92e0b02f839260b508a6fdf258539a26e96903 10.53.52.144:6376`\r\n\r\nLet's focus on how `Slot 16383` is conflicted in the cluster\r\n\r\n#### Timeline\r\n\r\n**1. failover election for `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372`**\r\n\r\nfailover_auth_epoch = 9\r\n\r\n**2. After Slot 0 is SETSLOT to `d69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 10.53.52.144:6378`**\r\n\r\nmaster nodeid | master node | config epoch | current epoch | slot 0 owner (configEpoch)  | slot 16383 owner (configEpoch)  | slave nodeid | slave node\r\n-- |-- | -- | -- | -- | -- | -- | --\r\n6ea4c2f8e7efe9180db83cf0adf1b055a557c74d | 10.53.52.144:6379 | 5 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | 626af417d334896000f5e23e6305f29242945948 | 10.53.52.144:6373\r\n**d69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59** | **10.53.52.144:6378** | **9** | **9** | **10.53.52.144:6378 (9)** | 10.53.52.144:6375 (1) | 95e75b159b3bcb7c33a8df9db93b5443413dfccb | 10.53.52.144:6377\r\n0c92e0b02f839260b508a6fdf258539a26e96903 | 10.53.52.144:6376 | 8 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | 900e489fcf28e0648272fc6c8b61f0bd998a01a0 | 10.53.52.144:6371\r\nf567e9d0fb1ccbdfbfc6a5c505e1d84d224de0ef | 10.53.52.144:6374 | 3 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | f65353c097d7f7e0f44644a28ddb980c21fc37bf | 10.53.52.144:6380\r\nc013fefabf931b77a5488e4e0fe1e81db1eaf4aa | 10.53.52.144:6375 | 1 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | 2ddaf93f230facc9893bc95bf9234a3d580d0291 | 10.53.52.144:6372\r\n\r\n**3. After slot 16383 is SETSLOT to `0c92e0b02f839260b508a6fdf258539a26e96903 10.53.52.144:6376`**\r\n\r\nAfter `0c92e0b02f839260b508a6fdf258539a26e96903 10.53.52.144:6376` receiving `PONG` send by `d69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 10.53.52.144:6378`, configEpoch,currentEpoch is 9+1\r\n\r\nmaster nodeid | master node | config epoch | current epoch | slot 0 owner (configEpoch)  | slot 16383 owner (configEpoch)  | slave nodeid | slave node\r\n-- |-- | -- | -- | -- | -- | -- | --\r\n6ea4c2f8e7efe9180db83cf0adf1b055a557c74d | 10.53.52.144:6379 | 5 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | 626af417d334896000f5e23e6305f29242945948 | 10.53.52.144:6373\r\nd69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 | 10.53.52.144:6378 | 9 | 9 | 10.53.52.144:6378 (9) | 10.53.52.144:6375 (1) | 95e75b159b3bcb7c33a8df9db93b5443413dfccb | 10.53.52.144:6377\r\n**0c92e0b02f839260b508a6fdf258539a26e96903** | **10.53.52.144:6376** | **10** | **10** | **10.53.52.144:6378 (9)** | **10.53.52.144:6376 (10)** | 900e489fcf28e0648272fc6c8b61f0bd998a01a0 | 10.53.52.144:6371\r\nf567e9d0fb1ccbdfbfc6a5c505e1d84d224de0ef | 10.53.52.144:6374 | 3 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | f65353c097d7f7e0f44644a28ddb980c21fc37bf | 10.53.52.144:6380\r\nc013fefabf931b77a5488e4e0fe1e81db1eaf4aa | 10.53.52.144:6375 | 1 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | 2ddaf93f230facc9893bc95bf9234a3d580d0291 | 10.53.52.144:6372\r\n\r\n\r\n**4. After Failover is authed to `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372`**\r\n\r\nmaster nodeid | master node | config epoch | current epoch | slot 0 owner (configEpoch)  | slot 16383 owner (configEpoch)  | slave nodeid | slave node\r\n-- |-- | -- | -- | -- | -- | -- | --\r\n6ea4c2f8e7efe9180db83cf0adf1b055a557c74d | 10.53.52.144:6379 | 5 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | 626af417d334896000f5e23e6305f29242945948 | 10.53.52.144:6373\r\nd69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 | 10.53.52.144:6378 | 9 | 9 | 10.53.52.144:6378 (9) | 10.53.52.144:6375 (1) | 95e75b159b3bcb7c33a8df9db93b5443413dfccb | 10.53.52.144:6377\r\n0c92e0b02f839260b508a6fdf258539a26e96903 | 10.53.52.144:6376 | 10 | 10 | 10.53.52.144:6378 (9) | 10.53.52.144:6376 (10) | 900e489fcf28e0648272fc6c8b61f0bd998a01a0 | 10.53.52.144:6371\r\nf567e9d0fb1ccbdfbfc6a5c505e1d84d224de0ef | 10.53.52.144:6374 | 3 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6375 (1) | f65353c097d7f7e0f44644a28ddb980c21fc37bf | 10.53.52.144:6380\r\n**2ddaf93f230facc9893bc95bf9234a3d580d0291** | **10.53.52.144:6372**  | **9** | **9** | 10.53.52.144:6379 (5) | **10.53.52.144:6372 (9)** | **c013fefabf931b77a5488e4e0fe1e81db1eaf4aa** | **10.53.52.144:6375**\r\n\r\n\r\n**5. After other nodes receive `PONG` send by `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372`**\r\n\r\nmaster nodeid | master node | config epoch | current epoch | slot 0 owner (configEpoch)  | slot 16383 owner (configEpoch)  | slave nodeid | slave node\r\n-- |-- | -- | -- | -- | -- | -- | --\r\n6ea4c2f8e7efe9180db83cf0adf1b055a557c74d | 10.53.52.144:6379 | 5 | **9** | 10.53.52.144:6379 (5) | **10.53.52.144:6372 (9)** | 626af417d334896000f5e23e6305f29242945948 | 10.53.52.144:6373\r\nd69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 | 10.53.52.144:6378 | 9 | 9 | 10.53.52.144:6378 (9) | **10.53.52.144:6372 (9)** | 95e75b159b3bcb7c33a8df9db93b5443413dfccb | 10.53.52.144:6377\r\n0c92e0b02f839260b508a6fdf258539a26e96903 | 10.53.52.144:6376 | 10 | 10 | 10.53.52.144:6378 (9) | 10.53.52.144:6376 (10) | 900e489fcf28e0648272fc6c8b61f0bd998a01a0 | 10.53.52.144:6371\r\nf567e9d0fb1ccbdfbfc6a5c505e1d84d224de0ef | 10.53.52.144:6374 | 3 | **9** | 10.53.52.144:6379 (5) | **10.53.52.144:6372 (9)** | f65353c097d7f7e0f44644a28ddb980c21fc37bf | 10.53.52.144:6380\r\n2ddaf93f230facc9893bc95bf9234a3d580d0291 | 10.53.52.144:6372  | 9 | 9 | 10.53.52.144:6379 (5) | 10.53.52.144:6372 (9) | c013fefabf931b77a5488e4e0fe1e81db1eaf4aa | 10.53.52.144:6375\r\n\r\n\r\n**6. After `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372` receiving `PONG` send by `0c92e0b02f839260b508a6fdf258539a26e96903 10.53.52.144:6376`**\r\n\r\nmaster nodeid | master node | config epoch | current epoch | slot 0 owner (configEpoch)  | slot 16383 owner (configEpoch)  | slave nodeid | slave node\r\n-- |-- | -- | -- | -- | -- | -- | --\r\n6ea4c2f8e7efe9180db83cf0adf1b055a557c74d | 10.53.52.144:6379 | 5 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6372 (9) | 626af417d334896000f5e23e6305f29242945948 | 10.53.52.144:6373\r\nd69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 | 10.53.52.144:6378 | 9 | 9 | 10.53.52.144:6378 (9) | 10.53.52.144:6372 (9) | 95e75b159b3bcb7c33a8df9db93b5443413dfccb | 10.53.52.144:6377\r\n0c92e0b02f839260b508a6fdf258539a26e96903 | 10.53.52.144:6376 | 10 | 10 | 10.53.52.144:6378 (9) | 10.53.52.144:6376 (10) | 900e489fcf28e0648272fc6c8b61f0bd998a01a0 | 10.53.52.144:6371\r\nf567e9d0fb1ccbdfbfc6a5c505e1d84d224de0ef | 10.53.52.144:6374 | 3 | 8 | 10.53.52.144:6379 (5) | 10.53.52.144:6372 (9) | f65353c097d7f7e0f44644a28ddb980c21fc37bf | 10.53.52.144:6380\r\n2ddaf93f230facc9893bc95bf9234a3d580d0291 | 10.53.52.144:6372  | 9 | **10** | **10.53.52.144:6378 (9)** | **10.53.52.144:6376 (10)** | c013fefabf931b77a5488e4e0fe1e81db1eaf4aa | 10.53.52.144:6375\r\n\r\n**7. `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372` resolve configEpoch Collision and send `PONG` msg**\r\n\r\n- configEpoch Collision between `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372` and `d69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 10.53.52.144:6378`\r\n- `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372` has smaller Node ID\r\n\r\n_slot 0 and 16383 ownership on other nodes will not be affected because `myslots` in `clusterMsg` header of `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372` don't have those two slots._\r\n\r\nmaster nodeid | master node | config epoch | current epoch | slot 0 owner (configEpoch)  | slot 16383 owner (configEpoch)  | slave nodeid | slave node\r\n-- |-- | -- | -- | -- | -- | -- | --\r\n6ea4c2f8e7efe9180db83cf0adf1b055a557c74d | 10.53.52.144:6379 | 5 | 11 | 10.53.52.144:6379 (5) | 10.53.52.144:6372 (**11**) | 626af417d334896000f5e23e6305f29242945948 | 10.53.52.144:6373\r\nd69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 | 10.53.52.144:6378 | 9 | 11 | 10.53.52.144:6378 (9) | 10.53.52.144:6372 (**11**) | 95e75b159b3bcb7c33a8df9db93b5443413dfccb | 10.53.52.144:6377\r\n0c92e0b02f839260b508a6fdf258539a26e96903 | 10.53.52.144:6376 | 10 | 11 | 10.53.52.144:6378 (9) | 10.53.52.144:6376 (10) | 900e489fcf28e0648272fc6c8b61f0bd998a01a0 | 10.53.52.144:6371\r\nf567e9d0fb1ccbdfbfc6a5c505e1d84d224de0ef | 10.53.52.144:6374 | 3 | 11 | 10.53.52.144:6379 (5) | 10.53.52.144:6372 (**11**) | f65353c097d7f7e0f44644a28ddb980c21fc37bf | 10.53.52.144:6380\r\n2ddaf93f230facc9893bc95bf9234a3d580d0291 | 10.53.52.144:6372  | **11** | **11** | 10.53.52.144:6378 (9) | 10.53.52.144:6376 (10) | c013fefabf931b77a5488e4e0fe1e81db1eaf4aa | 10.53.52.144:6375\r\n\r\n\r\n**8. `0c92e0b02f839260b508a6fdf258539a26e96903  10.53.52.144:6376` is reset by `CLUSTERMSG_TYPE_UPDATE` message**\r\n\r\n- `0c92e0b02f839260b508a6fdf258539a26e96903  10.53.52.144:6376`: _slot 16383 ownership is reset because `0c92e0b02f839260b508a6fdf258539a26e96903  10.53.52.144:6376` [node's configEpoch(`senderConfigEpoch`) 10 which is smaller than `server.cluster->slots[j]->configEpoch` 11](https://github.com/redis/redis/blob/unstable/src/cluster.c#L2942), then other nodes will find it has an old configurations and send `CLUSTERMSG_TYPE_UPDATE` message to `0c92e0b02f839260b508a6fdf258539a26e96903  10.53.52.144:6376` ask it to reset the ownership to `6372`, and finally succeed because slot 16383 is guarded by configEpoch 10 is smaller than senderConfigEpoch\\(11, [`it is not actually the \"Sender\" of the information`](https://github.com/redis/redis/blob/unstable/src/cluster.c#L2213), so it's 11 according by slot\\)_\r\n\r\n- `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372`: _slot 16383 ownership is not reset because_ \r\n\r\n     1. `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372` [not claimed slot 16383](https://github.com/redis/redis/blob/unstable/src/cluster.c#L2939)\r\n     2. `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372` node's [configEpoch(`senderConfigEpoch`) 11 which is equal to `server.cluster->slots[j]->configEpoch` 11](https://github.com/redis/redis/blob/unstable/src/cluster.c#L2942)\r\n     \r\n     _so there is no `CLUSTERMSG_TYPE_UPDATE` message. And also, There is no Sender claim slot 16383 whose configEpoch is greater than 10, so there is no `clusterUpdateSlotsConfigWith` through `CLUSTERMSG_TYPE_PING/PONG`... messages_\r\n\r\nmaster nodeid | master node | config epoch | current epoch | slot 0 owner (configEpoch)   | slot 16383 owner (configEpoch)  | slave nodeid | slave node\r\n-- |-- | -- | -- | -- | -- | -- | --\r\n6ea4c2f8e7efe9180db83cf0adf1b055a557c74d | 10.53.52.144:6379 | 5 | 11 | 10.53.52.144:6379 (5) | 10.53.52.144:6372 (11) | 626af417d334896000f5e23e6305f29242945948 | 10.53.52.144:6373\r\nd69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 | 10.53.52.144:6378 | 9 | 11 | 10.53.52.144:6378 (9) | 10.53.52.144:6372 (11) | 95e75b159b3bcb7c33a8df9db93b5443413dfccb | 10.53.52.144:6377\r\n0c92e0b02f839260b508a6fdf258539a26e96903 | 10.53.52.144:6376 | 10 | 11 | 10.53.52.144:6378 (9) | **10.53.52.144:6372 (11)** | 900e489fcf28e0648272fc6c8b61f0bd998a01a0 | 10.53.52.144:6371\r\nf567e9d0fb1ccbdfbfc6a5c505e1d84d224de0ef | 10.53.52.144:6374 | 3 | 11 | 10.53.52.144:6379 (5) | 10.53.52.144:6372 (11) | f65353c097d7f7e0f44644a28ddb980c21fc37bf | 10.53.52.144:6380\r\n2ddaf93f230facc9893bc95bf9234a3d580d0291 | 10.53.52.144:6372  | 11 | 11 | 10.53.52.144:6378 (9) | 10.53.52.144:6376 (10) | c013fefabf931b77a5488e4e0fe1e81db1eaf4aa | 10.53.52.144:6375\r\n\r\n\r\n**9. Slot 0 finished Gossip among cluster**\r\n\r\nmaster nodeid | master node | config epoch | current epoch | slot 0 owner (configEpoch)  | slot 16383 owner (configEpoch)  | slave nodeid | slave node\r\n-- |-- | -- | -- | -- | -- | -- | --\r\n6ea4c2f8e7efe9180db83cf0adf1b055a557c74d | 10.53.52.144:6379 | 5 | 11 | 10.53.52.144:6378 (9) | 10.53.52.144:6372 (11) | 626af417d334896000f5e23e6305f29242945948 | 10.53.52.144:6373\r\nd69ae698cb4ec7af73e8c15eb64d6f8b6cde4f59 | 10.53.52.144:6378 | 9 | 11 | 10.53.52.144:6378 (9) | 10.53.52.144:6372 (11) | 95e75b159b3bcb7c33a8df9db93b5443413dfccb | 10.53.52.144:6377\r\n0c92e0b02f839260b508a6fdf258539a26e96903 | 10.53.52.144:6376 | 10 | 11 | 10.53.52.144:6378 (9) | 10.53.52.144:6372 (11) | 900e489fcf28e0648272fc6c8b61f0bd998a01a0 | 10.53.52.144:6371\r\nf567e9d0fb1ccbdfbfc6a5c505e1d84d224de0ef | 10.53.52.144:6374 | 3 | 11 | 10.53.52.144:6378 (9) | 10.53.52.144:6372  (11)| f65353c097d7f7e0f44644a28ddb980c21fc37bf | 10.53.52.144:6380\r\n2ddaf93f230facc9893bc95bf9234a3d580d0291 | 10.53.52.144:6372  | 11 | 11 | 10.53.52.144:6378 (9) | 10.53.52.144:6376 (10) | c013fefabf931b77a5488e4e0fe1e81db1eaf4aa | 10.53.52.144:6375\r\n\r\n#### Result\r\nThe cluster has **inconsistent slots mapping about slot 16383, between `2ddaf93f230facc9893bc95bf9234a3d580d0291 10.53.52.144:6372` and other nodes**",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-21T08:01:41Z",
        "closed_at": "2023-06-21T14:57:11Z",
        "merged_at": "2023-06-21T14:57:11Z",
        "body": "Now we can see something like this:\r\n```\r\nFatal: Can't initialize Background Jobs. Error message: Cannot allocate memory\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-06-20T03:39:42Z",
        "closed_at": "2023-06-20T08:15:41Z",
        "merged_at": "2023-06-20T08:15:41Z",
        "body": "The parameter name is WITHSCORE instead of WITHSCORES.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 24,
        "changed_files": 3,
        "created_at": "2023-06-20T00:12:52Z",
        "closed_at": "2023-06-21T01:00:55Z",
        "merged_at": "2023-06-21T01:00:55Z",
        "body": "Resolves issue with nodename test as seen in https://github.com/redis/redis/actions/runs/5313162983/jobs/9618669303?pr=12159.\r\n\r\nTo determine when everything was stable, we couldn't just query the nodename since they aren't API visible by design. Instead, we were using a proxy piece of information which was bumping the epoch and waiting for everyone to observe that. This works for making source Node 0 and Node 1 had pinged, and Node 0 and Node 2 had pinged, but did not guarantee that Node 1 and Node 2 had pinged. Although unlikely, this can cause this failure message. To fix it I hijacked hostnames and used its validation that it has been propagated, since we know that it is stable.\r\n\r\nI also noticed while stress testing this sometimes the test took almost 4.5 seconds to finish, which is really close to the current 5 second limit of the log check, so I bumped that up as well just to make it a bit more consistent.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-19T09:19:21Z",
        "closed_at": "2023-06-20T08:15:11Z",
        "merged_at": "2023-06-20T08:15:11Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 134,
        "deletions": 5,
        "changed_files": 7,
        "created_at": "2023-06-18T13:36:35Z",
        "closed_at": "2023-06-20T17:41:42Z",
        "merged_at": "2023-06-20T17:41:42Z",
        "body": "When a connection that's subscribe to a channel emits PUBLISH inside MULTI-EXEC, the push notification messes up the EXEC response.\r\n\r\ne.g. MULTI, PING, PUSH foo bar, PING, EXEC\r\nthe EXEC's response will contain: PONG, {message foo bar}, 1. and the second PONG will be delivered outside the EXEC's response.\r\n\r\nAdditionally, this PR changes the order of responses in case of a plain PUBLISH (when the current client also subscribed to it), by delivering the push after the command's response instead of before it.\r\nThis also affects modules calling RM_PublishMessage in a similar way, so that we don't run the risk of getting that push mixed together with the module command's response.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-18T09:35:52Z",
        "closed_at": "2023-06-20T00:13:18Z",
        "merged_at": "2023-06-20T00:13:18Z",
        "body": "auxHumanNodenameGetter limited to %.40s, since we did not limit the\r\nlength of config cluster-announce-human-nodename, %.40s will cause\r\nnodename data loss (we will persist it in nodes.conf).\r\n\r\nAdditional modified auxHumanNodenamePresent to use sdslen.\r\n\r\nIntroduced in #9564.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-06-16T10:06:08Z",
        "closed_at": "2023-06-16T12:39:00Z",
        "merged_at": "2023-06-16T12:39:00Z",
        "body": "In #12301, we observed that if the\r\n`while(listLength(server.ready_keys) != 0)`\r\nin handleClientsBlockedOnKeys is changed to\r\n`if(listLength(server.ready_keys) != 0)`,\r\nthe order of command execution will change.\r\n\r\nIt is wrong to change that. It means that if a command\r\nbeing unblocked causes another command to get unblocked\r\n(like a BLMOVE would do), then the new unblocked command\r\nwill wait for later to get processed rather than right away.\r\n\r\nIt'll not have any real implication if we change that since\r\nwe do call handleClientsBlockedOnKeys in beforeSleep again,\r\nand redis will still behave correctly, but we don't change that.\r\n\r\nAn example:\r\n1. $rd1 blmove src{t} dst{t} left right 0\r\n2. $rd2 blmove dst{t} src{t} right left 0\r\n3. $rd3 set key1{t}, $rd3 lpush src{t}, $rd3 set key2{t} in a pipeline\r\n\r\nThe correct order would be:\r\n1. set key1{t}\r\n2. lpush src{t}\r\n3. lmove src{t} dst{t} left right\r\n4. lmove dst{t} src{t} right left\r\n5. set key2{t}\r\n\r\nThe wrong order would be:\r\n1. set key1{t}\r\n2. lpush src{t}\r\n3. lmove src{t} dst{t} left right\r\n4. set key2{t}\r\n5. lmove dst{t} src{t} right left\r\n\r\nThis PR adds corresponding test to cover it.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-06-15T20:42:09Z",
        "closed_at": "2023-06-20T17:58:27Z",
        "merged_at": null,
        "body": "It display same error \" ERR LIMIT can't be negative\" for string and out of range.\r\nChanging the error msg so that it can be applicable to all the cases, i.e negative integer, string and outofrange\r\n\r\n**Before** \r\n```\r\n127.0.0.1:6379> ZINTERCARD 2 z1 z2 LIMIT a\r\n(error) ERR LIMIT can't be negative\r\n127.0.0.1:6379> ZINTERCARD 2 z1 z2 LIMIT 1111111111111111111111111111111111111111111111111111111111\r\n(error) ERR LIMIT can't be negative\r\n127.0.0.1:6379> sintercard 2 s1 s2 LIMIT a\r\n(error) ERR LIMIT can't be negative\r\n127.0.0.1:6379> sintercard 2 s1 s2 LIMIT 1111111111111111111111111111111111111111111111111111111111\r\n(error) ERR LIMIT can't be negative\r\n127.0.0.1:6379>\r\n```\r\n\r\n**After**\r\n```\r\n127.0.0.1:6379> ZINTERCARD 2 z1 z2 LIMIT a\r\n(error) ERR LIMIT is not a positive integer or out of range\r\n127.0.0.1:6379>  ZINTERCARD 2 z1 z2 LIMIT 1111111111111111111111111111111111111111111111111111111111\r\n(error) ERR LIMIT is not a positive integer or out of range\r\n127.0.0.1:6379>  sintercard 2 s1 s2 LIMIT a\r\n(error) ERR LIMIT is not a positive integer or out of range\r\n127.0.0.1:6379> sintercard 2 s1 s2 LIMIT 1111111111111111111111111111111111111111111111111111111111\r\n(error) ERR LIMIT is not a positive integer or out of range\r\n127.0.0.1:6379>\r\n```",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-06-15T18:15:06Z",
        "closed_at": "2023-06-20T07:07:29Z",
        "merged_at": "2023-06-20T07:07:29Z",
        "body": "Observed that the sanitizer reported memory leak as clean up is not done before the process termination in negative/following cases:\r\n\r\n**- when we passed '--invalid' as option to redis-server.**\r\n\r\n```\r\n -vm:~/mem-leak-issue/redis$ ./src/redis-server --invalid\r\n\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 2\r\n>>> 'invalid'\r\nBad directive or wrong number of arguments\r\n\r\n=================================================================\r\n==865778==ERROR: LeakSanitizer: detected memory leaks\r\n\r\nDirect leak of 8 byte(s) in 1 object(s) allocated from:\r\n    #0 0x7f0985f65867 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cpp:145\r\n    #1 0x558ec86686ec in ztrymalloc_usable_internal /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:117\r\n    #2 0x558ec86686ec in ztrymalloc_usable /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:135\r\n    #3 0x558ec86686ec in ztryrealloc_usable_internal /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:276\r\n    #4 0x558ec86686ec in zrealloc /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:327\r\n    #5 0x558ec865dd7e in sdssplitargs /home/ubuntu/mem-leak-issue/redis/src/sds.c:1172\r\n    #6 0x558ec87a1be7 in loadServerConfigFromString /home/ubuntu/mem-leak-issue/redis/src/config.c:472\r\n    #7 0x558ec87a13b3 in loadServerConfig /home/ubuntu/mem-leak-issue/redis/src/config.c:718\r\n    #8 0x558ec85e6f15 in main /home/ubuntu/mem-leak-issue/redis/src/server.c:7258\r\n    #9 0x7f09856e5d8f in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58\r\n\r\nSUMMARY: AddressSanitizer: 8 byte(s) leaked in 1 allocation(s).\r\n\r\n```\r\n\r\n**- when we pass '--port' as option and missed to add port number to redis-server.**\r\n\r\n```\r\nvm:~/mem-leak-issue/redis$ ./src/redis-server --port\r\n\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 2\r\n>>> 'port'\r\nwrong number of arguments\r\n\r\n=================================================================\r\n==865846==ERROR: LeakSanitizer: detected memory leaks\r\n\r\nDirect leak of 8 byte(s) in 1 object(s) allocated from:\r\n    #0 0x7fdcdbb1f867 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cpp:145\r\n    #1 0x557e8b04f6ec in ztrymalloc_usable_internal /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:117\r\n    #2 0x557e8b04f6ec in ztrymalloc_usable /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:135\r\n    #3 0x557e8b04f6ec in ztryrealloc_usable_internal /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:276\r\n    #4 0x557e8b04f6ec in zrealloc /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:327\r\n    #5 0x557e8b044d7e in sdssplitargs /home/ubuntu/mem-leak-issue/redis/src/sds.c:1172\r\n    #6 0x557e8b188be7 in loadServerConfigFromString /home/ubuntu/mem-leak-issue/redis/src/config.c:472\r\n    #7 0x557e8b1883b3 in loadServerConfig /home/ubuntu/mem-leak-issue/redis/src/config.c:718\r\n    #8 0x557e8afcdf15 in main /home/ubuntu/mem-leak-issue/redis/src/server.c:7258\r\n    #9 0x7fdcdb29fd8f in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58\r\n\r\nIndirect leak of 10 byte(s) in 1 object(s) allocated from:\r\n    #0 0x7fdcdbb1fc18 in __interceptor_realloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cpp:164\r\n    #1 0x557e8b04f9aa in ztryrealloc_usable_internal /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:287\r\n    #2 0x557e8b04f9aa in ztryrealloc_usable /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:317\r\n    #3 0x557e8b04f9aa in zrealloc_usable /home/ubuntu/mem-leak-issue/redis/src/zmalloc.c:342\r\n    #4 0x557e8b033f90 in _sdsMakeRoomFor /home/ubuntu/mem-leak-issue/redis/src/sds.c:271\r\n    #5 0x557e8b033f90 in sdsMakeRoomFor /home/ubuntu/mem-leak-issue/redis/src/sds.c:295\r\n    #6 0x557e8b033f90 in sdscatlen /home/ubuntu/mem-leak-issue/redis/src/sds.c:486\r\n    #7 0x557e8b044e1f in sdssplitargs /home/ubuntu/mem-leak-issue/redis/src/sds.c:1165\r\n    #8 0x557e8b188be7 in loadServerConfigFromString /home/ubuntu/mem-leak-issue/redis/src/config.c:472\r\n    #9 0x557e8b1883b3 in loadServerConfig /home/ubuntu/mem-leak-issue/redis/src/config.c:718\r\n    #10 0x557e8afcdf15 in main /home/ubuntu/mem-leak-issue/redis/src/server.c:7258\r\n    #11 0x7fdcdb29fd8f in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58\r\n\r\nSUMMARY: AddressSanitizer: 18 byte(s) leaked in 2 allocation(s).\r\n\r\n```\r\n\r\nAs part analysis found that the sdsfreesplitres is not called when this condition checks are being hit.\r\n\r\nOutput after the fix:\r\n\r\n\r\n```\r\nvm:~/mem-leak-issue/redis$ ./src/redis-server --invalid\r\n\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 2\r\n>>> 'invalid'\r\nBad directive or wrong number of arguments\r\nvm:~/mem-leak-issue/redis$\r\n\r\n===========================================\r\nvm:~/mem-leak-issue/redis$ ./src/redis-server --jdhg\r\n\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 2\r\n>>> 'jdhg'\r\nBad directive or wrong number of arguments\r\n\r\n---------------------------------------------------------------------------\r\nvm:~/mem-leak-issue/redis$ ./src/redis-server --port\r\n\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 2\r\n>>> 'port'\r\nwrong number of arguments\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2023-06-15T13:47:02Z",
        "closed_at": "2023-06-20T17:44:43Z",
        "merged_at": "2023-06-20T17:44:43Z",
        "body": "Introduced by https://github.com/redis/redis/pull/11923 (Redis 7.2 RC2)\r\n\r\nIt's very weird and counterintuitive that `RM_ReplyWithError` requires the error-code **without** a hyphen while `RM_ReplyWithErrorFormat` requires either the error-code **with** a hyphen or no error-code at all\r\n```\r\nRedisModule_ReplyWithError(ctx, \"BLA bla bla\");\r\n```\r\nvs.\r\n```\r\nRedisModule_ReplyWithErrorFormat(ctx, \"-BLA %s\", \"bla bla\");\r\n```\r\n\r\nThis commit aligns RM_ReplyWithErrorFormat to behvae like RM_ReplyWithError.\r\nit's a breaking changes but it's done before 7.2 goes GA.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 57,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2023-06-15T12:48:03Z",
        "closed_at": "2023-06-16T15:14:12Z",
        "merged_at": "2023-06-16T15:14:11Z",
        "body": "In SPOP, when COUNT is greater than or equal to set's size,\r\nwe will remove the set. In dbDelete, we will do DEL or UNLINK\r\naccording to the lazy flag. This is also required for propagate.\r\n\r\nIn RESTORE, we won't store expired keys into the db, see #7472.\r\nWhen used together with REPLACE, it should emit a DEL or UNLINK\r\naccording to the lazy flag.\r\n\r\nThis PR also adds tests to cover the propagation. The RESTORE\r\ntest will also cover #7472.\r\n\r\n```\r\nRelease notes (Bug fix or minor improvement)\r\nUpdate SPOP and RESTORE commands to replicate unlink commands to replicas when the server is configured to use async server deletes.\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 21,
        "changed_files": 1,
        "created_at": "2023-06-15T07:18:51Z",
        "closed_at": "2023-06-16T12:55:25Z",
        "merged_at": "2023-06-16T12:55:25Z",
        "body": "In 4ba47d2d2 the following tests added in both tracking.tcl and introspection.tcl\r\n\r\n- Coverage: Basic CLIENT CACHING\r\n- Coverage: Basic CLIENT REPLY\r\n- Coverage: Basic CLIENT TRACKINGINFO\r\n- Coverage: Basic CLIENT GETREDIR",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-06-14T14:06:37Z",
        "closed_at": "2023-06-15T07:07:48Z",
        "merged_at": "2023-06-15T07:07:48Z",
        "body": "In PXAT case, there is no need to do the rewriteClientCommandVector,\r\na simply benchmark show we gain a improvement of 10%.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-14T09:38:52Z",
        "closed_at": "2023-06-18T07:30:23Z",
        "merged_at": "2023-06-18T07:30:23Z",
        "body": "Apparently for large size classes Jemalloc allocate some extra memory (can be up to 25% overhead for allocations of 16kb). see https://github.com/jemalloc/jemalloc/issues/1098#issuecomment-1589870476\r\n\r\np.s. from Redis's perspective that looks like external fragmentation, (i.e. allocated bytes will be low, and active pages bytes will be large) which  can cause active-defrag to eat CPU cycles in vain.\r\n\r\nSome details about this mechanism we disable:\r\n---------------------------------------------------------------\r\nDisabling this mechanism only affects large allocations (above 16kb)\r\nNot only that it isn't expected to cause any performance regressions, it's actually recommended, unless you have a specific workload pattern and hardware that benefit from this feature -- by default it's enabled and adds address randomization to all large buffers, by over allocating 1 page per large size class, and offsetting into that page to make the starting address of the user buffer randomized. Workloads such as scientific computation often handle multiple big matrixes at the same time, and the randomization makes sure that the cacheline level accesses don't suffer bad conflicts (when they all start from page-aligned addresses).\r\n\r\nHowever the downsize is also quite noticeable, like you observed that extra page per large size can cause memory overhead, plus the extra TLB entry. The other factor is, hardware in the last few years started doing the randomization at the hardware level, i.e. the address to cacheline mapping isn't a direct mapping anymore. So there's debate to disable the randomization by default, but we are still hesitant because when it matters, it could matter a lot, and having it enabled by default limits that worst case behavior, even though it means the majority of workloads suffers a regression.\r\n\r\nSo in short, it's safe and offers better performance in most cases.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-06-14T01:15:40Z",
        "closed_at": "2023-06-19T00:28:44Z",
        "merged_at": null,
        "body": "Hi!\r\n\r\nWe don't have to check if the number of elements inside the listpack exceeds a certain amount if we don't actually append anything. So only do the check if we do insert something.\r\n\r\nThank you",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-13T15:08:27Z",
        "closed_at": "2023-06-14T07:03:34Z",
        "merged_at": "2023-06-14T07:03:34Z",
        "body": "Looks like the Zadd test case was copied to create Zincrby test case ,but missed to change the command.\r\n\r\n**Before :**\r\n test \"ZSET element can't be set to NaN with ZADD - $encoding\" {\r\n            assert_error \"*not*float*\" {r zadd myzset nan abc}\r\n }\r\n test \"ZSET element can't be set to NaN with **ZINCRBY** - $encoding\" {\r\n            assert_error \"*not*float*\" {r **zadd** myzset nan abc}\r\n }\r\n\r\n**After.**\r\n test \"ZSET element can't be set to NaN with ZADD - $encoding\" {\r\n            assert_error \"*not*float*\" {r zadd myzset nan abc}\r\n }\r\n test \"ZSET element can't be set to NaN with **ZINCRBY** - $encoding\" {\r\n            assert_error \"*not*float*\" {r **zincrby** myzset nan abc}\r\n }",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-12T10:28:11Z",
        "closed_at": "2023-06-15T07:10:23Z",
        "merged_at": "2023-06-15T07:10:23Z",
        "body": "Fix #11643\r\n\r\nWhile Redis loading data from disk (AOF or RDB), modules will get key space notifications. In such stage the module should not register any PEJ, the main reason this is forbidden is that PEJ purpose is to perform a write operation as a reaction to the key space notification. Write operations should not be performed while loading data and so there is no reason to register a PEJ. \r\n\r\nSame argument also apply to readonly replica. module should not perform any writes as a reaction to key space notifications and so it should not register a PEJ.\r\n\r\nIf a module need to perform some other task which is not involve writing, he can do it on the key space notification callback itself.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-06-12T00:57:10Z",
        "closed_at": "2023-06-13T10:27:05Z",
        "merged_at": "2023-06-13T10:27:05Z",
        "body": "For the XREADGROUP BLOCK > scenario, there is an endless loop.\r\nDue to #11012, XREADGROUP with \">\" may repeatedly reprocess command and blockForKeys.\r\n\r\nThe right fix is to avoid an endless loop in handleClientsBlockedOnKey and handleClientsBlockedOnKeys,\r\nlooks like there was some attempt in handleClientsBlockedOnKeys but maybe not sufficiently good,\r\nand it looks like using a similar trick in handleClientsBlockedOnKey is complicated.\r\ni.e. stashing the list on the stack and iterating on it after creating a fresh one for future use,\r\nis problematic since the code keeps accessing the global list.\r\n\r\nThe fix is proposed by oranagra.\r\n\r\nFixes #12290",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 38,
        "changed_files": 9,
        "created_at": "2023-06-11T09:43:20Z",
        "closed_at": "2023-06-19T13:31:19Z",
        "merged_at": "2023-06-19T13:31:18Z",
        "body": "In the original implementation, the time complexity of the commands\r\nis actually O(N*M), where N is the number of patterns the client is\r\nalready subscribed and M is the number of patterns to subscribe to.\r\nThe docs are all wrong about this.\r\n\r\nSpecifically, because the original client->pubsub_patterns is a list,\r\nso we need to do listSearchKey which is O(N). In this PR, we change it\r\nto a dict, so the search becomes O(1).\r\n\r\nAt the same time, both pubsub_channels and pubsubshard_channels are dicts.\r\nChanging pubsub_patterns to a dictionary improves the readability and\r\nmaintainability of the code.\r\n\r\nA simple benchmark test show:\r\n```\r\nredis-benchmark -r  100000 -n 10000 punsubscribe __rand_int__ `python -c \"print(' '.join([str(i) for i in range(400)]))\"`\r\n```\r\n\r\nunstable:\r\n```\r\nSummary:\r\n  throughput summary: 472.41 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n      105.535     5.560   103.423   125.119   152.319   189.055\r\n```\r\n\r\nthis branch:\r\n```\r\nSummary:\r\n  throughput summary: 3880.48 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n       12.830     4.056    12.439    15.079    20.015    46.207\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-06-10T06:38:29Z",
        "closed_at": "2023-06-11T01:11:17Z",
        "merged_at": "2023-06-11T01:11:17Z",
        "body": "This leak will only happen in loadServerConfigFromString,\r\nthat is, when we are loading a redis.conf, and the user is wrong.\r\n\r\nBecause it happens in loadServerConfigFromString, redis will\r\nexit if there is an error, so this is actually just a cleanup.\r\n\r\nFixes #12293",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-08T19:51:17Z",
        "closed_at": "2023-06-11T05:36:27Z",
        "merged_at": "2023-06-11T05:36:27Z",
        "body": "Added missing test case coverage for below senarios\r\n![image](https://github.com/redis/redis/assets/51993843/408b6d1b-ebcc-4ddf-b20c-8b47a3f8103c)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-06-07T14:30:54Z",
        "closed_at": "2023-08-16T07:54:38Z",
        "merged_at": "2023-08-16T07:54:37Z",
        "body": "In the past we hardcoded it to 20, causing it to not count keys\r\nfor more databases.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-07T10:59:51Z",
        "closed_at": "2023-06-08T12:13:54Z",
        "merged_at": "2023-06-08T12:13:54Z",
        "body": "We now no longer propagate scripts (started from 7.0), so this is a\r\nvery rare issue that in nearly-dead-code.\r\n\r\nThis is an overlook in #9780",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 147,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2023-06-07T06:04:40Z",
        "closed_at": "2023-06-16T16:13:08Z",
        "merged_at": "2023-06-16T16:13:08Z",
        "body": "## Issue\r\nWhen a dict has a long chain or the length of the chain is longer than the number of samples, we will never be able to sample the elements at the end of the chain using dictGetSomeKeys().\r\nThis means that there's an unfair random sampling affecting: RANDOMKEY, SRANDMEMBER, SPOP, ZRANDMEMBER, HRANDFIELD, and the eviction mechanism.\r\n\r\n## Severe issue\r\nThis could mean that SRANDMEMBER, ZRANDMEMBER and HRANDFIELD (when used with `<count>`) can be hang in and endless loop.\r\nThe most severe case, is the pathological case of when someone uses SCAN+DEL or SSCAN+SREM creating an unevenly distributed dict.\r\n\r\n## Recent regression\r\nThe above was amplified by the recent change in #11692 which prevented a down-sizing rehashing while there is a fork.\r\nSo in a pathological case of SSCAN+SREM causing an uneven hash table, followed by down-sizing of the hash table, if the rehashing would be suspended during a fork, and then calling SRANDMEMBER with `<count>` can lead to a hang.\r\n\r\n## Solution\r\n1. Before, we will stop sampling when we reach the maximum number of samples, even if there is more data after the current chain.\r\nNow when we reach the maximum we use the Reservoir Sampling algorithm to fairly sample the end of the chain that cannot be sampled\r\n2. Fix the rehashing code, so that the same as it allows rehashing for up-sizing during fork when the ratio is extreme, it will allow it for down-sizing as well.\r\n\r\nFix #12200",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-06-06T21:13:35Z",
        "closed_at": "2023-06-20T09:50:04Z",
        "merged_at": "2023-06-20T09:50:04Z",
        "body": "For geosearch and georadius we have already test coverage for wrong type, but we dont have for geodist, geohash, geopos commands. So adding the wrong type test cases for geodist, geohash, geopos commands.\r\n\r\nExisting code, we have verify_geo_edge_response_bymember function for wrong type test cases which has member as an option. But the function is being called in other test cases where the output is not inline with these commnds(geodist, geohash, geopos). So I could not include these commands(geodist, geohash, geopos) as part of existing function, hence implemented a new function verify_geo_edge_response_generic and called from the test case.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-06-06T16:10:38Z",
        "closed_at": "2023-07-20T22:31:07Z",
        "merged_at": "2023-07-20T22:31:07Z",
        "body": "See #12266 \r\n\r\n\r\nWhen the redis server cluster running on `cluster-preferred-endpoint-type unknown-endpoint` mode, and receive a request that should be redirected to another redis server node, it does not reply the hostip, but a empty host like `MOVED 3999 :6381`. \r\n\r\nThis is because cluster server choose \"\" as `PreferredEndpoint` [here](https://github.com/redis/redis/blob/unstable/src/cluster.c#L5249)\r\n```\r\nconst char *getPreferredEndpoint(clusterNode *n) {\r\n    switch(server.cluster_preferred_endpoint_type) {\r\n    case CLUSTER_ENDPOINT_TYPE_IP: return n->ip;\r\n    case CLUSTER_ENDPOINT_TYPE_HOSTNAME: return (sdslen(n->hostname) != 0) ? n->hostname : \"?\";\r\n    case CLUSTER_ENDPOINT_TYPE_UNKNOWN_ENDPOINT: return \"\";\r\n    }\r\n    return \"unknown\";\r\n}\r\n```\r\n\r\nAnd then, the redis-cli would try to connect to an address without a host, which cause the issue:\r\n```\r\n127.0.0.1:7002> set bar bar\r\n-> Redirected to slot [5061] located at :7000\r\nCould not connect to Redis at :7000: No address associated with hostname\r\nCould not connect to Redis at :7000: No address associated with hostname\r\nnot connected> exit\r\n```\r\n\r\nIn this case, the redis-cli should use the previous hostip when there's no host provided by the server.\r\n\r\n```\r\nRelease notes (Tooling):\r\nAllow redis-cli in cluster mode to handle `unknown-endpoint`.\r\n```",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2023-06-05T21:32:11Z",
        "closed_at": "2023-06-13T15:16:32Z",
        "merged_at": "2023-06-13T15:16:32Z",
        "body": "It would be helpful for clients to get cluster slots/shards information during a node failover and is loading data.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 36,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-04T00:26:08Z",
        "closed_at": "2023-06-04T13:30:05Z",
        "merged_at": null,
        "body": "The new code is significantly faster and its speed is not affected by the value of dictionary size.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 476,
        "deletions": 492,
        "changed_files": 5,
        "created_at": "2023-06-02T16:01:21Z",
        "closed_at": "2023-09-28T06:21:23Z",
        "merged_at": "2023-09-28T06:21:23Z",
        "body": "In a long printf call with many placeholders, it's hard to see which argument belongs to which placeholder.\r\n\r\nThe long printf-like calls in the INFO and CLIENT commands are rewritten into pairs of (format, argument). These pairs are then rewritten to a single call with a long format string and a long list of arguments, using a macro called FMTARGS.\r\n\r\nThe file `fmtargs.h` is added to the repo.\r\n\r\nCo-Authored-By: @madolson",
        "comments": 22
    },
    {
        "merged": true,
        "additions": 2561,
        "deletions": 308,
        "changed_files": 42,
        "created_at": "2023-06-01T10:57:05Z",
        "closed_at": "2023-06-06T03:40:44Z",
        "merged_at": "2023-06-06T03:40:44Z",
        "body": "Fixes #12154, an issue with redis-cli being unable to handle RESP3 double responses that contain a NaN.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-06-01T08:52:00Z",
        "closed_at": "2023-06-05T09:11:31Z",
        "merged_at": "2023-06-05T09:11:31Z",
        "body": "On recent alpine/musl systems, using `std=c11` breaks compilation, as none of the feature compatibility defines is being set. On other systems it's not necessary but doesn't seem incorrect.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 113,
        "changed_files": 6,
        "created_at": "2023-05-31T09:56:16Z",
        "closed_at": "2023-06-20T12:14:45Z",
        "merged_at": "2023-06-20T12:14:45Z",
        "body": "A value of type long long is always less than 21 bytes when convert to a string, so always meets the conditions for using embedded string object which can always get memory reduction and performance gain (less calls to the heap allocator).\r\nAdditionally, for the conversion of longlong type to sds, we also use a faster algorithm (the one in util.c instead of the one that used to be in sds.c). \r\n\r\nFor the DECR command on 32-bit Redis, we get about a 5.7% performance improvement. There will also be some performance gains for some commands that heavily use sdscatfmt to convert numbers, such as INFO.",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-05-31T04:59:32Z",
        "closed_at": "2023-06-27T03:59:11Z",
        "merged_at": null,
        "body": "It appears that currently this configuration directive is not mentioned anywhere outside of the codebase, so documenting it in redis.conf would make it more visible to users.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-05-30T08:29:44Z",
        "closed_at": "2023-06-11T20:12:05Z",
        "merged_at": "2023-06-11T20:12:05Z",
        "body": "In 7.2, After 971b177fa we make sure (assert) that\r\nthe duration has been recorded when resetting the client.\r\n\r\nThis is not true for rejected commands.\r\nThe use case I found is a blocking command that an ACL rule changed before it was unblocked, and while reprocessing it, the command rejected and triggered the assert.\r\n\r\nThe PR reset the command duration inside rejectCommand / rejectCommandSds.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-05-29T11:21:52Z",
        "closed_at": "2023-05-29T12:39:32Z",
        "merged_at": "2023-05-29T12:39:32Z",
        "body": "We should emit DB_FLAG_KEY_EXPIRED instead of DB_FLAG_KEY_DELETED.\r\nThis is an overlook in #9406.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 11,
        "changed_files": 4,
        "created_at": "2023-05-29T11:00:45Z",
        "closed_at": "2023-06-12T11:05:35Z",
        "merged_at": "2023-06-12T11:05:35Z",
        "body": "This will increase the size of an already large COB (one already passed the threshold for disconnection)\r\n\r\nThis could also mean that we'll attempt to write that data to the socket and the replica will manage to read it, which will result in an undesired partial sync (undesired for the test)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-05-29T09:17:15Z",
        "closed_at": "2023-05-30T07:43:26Z",
        "merged_at": "2023-05-30T07:43:26Z",
        "body": "This is a followup fix for #11817\r\ncould possibly reduce loading time a bit.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-05-26T20:03:59Z",
        "closed_at": "2023-05-28T05:37:32Z",
        "merged_at": "2023-05-28T05:37:32Z",
        "body": "XREAD only supports a special ID of $ and XREADGROUP only supports ^.\r\nmake sure not to suggest the wrong one when rerunning an error about unbalanced ID arguments\r\n\r\n**Current Behavior**\r\n\r\nXREAD display wrong error message for use of the special ID and >. But when we try to use > with XREAD its display the error message as \"  ERR The > ID can be specified only when calling XREADGROUP using the GROUP <group> <consumer> option.\"\r\n\r\n![image](https://github.com/redis/redis/assets/51993843/3435a8cc-1157-4f0c-a79b-bed76cb6d0d7)\r\n\r\nExpected Behaviour\r\nXREAD should display \"ERR Unbalanced 'xread' list of streams: for each stream key an ID or '$' must be specified.\"\r\n![xadd 2](https://github.com/redis/redis/assets/51993843/40f77609-bc66-4baa-8074-f40ec815042b)\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-05-26T15:55:05Z",
        "closed_at": "2023-05-28T05:44:28Z",
        "merged_at": "2023-05-28T05:44:28Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-05-26T02:30:55Z",
        "closed_at": "2023-05-26T06:12:23Z",
        "merged_at": "2023-05-26T06:12:23Z",
        "body": "It was missing in #12223, and the reply-schemas daily\r\nwas failing:\r\n```\r\njsonschema.exceptions.ValidationError: 'nothing' is not valid under any of the given schemas\r\n\r\nFailed validating 'oneOf' in schema[0]['properties']['loglevel']:\r\n    {'oneOf': [{'const': 'debug'},\r\n               {'const': 'verbose'},\r\n               {'const': 'notice'},\r\n               {'const': 'warning'},\r\n               {'const': 'unknown'}]}\r\n\r\nOn instance['loglevel']:\r\n    'nothing'\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 4,
        "changed_files": 5,
        "created_at": "2023-05-25T08:58:32Z",
        "closed_at": "2023-05-28T05:35:27Z",
        "merged_at": "2023-05-28T05:35:27Z",
        "body": "Hi,\r\nChanges in this PR:\r\n* Rather than a fixed iovcnt for connWritev, support maxiov per connection type instead.\r\n* A minor change to reduce memory for struct connection.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 334,
        "deletions": 96,
        "changed_files": 9,
        "created_at": "2023-05-25T07:06:56Z",
        "closed_at": "2023-06-26T14:43:38Z",
        "merged_at": "2023-06-26T14:43:38Z",
        "body": "To resolve #12143 and resolve #12006.\r\n\r\nOriginally, when \"tls-cluster\" is enabled, `port` is set to TLS port. In order to support non-TLS clients, `pport` is used to propagate TCP port across cluster nodes. However when \"tls-cluster\" is disabled, `port` is set to TCP port, and `pport` is not used, which means the cluster cannot provide TLS service unless \"tls-cluster\" is on.\r\n```\r\ntypedef struct {\r\n    // ...\r\n    uint16_t port;  /* Latest known clients port (TLS or plain). */\r\n    uint16_t pport; /* Latest known clients plaintext port. Only used if the main clients port is for TLS. */\r\n    // ...\r\n} clusterNode;\r\n```\r\n```\r\ntypedef struct {\r\n    // ...\r\n    uint16_t port;   /* TCP base port number. */\r\n    uint16_t pport;  /* Sender TCP plaintext port, if base port is TLS */\r\n    // ...\r\n} clusterMsg;\r\n```\r\nThis PR renames `port` and `pport` in `clusterNode` to `tcp_port` and `tls_port`, to record both ports no matter \"tls-cluster\" is enabled or disabled.\r\n\r\nThis allows to provide TLS service to clients when \"tls-cluster\" is disabled: when displaying cluster topology, or giving `MOVED` error, server can provide TLS or TCP port according to client's connection type, no matter what type of connection cluster bus is using.\r\n\r\nFor backwards compatibility, `port` and `pport` in `clusterMsg` are preserved, when \"tls-cluster\" is enabled, `port` is set to TLS port and `pport` is set to TCP port, when \"tls-cluster\" is disabled, `port` is set to TCP port and `pport` is set to TLS port (instead of 0).\r\n\r\nAlso, in the nodes.conf file, a new aux field displaying an extra port is added to complete the persisted info. We may have `tls_port=xxxxx` or `tcp_port=xxxxx` in the aux field, to complete the cluster topology, while the other port is stored in the normal `<ip>:<port>` field. The format is shown below.\r\n```\r\n<node-id> <ip>:<tcp_port>@<cport>,<hostname>,shard-id=...,tls-port=6379 myself,master - 0 0 0 connected 0-1000\r\n```\r\nOr we can switch the position of two ports, both can be correctly resolved.\r\n```\r\n<node-id> <ip>:<tls_port>@<cport>,<hostname>,shard-id=...,tcp-port=6379 myself,master - 0 0 0 connected 0-1000\r\n```\r\n",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2023-05-24T15:29:50Z",
        "closed_at": "2023-05-29T06:55:17Z",
        "merged_at": "2023-05-29T06:55:17Z",
        "body": "We check lazyfree_lazy_server_del in sunionDiffGenericCommand\r\nto see if we need to lazyfree the temp set. Now do the same in\r\nzunionInterDiffGenericCommand to lazyfree the temp zset.\r\n\r\nThis is a minor change, follow #5903. Also improved the comments.\r\n\r\nAdditionally, avoid creating unused zset object in ZINTERCARD,\r\nresults in some 10% performance improvement. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-05-24T10:34:13Z",
        "closed_at": "2023-05-24T13:21:19Z",
        "merged_at": "2023-05-24T13:21:19Z",
        "body": "and update recent SENTINEL CONFIG changes.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 52,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-05-24T09:14:39Z",
        "closed_at": "2023-05-24T10:15:29Z",
        "merged_at": null,
        "body": "1. when i use redis-cli with tls\uff0center repeated parameters each time. e.g. `--cacert --cert --key`.  the process is too cumbersome and error-prone. so i think it would be convenient to use environment variables. it simplified redis-cli parameters.\r\n2. i added the `--keyfile-pass` parameter to redis-cli. this is keypass of Private key file. it also can also be read through environment variables.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-05-24T04:05:42Z",
        "closed_at": "2023-05-24T06:28:39Z",
        "merged_at": "2023-05-24T06:28:39Z",
        "body": "In #12166, we removed a call to CLUSTER SLAVES, which\r\nthen caused reply-schemas ci to fail:\r\n```\r\nWARNING! The following commands were not hit at all:\r\n  cluster|slaves\r\n  ERROR! at least one command was not hit by the tests\r\n```\r\n\r\nBecause we already have command output that cover CLUSTER REPLICAS\r\nelsewhere, here we simply add some dummy tests to fix the ci.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2023-05-24T02:59:51Z",
        "closed_at": "2023-05-24T06:32:39Z",
        "merged_at": "2023-05-24T06:32:39Z",
        "body": "We add a new loglevel 'nothing' to disable logging in #12133.\r\nThis PR syncs that config change to sentinel. Because in #11214\r\nwe support modifying loglevel in runtime.\r\n\r\nAlthough I think sentinel doesn't need this nothing config,\r\nit's better to be consistent.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 86,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-05-23T13:26:01Z",
        "closed_at": "2023-05-28T07:10:52Z",
        "merged_at": "2023-05-28T07:10:52Z",
        "body": "So far clients being blocked and unblocked by a module command would update the c->woff variable and so WAIT was ineffective and got released without waiting for the command actions to propagate.\r\n\r\nThis seems to have existed since forever, but not for RM_BlockClientOnKeys.\r\n\r\nIt is problematic though to know if the module did or didn't propagate anything in that command, so for now, instead of adding an API, we'll just update the woff to the latest offset when unblocking, this will cause the client to possibly wait excessively, but that's not that bad.\r\n\r\nfix #12124",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 63,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2023-05-23T13:07:55Z",
        "closed_at": "2023-06-20T06:03:53Z",
        "merged_at": "2023-06-20T06:03:53Z",
        "body": "Adds API\r\n- RedisModule_CommandFilterGetClientId()\r\n\r\nIncludes addition to commandfilter test module to validate that it works by performing the same command from 2 different clients",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-23T10:15:19Z",
        "closed_at": "2023-05-24T22:03:17Z",
        "merged_at": "2023-05-24T22:03:17Z",
        "body": "Minor typo\r\n- functoin -> function",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 39,
        "deletions": 17,
        "changed_files": 3,
        "created_at": "2023-05-22T13:31:36Z",
        "closed_at": "2023-05-24T13:27:44Z",
        "merged_at": "2023-05-24T13:27:44Z",
        "body": "The light version only shows the table sizes, while the pre-existing version that shows chain length stats is reachable with the `full` argument.\r\n\r\nThis should allow looking into rehashing state, even on huge dicts, on which we're afraid to run the command for fear of causing a server freeze.\r\n\r\nAlso, fix a possible overflow in dictGetStats.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 352,
        "deletions": 118,
        "changed_files": 5,
        "created_at": "2023-05-22T09:37:14Z",
        "closed_at": "2023-06-27T13:43:46Z",
        "merged_at": "2023-06-27T13:43:46Z",
        "body": "Optimized the performance of the SCAN command in a few ways:\r\n1. Move the key filtering (by MATCH pattern) in the scan callback, so as to avoid collecting them for later filtering.\r\n2. Reduce a many memory allocations and copying (use a reference to the original sds, instead of creating an robj, an excessive 2 mallocs and one string duplication)\r\n3. Compare TYPE filter directly (as integers), instead of inefficient string compare per key.\r\n4. fixed a small bug: when scan zset and hash types, maxiterations uses a more accurate number to avoid wrong double maxiterations.\r\n\r\nChanges **postponed** for a later version (8.0):\r\n1. Prepare to move the TYPE filtering to the scan callback as well. this was put on hold since it has side effects that can be considered a breaking change, which is that we will not attempt to do lazy expire (delete) a key that was filtered by not matching the TYPE (changing it would mean TYPE filter starts behaving the same as MATCH filter already does in that respect). \r\n2. when the specified key TYPE filter is an unknown type, server will reply a error immediately instead of doing a full scan that comes back empty handed. \r\n\r\nBenchmark result:\r\nFor different scenarios, we obtained about 30% more performance improvement:\r\n* scene 1: scan the key space with different filter\r\n\r\n\u00a0 | no filter | scan pattern and 100% unmatched | scan pattern and 50% unmatched | scan type and 100% matched | scan type and 100% unmatched | scan type and 50% matched\r\n-- | -- | -- | -- | -- | -- | --\r\nunstable | 31173 | 50279 | 36174 | 22936 | 32931 | 27603\r\npr | 40264 | 110661 | 53689 | 32028 | 50437 | 37736\r\nimprovement | 29.16% | 120.09% | 48.42% | 39.64% | 53.16% | 36.71%\r\n\r\n* scene 2:  hscan key which use ht encoding\r\n\r\nhscan(hashtable   encoding) | 100% matched | hscan pattern and 100% unmatched\r\n-- | -- | --\r\nunstable | 17740 | 32457\r\nthis pr | 22734 | 108945\r\nimprovement | 28.15% | 235.66%\r\n\r\n* scene 3: sscan key which use intset encoding\r\n\r\nsscan intset | 100% matched | sscan pattern and 20% matched\r\n-- | -- | --\r\nunstable | 7529 | 14567\r\npr | 7418 | 21763\r\nimporovement | -1.47% | 49.40%\r\n\r\n* scene 4: zscan key which use listpack encoding\r\n\r\nzscan listpack | 100% matched | zscan pattern and 10% matched\r\n-- | -- | --\r\nunstable | 18422 | 28857\r\nthis pr | 18826 | 83565\r\nimporovement | 2.19% | 189.58%\r\n\r\n",
        "comments": 28
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 32,
        "changed_files": 4,
        "created_at": "2023-05-19T16:47:18Z",
        "closed_at": "2023-05-22T12:48:33Z",
        "merged_at": "2023-05-22T12:48:33Z",
        "body": "Optimized HRANDFIELD and ZRANDMEMBER commands as in #8444,\r\nCASE 3 under listpack encoding. Boost optimization to CASE 2.5. \r\n\r\nCASE 2.5 listpack only. Sampling unique elements, in non-random order.\r\nListpack encoded hashes / zsets are meant to be relatively small, so\r\nHRANDFIELD_SUB_STRATEGY_MUL / ZRANDMEMBER_SUB_STRATEGY_MUL\r\nisn't necessary and we rather not make copies of the entries. Instead, we\r\nemit them directly to the output buffer.\r\n\r\nSimple benchmarks shows it provides some 400% improvement in HRANDFIELD\r\nand ZRANGESTORE both in CASE 3.\r\n\r\nUnrelated changes: remove useless setTypeRandomElements and fix a typo.\r\n\r\nhash: a simple benchmark (listpack with 500 entries):\r\n```\r\nsrc/redis-cli hset hash 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999\r\n```\r\n\r\nunstable:\r\n```\r\nsrc/redis-benchmark -P 100 -n 400000 hrandfield hash 200\r\nSummary:\r\n  throughput summary: 5713.80 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n      674.346    17.024   769.023   813.055   843.775  1291.263\r\n```\r\n\r\nthis branch:\r\n```\r\nsrc/redis-benchmark -P 100 -n 400000 hrandfield hash 200\r\nSummary:\r\n  throughput summary: 24100.74 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n      106.469     4.440   110.335   153.599   173.183   188.415\r\n```\r\n\r\nzset: a simple benchmark (listpack with 121 entries):\r\n```\r\nsrc/redis-cli zadd zset 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240\r\n```\r\n\r\nunstable:\r\n```\r\nSummary:\r\n  throughput summary: 25461.49 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n      175.624     5.064   189.183   207.615   217.087   239.999\r\n```\r\n\r\nthis branch:\r\n```\r\nsrc/redis-benchmark -P 100 -n 400000 zrandmember zset 50\r\nSummary:\r\n  throughput summary: 98546.44 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n       39.635     3.328    38.783    47.487    63.551    74.175\r\n```",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-18T19:52:11Z",
        "closed_at": "2023-05-24T06:34:13Z",
        "merged_at": "2023-05-24T06:34:13Z",
        "body": "HVALS, HKEYS and HEXISTS commands wrong type test cases were not covered so added the test cases.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-18T03:47:41Z",
        "closed_at": "2023-05-29T10:43:27Z",
        "merged_at": "2023-05-29T10:43:27Z",
        "body": "This test was introduced in #12079, it works well most of the time, but\r\noccasionally fails:\r\n```\r\n00:34:45> SENTINEL SIMULATE-FAILURE crash-after-election works: OK\r\n00:34:45> SENTINEL SIMULATE-FAILURE crash-after-promotion works: FAILED: Sentinel set crash-after-promotion but did not exit\r\n```\r\n\r\nDon't know the reason, it may be affected by the exit of the previous\r\ncrash-after-election test. Because it doesn't really make much sense to\r\ngo deeper into it now, we re-source init-tests to get a clean environment\r\nbefore each test, to try to fix this.\r\n\r\nAfter applying this change, we found a new error:\r\n```\r\n16:39:33> SENTINEL SIMULATE-FAILURE crash-after-election works: FAILED: caught an error in the test couldn't open socket: connection refused\r\ncouldn't open socket: connection refused\r\n```\r\n\r\nI am guessing the sentinel triggers failover and exits before SENTINEL FAILOVER,\r\nadded a new || condition in wait_for_condition to fix it.",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-05-16T14:45:16Z",
        "closed_at": "2023-05-18T09:16:46Z",
        "merged_at": "2023-05-18T09:16:46Z",
        "body": "Current tests for BITFIELD_RO command are skipped in the external mode, and therefore reply-schemas-validator reports a coverage error. This PR adds basic tests to increase coverage. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 82,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2023-05-16T11:47:01Z",
        "closed_at": "2023-05-18T12:24:47Z",
        "merged_at": "2023-05-18T12:24:47Z",
        "body": "For zsets that will eventually be stored as the skiplist encoding (has a dict),\r\nwe can convert it to skiplist ahead of time. This change checks the number\r\nof arguments in the ZADD command, and converts the data-structure\r\nif the number of new entries exceeds the listpack-max-entries configuration.\r\nThis can cause us to over-allocate memory if there are duplicate entries in the\r\ninput, which is unexpected.\r\n\r\nFor ZRANGESTORE, we know the size of the zset, so we can expand\r\nthe dict in advance, to avoid the temporary dict from being rehashed\r\nwhile it grows.\r\n\r\nSimple benchmarks shows it provides some 4% improvement in ZADD and 20% in ZRANGESTORE\r\n\r\n## ZADD\r\non a 1c2g machine, some very basic benchmarking using:\r\n```\r\nsrc/redis-benchmark -P 100 -n 100000 ZADD __rand_int__ `python -c \"print(' '.join([str(a) for a in range(888)]))\"`\r\n```\r\n\r\nunstable:\r\n```\r\nSummary:\r\n  throughput summary: 9203.87 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n       15.874    12.392    15.359    21.391    24.959    27.055\r\n```\r\n\r\nthis branch:\r\n```\r\nSummary:\r\n  throughput summary: 9563.89 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n       15.336    12.224    14.615    21.791    24.959    28.271\r\n```\r\n\r\n## ZRANGESTORE\r\n\r\nzrangestore benchmark: unstable took some 5.84 seconds, and this branch\r\ntook 4.57 seconds. so about a 20% improvement. (1c2g machine)\r\n```\r\nsrc/redis-cli zadd z 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33 34 34 35 35 36 36 37 37 38 38 39 39 30 40\r\nsrc/redis-cli config set zset-max-listpack-entries 38\r\nsrc/redis-benchmark -P 100 -n 400000 zrangestore z2 z 0 -1\r\n```",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 57,
        "deletions": 10,
        "changed_files": 3,
        "created_at": "2023-05-16T08:32:43Z",
        "closed_at": "2023-05-16T18:32:21Z",
        "merged_at": "2023-05-16T18:32:21Z",
        "body": "In the judgment in setTypeCreate, we should judge size_hint <= max_entries.\r\nIn the judgment in setTypeMaybeConvert, we should judge size_hint > max_entries.\r\n\r\nThis (old setTypeCreate) results in the following inconsistencies (Don't take it to the commit message):\r\n```\r\n127.0.0.1:6379> config set set-max-intset-entries 5 set-max-listpack-entries 5\r\nOK\r\n\r\n127.0.0.1:6379> sadd intset_set1 1 2 3 4 5\r\n(integer) 5\r\n127.0.0.1:6379> object encoding intset_set1\r\n\"hashtable\"\r\n127.0.0.1:6379> sadd intset_set2 1 2 3 4\r\n(integer) 4\r\n127.0.0.1:6379> sadd intset_set2 5\r\n(integer) 1\r\n127.0.0.1:6379> object encoding intset_set2\r\n\"intset\"\r\n\r\n127.0.0.1:6379> sadd listpack_set1 a 1 2 3 4\r\n(integer) 5\r\n127.0.0.1:6379> object encoding listpack_set1\r\n\"hashtable\"\r\n127.0.0.1:6379> sadd listpack_set2 a 1 2 3\r\n(integer) 4\r\n127.0.0.1:6379> sadd listpack_set2 4\r\n(integer) 1\r\n127.0.0.1:6379> object encoding listpack_set2\r\n\"listpack\"\r\n```\r\n\r\nThis was introduced in #12019, added corresponding tests.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-05-15T14:57:20Z",
        "closed_at": "2023-06-11T06:02:41Z",
        "merged_at": "2023-06-11T06:02:41Z",
        "body": "In #11963, some new tests about eventloop duration were added, which includes time measurement in TCL scripts. This has caused some unexpected CI failures, such as #12169 and #12177, due to slow test servers or some performance jittering.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-05-15T12:22:56Z",
        "closed_at": "2023-06-14T08:17:39Z",
        "merged_at": "2023-06-14T08:17:39Z",
        "body": "This change only affects keys with expiry time.\r\nFor SETEX, the average improvement is 5%, and for GET with expiation key, we gain a improvement of 13%.\r\n\r\nWhen keys have expiration time, Redis has an assertion to look up the main dict every time when it touches the expires.\r\nThis comes with a performance const, especially during rehash. the damage will be double.\r\n\r\nIt looks like that assert was added some ten years old, maybe out of paranoia, and there's probably no reason to keep it at that cost.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2023-05-15T10:14:12Z",
        "closed_at": "2023-05-15T13:50:13Z",
        "merged_at": null,
        "body": "There are situations (especially in TLS) in which the engine get too occupied managing new connections.\r\nTo better manage the tradeoff between new connection rate and other workloads, I suggest  adding a new parameter to manage maximum number of new connections per cycle, instead of using a predetermined number (currently 1000)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-05-15T08:48:23Z",
        "closed_at": "2023-06-21T09:48:14Z",
        "merged_at": "2023-06-21T09:48:14Z",
        "body": "Continuation of https://github.com/redis/redis/pull/11338\r\navoid overflow adding input timeout to \"now\" in moduleBlockClient.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 67005,
        "deletions": 31527,
        "changed_files": 907,
        "created_at": "2023-05-15T06:55:03Z",
        "closed_at": "2023-05-15T10:08:16Z",
        "merged_at": "2023-05-15T10:08:15Z",
        "body": "Upgrade urgency LOW: This is the second Release Candidate for Redis 7.2.\r\n\r\nINFO fields and introspection changes\r\n=====================================\r\n\r\n* Add a few low level event loop metrics to help diagnose latency (#11963)\r\n\r\nPerformance and resource utilization improvements\r\n=================================================\r\n\r\n* Minor performance improvement to SADD and HSET (#12019)\r\n\r\nPlatform / toolchain support related changes\r\n=================================================\r\n\r\n* Upgrade to Jemalloc 5.3.0, resolves a rare fork child hang (#12115)\r\n* Fix a compiler fortification induced crash when used with link time optimizations (#11982)\r\n* Fix local clients detection, 127.*.*.* instead of 127.0.0.1 (#11664)\r\n* Report AOF failure status to systemd in shutdown (#12065)\r\n\r\nChanges in CLI tools\r\n====================\r\n\r\n* redis-cli: Reimplement and improve help hints based on actual command arg docs (#10515)\r\n* redis-cli: Add option --count for tuning SCAN based features (#12042)\r\n* redis-benchmark: Add --seed option to seed the random number generator (#11945)\r\n\r\nModule API changes\r\n==================\r\n\r\n* Add RM_RdbLoad and RM_RdbSave APIs (#11852)\r\n* Add RM_ReplyWithErrorFormat that can support format string (#11923)\r\n* Fix: Delete empty key when RM_ZsetAdd, RM_ZsetIncrby, RM_StreamAdd fail (#12129)\r\n\r\nBug Fixes\r\n=========\r\n\r\n* LPOS with RANK set to LONG_MIN returning wrong result (#12167)\r\n* Avoid unnecessary full sync after master restart in a rare case (#12088)\r\n* Iterate clients fairly when processing background chores (#12025)\r\n* Avoid incorrect shrinking of query buffer when reading large data from clients (#12000)\r\n* Sentinel: Fix config rewrite error when old known-slave is used (#11775)\r\n* ACL: Disconnect pub-sub subscribers when revoking allchannels permission (#11992)\r\n* Add a missing fsync of AOF file in rare cases (#11973)\r\n\r\nFixes for issues in previous releases of Redis 7.2\r\n--------------------------------------------------\r\n\r\n* Fix tracking of command duration metrics for MULTI, EVAL, WAIT and modules (#11970)\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-14T06:08:56Z",
        "closed_at": "2023-05-14T14:40:29Z",
        "merged_at": "2023-05-14T14:40:29Z",
        "body": "new test added in #11963, fails on freebsd CI which is slow.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-05-13T10:31:40Z",
        "closed_at": "2023-05-14T06:04:33Z",
        "merged_at": "2023-05-14T06:04:33Z",
        "body": "Fix a minor bug reported by #12165 \r\n\r\nLimit the range of `RANK` to `-LONG_ MAX ~ LONG_ MAX`.\r\nWithout this limit, passing -9223372036854775808 would effectively be the same as passing -1.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 36,
        "changed_files": 6,
        "created_at": "2023-05-12T19:04:09Z",
        "closed_at": "2023-05-23T15:32:38Z",
        "merged_at": "2023-05-23T15:32:38Z",
        "body": "This commit excludes aux fields from the output of the `cluster nodes` and `cluster replicas` command.\r\nSpecifically `shard-id`, which was introduced in 7.2 RC1 (see #10536).\r\nWe may decide to re-introduce them in some form or another in the future, but not in v7.2.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 21,
        "changed_files": 1,
        "created_at": "2023-05-11T09:58:37Z",
        "closed_at": "2023-05-23T11:29:28Z",
        "merged_at": "2023-05-23T11:29:28Z",
        "body": "previously the argv wasn't freed so would leak.  not a common case, but should be handled.\r\n\r\nSolution: move RUN_AS_USER setup and error exit to the right place.\r\nthis way, when we do `goto cleanup` (instead of return) it'll automatically do the right thing (including autoMemoryAdd)\r\nRemoved the user argument from moduleAllocTempClient (reverted to the state before 6e993a5)\r\n\r\nfixes: #12157 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-11T08:40:58Z",
        "closed_at": "2023-05-11T15:16:09Z",
        "merged_at": "2023-05-11T15:16:08Z",
        "body": "just a plain overlook by #9406",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-05-10T21:27:07Z",
        "closed_at": "2023-05-18T06:25:56Z",
        "merged_at": "2023-05-18T06:25:56Z",
        "body": "When `replicationFeedSlaves()` serializes a command, it repeatedly calls `feedReplicationBuffer()` to feed it to the replication backlog piece by piece. It is unnecessary to call `incrementalTrimReplicationBacklog()` for every small amount of data added with `feedReplicationBuffer()` as the chance of the conditions being met for trimming are very low and these frequent calls add up to a notable performance cost. Instead, we will only attempt trimming when a new block is added to the replication backlog.\r\n\r\nUsing redis-benchmark to saturate a local redis server indicated a performance improvement of around 3-3.5% for 100 byte SET commands with this change.\r\n\r\n`./redis-benchmark -p 6379 -t set -n 100000000 -c 1000 -d 100 -P 100`\r\n\r\n**Baseline Example**\r\n\r\n```\r\nSummary:\r\n  throughput summary: 988415.81 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n       95.715    11.600    98.815   108.735   112.319   128.703\r\n```\r\n\r\n**Preventing Repetitive Trimming Example**\r\n```\r\nSummary:\r\n  throughput summary: 1021606.94 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n       92.407    11.960    95.359   105.407   108.927   166.015\r\n```\r\n\r\n**Performance Improvement: 3.35%**\r\n\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 10,
        "changed_files": 3,
        "created_at": "2023-05-09T14:49:26Z",
        "closed_at": "2023-05-10T05:26:46Z",
        "merged_at": "2023-05-10T05:26:46Z",
        "body": "This pattern is from COMMAND INFO:\r\nReturns information about one, multiple or all commands.\r\n\r\nAlso re-generate commands.def, the GEO change was missing in #12151.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 38,
        "changed_files": 5,
        "created_at": "2023-05-09T06:34:39Z",
        "closed_at": "2023-05-09T11:24:37Z",
        "merged_at": "2023-05-09T11:24:37Z",
        "body": "in GEO commands, `STORE` and `STOREDIST` are mutually exclusive.\r\nuse `oneof` block to contain them\r\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-05-09T04:05:44Z",
        "closed_at": "2023-05-09T11:14:23Z",
        "merged_at": "2023-05-09T11:14:23Z",
        "body": "The test failed on MacOS:\r\n```\r\n*** [err]: EXPIRE precision is now the millisecond in tests/unit/expire.tcl\r\nExpected 'somevalue {}' to equal or match '{} {}'\r\n```\r\n\r\n`set a [r get x]`, even though we tried 10 times, sometimes we\r\nstill get {}, this is a time-sensitive test.\r\n\r\nIn this PR, we add the following changes:\r\n1. More attempts, change it from 10 to 30.\r\n2. More tolerant, change the `after 900` to `after 800`.\r\n\r\nIn addition, we judging $a in advance and changing `after 1100`\r\nto `after 300`, this will save us some times.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-05-08T15:09:57Z",
        "closed_at": "2023-05-16T16:26:49Z",
        "merged_at": "2023-05-16T16:26:49Z",
        "body": "Minor missing test case addition.\r\n\r\n1. SMEMBERS SCARD against non set\r\n2. SMEMBERS SCARD against non existing key",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-08T09:30:29Z",
        "closed_at": "2023-05-09T13:18:14Z",
        "merged_at": null,
        "body": "Fixed issue #3768\r\n\r\n\r\nI use Macbook m1. It seems the Tcl doesn't support arm64. The command `uname -a` will return x86_64 and make it error.\r\n\r\n```\r\n% exec uname -a\r\nDarwin Kernel Version 21.6.0: Thu Mar  9 20:12:21 PST 2023; root:xnu-8020.240.18.700.8~1/RELEASE_ARM64_T6000 x86_64\r\n% exec file /bin/bash\r\n/bin/bash: Mach-O universal binary with 2 architectures: [x86_64:Mach-O 64-bit executable x86_64\r\n- Mach-O 64-bit executable x86_64] [arm64e:Mach-O 64-bit executable arm64e\r\n- Mach-O 64-bit executable arm64e]\r\n/bin/bash (for architecture x86_64):\tMach-O 64-bit executable x86_64\r\n/bin/bash (for architecture arm64e):\tMach-O 64-bit executable arm64e\r\n```\r\n\r\nChange \"uname -a\" into \"file /bin/bash\" to get architecture correctly. \r\nTest: Test HINCRBYFLOAT for correct float representation (issue #2846)",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-05-08T09:03:21Z",
        "closed_at": "2023-05-08T23:12:45Z",
        "merged_at": "2023-05-08T23:12:45Z",
        "body": "Remove several instances of duplicate \"the\" in comments.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-08T05:33:52Z",
        "closed_at": "2023-05-08T06:44:21Z",
        "merged_at": "2023-05-08T06:44:20Z",
        "body": "The new mallctl seems to set the output sz when EINVAL occors. that messes up the retry mechanism that does /2 on each iteration.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-06T16:20:38Z",
        "closed_at": "2023-05-08T23:13:32Z",
        "merged_at": "2023-05-08T23:13:32Z",
        "body": "TRIVIAL AS IS",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-05-06T03:48:25Z",
        "closed_at": "2023-05-23T15:30:44Z",
        "merged_at": "2023-05-23T15:30:44Z",
        "body": "Users can record logs of different levels by setting the `loglevel`. However, sometimes there are many logs even at the warning level, which can affect the performance of Redis.\r\n\r\nFor example, when a user accesses the tls-port using a non-encrypted link, Redis will log lots of \"# Error accepting a client connection: ...\".\r\n\r\nWe can provide the ability to disable logging so that users can temporarily turn off logging and turn it back on after the problem is resolved.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 86,
        "deletions": 36,
        "changed_files": 4,
        "created_at": "2023-05-05T22:07:18Z",
        "closed_at": "2023-05-09T15:49:09Z",
        "merged_at": null,
        "body": "**Current Command**\r\n\"SMOVE source destination member\" command returns \r\n1 if the element is moved.\r\n0 if the element is not a member of source and no operation was performed\r\n\r\n**Proposed Command**\r\nSMOVE source destination member [member..]\r\nReturns\r\n1 if at least one element is moved.\r\n0 if the none of the element is a member of source and no operation was performed\r\n\r\n![smove1](https://user-images.githubusercontent.com/51993843/236577736-60a2e6c0-d321-4d0b-8af2-253d82c78bc0.jpg)\r\n\r\n![smove 2](https://user-images.githubusercontent.com/51993843/236577748-42441912-e7ac-441f-a037-0e638b6c7189.jpg)\r\n\r\n![smove 3](https://user-images.githubusercontent.com/51993843/236577766-c0f9528b-4f9b-42ca-a679-d645b47d94d4.jpg)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 107,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2023-05-05T03:38:30Z",
        "closed_at": "2023-05-07T07:13:20Z",
        "merged_at": "2023-05-07T07:13:19Z",
        "body": "When `RM_ZsetAdd()`/`RM_ZsetIncrby()`/`RM_StreamAdd()` fails, if a new key happens to \r\nbe created using `moduleCreateEmptyKey()`, we should clean up the empty key.\r\n\r\n## Test\r\n1) Add new module commands(`zset.add` and `zset.incrby`) to cover  `RM_ZsetAdd()`/`RM_ZsetIncrby()`.\r\n2) Add a large-memory test to cover `RM_StreamAdd()`.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2356,
        "deletions": 1962,
        "changed_files": 2,
        "created_at": "2023-05-04T11:58:05Z",
        "closed_at": "2023-05-07T05:42:28Z",
        "merged_at": null,
        "body": "curl -sL -o config.guess 'https://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess;hb=HEAD' \r\ncurl -sL -o config.sub 'https://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.sub;hb=HEAD'",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-05-04T06:44:39Z",
        "closed_at": "2023-05-04T10:02:08Z",
        "merged_at": "2023-05-04T10:02:08Z",
        "body": "Tests occasionally fail since #12000:\r\n```\r\n*** [err]: query buffer resized correctly when not idle in tests/unit/querybuf.tcl\r\nExpected 0 > 32768 (context: type eval line 11 cmd {assert {$orig_test_client_qbuf > 32768}} proc ::test)\r\n\r\n*** [err]: query buffer resized correctly with fat argv in tests/unit/querybuf.tcl\r\nquery buffer should not be resized when client idle time smaller than 2s\r\n```\r\n\r\nThe reason may be because we set hz to 100, querybuf shrinks before we count\r\nclient_query_buffer. We avoid this problem by setting pause-cron to 1.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-05-03T17:40:13Z",
        "closed_at": "2023-05-07T08:46:11Z",
        "merged_at": "2023-05-07T08:46:11Z",
        "body": "adding test case of expired key or not exist for GET and GETEX.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-05-03T14:06:21Z",
        "closed_at": "2023-05-04T08:58:31Z",
        "merged_at": "2023-05-04T08:58:31Z",
        "body": "1. reset the readraw mode after a test that uses it. undetected since the\r\n  only test after that on the same server didn't read any replies.\r\n2. fix a cross slot issue that was undetected in cluster mode because\r\n  readraw doesn't throw exceptions on errors.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-05-03T01:29:00Z",
        "closed_at": "2023-05-03T17:35:28Z",
        "merged_at": "2023-05-03T17:35:28Z",
        "body": "API incorrectly uses ExecutionUnit instead of Notification.\r\n\r\nThis confused me while reading the documentation.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-05-01T21:16:17Z",
        "closed_at": "2023-05-08T14:39:26Z",
        "merged_at": null,
        "body": "In bitops.c local buffer sizes were declared with digits, so went through the subsequent code part and updated these with macros accordingly.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-05-01T14:48:01Z",
        "closed_at": "2023-05-01T18:00:08Z",
        "merged_at": "2023-05-01T18:00:08Z",
        "body": "Adding missing test case against wrong type for HRANDFIELD HGET HGETALL HDEL HINCRBY HINCRBYFLOAT HSTRLEN.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 52038,
        "deletions": 20516,
        "changed_files": 382,
        "created_at": "2023-05-01T14:36:20Z",
        "closed_at": "2023-05-07T14:41:18Z",
        "merged_at": "2023-05-07T14:41:18Z",
        "body": "Upgrade to jemalloc 5.3.0 hoping to resolve a potential for deadlock in a fork child\r\nsee https://github.com/jemalloc/jemalloc/issues/2402\r\n\r\nsteps:\r\n* Upgrade subtree according to the instructions in deps/README\r\n* update iget_defrag_hint by following changes to arena_dalloc_no_tcache\r\n\r\ntodo:\r\n* [ ] don't squash. merge\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-25T07:35:02Z",
        "closed_at": "2023-04-25T11:09:38Z",
        "merged_at": "2023-04-25T11:09:38Z",
        "body": "We were using `oldstable` Debian as a CI with an older toolchain, but that image is now offline so move to debian:buster\n\n```\nE: Failed to fetch http://security.debian.org/debian-security/dists/oldoldstable/updates/main/binary-amd64/Packages 404 Not Found [IP: 151.101.2.132 80]\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2023-04-21T07:15:52Z",
        "closed_at": "2023-05-06T03:53:28Z",
        "merged_at": "2023-05-06T03:53:28Z",
        "body": "As discusion in #12080 , when master reboot from RDB, if `rsi` in RDB is valid we should not free replication backlog, even if `master_repl_offset` or `repl-offset` is 0.\r\n\r\nSince if master doesn't send any data to replicas `master_repl_offset` is 0, it's a valid number.\r\n\r\nA clear example:\r\n\r\n1. start a master and apply some write commands, the master's `master_repl_offset` is 0 since it has no replicas.\r\n2. stop write commands on master, and start another instance and replicaof the master, trigger an FULLRESYNC\r\n3. the master's `master_repl_offset` is still 0 (set a large number for `repl-ping-replica-period`), do BGSAVE and restart the master\r\n4. master load `master_repl_offset` from RDB's rsi and it's still 0, and we should make sure replica can partially resync with master.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 20,
        "changed_files": 2,
        "created_at": "2023-04-20T14:37:14Z",
        "closed_at": "2023-04-20T20:28:45Z",
        "merged_at": "2023-04-20T20:28:45Z",
        "body": "1. Check for missing schema only after the docs contain sentinel commands\r\n2. The ignore-list in the C file contain only commands that cannot have a reply schema. The one in the py file is an extension of that list\r\n3. Temp: skipsentinel commands don't have a schema or test coverage yet, add them to the py list\r\n\r\nSolve CI error introduced by #12018",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 66,
        "deletions": 21,
        "changed_files": 2,
        "created_at": "2023-04-20T12:08:50Z",
        "closed_at": "2023-05-22T07:27:14Z",
        "merged_at": "2023-05-22T07:27:14Z",
        "body": "A single SPOP with command with count argument resulted in many SPOP commands being propagated to the replica.\r\nThis is inefficient because the key name is repeated many times, and is also being looked-up many times.\r\nalso it results in high QPS metrics on the replica.\r\nTo solve that, we flush batches of 1024 fields per SPOP command.\r\n\r\nFix #12053.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-04-20T10:14:05Z",
        "closed_at": "2023-04-21T08:27:14Z",
        "merged_at": null,
        "body": "After #8015, When restarting a mater instance, before loading the rdb, we create the repl backlog in advance to ensure that any keys that have expired in the meantime are also propagated to the replication.\r\nAfter #9166, We will determine whether `server.master_repl_offset` is 0 after loading RDB to decide whether to release the unnecessary repl baklog.\r\nThe problem is that if we restart an instance without any replication and have expired keys, the repl backlog will be created and will not be released after loading the RDB until times out (default server.repl_backlog_time_limit).\r\n\r\n## Solutio\r\nShould use `repl_offset` from RDB info to determine if need to release the repl backlog, as the comment `if RDB doesn't have replication info or there is no rdb, it is not possible to support partial resynchronization`.\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 197,
        "deletions": 17,
        "changed_files": 9,
        "created_at": "2023-04-20T07:01:27Z",
        "closed_at": "2023-04-27T06:32:14Z",
        "merged_at": "2023-04-27T06:32:14Z",
        "body": "The change in #12018 break the CI (fixed by #12083).\r\nThere are quite a few sentinel commands that are missing both test coverage and also schema.\r\n\r\nPR added reply-schema to the following commands:\r\n- sentinel debug\r\n- sentinel info-cache\r\n- sentinel pendding-scripts\r\n- sentinel reset\r\n- sentinel simulate-failure\r\n\r\nAdded some very basic tests for other sentinel commands, just so that they have some coverage.\r\n- sentinel help\r\n- sentinel masters\r\n- sentinel myid\r\n- sentinel sentinels\r\n- sentinel slaves\r\n\r\nThese tests should be improved / replaced in a followup PR.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2023-04-19T20:32:38Z",
        "closed_at": "2023-04-20T08:50:29Z",
        "merged_at": "2023-04-20T08:50:29Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-18T20:52:33Z",
        "closed_at": "2023-04-19T06:55:56Z",
        "merged_at": "2023-04-19T06:55:56Z",
        "body": "#Minor test case addition for DECR and DECRBY.\r\n\r\nCurrently DECR and DECRBY do not have test case coverage for following scenarios:  DECR or DECRBY commands returns \"-1\" If the key is not exists.\r\n\r\nAdded the test cases for both commnands accordingly.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 27,
        "changed_files": 1,
        "created_at": "2023-04-18T14:18:33Z",
        "closed_at": "2023-04-23T14:02:09Z",
        "merged_at": "2023-04-23T14:02:09Z",
        "body": "The code in aeProcessEvent was testing AE_DONT_WAIT flag at the wrong time.\r\nThe flag is set by by beforeSleep, but was was tested before calling beforeSleep,\r\nwhich would result in aeProcessEvent waiting when it shouldn't have, impacting TLS's HasPendingData.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 24,
        "changed_files": 1,
        "created_at": "2023-04-18T13:18:51Z",
        "closed_at": "2023-04-19T06:23:47Z",
        "merged_at": "2023-04-19T06:23:47Z",
        "body": "1. it's a bad idea to print these errors and exit after daemonization (if we'll exit, the failure may not be detected)\r\n2. it's not nice to exit (or even do the check that uses `fork`) after modules already started (could create threads, or do some changes)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-18T10:35:07Z",
        "closed_at": "2023-04-24T05:20:01Z",
        "merged_at": "2023-04-24T05:20:01Z",
        "body": "Since we do report the RDB error in below:\r\n```\r\nserverLog(LL_WARNING,\"Error trying to save the DB, can't exit.\");\r\nif (server.supervised_mode == SUPERVISED_SYSTEMD)\r\n    redisCommunicateSystemd(\"STATUS=Error trying to save the DB, can't exit.\\n\");\r\ngoto error;\r\n```\r\n\r\nThis may be an overlook in #6052",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 19,
        "changed_files": 10,
        "created_at": "2023-04-18T09:53:53Z",
        "closed_at": "2023-04-18T13:14:27Z",
        "merged_at": "2023-04-18T13:14:27Z",
        "body": "In order to speed up tests, avoid saving an RDB (mostly notable on shutdown),\r\nexcept for tests that explicitly test the RDB mechanism\r\n\r\nIn addition, use `shutdown-on-sigterm force` to prevetn shutdown from failing\r\nin case the server is in the middle of the initial AOFRW\r\n\r\nAlso a a test that checks that the `shutdown-on-sigterm default` is to refuse\r\nshutdown if there's an initial AOFRW\r\n\r\nBase on #12049 and refresh it",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-17T16:38:44Z",
        "closed_at": "2023-04-18T05:32:10Z",
        "merged_at": "2023-04-18T05:32:10Z",
        "body": "#Minor test case addition.\r\nCurrently GETRANGE command does not have the test case coverage for the scenarios: An error is returned when key exists but of different type\r\nAdded  missing test cases for getrange command.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-04-17T12:07:36Z",
        "closed_at": "2023-04-18T14:17:22Z",
        "merged_at": "2023-04-18T14:17:22Z",
        "body": "The comment for dict_can_resize in dict.c should be updated. Currently 0 means DICT_RESIZE_ENABLE, but should actually be DICT_RESIZE_AVOID or 1.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 302,
        "deletions": 23,
        "changed_files": 13,
        "created_at": "2023-04-17T07:26:40Z",
        "closed_at": "2023-04-17T12:54:33Z",
        "merged_at": "2023-04-17T12:54:33Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2023-28856) Authenticated users can use the HINCRBYFLOAT command to create\r\n  an invalid hash field that will crash Redis on access\r\n\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Fix CLIENT REPLY OFF|SKIP to not silence push notifications (#11875)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 271,
        "deletions": 15,
        "changed_files": 16,
        "created_at": "2023-04-17T07:25:12Z",
        "closed_at": "2023-04-17T12:54:27Z",
        "merged_at": "2023-04-17T12:54:27Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2023-28856) Authenticated users can use the HINCRBYFLOAT command to create\r\n  an invalid hash field that will crash Redis on access\r\n\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Fix CLIENT REPLY OFF|SKIP to not silence push notifications (#11875)\r\n* Disconnect pub-sub subscribers when revoking allchannels permission (#11992)\r\n* Trim excessive memory usage in stream nodes when exceeding `stream-node-max-bytes` (#11885)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 209,
        "deletions": 47,
        "changed_files": 16,
        "created_at": "2023-04-17T07:22:12Z",
        "closed_at": "2023-04-17T12:54:04Z",
        "merged_at": "2023-04-17T12:54:04Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2023-28856) Authenticated users can use the HINCRBYFLOAT command to create\r\n  an invalid hash field that will crash Redis on access\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Add a missing fsync of AOF file in rare cases (#11973)\r\n* Disconnect pub-sub subscribers when revoking allchannels permission (#11992)\r\n\r\nPlatform / toolchain support related improvements\r\n=================================================\r\n\r\n* Fix a compiler fortification induced crash when used with link time optimizations (#11982)\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 13,
        "changed_files": 5,
        "created_at": "2023-04-16T16:15:31Z",
        "closed_at": "2023-04-17T18:05:36Z",
        "merged_at": "2023-04-17T18:05:36Z",
        "body": "The nightly tests showed that the recent PR #12022 caused random failures in aof.tcl on checking RDB preamble inside an AOF file.\r\n\r\nRoot cause:\r\nWhen checking RDB preamble in an AOF file, what's passed into redis_check_rdb is aof_filename, not aof_filepath. The newly introduced isFifo function does not check return status of the stat call and hence uses the uninitailized stat_p object.\r\n\r\nFix:\r\n1. Fix isFifo by checking stat call's return code.\r\n2. Pass aof_filepath instead of aof_filename to redis_check_rdb.\r\n3. move the FIFO check to rdb.c since the limitation is the re-opening of the file, and not anything specific about redis-check-rdb.",
        "comments": 15
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 13,
        "changed_files": 6,
        "created_at": "2023-04-16T09:08:03Z",
        "closed_at": "2023-04-18T10:32:20Z",
        "merged_at": null,
        "body": "Handles https://github.com/redis/redis/pull/12013#discussion_r1161699047\r\n\r\nIn order to speed up tests, avoid saving an RDB (most notable on shutdown), except for tests that explicitly test the RDB mechanism\r\n\r\nIn addition, use `shutdown-on-sigterm force` to prevent shutdown from failing in case the server is in the middle of the initial AOFRW",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-04-14T07:10:37Z",
        "closed_at": "2023-04-16T12:58:54Z",
        "merged_at": null,
        "body": "Without this fix, when the total is greater than LAZYFREE_THRESHOLD\r\nbut each is less than LAZYFREE_THRESHOLD, we will free these in a\r\nforeground way which could be inefficient (unlink how it works with\r\nother structures).\r\n\r\nLAZYFREE_THRESHOLD is more tied to the number of allocations, compute\r\nthe total for comparison.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-04-14T05:04:02Z",
        "closed_at": "2023-05-11T05:15:00Z",
        "merged_at": "2023-05-11T05:15:00Z",
        "body": "When using scan in redis-cli, the SCAN COUNT is fixed, which means the\r\nfull scan can take a long time if there are a lot of keys, this will let users specify\r\na bigger COUNT option.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2023-04-13T19:52:21Z",
        "closed_at": "2023-04-17T10:16:30Z",
        "merged_at": null,
        "body": "**Problem statement :** \r\nAs a part of pr ( https://github.com/redis/redis/pull/9504 (Treat subcommands as commands) which was merged in last release) subcommands are also treated as command and it is now possible to use COMMAND directly on subcommands :\r\n\r\n**Current behaviors:**\r\nWe have another subcommand \"memory usage\" which seems to work fine\r\n\r\n**_127.0.0.1:6379> command getkeys memory usage key_**\r\n**_1) \"key\"_**\r\n\r\n**_127.0.0.1:6379> command getkeysandflags memory usage key_**\r\n**_1) 1) \"key\"_**\r\n    **_2) 1) RO_**\r\n\r\nWhen I tried to run the command \"COMMAND GETKEYS CLUSTER KEYSLOT key1\" I have encountered the error \r\n\r\n**_127.0.0.1:6379> command getkeys CLUSTER KEYSLOT k\r\n(error) ERR Invalid command specified_**\r\n\r\n**_127.0.0.1:6379> command getkeysandflags CLUSTER KEYSLOT k\r\n(error) ERR Invalid command specified_**\r\n\r\n**Expected behavior :** \r\n_**127.0.0.1:6379> command getkeys CLUSTER KEYSLOT key**_\r\n**_1) \"key\"_**\r\n_**127.0.0.1:6379> command getkeysandflags CLUSTER KEYSLOT key**_\r\n**_1) 1) \"key\"_**\r\n    **_2) 1) RO_**\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-13T14:19:45Z",
        "closed_at": "2023-04-13T18:48:26Z",
        "merged_at": "2023-04-13T18:48:26Z",
        "body": "There is are some missing test cases for substr command.\r\n\r\nAdded 3 test case.\r\n1.  start > stop\r\n2.  start and stop both greater than string length\r\n3.  when no key is present.\r\n![image](https://user-images.githubusercontent.com/51993843/231789451-dc95883f-e86b-4a7f-a78c-98d7a6e3014e.png)\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-04-13T10:04:53Z",
        "closed_at": "2023-04-16T10:02:47Z",
        "merged_at": "2023-04-16T10:02:47Z",
        "body": "Improve performance by avoiding redundancy memory malloc/free",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 128,
        "deletions": 66,
        "changed_files": 11,
        "created_at": "2023-04-13T08:59:41Z",
        "closed_at": "2023-04-18T06:53:51Z",
        "merged_at": "2023-04-18T06:53:51Z",
        "body": "This PR is to fix the compilation warnings and errors generated by the latest complier toolchain, and to add a new runner of the latest toolchain for daily CI.\r\n\r\n## Fix various compilation warnings and errors\r\n\r\n1) jemalloc.c\r\n\r\nCOMPILER: clang-14 with FORTIFY_SOURCE\r\n\r\nWARNING:\r\n```\r\nsrc/jemalloc.c:1028:7: warning: suspicious concatenation of string literals in an array initialization; did you mean to separate the elements with a comma? [-Wstring-concatenation]\r\n                    \"/etc/malloc.conf\",\r\n                    ^\r\nsrc/jemalloc.c:1027:3: note: place parentheses around the string literal to silence warning\r\n                \"\\\"name\\\" of the file referenced by the symbolic link named \"\r\n                ^\r\n```\r\n\r\nREASON:  the compiler to alert developers to potential issues with string concatenation that may miss a comma,\r\njust like #9534 which misses a comma.\r\n\r\nSOLUTION: use `()` to tell the compiler that these two line strings are continuous.\r\n\r\n2) config.h\r\n\r\nCOMPILER: clang-14 with FORTIFY_SOURCE\r\n\r\nWARNING:\r\n```\r\nIn file included from quicklist.c:36:\r\n./config.h:319:76: warning: attribute declaration must precede definition [-Wignored-attributes]\r\nchar *strcat(char *restrict dest, const char *restrict src) __attribute__((deprecated(\"please avoid use of unsafe C functions. prefer use of redis_strlcat instead\")));\r\n```\r\n\r\nREASON: Enabling _FORTIFY_SOURCE will cause the compiler to use `strcpy()` with check, it results in a deprecated attribute declaration after including <features.h>.\r\n\r\nSOLUTION: move the deprecated attribute declaration from config.h to fmacro.h before \"#include <features.h>\".\r\n\r\n3) networking.c\r\n\r\nCOMPILER: GCC-12\r\n\r\nWARNING: \r\n```\r\nnetworking.c: In function \u2018addReplyDouble.part.0\u2019:\r\nnetworking.c:876:21: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]\r\n  876 |         dbuf[start] = '$';\r\n      |                     ^\r\nnetworking.c:868:14: note: at offset -5 into destination object \u2018dbuf\u2019 of size 5152\r\n  868 |         char dbuf[MAX_LONG_DOUBLE_CHARS+32];\r\n      |              ^\r\nnetworking.c:876:21: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]\r\n  876 |         dbuf[start] = '$';\r\n      |                     ^\r\nnetworking.c:868:14: note: at offset -6 into destination object \u2018dbuf\u2019 of size 5152\r\n  868 |         char dbuf[MAX_LONG_DOUBLE_CHARS+32];\r\n```\r\n\r\nREASON: GCC-12 predicts that digits10() may return 9 or 10 through `return 9 + (v >= 1000000000UL)`.\r\n\r\nSOLUTION: add an assert to let the compiler know the possible length;\r\n\r\n4) redis-cli.c & redis-benchmark.c\r\n\r\nCOMPILER: clang-14 with FORTIFY_SOURCE\r\n\r\nWARNING:\r\n```\r\nredis-benchmark.c:1621:2: warning: embedding a directive within macro arguments has undefined behavior [-Wembedded-directive] #ifdef USE_OPENSSL\r\nredis-cli.c:3015:2: warning: embedding a directive within macro arguments has undefined behavior [-Wembedded-directive] #ifdef USE_OPENSSL\r\n```\r\n\r\nREASON: when _FORTIFY_SOURCE is enabled, the compiler will use the print() with check, which is a macro. this may result in the use of directives within the macro, which is undefined behavior.\r\n\r\nSOLUTION: move the directives-related code out of `print()`.\r\n\r\n5) server.c\r\n\r\nCOMPILER: gcc-13 with FORTIFY_SOURCE\r\n\r\nWARNING:\r\n```\r\nIn function 'lookupCommandLogic',\r\n    inlined from 'lookupCommandBySdsLogic' at server.c:3139:32:\r\nserver.c:3102:66: error: '*(robj **)argv' may be used uninitialized [-Werror=maybe-uninitialized]\r\n 3102 |     struct redisCommand *base_cmd = dictFetchValue(commands, argv[0]->ptr);\r\n      |                                                              ~~~~^~~\r\n```\r\n\r\nREASON: The compiler thinks that the `argc` returned by `sdssplitlen()` could be 0, resulting in an empty array of size 0 being passed to lookupCommandLogic.\r\nthis should be a false positive, `argc` can't be 0 when strings are not NULL.\r\n\r\nSOLUTION: add an assert to let the compiler know that `argc` is positive.\r\n\r\n6) sha1.c\r\n\r\nCOMPILER: gcc-12\r\n\r\nWARNING:\r\n```\r\nIn function \u2018SHA1Update\u2019,\r\n    inlined from \u2018SHA1Final\u2019 at sha1.c:195:5:\r\nsha1.c:152:13: warning: \u2018SHA1Transform\u2019 reading 64 bytes from a region of size 0 [-Wstringop-overread]\r\n  152 |             SHA1Transform(context->state, &data[i]);\r\n      |             ^\r\nsha1.c:152:13: note: referencing argument 2 of type \u2018const unsigned char[64]\u2019\r\nsha1.c: In function \u2018SHA1Final\u2019:\r\nsha1.c:56:6: note: in a call to function \u2018SHA1Transform\u2019\r\n   56 | void SHA1Transform(uint32_t state[5], const unsigned char buffer[64])\r\n      |      ^\r\nIn function \u2018SHA1Update\u2019,\r\n    inlined from \u2018SHA1Final\u2019 at sha1.c:198:9:\r\nsha1.c:152:13: warning: \u2018SHA1Transform\u2019 reading 64 bytes from a region of size 0 [-Wstringop-overread]\r\n  152 |             SHA1Transform(context->state, &data[i]);\r\n      |             ^\r\nsha1.c:152:13: note: referencing argument 2 of type \u2018const unsigned char[64]\u2019\r\nsha1.c: In function \u2018SHA1Final\u2019:\r\nsha1.c:56:6: note: in a call to function \u2018SHA1Transform\u2019\r\n   56 | void SHA1Transform(uint32_t state[5], const unsigned char buffer[64])\r\n```\r\n\r\nREASON: due to the bug[https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80922], when enable LTO, gcc-12 will not see `diagnostic ignored \"-Wstringop-overread\"`, resulting in a warning.\r\n\r\nSOLUTION: temporarily set SHA1Update to noinline to avoid compiler warnings due to LTO being enabled until the above gcc bug is fixed.\r\n\r\n7) zmalloc.h\r\n\r\nCOMPILER: GCC-12\r\n\r\nWARNING: \r\n```\r\nIn function \u2018memset\u2019,\r\n    inlined from \u2018moduleCreateContext\u2019 at module.c:877:5,\r\n    inlined from \u2018RM_GetDetachedThreadSafeContext\u2019 at module.c:8410:5:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:10: warning: \u2018__builtin_memset\u2019 writing 104 bytes into a region of size 0 overflows the destination [-Wstringop-overflow=]\r\n   59 |   return __builtin___memset_chk (__dest, __ch, __len,\r\n```\r\n\r\nREASON: due to the GCC-12 bug [https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96503], GCC-12 cannot see alloc_size, which causes GCC to think that the actual size of memory is 0 when checking with __glibc_objsize0().\r\n\r\nSOLUTION: temporarily set malloc-related interfaces to `noinline` to avoid compiler warnings due to LTO being enabled until the above gcc bug is fixed.\r\n\r\n## Other changes\r\n1) Fixed `ps -p [pid]`  doesn't output `<defunct>` when using procps 4.x causing `replication child dies when parent is killed - diskless` test to fail.\r\n2) Add a new fortify CI with GCC-13 and ubuntu-lunar docker image.",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 3,
        "changed_files": 7,
        "created_at": "2023-04-12T22:06:08Z",
        "closed_at": "2023-04-21T19:56:58Z",
        "merged_at": null,
        "body": "Issue: https://github.com/redis/redis/issues/12031\r\n\r\nThis change introduces a new FLUSHDB sub-event REDISMODULE_SUBEVENT_FLUSHDB_FULLSYNC_POST_FLUSHDB, which is fired during full sync on the replica, at the end of flushing old data from the replica. What drives this change is the need to indicate the end of FLUSHDB triggered by FULLSYNC replication (not by FLUSHDB/FLUSHALL command), so that the subscriber can do some additional cleanup.\r\n\r\nOne use case is that a redis module defines a module type aux data that represents a type of global resources living outside of the Redis keyspace. When FLUSHDB/FLUSHALL command is invoked, we do not want to drop the aux data because it is a global resource just like Redis functions. However, during FULLSYNC, when the old data in the replica is flushed (triggered by FULLSYNC), we want to drop the aux data. Otherwise, the old aux data will stay in the replica after full sync, ending up with the replica being not identical to the primary, which is a wrong behavior. This can be achieved by subscribing to the FLUSHDB event notification. If the sub-event is REDISMODULE_SUBEVENT_FLUSHDB_FULLSYNC_POST_FLUSHDB, we can perform some additional cleanup.",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-04-12T05:54:56Z",
        "closed_at": "2023-04-24T06:01:40Z",
        "merged_at": "2023-04-24T06:01:40Z",
        "body": "Every time when accept a connection, we add the client to `server.clients` list's tail, but in `clientsCron` we rotate the tail to head at first, and then process the head. It means that the \"new\" client would be processed before \"old\" client, moreover if connections established and then freed frequently, the \"old\" client may have no chance to be processed.\r\n\r\nTo fix it, we need take a fair way to iterate the list, that is take the current head and process, and then rotate the head to tail, thus we can make sure all clients could be processed step by step.\r\n\r\np.s. client has `client_list_node` pointer, we don't need put the current client to head to avoid O(N) when remove it.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-12T04:38:13Z",
        "closed_at": "2023-04-13T01:23:00Z",
        "merged_at": "2023-04-13T01:23:00Z",
        "body": "Add a print statement to indicate which IP/port is sending\r\nthe attack. So that the offending connection can be tracked\r\ndown, if necessary.\r\n\r\nClose #12007",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 105,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-04-12T03:29:24Z",
        "closed_at": "2023-04-12T09:23:51Z",
        "merged_at": "2023-04-12T09:23:51Z",
        "body": "In daily.yml, if the input suggests we don't run the full testsuite,\r\ndo not pass --fail-commands-not-all-hit to the validator.\r\n\r\nThis fixes the first point in #11954. Credit goes to the comment\r\non the open issue for GH actions: actions/runner#409\r\n\r\nAlso improve prints to show the dispatch arguments in every job.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-11T18:24:47Z",
        "closed_at": "2023-04-14T23:14:27Z",
        "merged_at": "2023-04-14T23:14:27Z",
        "body": "When loading RDB over the named piped, redis_check_rdb() is hung at fopen, because fopen blocks until another process opens the FIFO for writing. The fix is to check if RDB is FIFO. If yes, return an error.\r\n\r\nNote: currently redis doesn't use this function on a FIFO.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-11T04:48:58Z",
        "closed_at": "2023-04-11T08:14:17Z",
        "merged_at": "2023-04-11T08:14:17Z",
        "body": "We do have ZREMRANGEBYLEX tests, but it is a stress test\r\nmarked with slow tag and then skipped in reply-schemas daily.\r\n\r\nIn the past, we were able to succeed on a daily, i guess\r\nit was because there were some random command executions,\r\nsuch as corrupt-dump-fuzzy, which might call it.\r\n\r\nThese test examples are taken from ZRANGEBYLEX basics test.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 210,
        "deletions": 211,
        "changed_files": 43,
        "created_at": "2023-04-11T01:58:31Z",
        "closed_at": "2023-05-03T00:31:32Z",
        "merged_at": "2023-05-03T00:31:32Z",
        "body": "Technically declaring a prototype with an empty declaration has been deprecated since the early days of C, but we never got a warning for it. C2x will apparently be introducing a breaking change if you are using this type of declarator, so Clang 15 has started issuing a warning with `-pedantic`. Although not apparently a problem for any of the compiler we build on, if feels like the right thing is to properly adhere to the C standard and use (void). \r\n\r\nI'm not very happy this interferes with git history though, so maybe the better option is to just opt-out of the warning on Clang. ",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 39,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2023-04-11T01:55:56Z",
        "closed_at": "2023-05-08T23:11:21Z",
        "merged_at": "2023-05-08T23:11:20Z",
        "body": "For sets and hashes that will eventually be stores as the hash encoding, it's much faster to immediately convert them to their hash encoding and then perform the insertions since it avoids the O(N) search and frequent reallocations. This change checks the number of arguments in the incoming command, and converts the data-structure if the number of new entries exceeds the listpack-max-entries configuration. This can cause us to over-allocate memory if their are duplicate entries in the input, which is unexpected. \r\n\r\nSome very basic benchmarking using \r\n```\r\nredis-benchmark -r 1000000 -n 100000 HSET __rand_int__ `python -c \"print(' '.join([str(a) for a in range(1300)]))\"`\r\n```\r\n\r\nunstable\r\n```\r\nSummary:\r\n  throughput summary: 805.54 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n       61.908    25.680    68.351    73.279    75.967    79.295\r\n```\r\n\r\nhset-improvement\r\n```\r\nSummary:\r\n  throughput summary: 4701.46 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n       10.546     0.832    11.959    12.471    13.119    14.967\r\n```\r\n\r\nThis change was originally inspired by an AWS user who was running into performance bottlenecks because they were were inserting 600 hash entries. In the end these were being stored as a hashmap, but while inserting they were being stored as a listpack. These hashes are incrementally inserted into, so there was 600 O(N) searches to see if the element was in the list and then ~600 O(N) copies of the data to realloc the new item into it. We solved their problem by having them update the max_list_pack entry to 1 so the conversion happens sooner, but it seems unnecessary to use the expensive listpack insertions at all. So this change just immediately converts to a hashmap if the number of elements in the ARGV is larger than the conversion.\r\n\r\n\r\nThere is probably a lot more smart optimizations we could do here. I think there is two use cases:\r\n1. Creating a new record. (This seems not well optimized)\r\n2. Updating a single record in an existing field. (This seems fine)\r\n\r\n```\r\nRelease notes\r\nOptimize the performance of inserting entries toto hashes and sets that exceed the max size for listpacks.\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 5,
        "changed_files": 5,
        "created_at": "2023-04-10T20:39:23Z",
        "closed_at": "2023-04-19T06:08:11Z",
        "merged_at": "2023-04-19T06:08:11Z",
        "body": "Some sentinel subcommands are missing the reply_schema in the json file,\r\nso add the proper reply_schema part in json file as sentinel replicas commands.\r\n\r\nThe schema validator was skipping coverage test for sentinel commands, this was initially\r\ndone just in order to focus on redis commands and leave sentinel coverage for later,\r\nso this check is now removed.\r\n\r\nsentinel commands that were missing reply schema:\r\n* sentinel masters\r\n* sentinel myid\r\n* sentinel sentinels <master-name>\r\n* sentinel slaves (deprecated)  <master-name> ",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-04-10T19:31:49Z",
        "closed_at": "2023-04-13T09:17:28Z",
        "merged_at": "2023-04-13T09:17:28Z",
        "body": "**Problem:**\r\n\r\nThe description of reply_schema for the info and hset command was not updated and left without any decription field in the json file.\r\n\r\n\r\n**Solution:**\r\nUpdated the json file with reply_schema description for the commands as needed.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 143,
        "deletions": 109,
        "changed_files": 22,
        "created_at": "2023-04-10T08:16:07Z",
        "closed_at": "2023-04-12T06:19:21Z",
        "merged_at": "2023-04-12T06:19:21Z",
        "body": "The MacOS CI in github actions often hangs without any logs. GH argues that it's due to resource utilization, either running out of disk space, memory, or CPU starvation, and thus the runner is terminated.\r\n\r\nThis PR contains multiple attempts to resolve this:\r\n1. introducing pause_process instead of SIGSTOP, which waits for the process to stop before resuming the test, possibly resolving race conditions in some tests, this was a suspect since there was one test that could result in an infinite loop in that case, in practice this didn't help, but still a good idea to keep.\r\n2. disable the `save` config in many tests that don't need it, specifically ones that use heavy writes and could create large files.\r\n3. change the `populate` proc to use short pipeline rather than an infinite one.\r\n4. use `--clients 1` in the macos CI so that we don't risk running multiple resource demanding tests in parallel.\r\n5. enable `--verbose` to be repeated to elevate verbosity and print more info to stdout when a test or a server starts.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-04-04T12:54:09Z",
        "closed_at": "2023-04-05T11:45:42Z",
        "merged_at": "2023-04-05T11:45:42Z",
        "body": "This test produces 1GB of data and moves it around, and was expecting less than 500kb to be present in the system page cache.\r\nIt sometimes fails with up to some 6mb in the page cache (0 in the actual RDB files), increasing the threshold.\r\nIt looks like some background tasks in the container are occupying the page cache.\r\n\r\nIt is safe to ignore the above since we also explicitly check the pages of our dump.rdb are not cached (matching `vmtouch -v` to `0%`).\r\nAn additional fix is to match ` 0%` (add space), so that we don't successfully match `10%`.\r\n\r\ndetails in https://github.com/redis/redis/pull/11818",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-04-03T11:13:29Z",
        "closed_at": "2023-04-16T12:49:26Z",
        "merged_at": "2023-04-16T12:49:26Z",
        "body": "this pr fix two wrongs\uff1a\r\n1. When client\u2019s querybuf is pre-allocated for a fat argv, we need to update the querybuf_peak of the client immediately to completely avoid the unexpected shrinking of querybuf in the next clientCron (before data arrives to set the peak).\r\n2. the protocol's bulklen does not include `\\r\\n`, but the allocation and the data we read does. so in `clientsCronResizeQueryBuffer`, the `resize` or `querybuf_peak` should add these 2 bytes.\r\n\r\nthe first bug is likely to hit us on large payloads over slow connections, in which case transferring the payload can take longer and a cron event will be triggered (specifically if there are not a lot of clients)",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-04-03T10:13:43Z",
        "closed_at": "2023-04-04T13:01:06Z",
        "merged_at": null,
        "body": "This pull request fixed the outdated Redis documentation URLs.\r\n\r\n```diff\r\n-https://redis.io/documentation\r\n+https://redis.io/docs/\r\n```\r\n\r\nChecked by `grep` command:\r\n\r\n```shell\r\ngrep -R 'https://redis.io/documentation' .\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-04-03T09:08:03Z",
        "closed_at": "2023-04-04T06:05:53Z",
        "merged_at": "2023-04-04T06:05:53Z",
        "body": "In cluster mode, when a node restart as a replica, it doesn't immediately\r\nsync with the master, replication is enabled in clusterCron. It means that\r\nsometime server.masterhost is NULL and we wrongly judge it in beforeSleep.\r\n\r\nIn this case, we may trigger a fast activeExpireCycle in beforeSleep, but the\r\nnode's flag is actually a replica, that can lead to data inconsistency.  In this\r\nPR, we use iAmMaster to replace the `server.masterhost == NULL`\r\n\r\nThis is an overlook in #7001, and more discussion in #11783.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-04-03T07:16:53Z",
        "closed_at": "2023-04-04T12:56:34Z",
        "merged_at": "2023-04-04T12:56:34Z",
        "body": "```\r\n127.0.0.1:6379> client help\r\n ...\r\n38) SETINFO <option> <value>\r\n39)     Set client meta attr. Options are:\r\n40)     * LIB-NAME: the client lib name.\r\n41)     * LIB-VER: the client lib version.\r\n...\r\n127.0.0.1:6379>\r\n```\r\n\r\nI have tried to implement help message directly in `clientSetinfoCommand`, but its `arity` has been fixed to 4, so there will be more changes.\r\n\r\nTherefore, this PR directly modifies clientCommand.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-04-02T09:38:46Z",
        "closed_at": "2023-04-02T13:18:29Z",
        "merged_at": "2023-04-02T13:18:29Z",
        "body": "The existing logic for killing pub-sub clients did not handle the `allchannels` permission correctly. For example, if you:\r\n\r\n    ACL SETUSER foo allchannels\r\n\r\nHave a client authenticate as the user `foo` and subscribe to a channel, and then:\r\n\r\n    ACL SETUSER foo resetchannels\r\n\r\nThe subscribed client would not be disconnected, though new clients under that user would be blocked from subscribing to any channels.\r\n\r\nThis was caused by an incomplete optimization in `ACLKillPubsubClientsIfNeeded` checking whether the new channel permissions were a strict superset of the old ones.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 118,
        "deletions": 37,
        "changed_files": 6,
        "created_at": "2023-03-30T07:51:22Z",
        "closed_at": "2023-04-10T17:38:41Z",
        "merged_at": "2023-04-10T17:38:41Z",
        "body": "First of all, thanks for @siddhesh \r\nThis PR references https://github.com/systemd/systemd/commit/7929e180aa47a2692ad4f053afac2857d7198758 and https://github.com/systemd/systemd/commit/4f79f545b3c46c358666c9f5f2b384fe50aac4b4\r\n\r\nRelated to #11965\r\n\r\n## Issue\r\nWhen we use GCC-12 later or clang 9.0 later to build with `-D_FORTIFY_SOURCE=3`, we can see the following buffer overflow:\r\n```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n6263:M 06 Apr 2023 08:59:12.915 # Redis 255.255.255 crashed by signal: 6, si_code: -6\r\n6263:M 06 Apr 2023 08:59:12.915 # Crashed running the instruction at: 0x7f03d59efa7c\r\n\r\n------ STACK TRACE ------\r\nEIP:\r\n/lib/x86_64-linux-gnu/libc.so.6(pthread_kill+0x12c)[0x7f03d59efa7c]\r\n\r\nBacktrace:\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x42520)[0x7f03d599b520]\r\n/lib/x86_64-linux-gnu/libc.so.6(pthread_kill+0x12c)[0x7f03d59efa7c]\r\n/lib/x86_64-linux-gnu/libc.so.6(raise+0x16)[0x7f03d599b476]\r\n/lib/x86_64-linux-gnu/libc.so.6(abort+0xd3)[0x7f03d59817f3]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x896f6)[0x7f03d59e26f6]\r\n/lib/x86_64-linux-gnu/libc.so.6(__fortify_fail+0x2a)[0x7f03d5a8f76a]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x1350c6)[0x7f03d5a8e0c6]\r\nsrc/redis-server 127.0.0.1:25111(+0xd5e80)[0x557cddd3be80]\r\nsrc/redis-server 127.0.0.1:25111(feedReplicationBufferWithObject+0x78)[0x557cddd3c768]\r\nsrc/redis-server 127.0.0.1:25111(replicationFeedSlaves+0x1a4)[0x557cddd3cbc4]\r\nsrc/redis-server 127.0.0.1:25111(+0x8721a)[0x557cddced21a]\r\nsrc/redis-server 127.0.0.1:25111(call+0x47a)[0x557cddcf38ea]\r\nsrc/redis-server 127.0.0.1:25111(processCommand+0xbf4)[0x557cddcf4aa4]\r\nsrc/redis-server 127.0.0.1:25111(processInputBuffer+0xe6)[0x557cddd22216]\r\nsrc/redis-server 127.0.0.1:25111(readQueryFromClient+0x3a8)[0x557cddd22898]\r\nsrc/redis-server 127.0.0.1:25111(+0x1b9134)[0x557cdde1f134]\r\nsrc/redis-server 127.0.0.1:25111(aeMain+0x119)[0x557cddce5349]\r\nsrc/redis-server 127.0.0.1:25111(main+0x466)[0x557cddcd6716]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7f03d5982d90]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7f03d5982e40]\r\nsrc/redis-server 127.0.0.1:25111(_start+0x25)[0x557cddcd7025]\r\n```\r\n\r\nThe main reason is that when FORTIFY_SOURCE is enabled, GCC or clang will enhance some \r\ncommon functions, such as `strcpy`, `memcpy`, `fgets`, etc, so that they can detect buffer \r\noverflow errors and stop program execution, thus improving the safety of the program.\r\nWe use `zmalloc_usable_size()` everywhere to use memory blocks, but that is an abuse since the\r\nmalloc_usable_size() isn't meant for this kind of use, it is for diagnostics only. That is also why the\r\nbehavior is flaky when built with _FORTIFY_SOURCE, the compiler can sense that we reach outside\r\nthe allocated block and SIGABRT.\r\n\r\n### Solution\r\nIf we need to use the additional memory we got, we need to use a dummy realloc with `alloc_size` attribute\r\nand no inlining, (see `extend_to_usable`) to let the compiler see the large of memory we need to use.\r\nThis can either be an implicit call inside `z*usable` that returns the size, so that the caller doesn't have any other worry, or it can be a normal zmalloc call which means that if the caller wants to use zmalloc_usable_size it must also use extend_to_usable.\r\n\r\n### Changes\r\n\r\nThis PR does the following:\r\n1) rename the current\u00a0z[try]malloc_usable\u00a0family to\u00a0z[try]malloc_internal\u00a0and don't expose them to users outside zmalloc.c,\r\n2) expose a new set of\u00a0`z[*]_usable`\u00a0family that use\u00a0z[*]_internal\u00a0and\u00a0`extend_to_usable()`\u00a0implicitly, the caller gets the size of the allocation and it is safe to use.\r\n3) go over all the users of\u00a0`zmalloc_usable_size`\u00a0and convert them to use the\u00a0`z[*]_usable`\u00a0family if possible.\r\n4) in the places where the caller can't use `z[*]_usable`\u00a0and store the real size, and must still rely on zmalloc_usable_size, we still make sure that the allocation used `z[*]_usable` (which has a call to `extend_to_usable()`) and ignores the returning size, this way a later call to `zmalloc_usable_size` is still safe.\r\n\r\n[4] was done for module.c and listpack.c, all the others places (sds, reply proto list, replication backlog, client->buf) are using [3].\r\n\r\n## NOTE\r\nfor 7.0 backport we don't declare malloc_size attributes in zmalloc.h so that we don't take the risk of inducing any crashes in a bugfix release, so will only have effect if LTO was enforced from outside.\r\n",
        "comments": 29
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-03-30T00:53:52Z",
        "closed_at": "2023-03-30T03:05:59Z",
        "merged_at": null,
        "body": "Added a test example where the string is empty under scan in Scan.tcl",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-03-29T15:01:28Z",
        "closed_at": "2023-03-30T00:45:46Z",
        "merged_at": null,
        "body": "@hwware I have added the test example when scan is an empty string in Scan.tcl. Could you please help me merge the code ?",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-03-27T20:05:55Z",
        "closed_at": "2023-04-02T16:19:45Z",
        "merged_at": "2023-04-02T16:19:45Z",
        "body": "In the Redis 7.0 and newer version,\r\nconfig set command support multiply `<parameter> <value>` pairs, thus the previous sensitive command condition\r\ndoes not apply any more\r\nFor example:\r\n\r\nThe command:\r\n**config set maxmemory 1GB masteruser aa** will be written to redis_cli historyfile\r\n\r\nIn this PR, we update the condition for these sensitive commands\r\nconfig set masteruser <username>\r\nconfig set masterauth <master-password>\r\nconfig set requirepass foobared\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2023-03-26T17:59:35Z",
        "closed_at": "2023-04-12T09:10:14Z",
        "merged_at": null,
        "body": "Modified the judgment statement if in parseScanCursorOrReply so that an empty string is considered an error.\r\nFor empty string, strtoul returns 0, and eptr is set to the end of the string (`'\\0'`), the existing code would have treated that as success.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 64,
        "deletions": 13,
        "changed_files": 4,
        "created_at": "2023-03-26T14:06:50Z",
        "closed_at": "2023-03-29T12:17:06Z",
        "merged_at": "2023-03-29T12:17:06Z",
        "body": "This PR fix several unrelated bugs that were discovered by the same set of tests\r\n(WAITAOF tests in #11713), could make the `WAITAOF` test hang. \r\n\r\nThe change in `backgroundRewriteDoneHandler` is about MP-AOF.\r\nThat leftover / old code assumes that we started a new AOF file just now\r\n(when we have a new base into which we're gonna incrementally write), but\r\nthe fact is that with MP-AOF, the fork done handler doesn't really affect the\r\nincremental file being maintained by the parent process, there's no reason to\r\nre-issue `SELECT`, and no reason to update any of the fsync variables in that flow.\r\nThis should have been deleted with MP-AOF (introduced in #9788, 7.0).\r\nThe damage is that the update to `aof_fsync_offset` will cause us to miss an fsync\r\nin `flushAppendOnlyFile`, that happens if we stop write commands in `AOF_FSYNC_EVERYSEC`\r\nwhile an AOFRW is in progress. This caused a new `WAITAOF` test to sometime hang forever.\r\n\r\nAlso because of MP-AOF, we needed to change `aof_fsync_offset` to `aof_last_incr_fsync_offset`\r\nand match it to `aof_last_incr_size` in `flushAppendOnlyFile`. This is because in the past we compared\r\n`aof_fsync_offset` and `aof_current_size`, but with MP-AOF it could be the total AOF file will be\r\nsmaller after AOFRW, and the (already existing) incr file still has data that needs to be fsynced.\r\n\r\nThe change in `flushAppendOnlyFile`, about the `AOF_FSYNC_ALWAYS`, it is follow #6053\r\n(the details is in #5985), we also check `AOF_FSYNC_ALWAYS` to handle a case where\r\nappendfsync is changed from everysec to always while there is data that's written but not yet fsynced.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-03-25T02:56:56Z",
        "closed_at": "2023-03-26T05:39:05Z",
        "merged_at": "2023-03-26T05:39:05Z",
        "body": "Since we remove the COMMAND COUNT call in sentinel test in #11950,\r\nreply-schemas-validator started reporting this error:\r\n```\r\nWARNING! The following commands were not hit at all:\r\n  command|count\r\n  ERROR! at least one command was not hit by the tests\r\n```\r\n\r\nThis PR add a COMMAND COUNT test to cover it and also fix some\r\ntypos in req-res-log-validator.py",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2023-03-25T00:51:57Z",
        "closed_at": "2023-03-30T02:58:52Z",
        "merged_at": "2023-03-30T02:58:52Z",
        "body": "In https://github.com/redis/redis/pull/11012, we changed the way command durations were computed to handle the same command being executed multiple times. This commit fixes some misses from that commit.\r\n1. Wait commands were not correctly reporting their duration if the timeout was reached.\r\n2. Multi/scripts/and modules with RM_Call were not properly resetting the duration between inner calls, leading to them reporting cumulative duration.\r\n3. When a blocked client is freed, the call and duration are always discarded. \r\n\r\nThis commit also adds an assert if the duration is not properly reset, potentially indicating that a report to call statistics was missed. The assert potentially be removed in the future, as it's mainly intended to detect misses in tests.\r\n\r\n## Before:\r\n```\r\n(error) ERR EXEC without MULTI\r\n127.0.0.1:6379> multi\r\nOK\r\n127.0.0.1:6379> debug sleep 0.1\r\nQUEUED\r\n127.0.0.1:6379> debug sleep 0.1\r\nQUEUED\r\n127.0.0.1:6379> info commandstats\r\n127.0.0.1:6379> exec\r\n1) OK\r\n2) OK\r\n3) \"# Commandstats\\r\\ncmdstat_exec:calls=1,usec=4,usec_per_call=4.00,rejected_calls=0,failed_calls=1\\r\\ncmdstat_debug:calls=2,usec=300206,usec_per_call=150103.00,rejected_calls=0,failed_calls=0\\r\\ncmdstat_multi:calls=1,usec=5,usec_per_call=5.00,rejected_calls=0,failed_calls=0\\r\\n\"\r\n```\r\n\r\n## After:\r\n```\r\n127.0.0.1:6379> multi\r\nOK\r\n127.0.0.1:6379> debug sleep 0.1\r\nQUEUED\r\n127.0.0.1:6379> debug sleep 0.1\r\nQUEUED\r\n127.0.0.1:6379> exec\r\n1) OK\r\n2) OK\r\n127.0.0.1:6379> info commandstats\r\n# Commandstats\r\ncmdstat_exec:calls=2,usec=200176,usec_per_call=100088.00,rejected_calls=0,failed_calls=1\r\ncmdstat_info:calls=1,usec=35,usec_per_call=35.00,rejected_calls=0,failed_calls=0\r\ncmdstat_debug:calls=2,usec=200132,usec_per_call=100066.00,rejected_calls=0,failed_calls=0\r\ncmdstat_multi:calls=2,usec=4,usec_per_call=2.00,rejected_calls=0,failed_calls=0\r\n```",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 215,
        "deletions": 34,
        "changed_files": 5,
        "created_at": "2023-03-24T03:59:50Z",
        "closed_at": "2023-05-12T17:13:16Z",
        "merged_at": "2023-05-12T17:13:16Z",
        "body": "To implement the basic eventloop duration in #10719. \r\n\r\nThe measured latency(duration) includes the list below, which can be shown by `INFO STATS`.\r\n```\r\neventloop_cycles  // ever increasing counter\r\neventloop_duration_sum // cumulative duration of eventloop in microseconds\r\neventloop_duration_cmd_sum  // cumulative duration of executing commands in microseconds\r\ninstantaneous_eventloop_cycles_per_sec  // average eventloop count per second in recent 1.6s\r\ninstantaneous_eventloop_duration_usec  // average single eventloop duration in recent 1.6s\r\n```\r\nAlso added some experimental metrics, which are shown only when `INFO DEBUG` is called.\r\nThis section isn't included in the default INFO, or even in `INFO ALL` and the fields in this section can change in the future without considering backwards compatibility.\r\n```\r\neventloop_duration_aof_sum  // cumulative duration of writing AOF\r\neventloop_duration_cron_sum  // cumulative duration cron jobs (serverCron, beforeSleep excluding IO and AOF)\r\neventloop_cmd_per_cycle_max  // max number of commands executed in one eventloop\r\neventloop_duration_max  // max duration of one eventloop\r\n```\r\n\r\nAll of these are being reset by CONFIG RESETSTAT",
        "comments": 24
    },
    {
        "merged": false,
        "additions": 205,
        "deletions": 205,
        "changed_files": 41,
        "created_at": "2023-03-24T02:45:21Z",
        "closed_at": "2023-03-24T05:38:31Z",
        "merged_at": null,
        "body": "    Example:\r\n```\r\n    $ make CC=clang V=1\r\n    ...\r\n    ./server.h:3212:23: warning: a function declaration without a prototype is\r\n    deprecated in all versions of C [-Wstrict-prototypes]\r\n    sds getConfigDebugInfo();\r\n                          ^\r\n                           void\r\n    ./server.h:3214:37: warning: a function declaration without a prototype is\r\n    deprecated in all versions of C [-Wstrict-prototypes]\r\n    void initServerClientMemUsageBuckets();\r\n                                        ^\r\n                                         void\r\n    ...\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-03-23T15:00:27Z",
        "closed_at": "2023-03-27T09:55:18Z",
        "merged_at": "2023-03-27T09:55:18Z",
        "body": "Starting with the recent #11926 Makefile specifies `-flto=auto` which is unsupported on clang.\r\nAdditionally, detecting clang correctly requires actually running it, since on MacOS gcc can be an alias for clang.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-03-23T06:14:00Z",
        "closed_at": "2023-03-23T08:49:10Z",
        "merged_at": "2023-03-23T08:49:09Z",
        "body": "these latency threshold errors prevent the schema validation from running.\r\nhttps://github.com/redis/redis/actions/runs/4495621695/jobs/7909434097",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-20T17:31:01Z",
        "closed_at": "2023-05-02T13:20:38Z",
        "merged_at": "2023-05-02T13:20:38Z",
        "body": "Adds ability to set the random seed so that more consistent repeatable benchmarks are possible.\r\n\r\nExample usage:\r\n\r\nAdding 2 hash items\r\n```\r\nsrc/redis-benchmark -r 100 -n 2 --seed 250 hset myhash:__rand_int__ age __rand_int__\r\n```\r\n\r\nMonitor:\r\n\r\n1st benchmark invocation:\r\n```\r\n1679332814.824357 [0 127.0.0.1:36686] \"hset\" \"myhash:000000000022\" \"age\" \"000000000069\"\r\n1679332814.824404 [0 127.0.0.1:36690] \"hset\" \"myhash:000000000007\" \"age\" \"000000000043\"\r\n```\r\n\r\n2nd benchmark invocation:\r\n```\r\n1679332814.824357 [0 127.0.0.1:36686] \"hset\" \"myhash:000000000022\" \"age\" \"000000000069\"\r\n1679332814.824404 [0 127.0.0.1:36690] \"hset\" \"myhash:000000000007\" \"age\" \"000000000043\"\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2023-03-20T17:15:33Z",
        "closed_at": "2023-05-28T07:58:30Z",
        "merged_at": "2023-05-28T07:58:30Z",
        "body": "This is a redo of #11594 which got reverted in #11940\r\nIt improves performance by avoiding double lookup of the the key.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 783,
        "deletions": 784,
        "changed_files": 390,
        "created_at": "2023-03-20T16:24:03Z",
        "closed_at": "2023-03-29T17:48:59Z",
        "merged_at": "2023-03-29T17:48:59Z",
        "body": "This is an attempt to normalize/formalize command summaries.\r\n\r\nMain actions performed:\r\n* Starts with the continuation of the phrase \"The XXXX command, when called, ...\" for user commands.\r\n* Starts with \"An internal command...\", \"A container command...\", etc... when applicable.\r\n* Always uses periods.\r\n* Refrains from referring to other commands. If this is needed, backquotes should be used for command names.\r\n* Tries to be very clear about the data type when applicable.\r\n* Tries to mention additional effects, e.g. \"The key is created if it doesn't exist\" and \"The set is deleted if the last member is removed.\"\r\n* Prefers being terse over verbose.\r\n* Tries to be consistent.\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 441,
        "deletions": 97,
        "changed_files": 19,
        "created_at": "2023-03-20T13:20:29Z",
        "closed_at": "2023-03-20T17:16:50Z",
        "merged_at": "2023-03-20T17:16:50Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2023-28425) Specially crafted MSETNX command can lead to assertion and denial-of-service\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Large blocks of replica client output buffer may lead to PSYNC loops and unnecessary memory usage (#11666)\r\n* Fix CLIENT REPLY OFF|SKIP to not silence push notifications (#11875)\r\n* Trim excessive memory usage in stream nodes when exceeding `stream-node-max-bytes` (#11885)\r\n* Fix module RM_Call commands failing with OOM when maxmemory is changed to zero (#11319)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-03-20T12:47:57Z",
        "closed_at": "2023-03-20T16:50:45Z",
        "merged_at": "2023-03-20T16:50:45Z",
        "body": "Using the same key twice in MSETNX command would trigger an assertion.\r\n\r\nThis reverts #11594 (introduced in Redis 7.0.8)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-03-20T08:46:58Z",
        "closed_at": "2023-03-20T09:58:20Z",
        "merged_at": "2023-03-20T09:58:20Z",
        "body": "The reason is in reply-schemas-validator, the resp of the\r\nclient we create will be client_default_resp (currently 3):\r\n```\r\nclient *createClient(connection *conn) {\r\n    client *c = zmalloc(sizeof(client));\r\n #ifdef LOG_REQ_RES\r\n    reqresReset(c, 0);\r\n    c->resp = server.client_default_resp;\r\n #else\r\n    c->resp = 2;\r\n #endif\r\n}\r\n```\r\n\r\nBut current_resp3 in redis-cli will be inconsistent with it,\r\nthe test adds a simple hello 3 to avoid this failure, test\r\nwas added in #11873.\r\n\r\nAdded help descriptions for dont-pre-clean option, it was\r\nadded in #10273",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2023-03-17T20:11:28Z",
        "closed_at": "2023-03-24T13:40:24Z",
        "merged_at": null,
        "body": "We want to update the summary of Lpop, lpushx, rpop and rpushx commands in json file.\r\n\r\nCurrent summary values : \r\n\"LPOP\":  \"summary\": \"Remove and get the first elements in a list\",\r\n\"LPUSHX\":  \"summary\": \"Prepend an element to a list, only if the list exists\",\r\n\"RPOP\":  \"summary\": \"Remove and get the last elements in a list\",\r\n\"RPUSHX\":   \"summary\": \"Append an element to a list, only if the list exists\",\r\n\r\nLPOP and RPOP command can pop one or more element (if count is mentioned)\r\nLPUSHX and RPUSHX command can push one or multiple element.\r\n\r\nIn this pull request , we have updated the summary of Lpop, lpushx, rpop and rpushx commands\r\nUpdated summary values:\r\n\"LPOP\":  \"summary\": \"Remove and get one or multiple elements from the beginning of a list\",\r\n\"LPUSHX\":  \"summary\": \"Prepend one or multiple elements to a list, only if the list exists\",\r\n\"RPOP\":  \"summary\": \"Remove and get one or multiple elements from the end of a list\",\r\n\"RPUSHX\":   \"Append one or multiple elements to a list, only if the list exists\",\r\n\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-03-17T10:20:04Z",
        "closed_at": "2023-03-17T12:20:55Z",
        "merged_at": "2023-03-17T12:20:55Z",
        "body": "`{commands_filename}` the extra {} actually make it\r\nbecome a Set, and the output print was like this:\r\n```\r\nProcessing json files...\r\nLinking container command to subcommands...\r\nChecking all commands...\r\nGenerating {'commands'}.c...\r\nAll done, exiting.\r\n```\r\n\r\nIntroduced in #11920",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-17T09:09:33Z",
        "closed_at": "2023-04-10T01:22:51Z",
        "merged_at": null,
        "body": "when kill a master node.  connection will close , need refresh  again.  #11932 11932",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-03-17T07:07:23Z",
        "closed_at": "2023-03-20T06:18:39Z",
        "merged_at": "2023-03-20T06:18:39Z",
        "body": "When the server crashes during the AUTH command, or another command with an AUTH argument, the password was recorded in the log.\r\n\r\nNow, when the `auth` keyword is detected (could be in HELLO or MIGRATE, etc), the loop exits before printing any additional arguments.\r\n\r\n```\r\n------ CURRENT CLIENT INFO ------\r\nid=3 addr=127.0.0.1:33162 laddr=127.0.0.1:9091 fd=7 name= age=3 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 qbuf=23 qbuf-free=20451 argv-mem=7 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=22295 events=r cmd=auth user=default redir=-1 resp=2\r\nargv[0]: '\"auth\"'\r\nargv[1]: '\"123\"'\r\n\r\n------ EXECUTING CLIENT INFO ------\r\nid=3 addr=127.0.0.1:33162 laddr=127.0.0.1:9091 fd=7 name= age=3 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 qbuf=23 qbuf-free=20451 argv-mem=7 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=22295 events=r cmd=auth user=default redir=-1 resp=2\r\nargv[0]: '\"auth\"'\r\nargv[1]: '\"123\"'\r\n\r\n```\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-16T17:29:42Z",
        "closed_at": "2023-03-18T23:58:30Z",
        "merged_at": null,
        "body": "fix [#issue-1627953579](https://github.com/redis/redis/issues/11928#issue-1627953579)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-16T11:17:08Z",
        "closed_at": "2023-03-17T16:23:07Z",
        "merged_at": "2023-03-17T16:23:07Z",
        "body": "Use -flto=auto to use GNU make's job server, if available, or otherwise fall back to autodetection of the number of CPU threads present in your system.\r\n\r\n  Warnings:\r\n```\r\n  lto-wrapper: warning: using serial compilation of 2 LTRANS jobs\r\n  lto-wrapper: note: see the \u2018-flto\u2019 option documentation for more information\r\n  lto-wrapper: warning: using serial compilation of 4 LTRANS jobs\r\n  lto-wrapper: note: see the \u2018-flto\u2019 option documentation for more information\r\n  lto-wrapper: warning: using serial compilation of 31 LTRANS jobs\r\n  lto-wrapper: note: see the \u2018-flto\u2019 option documentation for more information\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 53,
        "deletions": 2,
        "changed_files": 6,
        "created_at": "2023-03-15T16:13:57Z",
        "closed_at": "2023-04-12T07:11:30Z",
        "merged_at": "2023-04-12T07:11:30Z",
        "body": "Reply with the error create from a printf format and arguments.\r\n\r\nIf the error code is already passed in the string 'fmt', the error\r\ncode provided is used, otherwise the string \"-ERR \" for the generic\r\nerror code is automatically added.\r\n\r\nThe usage is, for example:\r\n- RedisModule_ReplyWithErrorFormat(ctx, \"An error: %s\", \"foo\");\r\n- RedisModule_ReplyWithErrorFormat(ctx, \"-WRONGTYPE Wrong Type: %s\", \"foo\");\r\n\r\nThe function always returns REDISMODULE_OK.\r\n\r\nFixes #11919",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 86,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-03-15T12:54:13Z",
        "closed_at": "2023-03-15T16:16:17Z",
        "merged_at": "2023-03-15T16:16:16Z",
        "body": "There be a situation that satisfies WAIT, and then wrongly unblock\r\nWAITAOF because we mix-use last_offset and last_numreplicas.\r\n\r\nWe update last_offset and last_numreplicas only when the condition\r\nmatches. i.e. output of either replicationCountAOFAcksByOffset or\r\nreplicationCountAcksByOffset is right.\r\n\r\nIn this case, we need to have separate last_ variables for each of\r\nthem. Added a last_aof_offset and last_aof_numreplicas for WAITAOF.\r\n\r\nWAITAOF was added in #11713. Found while coding #11917.\r\nA Test was added to validate that case.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 98,
        "deletions": 33,
        "changed_files": 3,
        "created_at": "2023-03-15T05:14:08Z",
        "closed_at": "2023-03-15T09:07:04Z",
        "merged_at": "2023-03-15T09:07:04Z",
        "body": "WAITAOF wad added in #11713, its return is an array.\r\nBut forget to handle WAITAOF in last_offset and last_numreplicas,\r\ncausing WAITAOF to return a WAIT like reply.\r\n\r\nTests was added to validate that case (both WAIT and WAITAOF).\r\nThis PR also refactored processClientsWaitingReplicas a bit for better\r\nmaintainability and readability.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12015,
        "deletions": 9888,
        "changed_files": 15,
        "created_at": "2023-03-14T14:27:50Z",
        "closed_at": "2023-03-30T16:05:34Z",
        "merged_at": null,
        "body": "Based on #10515.\r\n\r\n* Merge conflicts solved. The changes from #11232 are lost as the affected code is deleted in this PR.\r\n* Tests of hints for `BITFIELD_RO` adjusted slightly to compensate for #11445. The implementation of this PR isn't perfect, so I added a TODO in the tests.\r\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 24,
        "changed_files": 12,
        "created_at": "2023-03-14T11:52:04Z",
        "closed_at": "2023-03-16T06:50:32Z",
        "merged_at": "2023-03-16T06:50:32Z",
        "body": "Fix some seen typos and wrong comments.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2023-03-14T03:22:44Z",
        "closed_at": "2023-03-19T10:51:35Z",
        "merged_at": "2023-03-19T10:51:35Z",
        "body": "`rewriteConfig` already calls `fsync` to make sure changes are committed to disk,\r\nso it is no need to call `fsync` again here.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-13T15:59:06Z",
        "closed_at": "2023-03-14T11:52:41Z",
        "merged_at": null,
        "body": "Bumps [codespell](https://github.com/codespell-project/codespell) from 2.2.2 to 2.2.4.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/codespell-project/codespell/releases\">codespell's releases</a>.</em></p>\n<blockquote>\n<h2>v2.2.4</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>BUG: Fix bug with toml triage by <a href=\"https://github.com/larsoner\"><code>@\u200blarsoner</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2774\">codespell-project/codespell#2774</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/codespell-project/codespell/compare/v2.2.3...v2.2.4\">https://github.com/codespell-project/codespell/compare/v2.2.3...v2.2.4</a></p>\n<h2>v2.2.3</h2>\n<h2>NOTE:</h2>\n<p>This release is broken for Python &lt; 3.11 without tomli installed in the presence of <code>.toml</code> files (e.g., <code>pyproject.toml</code>), see <a href=\"https://redirect.github.com/codespell-project/actions-codespell/issues/59\">codespell-project/actions-codespell#59</a> !</p>\n<h2>What's Changed</h2>\n<ul>\n<li>Misspelling of sufficient, sufficiently by <a href=\"https://github.com/vikivivi\"><code>@\u200bvikivivi</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2531\">codespell-project/codespell#2531</a></li>\n<li>insuffient-&gt;insufficient by <a href=\"https://github.com/tkoyama010\"><code>@\u200btkoyama010</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2527\">codespell-project/codespell#2527</a></li>\n<li>add anayltic(s|al|ally) -&gt; analytic(s|al|ally) by <a href=\"https://github.com/robin-wayve\"><code>@\u200brobin-wayve</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2507\">codespell-project/codespell#2507</a></li>\n<li>Ignore flake8 rule W503 by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2069\">codespell-project/codespell#2069</a></li>\n<li>Dead code found by vulture by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2101\">codespell-project/codespell#2101</a></li>\n<li>Move falsy and accreting to more appropriate files by <a href=\"https://github.com/janosh\"><code>@\u200bjanosh</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2511\">codespell-project/codespell#2511</a></li>\n<li>Add subtrate-&gt;substrate by <a href=\"https://github.com/janosh\"><code>@\u200bjanosh</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2525\">codespell-project/codespell#2525</a></li>\n<li>Add &quot;subtask&quot; by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2537\">codespell-project/codespell#2537</a></li>\n<li>Apply pyupgrade to project by <a href=\"https://github.com/jdufresne\"><code>@\u200bjdufresne</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2364\">codespell-project/codespell#2364</a></li>\n<li>{speherical,sperhical}-&gt;spherical by <a href=\"https://github.com/janosh\"><code>@\u200bjanosh</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2540\">codespell-project/codespell#2540</a></li>\n<li>interepolation-&gt;interpolation by <a href=\"https://github.com/tkoyama010\"><code>@\u200btkoyama010</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2512\">codespell-project/codespell#2512</a></li>\n<li>Ignore .mypy_cache folder by <a href=\"https://github.com/kianmeng\"><code>@\u200bkianmeng</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2361\">codespell-project/codespell#2361</a></li>\n<li>Fix uncaught exception on unreadable files by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2196\">codespell-project/codespell#2196</a></li>\n<li>Add constrainst to dictionary by <a href=\"https://github.com/jonathanberthias\"><code>@\u200bjonathanberthias</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2546\">codespell-project/codespell#2546</a></li>\n<li>Add relative hidden directory and basic subdir tests by <a href=\"https://github.com/peternewman\"><code>@\u200bpeternewman</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2542\">codespell-project/codespell#2542</a></li>\n<li>Add trignometric-&gt;trigonometric by <a href=\"https://github.com/janosh\"><code>@\u200bjanosh</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2549\">codespell-project/codespell#2549</a></li>\n<li>Add virtualied-&gt;virtualized, virtualised and friends by <a href=\"https://github.com/peternewman\"><code>@\u200bpeternewman</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2553\">codespell-project/codespell#2553</a></li>\n<li>Add several spelling corrections by <a href=\"https://github.com/luzpaz\"><code>@\u200bluzpaz</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2521\">codespell-project/codespell#2521</a></li>\n<li>Slightly simplify some boolean expressions by <a href=\"https://github.com/jdufresne\"><code>@\u200bjdufresne</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2556\">codespell-project/codespell#2556</a></li>\n<li>MAINT: Add CODEOWNERS by <a href=\"https://github.com/larsoner\"><code>@\u200blarsoner</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2535\">codespell-project/codespell#2535</a></li>\n<li>positivie-&gt;positive by <a href=\"https://github.com/janosh\"><code>@\u200bjanosh</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2558\">codespell-project/codespell#2558</a></li>\n<li>DOC/ENH: Clarify usage or configuration files and log about it by <a href=\"https://github.com/sappelhoff\"><code>@\u200bsappelhoff</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2552\">codespell-project/codespell#2552</a></li>\n<li>Document required setuptools version by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2560\">codespell-project/codespell#2560</a></li>\n<li>Redundant wheel dependency in pyproject.toml by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2562\">codespell-project/codespell#2562</a></li>\n<li>Remove parameterizes-&gt;parametrizes by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2563\">codespell-project/codespell#2563</a></li>\n<li>Replace codecs.open with open by <a href=\"https://github.com/jdufresne\"><code>@\u200bjdufresne</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2378\">codespell-project/codespell#2378</a></li>\n<li>Update subprocess usage to use modern subprocess.run() by <a href=\"https://github.com/jdufresne\"><code>@\u200bjdufresne</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2565\">codespell-project/codespell#2565</a></li>\n<li>Fix produce typo by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2561\">codespell-project/codespell#2561</a></li>\n<li>Add a few corrections by <a href=\"https://github.com/int-y1\"><code>@\u200bint-y1</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2566\">codespell-project/codespell#2566</a></li>\n<li>Migrate pytest config into pyproject.toml by <a href=\"https://github.com/cclauss\"><code>@\u200bcclauss</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2554\">codespell-project/codespell#2554</a></li>\n<li>Handle bad globs passed to if --skip/-S by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2159\">codespell-project/codespell#2159</a></li>\n<li>Isn't the mailing list obsolete? by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2090\">codespell-project/codespell#2090</a></li>\n<li>More CODEOWNERS by <a href=\"https://github.com/peternewman\"><code>@\u200bpeternewman</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2569\">codespell-project/codespell#2569</a></li>\n<li>Remove unused attribute Misspelling.fixword by <a href=\"https://github.com/jdufresne\"><code>@\u200bjdufresne</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2557\">codespell-project/codespell#2557</a></li>\n<li>Read config file without interpolation by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2545\">codespell-project/codespell#2545</a></li>\n<li>Add andriod-&gt;android and friends by <a href=\"https://github.com/peternewman\"><code>@\u200bpeternewman</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2570\">codespell-project/codespell#2570</a></li>\n<li>Add total type GB to US by <a href=\"https://github.com/peternewman\"><code>@\u200bpeternewman</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2577\">codespell-project/codespell#2577</a></li>\n<li>Catch all cases of missing pytest by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/codespell-project/codespell/pull/2568\">codespell-project/codespell#2568</a></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/ec0f41b9573937aebab66e3ca5b00d00a7b339fa\"><code>ec0f41b</code></a> BUG: Fix bug with toml triage (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/2774\">#2774</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/8fa416764c412c2adb68d48af577176395fbf34f\"><code>8fa4167</code></a> Switch from pytest tmpdir to tmp_path in tests (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/2625\">#2625</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/892922e11e524ffe72fe07258af925603fb8a021\"><code>892922e</code></a> ENH: Color matching wrong word in the interactive session (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/2771\">#2771</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/3601c9a8efb7aaff6020abd90cb418641ddf890d\"><code>3601c9a</code></a> tetrahedoren-&gt;tetrahedron (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/2769\">#2769</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/41ed86a5d77668737c4db6546643318c8acf61c4\"><code>41ed86a</code></a> Add spelling correction for vulnderabilit(y|ies)-&gt;vulnerabilit(y|ies)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/22ac85a4fe7e42f7cb5d2c24959edae86c906034\"><code>22ac85a</code></a> doc: tomli is not needed for Python &gt;= 3.11 (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/2751\">#2751</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/0c347972abc2942ed871a123e84dca6f79005243\"><code>0c34797</code></a> Add a spelling correction</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/5c9aeeb0136a0d0b81890ff08fc9bd802148921a\"><code>5c9aeeb</code></a> More precise ignore directive for mypy (<a href=\"https://redirect.github.com/codespell-project/codespell/issues/2760\">#2760</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/bf8d70ce2e05cc218ce9cbf2bc7cffc603391a20\"><code>bf8d70c</code></a> Add deffault(s|ed)-&gt;default(s|ed) correction</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/55fe25616b9b50fd5f4385cdc25df95d3b23ee30\"><code>55fe256</code></a> Add protlet(s)-&gt;portlet(s) correction</li>\n<li>Additional commits viewable in <a href=\"https://github.com/codespell-project/codespell/compare/v2.2.2...v2.2.4\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=codespell&package-manager=pip&previous-version=2.2.2&new-version=2.2.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 358,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2023-03-13T10:12:03Z",
        "closed_at": "2023-07-11T17:06:23Z",
        "merged_at": null,
        "body": "Solves #11699\r\n\r\nAdd subcommand `subscribers` to `pubsub` API to provide the list of connected clients for a given channel or pattern.\r\n\r\n#### API request:\r\n```\r\npubsub subscribers CHANNEL|SHARD-CHANNEL|PATTERN [MATCH pattern] [COUNT count]\r\n```\r\n\r\nOptional parameters:\r\n\r\n1. MATCH - Match supports glob based pattern. The default value is set to `*`.\r\n2. COUNT - Count supports [-1, LONG_MAX]. The default value is set to 1000 to restrict the no. of channels/pattern output in a single call. -1 returns all subscription for a given subscription type.\r\n\r\n\r\n#### Global (pubsub data propagated across all nodes, can be listened to via `subscribe`)\r\n```\r\n127.0.0.1:6379> pubsub subscribers channel\r\n1) \"ch2\"\r\n2) 1) 1) \"name\"\r\n      2) \"\"\r\n      3) \"id\"\r\n      4) \"3\"\r\n      5) \"addr\"\r\n      6) \"127.0.0.1:52100\"\r\n```\r\n\r\n#### Pattern (pubsub data propagated across all nodes, can be listened to via `psubscribe`)\r\n```\r\n127.0.0.1:6379> pubsub subscribers pattern\r\n1) \"ch*\"\r\n2) 1) 1) \"name\"\r\n      2) \"\"\r\n      3) \"id\"\r\n      4) \"3\"\r\n      5) \"addr\"\r\n      6) \"127.0.0.1:52100\"\r\n ```\r\n \r\n #### Shard (pubsub data propagated across a shard (primary + replicas), can be listened to via `ssubscribe`)\r\n ```\r\n127.0.0.1:6379> pubsub subscribers shard-channel match shardch* count 1\r\n1) \"shardch\"\r\n2) 1) 1) \"name\"\r\n      2) \"\"\r\n      3) \"id\"\r\n      4) \"3\"\r\n      5) \"addr\"\r\n      6) \"127.0.0.1:52100\"\r\n```\r\n\r\n#### Considerations\r\n1. Three sub-type are introduced to filter the subscription type: `channel`, `shard-channel` and `pattern`.\r\n2. For each client, `id`, `name` and `addr` (ip and port) attribute is returned. Clients could use `CLIENT LIST` API to pull further information. \r\n\r\nTasks:\r\n\r\n- [x]  API support\r\n- [x] Tests\r\n- [x] Reply schema\r\n- [ ] Document update",
        "comments": 20
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-03-13T09:38:44Z",
        "closed_at": "2023-07-30T09:19:06Z",
        "merged_at": null,
        "body": "I found the .gitignore has considering the compile_commands.json ,usually clangd-server will also generate .cache/* files in the workspace, it will be good to ignore these files too by default.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2023-03-13T09:24:08Z",
        "closed_at": "2023-03-13T14:12:29Z",
        "merged_at": "2023-03-13T14:12:29Z",
        "body": "In #11666, we added a while loop and will split a big reply\r\nnode to multiple nodes. The update of tail->repl_offset may\r\nbe wrong. Like before #11666, we would have created at most\r\none new reply node, and now we will create multiple nodes if\r\nit is a big reply node.\r\n\r\nNow we are creating more than one node, and the tail->repl_offset\r\nof all the nodes except the last one are incorrect. Because we\r\nupdate master_repl_offset at the beginning, and then use it to\r\nupdate the tail->repl_offset. This would have lead to an assertion\r\nduring PSYNC, a test was added to validate that case.\r\n\r\nBesides that, the calculation of size was adjusted to fix\r\ntests that failed due to a combination of a very low backlog size,\r\nand some thresholds of that get violated because of the relatively\r\nhigh overhead of replBufBlock. So now if the backlog size / 16 is too\r\nsmall, we'll take PROTO_REPLY_CHUNK_BYTES instead.\r\nThis size fix was provided by Oran.\r\n```\r\n*** [err]: slave buffer are counted correctly in tests/unit/maxmemory.tcl\r\nExpected 814808 < 500000 && 814808 > -500000 (context: type eval line 80 cmd {assert {$delta < $delta_max && $delta > -$delta_max}} proc ::test)\r\n\r\n*** [err]: replica buffer don't induce eviction in tests/unit/maxmemory.tcl\r\nExpected [::redis::redisHandle147 dbsize] == 100 (context: type eval line 77 cmd {assert {[$master dbsize] == 100}} proc ::test)\r\n\r\n*** [err]: All replicas share one global replication buffer in tests/integration/replication-buffer.tcl\r\nExpected '1153280' to be equal to '1148928' (context: type eval line 18 cmd {assert_equal $repl_buf_mem [s mem_total_replication_buffers]} proc ::test)\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-10T06:48:46Z",
        "closed_at": "2023-03-12T11:25:10Z",
        "merged_at": "2023-03-12T11:25:10Z",
        "body": "In #9408, we added some SENTINEL DEBUG to reduce default\r\ntimeouts and allow tests to execute faster. The change\r\nin 05-manual.tcl may cause a race that SENTINEL FAILOVER\r\nresponse with a NOGOODSLAVE:\r\n```\r\nManual failover works: FAILED: Expected NOGOODSLAVE No suitable replica to promote eq \"OK\" (context: type eval line 6 cmd {assert {$reply eq \"OK\"}} proc ::test)\r\n(Jumping to next unit after error)\r\nFAILED: caught an error in the test\r\nassertion:Expected NOGOODSLAVE No suitable replica to promote eq \"OK\" (context: type eval line 6 cmd {assert {$reply eq \"OK\"}} proc ::test)\r\n```\r\n\r\nThe reason is that the info-period value was reduced in #9408\r\n(the default value is 10000), and then manual failover was\r\nperformed immediately, but the INFO may not exchanged between\r\nthe sentinel and replicas, causing the sentinel to skip all\r\nthe replicas in sentinelSelectSlave (Because replica's info_refresh\r\nis not updated, see the code snippet below), then return a NOGOODSLAVE,\r\nbreak the test.\r\n\r\nCode snippet from sentinelSelectSlave:\r\n```\r\nwhile((de = dictNext(di)) != NULL) {\r\n    sentinelRedisInstance *slave = dictGetVal(de);\r\n    mstime_t info_validity_time;\r\n    if (master->flags & SRI_S_DOWN)\r\n        info_validity_time = sentinel_ping_period*5;\r\n    else\r\n        info_validity_time = sentinel_info_period*3;\r\n    if (mstime() - slave->info_refresh > info_validity_time) continue;\r\n}\r\n```\r\n\r\nBy adding a wait_for_condition, we have the opportunity to\r\nlet sentinel update the info_period of the replicas.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 59,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2023-03-09T12:57:22Z",
        "closed_at": "2023-03-20T06:04:14Z",
        "merged_at": "2023-03-20T06:04:14Z",
        "body": "Previously we would run the module command filters even upon blocked command reprocessing.  This could modify the command, and it's args.  This is irrelevant in the context of a command being reprocessed (it already went through the filters), as well as breaks the crashed command lookup that exists in the case of a reprocessed command.\r\n\r\nfixes #11894.",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-03-09T08:39:48Z",
        "closed_at": "2023-03-09T10:05:50Z",
        "merged_at": "2023-03-09T10:05:50Z",
        "body": "This was exposed by a new LATENCY GRAPH valgrind test.\r\nThere are no security implications, fix by initializing\r\nthese members.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-03-08T15:28:07Z",
        "closed_at": "2023-04-04T12:25:53Z",
        "merged_at": "2023-04-04T12:25:53Z",
        "body": "Redis supports syslog integration via these directives, documented in [redis.conf](https://github.com/redis/redis/blob/6.2/redis.conf#L306-L314):\r\n\r\n```\r\nsyslog-enabled no\r\nsyslog-ident redis\r\nsyslog-facility local0\r\n```\r\n \r\nWhile these directives are not documented in [sentinel.conf](https://github.com/redis/redis/blob/6.2/sentinel.conf), they do work with Redis-Sentinel. It took me a while to realize this.\r\n\r\nHow about documenting those in sentinel.conf, just to make it clear they can be used with Sentinel ?",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-03-08T05:26:39Z",
        "closed_at": "2023-03-21T15:13:31Z",
        "merged_at": "2023-03-21T15:13:31Z",
        "body": "SRI_MASTER_REBOOT flag was added in #9438",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-08T03:48:15Z",
        "closed_at": "2023-03-26T05:46:59Z",
        "merged_at": "2023-03-26T05:46:59Z",
        "body": "This test fails sporadically:\r\n```\r\n*** [err]: Migrate the last slot away from a node using redis-cli in tests/unit/cluster/cli.tcl\r\ncluster size did not reach a consistent size 4\r\n```\r\n\r\nI guess the time (5s) of wait_for_cluster_size is not enough,\r\nusually, the waiting time for our other tests for cluster\r\nconsistency is 50s, so also changing it to 50s.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2023-03-07T03:06:59Z",
        "closed_at": "2023-03-07T23:06:53Z",
        "merged_at": "2023-03-07T23:06:53Z",
        "body": "This change attempts to alleviate a minor memory degradation for Redis 6.2 and onwards when using rather large objects (~2k) in streams. Introduced in #6281, we pre-allocate the head nodes of a stream to be 4kb, to limit the amount of unnecessary initial reallocations that are done. However, if we only ever allocate one object because 2 objects exceeds the max_stream_entry_size, we never actually shrink it to fit the single item. This can lead to a lot of fragmentation. For smaller item sizes this becomes less of an issue, as the fragmentation decreases as the items become smaller in size.\r\n\r\nThis commit also changes the `MEMORY USAGE` of streams, since it was reporting the lpBytes instead of the allocated size. This introduced an observability issue when diagnosing the memory issue, since Redis reported the same amount of used bytes pre and post change, even though the new implementation allocated more memory.\r\n\r\nA note about #6281, I was trying to see if we discussed a reason to not also compress on size exceeded case, but couldn't find one. Assuming it was just a miss at the time, it looks like all the testing was done on *very* small sizes.\r\n\r\nBefore:\r\n```\r\ndev-dsk-matolson-2c-96119f9f % redis-cli XADD test \"*\" foo `python -c \"print('x'*2500)\"`\r\n\"1678157345210-0\"\r\n\r\n(23-03-07 2:49:05) <0> [~]  \r\ndev-dsk-matolson-2c-96119f9f % redis-cli XADD test \"*\" foo `python -c \"print('x'*2500)\"`\r\n\"1678157346184-0\"\r\n\r\n(23-03-07 2:49:06) <0> [~]  \r\ndev-dsk-matolson-2c-96119f9f % redis-cli XADD test \"*\" foo `python -c \"print('x'*2500)\"`\r\n\"1678157346706-0\"\r\n\r\n(23-03-07 2:49:06) <0> [~]  \r\ndev-dsk-matolson-2c-96119f9f % redis-cli memory usage test                              \r\n(integer) 14416\r\n```\r\n\r\nAfter:\r\n```\r\ndev-dsk-matolson-2c-96119f9f % redis-cli XADD test \"*\" foo `python -c \"print('x'*2500)\"`\r\n\"1678157253636-0\"\r\n\r\n(23-03-07 2:47:33) <0> [~]  \r\ndev-dsk-matolson-2c-96119f9f % redis-cli XADD test \"*\" foo `python -c \"print('x'*2500)\"`\r\n\"1678157254074-0\"\r\n\r\n(23-03-07 2:47:34) <0> [~]  \r\ndev-dsk-matolson-2c-96119f9f % redis-cli XADD test \"*\" foo `python -c \"print('x'*2500)\"`\r\n\"1678157254523-0\"\r\n\r\n(23-03-07 2:47:34) <0> [~]  \r\ndev-dsk-matolson-2c-96119f9f % redis-cli memory usage test                              \r\n(integer) 11344\r\n```\r\n\r\n```\r\nRelease Notes\r\nRemove unused memory of stream nodes when new nodes are created for exceeding `stream-node-max-bytes`.\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-06T05:06:42Z",
        "closed_at": "2023-03-07T13:27:09Z",
        "merged_at": "2023-03-07T13:27:09Z",
        "body": "I've seen it fail here (test-centos7-tls-module-no-tls and test-freebsd):\r\n```\r\n*** [err]: Operations in no-touch mode do not alter the last access time of a key in tests/unit/introspection-2.tcl\r\nExpected '244296' to be more than '244296' (context: type eval line 12 cmd {assert_morethan $newlru $oldlru} proc ::test)\r\n```\r\n\r\nOur LRU_CLOCK_RESOLUTION value is 1000ms, and default hz is 10, so if the\r\ntest is really fast, or the timing is just right, newlru will be the same\r\nas oldlru. We fixed this by changing `after 1000` to `after 1100`.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-03-04T16:13:20Z",
        "closed_at": "2023-03-05T13:28:50Z",
        "merged_at": "2023-03-05T13:28:50Z",
        "body": "__builtin_unreachable is added for the first time in GCC 4.5.\r\n\r\nThe following error may occur in versions before GCC 4.5.\r\n\r\nGCC 4.4:\r\n`undefined reference to __builtin_unreachable`\r\n\r\nAdd a minor version check to ensure that can fallback to `abort`\r\n\r\n\r\n\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-03-03T07:42:31Z",
        "closed_at": "2023-03-07T07:06:58Z",
        "merged_at": "2023-03-07T07:06:58Z",
        "body": "Test `trim on SET with big value` (introduced from #11817) fails under mac m1 with libc mem_allocator.\r\nThe reason is that malloc(33000) will allocate 65536 bytes(>42000).\r\nThis test still passes under ubuntu with libc mem_allocator.\r\n\r\n```\r\n*** [err]: trim on SET with big value in tests/unit/type/string.tcl\r\nExpected [r memory usage key] < 42000 (context: type source line 471 file /Users/iospack/data/redis_fork/tests/unit/type/string.tcl cmd {assert {[r memory usage key] < 42000}} proc ::test)\r\n```\r\n\r\nsimple test under mac m1 with libc mem_allocator:\r\n```c\r\nvoid *p = zmalloc(33000);\r\nprintf(\"malloc size: %zu\\n\", zmalloc_size(p));\r\n\r\n# output\r\nmalloc size: 65536\r\n```\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 227,
        "deletions": 5,
        "changed_files": 9,
        "created_at": "2023-03-02T16:44:39Z",
        "closed_at": "2023-03-12T15:50:44Z",
        "merged_at": "2023-03-12T15:50:44Z",
        "body": "This bug seems to be there forever, CLIENT REPLY OFF|SKIP will\r\nmark the client with CLIENT_REPLY_OFF or CLIENT_REPLY_SKIP flags.\r\nWith these flags, prepareClientToWrite called by addReply* will\r\nreturn C_ERR directly. So the client can't receive the Pub/Sub\r\nmessages and any other push notifications, e.g client side tracking.\r\n\r\nIn this PR, we adding a CLIENT_PUSHING flag, disables the reply\r\nsilencing flags. When adding push replies, set the flag, after the reply,\r\nclear the flag. Then add the flag check in prepareClientToWrite.\r\n\r\nFixes #11874\r\n\r\nNote, the SUBSCRIBE command response is a bit awkward,\r\nsee https://github.com/redis/redis-doc/pull/2327",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 298,
        "deletions": 56,
        "changed_files": 3,
        "created_at": "2023-03-02T10:43:31Z",
        "closed_at": "2023-03-19T10:56:55Z",
        "merged_at": "2023-03-19T10:56:54Z",
        "body": "The message \"Reading messages... (press Ctrl-C to quit)\" is replaced by \"Reading messages... (press Ctrl-C to quit or any key to type command)\".\r\n\r\nThis allows users to subscribe to more channels, to try out UNSUBSCRIBE and to combine pubsub with other features such as push messages from client tracking.\r\n\r\nThe \"Reading messages\" info message is displayed in the bottom of the output in a distinct style and moves downward as more messages appear. When any key is pressed, the info message is replaced by the prompt with for entering commands. After entering a command and the reply is displayed, the \"Reading messages\" info messages appears again. This is added to the repl loop in redis-cli and in the corresponding place for non-interactive mode.\r\n\r\nAn indication \"(subscribed mode)\" is included in the prompt when entering commands in subscribed mode.\r\n\r\nAlso:\r\n\r\n* Fixes a problem that UNSUBSCRIBE hanged when used with RESP3 and push callback, without first entering subscribe mode. It hanged because UNSUBSCRIBE gets one or more push replies but no in-band reply.\r\n\r\n* Exit subscribed mode after RESET.",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-03-02T09:47:23Z",
        "closed_at": "2023-03-04T10:54:37Z",
        "merged_at": "2023-03-04T10:54:36Z",
        "body": "This test is very sensitive and fragile. It often fails in Daily,\r\nin most cases, it failed in test-ubuntu-32bit (the AOF loading one),\r\nwith the range in (31, 40):\r\n```\r\n[err]: Active defrag in tests/unit/memefficiency.tcl\r\nExpected 38 <= 30 (context: type eval line 113 cmd {assert {$max_latency <= 30}} proc ::test)\r\n```\r\n\r\nThe AOF loading part isn't tightly fixed to the cron hz. It calls\r\nprocessEventsWhileBlocked once in every 1024 command calls.\r\n```\r\n        /* Serve the clients from time to time */\r\n        if (!(loops++ % 1024)) {\r\n            off_t progress_delta = ftello(fp) - last_progress_report_size;\r\n            loadingIncrProgress(progress_delta);\r\n            last_progress_report_size += progress_delta;\r\n            processEventsWhileBlocked();\r\n            processModuleLoadingProgressEvent(1);\r\n        }\r\n```\r\n\r\nIn this case, we can either decrease the 1024 or increase the\r\nthreshold of just the AOF part of that test. Considering the test\r\nmachines are sometimes slow, and all sort of quirks could happen\r\n(which do not indicate a bug), and we've already set to 30, we suppose\r\nwe can set it a little bit higher, set it to 40. We can have this instead of\r\nadding another testing config (we can add it when we really need it).\r\n\r\nFixes #11868",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-02-28T10:35:33Z",
        "closed_at": "2023-02-28T13:15:27Z",
        "merged_at": "2023-02-28T13:15:27Z",
        "body": "Authenticated users can use string matching commands with a specially crafted pattern to trigger a denial-of-service attack on Redis, causing it to hang and consume 100% CPU time.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 5,
        "changed_files": 6,
        "created_at": "2023-02-28T10:35:00Z",
        "closed_at": "2023-02-28T13:15:47Z",
        "merged_at": "2023-02-28T13:15:47Z",
        "body": "Issue happens when passing a negative long value that greater than the max positive value that the long can store.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 563,
        "deletions": 19,
        "changed_files": 8,
        "created_at": "2023-02-28T07:46:47Z",
        "closed_at": "2023-04-09T09:07:32Z",
        "merged_at": "2023-04-09T09:07:32Z",
        "body": "Add `RM_RdbLoad()` and `RM_RdbSave()` to load/save RDB files from the module API. \r\n\r\nIn our use case, we have our clustering implementation as a module. As part of this implementation, the module needs to trigger RDB save operation at specific points. Also, this module delivers RDB files to other nodes (not using Redis' replication). When a node receives an RDB file, it should be able to load the RDB. Currently, there is no module API to save/load RDB files. \r\n\r\n\r\nThis PR adds four new APIs:\r\n```c\r\nRedisModuleRdbStream *RM_RdbStreamCreateFromFile(const char *filename);\r\nvoid RM_RdbStreamFree(RedisModuleRdbStream *stream);\r\n\r\nint RM_RdbLoad(RedisModuleCtx *ctx, RedisModuleRdbStream *stream, int flags);\r\nint RM_RdbSave(RedisModuleCtx *ctx, RedisModuleRdbStream *stream, int flags);\r\n```\r\n\r\nThe first step is to create a `RedisModuleRdbStream` object. This PR provides a function to create RedisModuleRdbStream from the filename. (You can load/save RDB with the filename). In the future, this API can be extended if needed: \r\ne.g., `RM_RdbStreamCreateFromFd()`, `RM_RdbStreamCreateFromSocket()` to save/load RDB from an `fd` or a `socket`. \r\n\r\n\r\nUsage:\r\n```c\r\n/* Save RDB */\r\nRedisModuleRdbStream *stream = RedisModule_RdbStreamCreateFromFile(\"example.rdb\");\r\nRedisModule_RdbSave(ctx, stream, 0);\r\nRedisModule_RdbStreamFree(stream);\r\n\r\n/* Load RDB */\r\nRedisModuleRdbStream *stream = RedisModule_RdbStreamCreateFromFile(\"example.rdb\");\r\nRedisModule_RdbLoad(ctx, stream, 0);\r\nRedisModule_RdbStreamFree(stream);\r\n```\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-02-27T16:43:11Z",
        "closed_at": "2023-03-03T08:28:55Z",
        "merged_at": "2023-03-03T08:28:55Z",
        "body": "Avoiding initializing the interactive help and the excessive call to the COMMAND command when using redis-cli with pipe.\r\ne.g.\r\n```\r\necho PING | redis-cli\r\n```\r\n\r\nSee https://github.com/redis/redis/issues/3978#issuecomment-1446032572\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 464,
        "deletions": 107,
        "changed_files": 36,
        "created_at": "2023-02-27T07:11:48Z",
        "closed_at": "2023-02-28T16:32:34Z",
        "merged_at": "2023-02-28T16:32:34Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2023-25155) Specially crafted SRANDMEMBER, ZRANDMEMBER, and HRANDFIELD\r\n  commands can trigger an integer overflow, resulting in a runtime assertion\r\n  and termination of the Redis server process.\r\n* (CVE-2022-36021) String matching commands (like SCAN or KEYS) with a specially\r\n  crafted pattern to trigger a denial-of-service attack on Redis, causing it to\r\n  hang and consume 100% CPU time.\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Fix a crash when reaching the maximum invalidations limit of client-side tracking (#11814)\r\n* Fix a crash when SPUBLISH is used after passing the cluster-link-sendbuf-limit (#11752)\r\n* Fix possible memory corruption in FLUSHALL when a client watches more than one key (#11854)\r\n* Fix cluster inbound link keepalive time (#11785)\r\n* Flush propagation list in active-expire of writable replicas to fix an assertion (#11615)\r\n* Avoid propagating DEL of lazy expire from SCAN and RANDOMKEY as MULTI-EXEC (#11788)\r\n\r\nPerformance and resource utilization improvements\r\n=================================================\r\n\r\n* Avoid realloc to reduce size of strings when it is unneeded (#11766)\r\n* Improve CLUSTER SLOTS reply efficiency for non-continuous slots (#11745)\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 246,
        "deletions": 57,
        "changed_files": 23,
        "created_at": "2023-02-27T07:10:20Z",
        "closed_at": "2023-02-28T16:32:15Z",
        "merged_at": "2023-02-28T16:32:15Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2023-25155) Specially crafted SRANDMEMBER, ZRANDMEMBER, and HRANDFIELD\r\n  commands can trigger an integer overflow, resulting in a runtime assertion\r\n  and termination of the Redis server process.\r\n* (CVE-2022-36021) String matching commands (like SCAN or KEYS) with a specially\r\n  crafted pattern to trigger a denial-of-service attack on Redis, causing it to\r\n  hang and consume 100% CPU time.\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Fix a crash when reaching the maximum invalidations limit of client-side tracking (#11814)\r\n* Fix cluster inbound link keepalive time (#11785)\r\n* Make sure that fork child doesn't do incremental rehashing (#11692)\r\n\r\nPerformance and resource utilization improvements\r\n=================================================\r\n\r\n* Avoid realloc to reduce size of strings when it is unneeded (#11766)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 92,
        "deletions": 27,
        "changed_files": 10,
        "created_at": "2023-02-27T07:08:16Z",
        "closed_at": "2023-02-28T16:31:59Z",
        "merged_at": "2023-02-28T16:31:59Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2023-25155) Specially crafted SRANDMEMBER, ZRANDMEMBER, and HRANDFIELD\r\n  commands can trigger an integer overflow, resulting in a runtime assertion\r\n  and termination of the Redis server process.\r\n* (CVE-2022-36021) String matching commands (like SCAN or KEYS) with a specially\r\n  crafted pattern to trigger a denial-of-service attack on Redis, causing it to\r\n  hang and consume 100% CPU time.\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Make sure that fork child doesn't do incremental rehashing (#11692)\r\n* Fix cluster inbound link keepalive time (#11785)\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 32,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-02-27T03:30:51Z",
        "closed_at": "2023-03-03T01:18:25Z",
        "merged_at": null,
        "body": "usage: usage: ./redis-cli --cluster create ... --cluster-sort [asc|desc|no] default:no\r\n#11682  after sort cluster node,  the  master-slave structure  will be more stable.  ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-02-27T03:15:44Z",
        "closed_at": "2023-03-03T02:18:46Z",
        "merged_at": null,
        "body": "fix: #11843  by: define __GNU_VISIBLE 1 under __CYGWIN__\r\n\r\n\r\n\r\n\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-02-26T14:25:37Z",
        "closed_at": "2023-03-21T15:51:47Z",
        "merged_at": "2023-03-21T15:51:47Z",
        "body": "I saw this error once, in the FreeBSD Daily CI:\r\n```\r\n*** [err]: Temp rdb will be deleted if we use bg_unlink when shutdown in tests/unit/shutdown.tcl\r\nExpected [file exists /xxx/temp-10336.rdb] (context: type eval line 15 cmd {assert {[file exists $temp_rdb]}} proc ::test)\r\n```\r\n\r\nThe log shows that bgsave was executed, and it was successfully executed in the end:\r\n```\r\nStarting test Temp rdb will be deleted if we use bg_unlink when shutdown in tests/unit/shutdown.tcl\r\n10251:M 22 Feb 2023 11:37:25.441 * Background saving started by pid 10336\r\n10336:C 22 Feb 2023 11:37:27.949 * DB saved on disk\r\n10336:C 22 Feb 2023 11:37:27.949 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB\r\n10251:M 22 Feb 2023 11:37:28.060 * Background saving terminated with success\r\n```\r\n\r\nThere may be two reasons:\r\n1. The child process has been created, but it has not created\r\n   the temp rdb file yet, so [file exists $temp_rdb] check failed.\r\n2. The child process bgsave has been executed successfully and the\r\n   temp file has been deleted, so [file exists $temp_rdb] check failed.\r\n\r\nFrom the logs pint, it should be the case 2, case 1 is too extreme,\r\nset rdb-key-save-delay to a higher value to ensure bgsave does not\r\nsucceed early to avoid this case.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2023-02-25T11:05:35Z",
        "closed_at": "2023-02-27T02:41:34Z",
        "merged_at": null,
        "body": "usage: ./redis-cli --cluster create ...  --cluster-sort [asc|desc|no] default:no \r\nfix #11682 \r\n$ ./redis-cli.exe --cluster create 127.0.0.1:30001 127.0.0.4:30003 127.0.0.2:30001 127.0.0.1:30002 127.0.0.4:30001 127.0.0.4:30005  127.0.0.3:30002 127.0.0.2:30002  --cluster-replicas 1 **--cluster-sort asc**\r\n\r\n>>> Performing hash slots allocation on 8 nodes...\r\nMaster[0] -> Slots 0 - 4095\r\nMaster[1] -> Slots 4096 - 8191\r\nMaster[2] -> Slots 8192 - 12287\r\nMaster[3] -> Slots 12288 - 16383\r\nAdding replica 127.0.0.2:30002 to 127.0.0.1:30001\r\nAdding replica 127.0.0.4:30003 to 127.0.0.2:30001\r\nAdding replica 127.0.0.4:30005 to 127.0.0.3:30002\r\nAdding replica 127.0.0.1:30002 to 127.0.0.4:30001\r\n>>> Trying to optimize slaves allocation for anti-affinity\r\n[WARNING] Some slaves are in the same host as their master\r\nM: 5820d30f307492f1bbb450ddb18f85ea91eb15f7 127.0.0.1:30001\r\n   slots:[0-4095] (4096 slots) master\r\nS: e88b4e33e78ab7d33a43ed7901b8a248eacc6c18 127.0.0.1:30002\r\n   replicates 5820d30f307492f1bbb450ddb18f85ea91eb15f7\r\nM: 5820d30f307492f1bbb450ddb18f85ea91eb15f7 127.0.0.2:30001\r\n   slots:[4096-8191] (4096 slots) master\r\nS: e88b4e33e78ab7d33a43ed7901b8a248eacc6c18 127.0.0.2:30002\r\n   replicates e88b4e33e78ab7d33a43ed7901b8a248eacc6c18\r\nM: e88b4e33e78ab7d33a43ed7901b8a248eacc6c18 127.0.0.3:30002\r\n   slots:[8192-12287] (4096 slots) master\r\nM: 5820d30f307492f1bbb450ddb18f85ea91eb15f7 127.0.0.4:30001\r\n   slots:[12288-16383] (4096 slots) master\r\nS: 22d95728365c06b2da98f14f0b79307d1eba90e4 127.0.0.4:30003\r\n   replicates 5820d30f307492f1bbb450ddb18f85ea91eb15f7\r\nS: 2490820d802becdcde54b69559373b91333fe5f5 127.0.0.4:30005\r\n   replicates 5820d30f307492f1bbb450ddb18f85ea91eb15f7\r\nCan I set the above configuration? (type 'yes' to accept):\r\n\r\n-------------------------------\r\nafter sort .  the structure  will be more stable.  \r\n\r\n\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-02-24T11:33:16Z",
        "closed_at": "2023-06-29T16:32:01Z",
        "merged_at": "2023-06-29T16:32:01Z",
        "body": "redis-cli -u <uri> does not support ipv6 URL.\r\n```shell\r\n>>> redis-cli -u \"redis://[::1]:6379/0\"                   \r\nCould not connect to Redis at [:0: nodename nor servname provided, or not known\r\n```\r\n\r\nReferring to the http protocol([rfc2732](https://www.ietf.org/rfc/rfc2732.txt)), we should add ipv6 support\r\nNow(ipv6 support):\r\n```shell\r\n>>> ./src/redis-cli -u \"redis://[::1]:6379/0\"                   \r\n[::1]:6379> \r\n```\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2023-02-23T19:10:41Z",
        "closed_at": "2023-03-08T08:08:55Z",
        "merged_at": "2023-03-08T08:08:55Z",
        "body": "Currently (starting at #11012) When a module is blocked on keys it sets the CLIENT_PENDING_COMMAND flag.\r\nHowever in case the module decides to unblock the client not via the regular flow (eg timeout, key signal or CLIENT UNBLOCK command) it will attempt to reprocess the module command and potentially blocked again.\r\n\r\nThis fix remove the CLIENT_PENDING_COMMAND flag in case blockedForKeys is issued from module context.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-02-23T07:31:59Z",
        "closed_at": "2023-02-23T08:56:52Z",
        "merged_at": "2023-02-23T08:56:52Z",
        "body": "CLIENT NO-TOUCH added in #11483, but forgot to add the since\r\nfield in the JSON file. This PR adds the since field to it\r\nwith a value of 7.2.0",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 51,
        "changed_files": 1,
        "created_at": "2023-02-21T09:03:51Z",
        "closed_at": "2023-02-21T16:58:56Z",
        "merged_at": "2023-02-21T16:58:56Z",
        "body": "We noticed that `client evicted due to client tracking prefixes`\r\ntakes over 200 seconds with valgrind.\r\n\r\nWe combine three prefixes in each command, this will probably\r\nsave us half the testing time.\r\n\r\nBefore: normal: 3508ms, valgrind: 289503ms -> 290s\r\nWith three prefixes, normal: 1500ms, valgrind: 135742ms -> 136s\r\n\r\nSince we did not actually count the memory usage of all prefixes, see\r\ngetClientMemoryUsage, so we can not use larger prefixes to speed up the\r\ntest here. Also this PR cleaned up some spaces (IDE jobs) and typos.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-02-20T14:55:13Z",
        "closed_at": "2023-02-23T11:36:32Z",
        "merged_at": "2023-02-23T11:36:32Z",
        "body": "The test sporadically failed with valgrind trying to match `no client named obuf-client1 found*`\r\nin the log it looks like `obuf-client1` was indeed dropped, so i'm guessing it's because CLIENT LIST was processed first.\r\n\r\nhttps://github.com/redis/redis/actions/runs/4199325173/jobs/7284050432",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-02-19T10:27:22Z",
        "closed_at": "2023-02-19T16:38:08Z",
        "merged_at": "2023-02-19T16:38:08Z",
        "body": "stabilize the test introduced in #11248\r\n* remove random aspect of the test by using DEBUG POPULATE instead of redis-benchmark\r\n* disable rdbcompression, so that the rdb file is always about 1GB.\r\n\r\nwhen fadvise was disabled, i get about 1GB in the page cace\r\nhttps://github.com/oranagra/redis/actions/runs/4215736478/jobs/7317091519\r\nwhen enabled i get less than 200KB\r\nhttps://github.com/oranagra/redis/actions/runs/4215715176/jobs/7317056275\r\n\r\nso for now, i'll keep the 500kb threshold.",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 84,
        "deletions": 23,
        "changed_files": 7,
        "created_at": "2023-02-19T06:58:19Z",
        "closed_at": "2023-02-28T17:38:59Z",
        "merged_at": "2023-02-28T17:38:59Z",
        "body": "As `sdsRemoveFreeSpace` have an impact on performance even if it is a no-op (see details at #11508). \r\nOnly call the function when there is a possibility that the string contains free space.\r\n* For strings coming from the network, it's only if they're bigger than PROTO_MBULK_BIG_ARG\r\n* For strings coming from scripts, it's only if they're smaller than LUA_CMD_OBJCACHE_MAX_LEN\r\n* For strings coming from modules, it could be anything.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-02-17T05:14:31Z",
        "closed_at": "2023-02-21T16:14:41Z",
        "merged_at": "2023-02-21T16:14:41Z",
        "body": "Resolves https://github.com/redis/redis/issues/11715.\r\n\r\nThere is a built in limit to client side tracking keys, which when exceeded will invalidate keys. This occurs in two places, one in the server cron and other before executing a command. If it happens in the second scenario, the invalidations will be queued for later since current client is set. This queue is never drained if a command is not executed (through call) such as a multi-exec command getting queued. This results in a later server assert crashing.\r\n\r\nRelated: https://github.com/redis/redis/pull/9422.\r\n\r\n```\r\nRelease notes\r\nFix a crash that can occur when client side tracking keys are invalidated when the max number of tracked keys has been reached.\r\n```",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-02-15T11:10:43Z",
        "closed_at": "2023-02-21T13:05:59Z",
        "merged_at": "2023-02-21T13:05:59Z",
        "body": "A simple `HELLO` command to a password protected Redis server replies with an error with another command suggestion. This omits protocol version from HELLO command arguments which causes another error.\r\n\r\n```\r\n127.0.0.1:6379> HELLO \r\n(error) NOAUTH HELLO must be called with the client already authenticated, otherwise the HELLO AUTH <user> <pass> option can be used to authenticate the client and select the RESP protocol version at the same time\r\n127.0.0.1:6379> HELLO AUTH <user> <pass>\r\n(error) ERR Protocol version is not an integer or out of range\r\n```\r\n\r\nThis PR adds the protocol version in the command suggestion.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2023-02-15T06:17:08Z",
        "closed_at": "2023-02-16T08:50:58Z",
        "merged_at": "2023-02-16T08:50:58Z",
        "body": "the new test is incompatible with valgrind.\r\nadded a new `--valgrind` argument to `redis-server tests` mode, which will cause that test to be skipped..",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-02-15T02:41:53Z",
        "closed_at": "2023-02-15T05:46:57Z",
        "merged_at": "2023-02-15T05:46:57Z",
        "body": "In #9373, actually need to replace `$rd $pop blist1{t} blist2{t} 1`\r\nwith `bpop_command_two_key $rd $pop blist1{t} blist2{t} 1` but forgot\r\nto delete the latter.\r\n\r\nThis doesn't affect the test, because the later assert_error \"WRONGTYPE\"\r\nis expected (and right). And if we read $rd again, it will get the\r\nwrong result, like 'ERR unknown command 'BLMPOP_LEFT' | 'BLMPOP_RIGHT'",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 74,
        "deletions": 149,
        "changed_files": 3,
        "created_at": "2023-02-14T09:27:01Z",
        "closed_at": "2023-02-14T18:06:31Z",
        "merged_at": "2023-02-14T18:06:31Z",
        "body": "All of the POP commands must not decrement length below 0. So, get_fsl will delete the key if the length is 0 (unless the caller wished to create if doesn't exist)\r\n\r\nOther:\r\n1. Use REDISMODULE_WRITE where needed (POP commands)\r\n2. Use wait_for_blokced_clients in tests\r\n\r\nUnrelated:\r\nUse quotes instead of curly braces in zset.tcl, for variable expansion",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-02-13T09:17:56Z",
        "closed_at": "2023-03-08T09:57:33Z",
        "merged_at": "2023-03-08T09:57:33Z",
        "body": "XREADGROUP can output a misleading error message regarding use of the $ special ID.\r\n\r\nHere is the example (with some newlines):\r\n```\r\nredis> xreadgroup group workers worker1 count 1 streams mystream\r\n(error) ERR Unbalanced XREAD list of streams: for each stream key an ID or '$' must be specified.\r\n\r\nredis> xreadgroup group workers worker1 count 1 streams mystream $\r\n(error) ERR The $ ID is meaningless in the context of XREADGROUP: you want to read the history of this\r\nconsumer by specifying a proper ID, or use the > ID to get new messages. The $ ID would just return an empty result set.\r\n\r\nredis> xreadgroup group workers worker1 count 1 streams mystream >\r\n1) 1) \"mystream\"\r\n   2) 1) 1) \"1673544607848-0\"\r\n         2) 1) \"n\"\r\n            2) \"1\"\r\n```\r\n\r\nNote that XREADGROUP first returns an error with the following problems in it:\r\n- Command name in the error should be XREADGROUP not XREAD.\r\n- It recommends using $ as an option for a stream ID, then when you try this\r\n  (see second XREADGROUP command above), it errors telling you that `$` doesn't\r\n  make sense in this context even though the previous error message told you to use it\r\n\r\nSuggest that the command name be fixed in the first message, and the second part error\r\nmessage be amended not to talk about using `$` but `>` instead, this works, see the third\r\nand final XREADGROUP example above.\r\n\r\nFixes #11730, commit message took from simonprickett.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-02-09T13:47:38Z",
        "closed_at": "2023-02-09T19:13:42Z",
        "merged_at": null,
        "body": "Make `io_threads_op` an atomic variable so that modifications to it are visible to other threads",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-02-09T13:05:19Z",
        "closed_at": "2023-03-08T20:39:55Z",
        "merged_at": "2023-03-08T20:39:55Z",
        "body": "This PR has two parts:\r\n\r\n1. Fix flaky test case, the previous tests set a lot of volatile keys,\r\nit injects an unexpected DEL command into the replication stream during\r\nthe later test, causing it to fail. Add a flushall to avoid it.\r\n\r\n2. Improve assert_replication_stream, now it can print the whole stream\r\nrather than just the failing line.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4005,
        "deletions": 912,
        "changed_files": 137,
        "created_at": "2023-02-09T11:45:40Z",
        "closed_at": "2023-03-01T09:32:31Z",
        "merged_at": null,
        "body": "```\r\n\r\nvoid getrangeCommand(client *c) {\r\n    robj *o;\r\n    long long start, end;\r\n    char *str, llbuf[32];\r\n    size_t strlen;\r\n\r\n    if (getLongLongFromObjectOrReply(c,c->argv[2],&start,NULL) != C_OK)\r\n        return;\r\n    if (getLongLongFromObjectOrReply(c,c->argv[3],&end,NULL) != C_OK)\r\n        return;\r\n    if ((o = lookupKeyReadOrReply(c,c->argv[1],shared.emptybulk)) == NULL ||\r\n        checkType(c,o,OBJ_STRING)) return;\r\n\r\n    if (o->encoding == OBJ_ENCODING_INT) {\r\n        str = llbuf;\r\n        strlen = ll2string(llbuf,sizeof(llbuf),(long)o->ptr);\r\n    } else {\r\n        str = o->ptr;\r\n        strlen = sdslen(str);\r\n    }\r\n\r\n    /* Convert negative indexes */\r\n    if (start < 0 && end < 0 && start > end) {\r\n        addReply(c,shared.emptybulk);\r\n        return;\r\n    }\r\n    /* Convert negative indexes */\r\n    if (start < 0) start = strlen+start;\r\n    if (end < 0) end = strlen+end;\r\n\r\n    /* Bounds enforcement */\r\n    if (start < 0) start = 0;\r\n    if ((unsigned long long)start >= strlen || end < start) {\r\n        addReply(c,shared.emptybulk);\r\n        return;\r\n    }\r\n\r\n    if (end < 0) end = 0;\r\n    if ((unsigned long long)end >= strlen) end = strlen-1;\r\n\r\n    /* Precondition: end >= 0 && end < strlen, so the only condition where\r\n     * nothing can be returned is: start > end. */\r\n    if (start > end || strlen == 0) {\r\n        addReply(c,shared.emptybulk);\r\n    } else {\r\n        addReplyBulkCBuffer(c,(char*)str+start,end-start+1);\r\n    }\r\n}\r\n\r\n\r\n```\r\n\r\n//  The code above is a rewritten example of the getRangeCommand function in the t_string.c file, which was written for issue number 11738.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-02-08T08:12:01Z",
        "closed_at": "2023-02-09T05:57:20Z",
        "merged_at": "2023-02-09T05:57:20Z",
        "body": "Return an error when loadAppendOnlyFiles fails instead of\r\nexiting. DEBUF LOADAOF command is only meant to be used by\r\nthe test suite, and only by tests that generated an AOF file\r\nfirst. So this change is ok (considering that the caller is\r\nlikely to catch this error and die).\r\n\r\nThis actually revert part of the code in #9012, and now\r\nDEBUG LOADAOF behaves the same as DEBUG RELOAD (returns an\r\nerror when the load fails).\r\n\r\nPlus remove a `after 2000` in a test, which can save times (looks like copy paste error).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-02-08T07:19:40Z",
        "closed_at": "2023-02-20T08:23:25Z",
        "merged_at": "2023-02-20T08:23:25Z",
        "body": "This test case is to cover a edge scenario: when a writable replica enabled AOF at the same time, active expiry keys which was created in writable replicas should propagate to the AOF file, and some versions might crash (fixed by #11615). For details, please refer to #11778 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 75,
        "deletions": 17,
        "changed_files": 7,
        "created_at": "2023-02-07T19:50:12Z",
        "closed_at": "2023-02-14T07:33:21Z",
        "merged_at": "2023-02-14T07:33:21Z",
        "body": "Starting from Redis 7.0 (#9890) we started wrapping everything a command propagates with MULTI/EXEC.\r\nThe problem is that both SCAN and RANDOMKEY can lazy-expire arbitrary keys (similar behavior to active-expire), and put DELs in a transaction.\r\n\r\nFix: When these commands are called without a parent exec-unit (e.g. not in EVAL or MULTI) we avoid wrapping their DELs in a transaction (for the same reasons active-expire and eviction avoids a transaction)\r\n\r\nThis PR adds a per-command flag that indicates that the command may touch arbitrary keys (not the ones in the arguments), and uses that flag to avoid the MULTI-EXEC.\r\nFor now, this flag is internal, since we're considering other solutions for the future.\r\n\r\nNote for cluster mode: if SCAN/RANDOMKEY is inside EVAL/MULTI it can still cause the same situation (as it always did), but it won't cause a CROSSSLOT because replicas and AOF do not perform slot checks.\r\nThe problem with the above is mainly for 3rd party ecosystem tools that propagate commands from master to master, or feed an AOF file with redis-cli into a master.\r\nThis PR aims to fix the regression in redis 7.0, and we opened #11792 to try to handle the bigger problem with lazy expire better for another release.",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-02-07T07:34:00Z",
        "closed_at": "2023-02-16T03:21:18Z",
        "merged_at": "2023-02-16T03:21:18Z",
        "body": "since `cluster_node_timeout` is millisecond, we need correct it when set keep alive.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-02-03T05:28:21Z",
        "closed_at": "2023-02-03T09:18:05Z",
        "merged_at": "2023-02-03T09:18:05Z",
        "body": "Failure happens in FreeBSD daily:\r\n```\r\n*** [err]: Test replication with parallel clients writing in different DBs in tests/integration/replication-4.tcl\r\nExpected [::redis::redisHandle2 dbsize] > 0 (context: type eval line 19 cmd {assert {[$master dbsize] > 0}} proc ::test)\r\n```\r\n\r\nThe test is failing because db 9 has no data (default db), and\r\naccording to the log, we can see that db 9 does not have a key:\r\n```\r\n ### Starting test Test replication with parallel clients writing in different DBs in tests/integration/replication-4.tcl\r\n3338:S 03 Feb 2023 00:15:18.723 - DB 11: 1 keys (0 volatile) in 4 slots HT.\r\n3338:S 03 Feb 2023 00:15:18.723 - DB 12: 141 keys (0 volatile) in 256 slots HT.\r\n```\r\n\r\nWe use `wait_for_condition` to ensure that parallel clients have\r\nwritten data before calling stop_bg_complex_data. At the same time,\r\n`wait_for_condition` is also used to remove the above `after 1000`,\r\nwhich can save time in most cases.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-02-03T03:43:50Z",
        "closed_at": "2023-02-03T08:51:16Z",
        "merged_at": "2023-02-03T08:51:16Z",
        "body": "There is a timing issue in the new ACL log test:\r\n```\r\n*** [err]: ACL LOG aggregates similar errors together and assigns unique entry-id to new errors in tests/unit/acl.tcl\r\nExpected 1675382873989 < 1675382873989 (context: type eval line 15 cmd {assert {$timestamp_last_update_original < $timestamp_last_updated_after_update}} proc ::test)\r\n```\r\n\r\nLooking at the test code, we will check the `timestamp-last-updated` before\r\nand after a new ACL error occurs. Actually `WRONGPASS` errors can be executed\r\nvery quickly on fast machines. For example, in the this case, the execution is\r\ncompleted within one millisecond.\r\n\r\nThe error is easy to reproduce, if we reduce the number of the for loops, for\r\nexample set to 2, and using --loop and --stop. Avoid this timing issue by adding\r\nan `after 1` before the new errors.\r\n\r\nThe test was introduced in #11477.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 49,
        "changed_files": 3,
        "created_at": "2023-02-03T00:01:42Z",
        "closed_at": "2023-02-06T16:26:40Z",
        "merged_at": "2023-02-06T16:26:40Z",
        "body": "I started looking at the profile of  `ZRANGE` related commands, like `ZREVRANGEBYSCORE` due to the benchmark we've added https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zrevrangebyscore-256K-elements-pipeline-1.yml .\r\n\r\nputting it simple, if we have integer scores on the sorted set we're not using the fastest way to reply by calling `d2string` which uses `double2ll` and `ll2string` when it can, instead of `fpconv_dtoa`. \r\n\r\n\r\n**To reproduce:**\r\n\r\npopulate sorted sets (cardinality is not really important here -- the score reply type yes) :\r\n```\r\nredis-cli \"ZADD\" \"zset:10:double_score\" \"0.306275\" \"lysbgqqfqw\" \"0.486004\" \"mtccjerdon\" \"0.941626\" \"jekkafodvk\" \"0.602656\" \"nmgxcctxpn\" \"0.630771\" \"vyqqkuszzh\" \"0.406379\" \"pytrnqdhvs\" \"0.521814\" \"oguwnmniig\" \"0.182854\" \"gekntrykfh\" \"0.657658\" \"nhfnbxqgol\" \"0.218066\" \"cgoeihlnei\"\r\nredis-cli \"ZADD\" \"zset:10:long_score\" \"10000000\" \"lysbgqqfqw\" \"10000001\" \"mtccjerdon\" \"10000002\" \"jekkafodvk\" \"10000003\" \"nmgxcctxpn\" \"10000004\" \"vyqqkuszzh\" \"10000005\" \"pytrnqdhvs\" \"10000006\" \"oguwnmniig\" \"10000007\" \"gekntrykfh\" \"10000008\" \"nhfnbxqgol\" \"10000009\" \"cgoeihlnei\"\r\n```\r\n\r\n**benchmark:**\r\n1) double replies \r\n\r\n```\r\n# resp2\r\nmemtier_benchmark --command=\"ZREVRANGEBYSCORE zset:10:double_score 100000000 0 LIMIT 1 10 WITHSCORES\" --hide-histogram --test-time 60 --pipeline 10 --protocol resp2\r\n\r\n# resp3\r\nmemtier_benchmark --command=\"ZREVRANGEBYSCORE zset:10:double_score 100000000 0 LIMIT 1 10 WITHSCORES\" --hide-histogram --test-time 60 --pipeline 10 --protocol resp3\r\n```\r\n\r\n2) long replies\r\n\r\n```\r\n# resp2\r\nmemtier_benchmark --command=\"ZREVRANGEBYSCORE zset:10:long_score 100000000 0 LIMIT 1 10 WITHSCORES\" --hide-histogram --test-time 60 --pipeline 10 --protocol resp2\r\n\r\n# resp3\r\nmemtier_benchmark --command=\"ZREVRANGEBYSCORE zset:10:long_score 100000000 0 LIMIT 1 10 WITHSCORES\" --hide-histogram --test-time 60 --pipeline 10 --protocol resp3\r\n```\r\n\r\n**profile info:**\r\n\r\nNotice that `addReplyDouble` is taking ~36% of CPU cycles:\r\n![image](https://user-images.githubusercontent.com/5832149/216477204-19c00991-02bb-45a0-89f2-13336b1a9fc8.png)\r\n\r\n**Impact of this PR in the achievable ops/sec for both double and long scores:**\r\n\r\n\r\n\r\nVariation | v7.0.8 | unstable | this PR | % change this PR vs v7.0.8 | % change this PR vs unstable | Note\r\n-- | -- | -- | -- | -- | -- | --\r\nRESP2 - double score | 142997 | 187233 | 185812 | 29.9% | -0.8% | as confirmed, the overhead of double2ll is neglectible. Notice that this change+unstable vs v7.0.8 still retains a large improvement\r\nRESP3 - double score | 142997 | 184714 | 183919 | 28.6% | -0.4% | as confirmed, the overhead of double2ll is neglectible. Notice that this change+unstable vs v7.0.8 still retains a large improvement\r\nRESP2 - long score | 192704 | 250218 | 386702 | 100.7% | 54.5% | ---\r\nRESP3 - long score | 187414 | 246445 | 374957 | 100.1% | 52.1% | ---\r\n\r\nNote that the existing change in unstable missing in 7.0 is #10587",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2023-02-01T22:27:39Z",
        "closed_at": "2023-04-04T08:53:58Z",
        "merged_at": "2023-04-04T08:53:58Z",
        "body": "closes the following issue: https://github.com/redis/redis/issues/11774\r\n\r\n\r\nThis PR attempts to fix the following config file error\r\n\r\n```\r\n*** FATAL CONFIG FILE ERROR (Redis 6.2.7) ***\r\nReading the configuration file, at line 152\r\n>>> 'sentinel known-replica XXXX 127.0.0.1 5001'\r\nDuplicate hostname and port for replica.\r\n```\r\n\r\n\r\nthat is happening when a user uses the legacy key \"known-slave\" in the config file and a config rewrite occurs. The config rewrite logic won't replace the old  line \"sentinel known-slave XXXX 127.0.0.1 5001\" and would add a new line with \"sentinel known-replica XXXX 127.0.0.1 5001\" which results in the error above \"Duplicate hostname and port for replica.\"\r\n\r\n\r\nexample:\r\n\r\nCurrent sentinal.conf\r\n```\r\n...\r\n\r\nsentinel known-slave XXXX 127.0.0.1 5001\r\nsentinel example-random-option X\r\n...\r\n```\r\nafter the config rewrite logic runs:\r\n```\r\n....\r\nsentinel known-slave XXXX 127.0.0.1 5001\r\nsentinel example-random-option X\r\n\r\n# Generated by CONFIG REWRITE\r\nsentinel known-replica XXXX 127.0.0.1 5001\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis bug only exists in Redis versions >=6.2 because prior to that it was hidden by the effects of this bug https://github.com/redis/redis/issues/5388 that that @hwware fixed in https://github.com/redis/redis/pull/8271  and was released in versions >=6.2\r\n",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 119,
        "deletions": 6,
        "changed_files": 6,
        "created_at": "2023-02-01T11:38:01Z",
        "closed_at": "2023-02-09T12:59:05Z",
        "merged_at": "2023-02-09T12:59:05Z",
        "body": "The PR adds support for the following flags on RedisModule_OpenKey:\r\n\r\n* REDISMODULE_OPEN_KEY_NONOTIFY - Don't trigger keyspace event on key misses.\r\n* REDISMODULE_OPEN_KEY_NOSTATS - Don't update keyspace hits/misses counters.\r\n* REDISMODULE_OPEN_KEY_NOEXPIRE - Avoid deleting lazy expired keys.\r\n* REDISMODULE_OPEN_KEY_NOEFFECTS - Avoid any effects from fetching the key\r\n\r\nIn addition, added `RM_GetOpenKeyModesAll`, which returns the mask of all supported OpenKey modes. This allows the module to check, in runtime, which OpenKey modes are supported by the current Redis instance.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 273,
        "deletions": 107,
        "changed_files": 17,
        "created_at": "2023-01-31T10:30:46Z",
        "closed_at": "2023-02-16T06:07:35Z",
        "merged_at": "2023-02-16T06:07:35Z",
        "body": "* Make it clear that current_client is the root client that was called by external connection\r\n* add executing_client which is the client that runs the current command (can be a module or a script)\r\n* Remove script_caller that was used for commands that have CLIENT_SCRIPT to get the client that called the script. in most cases, that's the current_client, and in others (when being called from a module), it could be an intermediate client when we actually want the original one used by the external connection.\r\n\r\nbugfixes:\r\n* RM_Call with C flag should log ACL errors with the requested user rather than the one used by the original client, this also solves a crash when RM_Call is used with C flag from a detached thread safe context.\r\n* addACLLogEntry would have logged info about the script_caller, but in case the script was issued by a module command we actually want the current_client. the exception is when RM_Call is called from a timer event, in which case we don't have a current_client.\r\n\r\nbehavior changes:\r\n* client side tracking for scripts now tracks the keys that are read by the script instead of the keys that are declared by the caller for EVAL\r\n\r\nother changes:\r\n* Log both current_client and executing_client in the crash log.\r\n* remove prepareLuaClient and resetLuaClient, being dead code that was forgotten.\r\n* remove scriptTimeSnapshot and snapshot_time and instead add cmd_time_snapshot that serves all commands and is reset only when execution nesting starts.\r\n* remove code to propagate CLIENT_FORCE_REPL from the executed command to the script caller since scripts aren't propagated anyway these days and anyway this flag wouldn't have had an effect since CLIENT_PREVENT_PROP is added by scriptResetRun.\r\n* fix a module GIL violation issue in afterSleep that was introduced in #10300 (unreleased)",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 55,
        "changed_files": 7,
        "created_at": "2023-01-30T10:29:26Z",
        "closed_at": "2023-01-31T15:26:36Z",
        "merged_at": "2023-01-31T15:26:36Z",
        "body": "In #7875 (Redis 6.2), we changed the sds alloc to be the usable allocation size in order to:\r\n>reduce the need for realloc calls by making the sds implicitly take over\r\nthe internal fragmentation\r\n\r\nThis change was done most sds functions, excluding `sdsRemoveFreeSpace` and `sdsResize`, the reason is that in some places (e.g. clientsCronResizeQueryBuffer) we call sdsRemoveFreeSpace when we see excessive free space and want to trim it.\r\nso if we don't trim it exactly to size, the caller may still see excessive free space and call it again and again.\r\n\r\nHowever, this resulted in some excessive calls to realloc, even when there's no need and it's gonna be a no-op (e.g. when reducing 15 bytes allocation to 13).\r\n\r\nIt turns out that a call for realloc with jemalloc can be expensive even if it ends up doing nothing, so this PR adds a check using `je_nallocx`, which is cheap to avoid the call for realloc.\r\n\r\nin addition to that this PR unifies sdsResize and sdsRemoveFreeSpace into common code.\r\nthe difference between them was that sdsResize would avoid using SDS_TYPE_5, since it want to keep the string ready to be resized again, while sdsRemoveFreeSpace would permit using SDS_TYPE_5 and get an optimal memory consumption.\r\nnow both methods take a `would_regrow` argument that makes it more explicit.\r\n\r\nthe only actual impact of that is that in clientsCronResizeQueryBuffer we call both sdsResize and sdsRemoveFreeSpace for in different cases, and we now prevent the use of SDS_TYPE_5 in both.\r\n\r\nThe new test that was added to cover this concern used to pass before this PR as well, this PR is just a performance optimization and cleanup.\r\n\r\nBenchmark:\r\n`redis-benchmark -c 100 -t set  -d 512 -P 10  -n  100000000`\r\non i7-9850H with jemalloc, shows improvement from 1021k ops/sec to 1067k (average of 3 runs).\r\nsome 4.5% improvement.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-01-29T01:13:48Z",
        "closed_at": "2023-02-02T17:18:22Z",
        "merged_at": "2023-02-02T17:18:22Z",
        "body": "Added code to support xadd as one of the commands that can be run via redis-benchmarking tool\r\n\r\nHere is how you would use this command\r\n\r\n``` redis-benchmark --cluster -h HOST -p PORT -t xadd -n 1000 -c 20 ```\r\n\r\nHappy to make improvements as needed - please review.\r\n\r\nThanks,\r\nMaheedhar",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-01-28T20:23:25Z",
        "closed_at": "2023-01-30T02:00:24Z",
        "merged_at": "2023-01-30T02:00:24Z",
        "body": "Redis 7.0 introduced new logic in `expireIfNeeded()` where a read-only replica would never consider a key as expired when replicating commands from the master. See https://github.com/redis/redis/commit/acf3495eb823df8d1f358b1fe59b759fcc49666f. This was done by checking `server.current_client` with `server.master`. However, we should instead check for `CLIENT_MASTER` flag for this logic to be more robust and consistent with the rest of the Redis code base. \r\n\r\nThis was discovered via one of our internal features where replica can't immediately process the incoming `SET` commands from the master due to various conditions, and we would create a separate AOF client as a fake master client to process this command later. In this case, this client would fall through this check if it tries to update a key that has logically expired on the replica and led to a crash later on. i.e. in `setKey()`, `dbLookup()` somehow believes the key doesn't exist due to this logic here https://github.com/redis/redis/blob/unstable/src/db.c#L284, and later `dbAdd()` will assert because it finds the key actually exists in Redis. This fix would address this issue. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 149,
        "deletions": 11,
        "changed_files": 5,
        "created_at": "2023-01-27T18:19:22Z",
        "closed_at": "2023-03-22T06:17:20Z",
        "merged_at": "2023-03-22T06:17:20Z",
        "body": "This PR allows clients to send information about the client library to redis to be displayed in CLIENT LIST and CLIENT INFO.\r\n\r\nCurrently supports:\r\n`CLIENT [lib-name | lib-ver] <value>`\r\nClient libraries are expected to pipeline these right after AUTH, and ignore the failure in case they're talking to an older version of redis.\r\n\r\nThese will be shown in CLIENT LIST and CLIENT INFO as:\r\n* `lib-name` - meant to hold the client library name.\r\n* `lib-ver` - meant to hold the client library version.\r\n\r\nThe values cannot contain spaces, newlines and any wild ASCII characters, but all other normal chars are accepted, e.g `.`, `=` etc (same as CLIENT NAME).\r\n\r\nThe RESET command does NOT clear these, but they can be cleared to the default by sending a command with a blank string.\r\n\r\nDocs PR: https://github.com/redis/redis-doc/pull/2362",
        "comments": 22
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-01-25T19:32:07Z",
        "closed_at": "2023-01-25T20:42:40Z",
        "merged_at": "2023-01-25T20:42:40Z",
        "body": "Another minor fix for json file format\r\n\r\nWe should always use space instead of Tab\r\n\r\nThis PR fix the wrong code format",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-01-25T17:02:24Z",
        "closed_at": "2023-01-25T18:11:38Z",
        "merged_at": "2023-01-25T18:11:38Z",
        "body": "We should always **use space instead of Tab**\r\n\r\nThis PR fix the wrong code format",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 91,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2023-01-25T02:03:57Z",
        "closed_at": "2023-02-02T17:06:25Z",
        "merged_at": "2023-02-02T17:06:25Z",
        "body": "Currently while a sharded pubsub message publish tries to propagate the message across the cluster, a NULL check is missing for clusterLink.  clusterLink could be NULL if the link is causing memory beyond the set threshold `cluster-link-sendbuf-limit` and server terminates the link. \r\n\r\nThis change introduces two things:\r\n\r\n1. Avoids the engine crashes on the publishing node if a message is tried to be sent to a node and the link is NULL.\r\n2. Adds a debugging tool `CLUSTERLINK KILL` to terminate the clusterLink between two nodes.\r\n\r\n```\r\nRelease Notes:\r\nFix a potential crash if `SPUBLISH` was used when the link between nodes were disconnected or the nodes within the shard were in handshaking state.\r\n```",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-01-24T20:46:24Z",
        "closed_at": "2023-01-26T08:10:18Z",
        "merged_at": "2023-01-26T08:10:18Z",
        "body": "The commnd:\r\n\r\n**sentinel config set option value** \r\nand \r\n**sentinel config get option**\r\n\r\nThey should include at least 4 arguments instead of 3,\r\nThis PR fixs this issue",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 58,
        "deletions": 6,
        "changed_files": 5,
        "created_at": "2023-01-24T15:48:23Z",
        "closed_at": "2023-04-27T14:47:26Z",
        "merged_at": null,
        "body": "We want to add one more parameter in redis.conf: maxmemory-reserved-scale\r\nThe goal is to begin the key eviction process earlier if the maxmemory parameter is set as well.\r\n\r\nCurrently key eviction will be triggered after used memory is greater than maxmemory. This PR introduces a new parameter to trigger key eviction earlier, so that redis still have some reserved memoery for other memory consumption counted as used memory during eviction. \r\n\r\nThis would be helpful if redis has active eviction in the future like active expire, so key can be evicted before maxmemory is reached and when redis is not busy. \r\n\r\nOne example for this paramter\uff1a\r\nAssume \r\n\r\nmaxmemory 4GB\r\nmaxmemory-reserved-scale 20\r\n\r\nThen we could check the detail by info memory command:\r\n\r\nmaxmemory:4294967296  \r\nmaxmemory_human:4.00G  \r\nmaxmemory_policy:allkeys-lru  \r\nmaxmemory_reserved_scale:20  \r\nmaxmemory_available:3435973836  \r\nmaxmemory_available_human:3.20G\r\n\r\nWe could also update and get the maxmemory-reserved-scale value during runtime as following:\r\n\r\nconfig set maxmemory-reserved-scale value\r\nconfig get maxmemory-reserved-scale\r\n\r\nNote: the feature reference the Azure link:\r\nhttps://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-best-practices-memory-management\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2023-01-23T05:46:28Z",
        "closed_at": "2023-01-30T02:04:54Z",
        "merged_at": "2023-01-30T02:04:54Z",
        "body": "This change improves the performance of cluster slots by removing the deferring lengths that are used. Deferring lengths are used in two contexts, the first is for determining the number of replicas that serve a slot (Added in 6.2 as part of a different performance improvement) and the second is for determining the extra networking options for each node (Added in 7.0). For continuous slots, (e.g. 0-8196) this improvement is very negligible, however it becomes more significant when slots are not continuous (e.g. 0 2 4 6 etc) which can happen in production for various users.\r\n\r\nThe `cluster slots` command is deprecated in favor of `cluster shards`, but since most clients don't support the new command yet I think it's important to not degrade performance here.\r\n\r\nBenchmarking shows about 2x improvement, however I wasn't able to get a coherent TPS number since the benchmark process was being saturated long before Redis was, so had to run with multiple benchmarks and merge results. If needed I can add this to our memtier framework. Instead the next section shows the number of usec per call from the benchmark results, which shows significant improvement as well as having a more coherent response in the CoB.\r\n\r\n| | New Code | Old Code | % Improvements\r\n|----|----|----- |-----\r\n| Uniform slots| usec_per_call=10.46 | usec_per_call=11.03 | 5.7%\r\n| Worst case (Only even slots)| usec_per_call=963.80 | usec_per_call=2950.99 | 307%\r\n\r\n\r\nThis change also removes some extra white space that I added a when making a code change for adding hostnames.\r\n\r\nI'm suggesting we also backport this to 7.0, since this was observed by a user who upgraded from 6.2 -> 7.0 and saw a decrease in number of new connections they could establish. Their flow was open a connection, issue slot to understand topology, issue command. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 114,
        "deletions": 32,
        "changed_files": 2,
        "created_at": "2023-01-18T03:42:00Z",
        "closed_at": "2023-08-21T16:48:31Z",
        "merged_at": "2023-08-21T16:48:31Z",
        "body": "BITCOUNT and BITPOS with non-existing key will return 0 even the\r\narguments are error, before this commit:\r\n```\r\n> flushall\r\nOK\r\n> bitcount s 0\r\n(integer) 0\r\n> bitpos s 0 0 1 hello\r\n(integer) 0\r\n\r\n> set s 1\r\nOK\r\n> bitcount s 0\r\n(error) ERR syntax error\r\n> bitpos s 0 0 1 hello\r\n(error) ERR syntax error\r\n```\r\n\r\nThe reason is that we judged non-existing before parameter checking and\r\nreturned. This PR fixes it, and after this commit:\r\n```\r\n> flushall\r\nOK\r\n> bitcount s 0\r\n(error) ERR syntax error\r\n> bitpos s 0 0 1 hello\r\n(error) ERR syntax error\r\n```\r\n\r\nAlso BITPOS made the same fix as #12394, check for wrong argument, before\r\nchecking for key.\r\n```\r\n> lpush mylist a b c\r\n(integer) 3                                                                                    \r\n> bitpos mylist 1 a b\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n```\r\n\r\nThis could somehow cause a breakage, will be part of Redis 8.0.\r\nFixes #11731",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2023-01-18T02:38:30Z",
        "closed_at": "2023-01-20T21:18:53Z",
        "merged_at": "2023-01-20T21:18:53Z",
        "body": "Remove duplicate code for removing the tail item from a list.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-01-17T15:51:36Z",
        "closed_at": "2023-02-01T09:48:48Z",
        "merged_at": "2023-02-01T09:48:48Z",
        "body": "Change history:\r\n- `user` added in 6.0.0, 0f42447a0ec841f0b3e83328ac16a573012e2880\r\n- `argv-mem` and `tot-mem` added in 6.2.0, bea40e6a41e31a52e9e6efee77ea5a4bd873b759\r\n- `redir` added in 6.2.0, dd1f20edc5ecda7848c31601782c5e9d7bce4788\r\n- `resp` added in 7.0.0, 7c376398b1cd827282e17804e230c41cbb48a89c\r\n- `multi-mem` added in 7.0.0, 2753429c99425e3d0216cba79e0e61192975f252\r\n- `rbs` and `rbp` added in 7.0.0, 47c51d0c7858dc8ce7747b78b73cf8cec2e59ff3\r\n- `ssub` added in 7.0.3, 35c2ee8716dc9b1d4edbbb409815a585af491335",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 11,
        "changed_files": 4,
        "created_at": "2023-01-17T11:37:14Z",
        "closed_at": "2023-01-17T15:08:17Z",
        "merged_at": "2023-01-17T15:08:17Z",
        "body": "Upgrade urgency: MODERATE, a quick followup fix for a recently released 6.2.9.\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Revert the change to KEYS in the recent client output buffer limit fix (#11676)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-01-17T06:47:15Z",
        "closed_at": "2023-01-22T07:16:17Z",
        "merged_at": "2023-01-22T07:16:17Z",
        "body": "`sdscatrepr` is not the hot path in redis, but it helpful for some project with sds.c, so it is still worth optimizing its performance",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2023-01-16T08:55:54Z",
        "closed_at": "2023-01-16T11:49:31Z",
        "merged_at": "2023-01-16T11:49:31Z",
        "body": "Authenticated users issuing specially crafted SETRANGE and SORT(_RO) commands can trigger an integer overflow, resulting with Redis attempting to allocate impossible amounts of memory and abort with an OOM panic.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 234,
        "deletions": 60,
        "changed_files": 26,
        "created_at": "2023-01-16T08:46:33Z",
        "closed_at": "2023-01-16T16:40:36Z",
        "merged_at": "2023-01-16T16:40:36Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-35977) Integer overflow in the Redis SETRANGE and SORT/SORT_RO\r\n  commands can drive Redis to OOM panic\r\n* (CVE-2023-22458) Integer overflow in the Redis HRANDFIELD and ZRANDMEMBER\r\n  commands can lead to denial-of-service\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Avoid possible hang when client issues long KEYS, SRANDMEMBER, HRANDFIELD,\r\n  and ZRANDMEMBER commands and gets disconnected by client output buffer limit (#11676)\r\n* Make sure that fork child doesn't do incremental rehashing (#11692)\r\n* Fix a bug where blocking commands with a sub-second timeout would block forever (#11688)\r\n* Fix sentinel issue if replica changes IP (#11590)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 127,
        "deletions": 22,
        "changed_files": 17,
        "created_at": "2023-01-16T08:46:07Z",
        "closed_at": "2023-01-16T16:41:08Z",
        "merged_at": "2023-01-16T16:41:08Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-35977) Integer overflow in the Redis SETRANGE and SORT/SORT_RO\r\n  commands can drive Redis to OOM panic\r\n* (CVE-2023-22458) Integer overflow in the Redis HRANDFIELD and ZRANDMEMBER\r\n  commands can lead to denial-of-service\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Avoid possible hang when client issues long KEYS, SRANDMEMBER, HRANDFIELD,\r\n  and ZRANDMEMBER commands and gets disconnected by client output buffer limit (#11676)\r\n* Fix sentinel issue if replica changes IP (#11590)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 233,
        "deletions": 85,
        "changed_files": 31,
        "created_at": "2023-01-16T08:45:36Z",
        "closed_at": "2023-01-17T12:59:42Z",
        "merged_at": "2023-01-17T12:59:42Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-35977) Integer overflow in the Redis SETRANGE and SORT/SORT_RO\r\n  commands can drive Redis to OOM panic\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Avoid hang when client issues long SRANDMEMBER command and gets\r\n  disconnected by client output buffer limit (#11676)\r\n* Lua: fix crash on a script call with many arguments, a regression in v6.0.16 (#9809)\r\n* Lua: Add checks for min-slave-* configs when evaluating Lua scripts (#10160)\r\n* Fix BITFIELD overflow detection on some compilers due to undefined behavior (#9601)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 623,
        "deletions": 82,
        "changed_files": 13,
        "created_at": "2023-01-15T10:22:53Z",
        "closed_at": "2023-03-14T18:26:22Z",
        "merged_at": "2023-03-14T18:26:22Z",
        "body": "Implementing the WAITAOF functionality requested in issue #10505, which would allow the user to block until a specified number of Redises have fsynced all previous write commands to the AOF.\r\n\r\nSyntax: `WAITAOF <num_local> <num_replicas> <timeout>`\r\nResponse: Array containing two elements: num_local, num_replicas\r\nnum_local is always either 0 or 1 representing the local AOF on the master.\r\nnum_replicas is the number of replicas that acknowledged the a replication\r\noffset of the last write being fsynced to the AOF.\r\n\r\nReturns an error when called on replicas, or when called with non-zero\r\nnum_local on a master with AOF disabled, in all other cases the response\r\njust contains number of fsync copies.\r\n\r\nMain changes:\r\n* Added code to keep track of replication offsets that are confirmed to have been fsynced to disk.\r\n* Keep advancing master_repl_offset even when replication is disabled (and there's no replication backlog, only if there's an AOF enabled).\r\n  This way we can use this command and it's mechanisms even when replication is disabled.\r\n* Extend REPLCONF ACK to `REPLCONF ACK <ofs> FACK <ofs>`, the FACK will be appended only if there's an AOF on the replica, and already ignored on old masters (thus backwards compatible)\r\n* WAIT now no longer wait for the replication offset after your last command, but rather the replication offset after your last write (or read command that caused propagation, e.g. lazy expiry).\r\n\r\nUnrelated changes:\r\n* WAIT command respects CLIENT_DENY_BLOCKING (not just CLIENT_MULTI)\r\n\r\nImplementation details:\r\n* Add an atomic var named `fsynced_reploff_pending` that's updated (usually by the bio thread) and later copied to the main `fsynced_reploff` variable (only if the AOF base file exists).\r\n  I.e. during the initial AOF rewrite it will not be used as the fsynced offset since the AOF base is still missing.\r\n* Replace close+fsync bio job with new BIO_CLOSE_AOF (AOF specific) job that will also update fsync offset the field.\r\n* Handle all AOF jobs (BIO_CLOSE_AOF, BIO_AOF_FSYNC) in the same bio\r\n  worker thread, to impose ordering on their execution. This solves a\r\n  race condition where a job could set `fsynced_reploff_pending` to a higher\r\n  value than another pending fsync job, resulting in indicating an offset\r\n  for which parts of the data have not yet actually been fsynced.\r\n  Imposing an ordering on the jobs guarantees that fsync jobs are executed\r\n  in increasing order of replication offset.\r\n* Drain bio jobs when switching `appendfsync` to \"always\"\r\n  This should prevent a write race between updates to `fsynced_reploff_pending`\r\n  in the main thread (`flushAppendOnlyFile` when set to ALWAYS fsync), and\r\n  those done in the bio thread.\r\n* Drain the pending fsync when starting over a new AOF to avoid race conditions\r\n  with the previous AOF offsets overriding the new one (e.g. after switching to replicate from a new master).\r\n* Make sure to update the fsynced offset at the end of the initial AOF rewrite.\r\n  a must in case there are no additional writes that trigger a periodic fsync, specifically for a replica that does a full sync.\r\n\r\nLimitations:\r\nIt is possible to write a module and a Lua script that propagate to the AOF and doesn't\r\npropagate to the replication stream. see REDISMODULE_ARGV_NO_REPLICAS and luaRedisSetReplCommand.\r\nThese features are incompatible with the WAITAOF command, and can result in two bad cases.\r\nThe scenario is that the user executes command that only propagates to AOF, and then immediately\r\nissues a WAITAOF, and there's no further writes on the replication stream after that.\r\n1. if the the last thing that happened on the replication stream is a PING (which increased the replication offset but won't trigger an fsync on the replica), then the client would hang forever (will wait for an fack that the replica will never send sine it doesn't trigger any fsyncs).\r\n2. if the last thing that happened is a write command that got propagated properly, then WAITAOF will be released immediately, without waiting for an fsync (since the offset didn't change)\r\n\r\nRefactoring:\r\n* Plumbing to allow bio worker to handle multiple job types\r\n  This introduces infrastructure necessary to allow BIO workers to\r\n  not have a 1-1 mapping of worker to job-type. This allows in the\r\n  future to assign multiple job types to a single worker, either as\r\n  a performance/resource optimization, or as a way of enforcing\r\n  ordering between specific classes of jobs.\r\n\r\nDiscussion:\r\n[Note currently this PR implements the most explicit approach of the options presented blow]\r\n\r\nIn its most general form, the command can be used to wait for fsync by replicas, by the master (or _only_ Redis if replication is not used), or some combination of the above. A few use-cases I can think of:\r\n\r\n1. I have no replication, and wish to wait for all of my previous writes to be persisted to disk before I continue.\r\n2. I have a master and _n_ replicas. I wish to wait for at least _m_ of those replicas to persist my writes to disk before I continue.\r\n3. I have a master and _n_ replicas. I wish to wait until at least _m_ Redises (including both master and replicas) have persisted my writes to disk before I continue.\r\n\r\nThe simplest approach would be to extend the existing `WAIT` command with an optional `AOF` keyword:\r\n\r\n```\r\nWAIT AOF <num> <timeout>\r\n```\r\n\r\nIn which case we need to define whether `<num>` includes the master or not. If it does, use-cases 1 and 3 are covered, but use-case 2\u2014which would probably be a pretty common one\u2014is impossible; If it does not, 2 is supported but 1 and 3 are not. This could be extended by giving \"0\" the special meaning of \"wait for current Redis only\" to enable use-case 1, but this might be somewhat surprising semantics.\r\n\r\nAnother option that crossed my mind is to use positive/negative numbers for the different meanings. e.g.: \"-5\" means \"wait for any 5 Redises\" while \"5\" means \"wait for 5 replicas.\" This is simple and supports all 3 use-cases above, but might not be intuitive or particularly easy to remember.\r\n\r\nYet another possibility is of course to split this functionality into a separate command entirely, in which case we need to define the arguments and semantics of that new command.\r\n\r\nTodo:\r\n- [x] make a redis-doc PR: https://github.com/redis/redis-doc/pull/2359",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-01-13T14:27:30Z",
        "closed_at": "2023-01-22T14:32:20Z",
        "merged_at": "2023-01-22T14:32:20Z",
        "body": "replace \"clokcsource\" with \"clocksource\"",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-01-13T08:47:37Z",
        "closed_at": "2023-01-13T09:50:49Z",
        "merged_at": null,
        "body": "This is just a modification of the code format. In vim, \u201cptr-PREFIX_SIZE\u201d will be recognized as a variable name and unable to jump to the definition",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 297,
        "deletions": 5,
        "changed_files": 10,
        "created_at": "2023-01-13T01:18:30Z",
        "closed_at": "2023-03-21T17:07:12Z",
        "merged_at": "2023-03-21T17:07:12Z",
        "body": "Related to issues #11516 #11083 \r\n\r\nThis allows modules to register commands to existing ACL categories and blocks the creation of [sub]commands, datatypes and registering the configs outside of the OnLoad function.\r\n\r\nFor allowing modules to register commands to existing ACL categories,\r\nThis PR implements a new API `int RM_SetCommandACLCategories()` which takes a pointer to a `RedisModuleCommand` and a C string `aclflags` containing the set of space separated ACL categories.\r\nExample, `'write slow'` marks the command as part of the write and slow ACL categories.\r\n\r\nThe C string `aclflags` is tokenized by implementing a helper function `categoryFlagsFromString()`. Theses tokens are matched and the corresponding ACL categories flags are set by a helper function `matchAclCategoriesFlags`. The helper function `categoryFlagsFromString()` returns the corresponding `categories_flags` or returns `-1` if some token not processed correctly.\r\n\r\nIf the module contains commands which are registered to existing ACL categories, the number of [sub]commands are tracked by `num_commands_with_acl_categories` in `struct RedisModule`. Further, the allowed command bit-map of the existing users are recomputed from the `command_rules` list, by implementing a function called `ACLRecomputeCommandBitsFromCommandRulesAllUsers()` for the existing users to have access to the module commands on runtime.\r\n\r\n\r\n## Breaking change\r\n\r\nThis change requires that registering commands and subcommands only occur during a modules \"OnLoad\" function, in order to allow efficient recompilation of ACL bits. We also chose to block registering configs and types, since we believe it's only valid for those to be created during onLoad. We check for this `onload` flag in `struct RedisModule` to check if the call is made from the `OnLoad` function.",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-01-06T18:52:30Z",
        "closed_at": "2023-08-16T07:48:49Z",
        "merged_at": "2023-08-16T07:48:49Z",
        "body": "When running cluster info, and the number of keys overflows the integer value, the summary no longer makes sense. This fixes by using an appropriate type to handle values over the max int value.\r\n\r\n```\r\ntylerbream@unique-service-redis-1:~$ redis-cli --cluster info localhost:6379\r\n20.0.*.*:6379 (5ba6400d...) -> 258386540 keys | 994 slots | 2 slaves.\r\n20.0.*.*:6379 (a6165089...) -> 257990944 keys | 991 slots | 2 slaves.\r\n20.0.*.*:6379 (e42fc6d7...) -> 258346175 keys | 994 slots | 2 slaves.\r\n20.0.*.*:6379 (3a918211...) -> 124965126 keys | 486 slots | 2 slaves.\r\n20.0.*.*:6379 (5ec5dcd8...) -> 127660401 keys | 496 slots | 2 slaves.\r\n20.0.*.*:6379 (70e711fd...) -> 127895119 keys | 497 slots | 2 slaves.\r\n20.0.*.*:6379 (e9beda89...) -> 127634622 keys | 496 slots | 2 slaves.\r\n20.0.*.*:6379 (ad2036f0...) -> 127620813 keys | 496 slots | 2 slaves.\r\n20.0.*.*:6379 (effb464e...) -> 127887459 keys | 497 slots | 2 slaves.\r\n20.0.*.*:6379 (e61f9cce...) -> 127582083 keys | 496 slots | 2 slaves.\r\n20.0.*.*:6379 (8d710b8c...) -> 127602269 keys | 496 slots | 2 slaves.\r\n20.0.*.*:6379 (300597f3...) -> 127932883 keys | 497 slots | 2 slaves.\r\n20.0.*.*:6379 (38b90fee...) -> 127658378 keys | 496 slots | 2 slaves.\r\n20.0.*.*:6379 (a66b9dd7...) -> 127633387 keys | 496 slots | 2 slaves.\r\n20.0.*.*:6379 (e692a243...) -> 258046481 keys | 993 slots | 2 slaves.\r\n20.0.*.*:6379 (da431ce4...) -> 258033765 keys | 993 slots | 2 slaves.\r\n20.0.*.*:6379 (7de68cb5...) -> 261471098 keys | 1006 slots | 2 slaves.\r\n20.0.*.*:6379 (116b1d64...) -> 258326490 keys | 994 slots | 2 slaves.\r\n20.0.*.*:6379 (f830e971...) -> 127646850 keys | 496 slots | 2 slaves.\r\n20.0.*.*:6379 (63d45ca2...) -> 254979307 keys | 981 slots | 2 slaves.\r\n20.0.*.*:6379 (435002bf...) -> 127887872 keys | 497 slots | 2 slaves.\r\n20.0.*.*:6379 (d2526757...) -> 260664072 keys | 1003 slots | 2 slaves.\r\n20.0.*.*:6379 (acc7d1ce...) -> 230822 keys | 1 slots | 2 slaves.\r\n20.0.*.*:6379 (143b0757...) -> 257767131 keys | 992 slots | 2 slaves.\r\n[OK] -53117209 keys in 24 masters.\r\n-3242.02 keys per slot on average.\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1156,
        "deletions": 564,
        "changed_files": 27,
        "created_at": "2023-01-06T02:04:54Z",
        "closed_at": "2023-10-15T06:58:26Z",
        "merged_at": "2023-10-15T06:58:26Z",
        "body": "This is an implementation of https://github.com/redis/redis/issues/10589 that eliminates 16 bytes per entry in cluster mode, that are currently used to create a linked list between entries in the same slot.  Main idea is splitting main dictionary into 16k smaller dictionaries (one per slot), so we can perform all slot specific operations, such as iteration, without any additional info in the `dictEntry`. The expire dictionary is also split up so that each slot is logically decoupled, so that in subsequent revisions we will be able to atomically flush a slot of data.\r\n\r\n## Important changes\r\n* Incremental rehashing - one big change here is that it's not one, but rather up to 16k dictionaries that can be rehashing at the same time, in order to keep track of them, we introduce a separate queue for dictionaries that are rehashing. Also instead of rehashing a single dictionary, cron job will now try to rehash as many as it can in 1ms.\r\n* getRandomKey - now needs to not only select a random key, from the random bucket, but also needs to select a random dictionary. Fairness is a major concern here, as it's possible that keys can be unevenly distributed across the slots. In order to address this search we introduced [binary index](https://www.topcoder.com/thrive/articles/Binary%20Indexed%20Trees) that allows us to search cumulative frequencies of slot ranges, better explanation in comments [here](https://github.com/redis/redis/pull/11695#issuecomment-1480454348). With that data structure we are able to efficiently find a random slot using binary search in O(log^2(slot count)) time.\r\n* Iteration efficiency - when iterating dictionary with a lot of empty slots, we want to skip them efficiently. We can do this using same binary index that is used for random key selection, this index allows us to find a slot for a specific key index. For example if there are 10 keys in the slot 0, then we can quickly find a slot that contains 11th key using binary search on top of the binary index tree.\r\n* scan API - in order to perform a scan across the entire DB, the cursor now needs to not only save position within the dictionary but also the slot id. In this change we append slot id into LSB of the cursor so it can be passed around between client and the server. This has interesting side effect, now you'll be able to start scanning specific slot by simply providing slot id as a cursor value. The plan is to not document this as defined behavior, however. It's also worth nothing the SCAN API is now technically incompatible with previous versions, although practically we don't believe it's an issue.\r\n* Checksum calculation optimizations - During command execution, we know that all of the keys are from the same slot (outside of a few notable exceptions such as cross slot scripts and modules). We don't want to compute the checksum multiple multiple times, hence we are relying on cached slot id in the client during the command executions. All operations that access random keys, either should pass in the known slot or recompute the slot. \r\n* Slot info in RDB - in order to resize individual dictionaries correctly, while loading RDB, it's not enough to know total number of keys (of course we could approximate number of keys per slot, but it won't be precise). To address this issue, we've added additional metadata into RDB that contains number of keys in each slot, which can be used as a hint during loading.\r\n* DB size - besides `DBSIZE` API, we need to know size of the DB in many places want, in order to avoid scanning all dictionaries and summing up their sizes in a loop, we've introduced a new field into `redisDb` that keeps track of `key_count`. This way we can keep DBSIZE operation O(1). This is also kept for O(1) expires computation as well.\r\n\r\n## Performance\r\nThis change improves SET performance in cluster mode by ~5%, most of gains come from us not having to maintain linked lists for keys in slot, non-cluster mode should have same performance. See benchmarks in the comments below.\r\n\r\n## Interface changes\r\n* Removed `overhead.hashtable.slot-to-keys` to `MEMORY STATS`\r\n* Scan API will now require 64 bits to store the cursor, even on 32 bit systems, as the slot information will be stored.\r\n\r\nFollow up items:\r\n- [ ] : Performance improvements https://github.com/redis/redis/issues/12654\r\n- [ ] : We are introducing an incompatible change for the SCAN API. During an engine upgrade, the scan cursor will become invalidated as it will not include slot information (See https://github.com/redis/redis/issues/12440). Currently we don't do anything to reject old cursors, since the cursor will just look like a valid cursor for slot 0. I think we don't care, but just pinning to make sure folks agree.\r\n",
        "comments": 62
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-01-05T23:01:05Z",
        "closed_at": "2023-01-10T07:09:51Z",
        "merged_at": "2023-01-10T07:09:51Z",
        "body": "While creating a generator of valid Redis commands based on the json specification files, I hit upon xsetid not able to execute on Redis6.\r\n\r\nTurns out this was because entries-added and max-deleted-id have been added in Redis7, but not marked as such in src/commands/xsetid.json.\r\n\r\nProbably not a big deal since xsetid is an internal command anyways. This raises the question though, would an \"internal\" flag make sense in these specifications?",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 21,
        "changed_files": 3,
        "created_at": "2023-01-05T09:59:45Z",
        "closed_at": "2023-01-10T06:40:41Z",
        "merged_at": "2023-01-10T06:40:41Z",
        "body": "Turns out that a fork child calling getExpire while persisting keys (and possibly also a result of some module fork tasks) could cause dictFind to do incremental rehashing in the child process, which is both a waste of time, and also causes COW harm.\r\n\r\nDiscovered in #11679",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 18,
        "changed_files": 1,
        "created_at": "2023-01-04T19:36:13Z",
        "closed_at": "2023-01-06T15:44:41Z",
        "merged_at": null,
        "body": "In our current test codes for XAUTOCLAIM, XCLAIM and XPENDING commands, the min-idle-time is 10ms, \r\nwe do not need 200ms as the idle time, we could decrease them to 15ms, \r\nthus the total testing runtime could decrease 60%, from 5 sec to 2 sec.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-01-04T15:35:39Z",
        "closed_at": "2023-01-08T09:02:49Z",
        "merged_at": "2023-01-08T09:02:49Z",
        "body": "Any value in the range of [0-1) turns to 0 when being cast from double to long long.\r\n\r\nhttps://github.com/redis/redis/issues/11687\r\n\r\n```\r\nRelease Notes\r\nFix a bug where blocking commands with a non-zero blocking time in seconds were blocking forever.\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2023-01-04T14:00:18Z",
        "closed_at": "2023-01-05T06:21:57Z",
        "merged_at": "2023-01-05T06:21:57Z",
        "body": "PR #11290 added listpack encoding for sets, but was missing two things:\r\n1. Correct handling of MEMORY USAGE (leading to an assertion).\r\n2. Had an uncontrolled scratch buffer size in SRANDMEMBER leading to OOM panic (reported in #11668). Fixed by copying logic from ZRANDMEMBER.\r\n\r\nnote that both issues didn't exist in any redis release.\r\n\r\nThe case of SRANDMEMBER in #11668 will now result in a hung likes in #11671 (instead of OOM panic), to be discussed in #11671\r\n\r\nBoth issues reported by @yype ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-01-04T12:36:21Z",
        "closed_at": "2023-01-05T05:08:08Z",
        "merged_at": null,
        "body": "When parsing string to integer, add a special test for -0, see issue #11683",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2023-01-02T09:44:13Z",
        "closed_at": "2023-01-16T11:51:19Z",
        "merged_at": "2023-01-16T11:51:19Z",
        "body": "Related to the hang reported in #11671\r\nCurrently, redis can disconnect a client due to reaching output buffer limit, it'll also avoid feeding that output buffer with more data, but it will keep running the loop in the command (despite the client already being marked for disconnection)\r\n\r\nThis PR is an attempt to mitigate the problem, specifically for commands that are easy to abuse, specifically: KEYS, HRANDFIELD, SRANDMEMBER, ZRANDMEMBER.\r\nThe RAND family of commands can take a negative COUNT argument (which is not bound to the number of elements in the key), so it's enough to create a key with one field, and then these commands can be used to hang redis.\r\nFor KEYS the caller can use the existing keyspace in redis (if big enough).\r\n\r\nNOTICE: in Redis 7.0 this fix covers KEYS as well, but in 6.2 and 6.0 it doesn't, this is because in 7.0 there's a mechanism to avoid sending partial replies to the client, and in older releases there isn't, and without it there's a risk that the client would be able to read what looks like a complete KEYS command.",
        "comments": 25
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-01-01T21:27:40Z",
        "closed_at": "2023-01-10T14:06:25Z",
        "merged_at": "2023-01-10T14:06:25Z",
        "body": "Fixes small typo in memory overcommit message in syscheck.c (double word can).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2023-01-01T18:07:19Z",
        "closed_at": "2023-01-16T11:50:28Z",
        "merged_at": "2023-01-16T11:50:28Z",
        "body": "missing range check in ZRANDMEMBER and HRANDIFLD leading to panic due\r\nto protocol limitations\r\n\r\nfixes #11670",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 15,
        "changed_files": 7,
        "created_at": "2023-01-01T01:58:09Z",
        "closed_at": "2023-01-04T08:52:56Z",
        "merged_at": "2023-01-04T08:52:56Z",
        "body": "Introduce .is_local method to connection, and implement for TCP/TLS/ Unix socket, also drop 'int islocalClient(client *c)'. Then we can hide the detail into the specific connection types. Uplayer tests a connection is local or not by abstract method only.\r\n\r\nSigned-off-by: zhenwei pi <pizhenwei@bytedance.com>",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 137,
        "deletions": 72,
        "changed_files": 2,
        "created_at": "2022-12-30T03:27:31Z",
        "closed_at": "2023-03-12T17:47:07Z",
        "merged_at": "2023-03-12T17:47:07Z",
        "body": "This can happen when a key almost equal or larger than the\r\nclient output buffer limit of the replica is written.\r\n\r\nExample:\r\n1. DB is empty\r\n2. Backlog size is 1 MB\r\n3. Client out put buffer limit is 2 MB\r\n4. Client writes a 3 MB key\r\n5. The shared replication buffer will have a single node which contains\r\nthe key written above, and it exceeds the backlog size.\r\n\r\nAt this point the client output buffer usage calculation will report the\r\nreplica buffer to be 3 MB (or more) even after sending all the data to\r\nthe replica.\r\nThe primary drops the replica connection for exceeding the limits,\r\nthe replica reconnects and successfully executes partial sync but the\r\nprimary will drop the connection again because the buffer usage is still\r\n3 MB. This happens over and over.\r\n\r\nTo mitigate the problem, this fix limits the maximum size of a single\r\nbacklog node to be (repl_backlog_size/16). This way a single node can't\r\nexceed the limits of the COB (the COB has to be larger than the\r\nbacklog).\r\nIt also means that if the backlog has some excessive data it can't trim,\r\nit would be at most about 6% overuse.\r\n\r\nother notes:\r\n1. a loop was added in feedReplicationBuffer which caused a massive LOC\r\n  change due to indentation, the actual changes are just the `min(max` and the loop.\r\n3. an unrelated change in an existing test to speed up a server termination which took 10 seconds.",
        "comments": 28
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-12-29T22:30:29Z",
        "closed_at": "2023-01-12T00:42:24Z",
        "merged_at": "2023-01-12T00:42:24Z",
        "body": "Hey, I am from AWS Elasticache Team.\r\n\r\nThis change increase the frequency of the failover log from 5 minutes to 10 seconds. This log is only emitted when a replica has an outstanding election is progress, and waiting 5 minutes for the next log makes debugging and alarming on the log messages too slow. It also now prints out the number of votes the replica has currently received as well as the number of votes it needs to achieve quorum so that we can track the progress if it's running slowly.\r\n\r\nHarry Lin",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-12-29T08:59:02Z",
        "closed_at": "2023-04-04T07:45:09Z",
        "merged_at": "2023-04-04T07:45:09Z",
        "body": "Match 127.0.0.0/8 instead of just `127.0.0.1` to detect the local clients.\r\n\r\nFixes https://github.com/redis/redis/issues/11637",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1057,
        "deletions": 103,
        "changed_files": 11,
        "created_at": "2022-12-28T01:50:12Z",
        "closed_at": "2023-03-15T22:18:42Z",
        "merged_at": "2023-03-15T22:18:42Z",
        "body": "(PR from AWS)\r\n\r\nThis PR adds new module callbacks that can override the default password based authentication associated with ACLs. With this, Modules can register auth callbacks through which they can implement their own Authentication logic. When `AUTH` and `HELLO AUTH ...` commands are used, Module based authentication is attempted after which normal password based authentication is attempted if needed.\r\nThe new Module APIs added in this PR are - `RM_RegisterCustomAuthCallback` and `RM_BlockClientOnAuth` and `RedisModule_ACLAddLogEntryByUserName `.\r\n\r\nNote\r\n- This follows the module interface explained here: https://github.com/redis/redis/issues/11256#issuecomment-1246335809\r\n- Implements #11256 and builds over a commit from an earlier PR which updates the HELLO command (https://github.com/redis/redis/pull/11037). This introduces a breaking change, see **Breaking changes** for more details.\r\n- [tests/modules/auth.c ](https://github.com/redis/redis/blob/0762b2ea019058bf607cf1aad50f472d705106bf/tests/modules/auth.c#L79-L221) contains sample usage of custom Module based authentication.\r\n\r\nModule based authentication will be attempted for all Redis users (created through the ACL SETUSER cmd or through Module APIs) even if the Redis user does not exist at the time of the command. This gives a chance for the Module to create the RedisModule user and then authenticate via the RedisModule API - from the custom auth callback.\r\n\r\nFor the AUTH command, we will support both variations - `AUTH <username> <password>` and `AUTH <password>`. In case of the `AUTH <password>` variation, the custom auth callbacks are triggered with \u201cdefault\u201d as the username and password as what is provided.\r\n\r\n\r\n### RedisModule_RegisterCustomAuthCallback\r\n```\r\nvoid RM_RegisterCustomAuthCallback(RedisModuleCtx *ctx, RedisModuleCustomAuthCallback cb) {\r\n```\r\nThis API registers a callback to execute to prior to normal password based authentication. Multiple callbacks can be registered across different modules. These callbacks are responsible for either handling the authentication, each authenticating the user or explicitly denying, or deferring it to other authentication mechanisms. Callbacks are triggered in the order they were registered. When a Module is unloaded, all the auth callbacks registered by it are unregistered. The callbacks are attempted, in the order of most recently registered callbacks, when the AUTH/HELLO (with AUTH field is provided) commands are called. The callbacks will be called with a module context along with a username and a password, and are expected to take one of the following actions:\r\n\r\n (1) Authenticate - Use the RM_Authenticate* API successfully and return `REDISMODULE_AUTH_HANDLED`. This will immediately end the auth chain as successful and add the OK reply.\r\n(2) Block a client on authentication - Use the `RM_BlockClientOnAuth` API and return `REDISMODULE_AUTH_HANDLED`. Here, the client will be blocked until the `RM_UnblockClient `API is used which will trigger the auth reply callback (provided earlier through the `RM_BlockClientOnAuth`). In this reply callback, the Module should authenticate, deny or skip handling authentication.\r\n(3) Deny Authentication - Return `REDISMODULE_AUTH_HANDLED` without authenticating or blocking the client. Optionally, `err` can be set to a custom error message. This will immediately end the auth chain as unsuccessful and add the ERR reply.\r\n(4) Skip handling Authentication - Return `REDISMODULE_AUTH_NOT_HANDLED` without blocking the client. This will allow the engine to attempt the next custom auth callback.\r\n\r\nIf none of the callbacks authenticate or deny auth, then password based auth is attempted and will authenticate or add failure logs and reply to the clients accordingly.\r\n\r\n### RedisModule_BlockClientOnAuth\r\n```\r\nRedisModuleBlockedClient *RM_BlockClientOnAuth(RedisModuleCtx *ctx, RedisModuleCustomAuthCallback reply_callback,\r\n                                               void (*free_privdata)(RedisModuleCtx*,void*))\r\n```\r\nThis API can only be used from a Module from the custom auth callback. If a client is not in the middle of custom module based authentication, ERROR is returned. Otherwise, the client is blocked and the `RedisModule_BlockedClient` is returned similar to the `RedisModule_BlockClient` API.\r\n\r\n### RedisModule_ACLAddLogEntryByUserName\r\n```\r\nint RM_ACLAddLogEntryByUserName(RedisModuleCtx *ctx, RedisModuleString *username, RedisModuleString *object, RedisModuleACLLogEntryReason reason)\r\n```\r\nAdds a new entry in the ACL log with the `username` RedisModuleString provided. This simplifies the Module usage because now, developers do not need to create a Module User just to add an error ACL Log entry. Aside from accepting username (RedisModuleString) instead of a RedisModuleUser, it is the same as the existing `RedisModule_ACLAddLogEntry` API.\r\n\r\n\r\n### Breaking changes\r\n- HELLO command - Clients can now only set the client name and RESP protocol from the `HELLO` command if they are authenticated. Also, we now finish command arg validation first and return early with a ERR reply if any arg is invalid. This is to avoid mutating the client name / RESP from a command that would have failed on invalid arguments.\r\n\r\n### Notable behaviors\r\n- Module unblocking - Now, we will not allow Modules to block the client from inside the context of a reply callback (triggered from the Module unblock flow `moduleHandleBlockedClients`).\r\n\r\n### Summary of previous discussions (https://github.com/redis/redis/issues/11256):\r\n- We will not overload / re-use the existing `RM_BlockClient` APIs. Instead, we will add a new dedicated API for blocking on auth - `RM_BlockClientOnAuth` - because (1) we will need to supply the username / password to the Module. (2) we need to know the authentication result from the reply callback (Allow, Deny, Skip). Both of these require a custom type of callbacks and reply callbacks.\r\n- Instead of adding a separate Module API (such as `RM_DenyAuth `) to indicate the Authentication Result, we will use return codes from the custom auth callback and reply callback\r\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-12-26T06:02:09Z",
        "closed_at": "2022-12-28T16:15:50Z",
        "merged_at": "2022-12-28T16:15:50Z",
        "body": "This call is introduced in #8687, but became irrelevant in #11348, and is currently a no-op.\r\nThe fact is that #11348 an unintended side effect, which is that even if the client eviction config\r\nis enabled, there are certain types of clients for which memory consumption is not accurately\r\ntracked, and so unlike normal clients, their memory isn't reported correctly in INFO.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 95,
        "deletions": 73,
        "changed_files": 3,
        "created_at": "2022-12-25T09:03:05Z",
        "closed_at": "2023-01-02T07:41:54Z",
        "merged_at": "2023-01-02T07:41:54Z",
        "body": "Description - When the server crashes we print the bug report of it, in that we print the SERVER INFO , but we dont print the CLUSTER INFO (if the server is cluster enabled), This data can be very helpful with crashes that occur on enabled servers, and espaciialy if the crash is related to the cluser enabled feature.\r\n\r\nI just finished debugging such a bug and if i had this info it would have been really helpful.\r\n\r\nThis PR , adds the CLUSTER INFO and CLUSTER NODES to the bug report string.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 62,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2022-12-22T11:38:49Z",
        "closed_at": "2023-01-04T09:03:56Z",
        "merged_at": "2023-01-04T09:03:56Z",
        "body": "TLDR: solve a problem introduce in Redis 7.0.6 (#11541) with RM_CommandFilterArgInsert being called from scripts, which can lead to memory corruption.\r\n\r\nLibc realloc can return the same pointer even if the size was changed. The code in freeLuaRedisArgv had an assumption that if the pointer didn't change, then the allocation didn't change, and the cache can still be reused.\r\nHowever, if rewriteClientCommandArgument or RM_CommandFilterArgInsert were used, it could be that we realloced the argv array, and the pointer didn't change, then a consecutive command being executed from Lua can use that argv cache reaching beyond its size.\r\nThis was actually only possible with modules, since the decision to realloc was based on argc, rather than argv_len.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-12-22T04:11:16Z",
        "closed_at": "2022-12-22T08:51:43Z",
        "merged_at": "2022-12-22T08:51:43Z",
        "body": "This test failed in FreeBSD:\r\n```\r\n*** [err]: PTTL returns time to live in milliseconds in tests/unit/expire.tcl\r\nExpected 836 > 900 && 836 <= 1000 (context: type eval line 5 cmd {assert {$ttl > 900 && $ttl <= 1000}} proc ::test)\r\n```\r\n\r\nOn some slow machines, sometimes the test take close to 200ms\r\nto finish. We only set aside 100ms, so that caused the failure.\r\nSince the failure was around 800, change the condition to be >500.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 74,
        "deletions": 22,
        "changed_files": 1,
        "created_at": "2022-12-21T10:33:01Z",
        "closed_at": "2023-01-04T09:13:22Z",
        "merged_at": "2023-01-04T09:13:22Z",
        "body": "The current redis-cli does not support the real PSYNC command, the older\r\nversion of redis-cli can support PSYNC is because that we actually issue\r\nthe SYNC command instead of PSYNC, so it act like SYNC (always full-sync).\r\nNoted that in this case we will send the SYNC first (triggered by sendSync),\r\nthen send the PSYNC (the one in redis-cli input).\r\n\r\nDidn't bother to find which version that the order changed, we send PSYNC\r\nfirst (the one in redis-cli input), and then send the SYNC (the one triggered\r\nby sendSync). So even full-sync is not working anymore, and it will result\r\nthis output (mentioned in issue #11246):\r\n```\r\npsync dummy 0\r\nEntering replica output mode...  (press Ctrl-C to quit)\r\nSYNC with master, discarding bytes of bulk transfer until EOF marker...\r\nError reading RDB payload while SYNCing\r\n```\r\n\r\nThis PR adds PSYNC support to redis-cli, which can handle +FULLRESYNC and\r\n+CONTINUE responses, and some examples will follow.\r\n\r\nSYNC command:\r\n```\r\n127.0.0.1:6379> sync\r\nEntering replica output mode...  (press Ctrl-C to quit)\r\nFull resync with master, discarding 177 bytes of bulk transfer...\r\nFull resync done. Logging commands from master.\r\n\"ping\"\r\n\"SELECT\",\"0\"\r\n\"incr\",\"counter\"\r\n```\r\n\r\nSYNC command with EOF marker:\r\n```\r\n127.0.0.1:6379> REPLCONF CAPA EOF\r\nOK\r\n127.0.0.1:6379> sync\r\nEntering replica output mode...  (press Ctrl-C to quit)\r\nFull resync with master, discarding bytes of bulk transfer until EOF marker...\r\nFull resync done after 233 bytes. Logging commands from master.\r\nsending REPLCONF ACK 0\r\n\"SELECT\",\"0\"\r\n\"incr\",\"counter\"\r\n```\r\n\r\nPSYNC command with +FULLRESYNC:\r\n```\r\n127.0.0.1:6379> psync dummy 0\r\nEntering replica output mode...  (press Ctrl-C to quit)\r\nPSYNC replied +FULLRESYNC 188d44f21759cf12551e0fd36a696bab81c0d794 128\r\nFull resync with master, discarding 194 bytes of bulk transfer...\r\nFull resync done. Logging commands from master.\r\n\"ping\"\r\n\"SELECT\",\"0\"\r\n\"incr\",\"counter\"\r\n```\r\n\r\nPSYNC command with +FULLRESYNC, with EOF marker:\r\n```\r\n127.0.0.1:6379> REPLCONF CAPA EOF\r\nOK\r\n127.0.0.1:6379> psync dummy 0\r\nEntering replica output mode...  (press Ctrl-C to quit)\r\nPSYNC replied +FULLRESYNC 188d44f21759cf12551e0fd36a696bab81c0d794 206\r\nFull resync with master, discarding bytes of bulk transfer until EOF marker...\r\nFull resync done after 234 bytes. Logging commands from master.\r\nsending REPLCONF ACK 0\r\n\"ping\"\r\n\"SELECT\",\"1\"\r\n\"incr\",\"counter\"\r\n```\r\n\r\nPSYNC command with +CONTINUE:\r\n```\r\n127.0.0.1:6379> psync 188d44f21759cf12551e0fd36a696bab81c0d794 206\r\nEntering replica output mode...  (press Ctrl-C to quit)\r\nPSYNC replied +CONTINUE\r\nPartial resync with master...\r\nPartial resync done. Logging commands from master.\r\n\"SELECT\"\r\n\"0\"\r\n\"incr\",\"counter\"\r\n\"ping\"\r\n\"ping\"\r\n\"SELECT\",\"1\"\r\n\"incr\",\"counter\"\r\n```\r\n\r\nPYSNC COMMAND with +CONTINUE replid:\r\n```\r\n127.0.0.1:6379> REPLCONF CAPA PSYNC2\r\nOK\r\n127.0.0.1:6379> psync 188d44f21759cf12551e0fd36a696bab81c0d794 206\r\nEntering replica output mode...  (press Ctrl-C to quit)\r\nPSYNC replied +CONTINUE 188d44f21759cf12551e0fd36a696bab81c0d794\r\nPartial resync with master...\r\nPartial resync done. Logging commands from master.\r\n\"SELECT\"\r\n\"0\"\r\n\"incr\",\"counter\"\r\n\"ping\"\r\n\"ping\"\r\n\"SELECT\",\"1\"\r\n\"incr\",\"counter\"\r\n```\r\n\r\nFixes #11246",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-12-20T10:14:07Z",
        "closed_at": "2022-12-21T06:04:32Z",
        "merged_at": null,
        "body": "This is to fix issue https://github.com/redis/redis/issues/11246 .\r\n\r\n`redis-cli` parses the response of `SYNC` to get payload and drop the payload. When `PSYNC` is used, the parsing cannot work and error is raised. \r\n\r\nThis patch makes redis-cli ignores the buffered `PSYNC` command and use `SYNC` instead. ",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2022-12-19T10:56:50Z",
        "closed_at": "2023-01-04T08:56:09Z",
        "merged_at": "2023-01-04T08:56:09Z",
        "body": "Test on ARM + TLS often fail with this error:\r\n```\r\n*** [err]: Slave is able to detect timeout during handshake in tests/integration/replication.tcl\r\nReplica is not able to detect timeout\r\n```\r\nhttps://github.com/redis/redis-extra-ci/actions/runs/3727554226/jobs/6321797837\r\n\r\nThe replica logs show that in this case the replica got timeout before even getting a response to the PING command (instead of the SYNC command).\r\n\r\nit should have shown these:\r\n```\r\n* MASTER <-> REPLICA sync started\r\n* REPLICAOF 127.0.0.1:22112 enabled ....\r\n### Starting test Slave enters handshake in tests/integration/replication.tcl\r\n* Non blocking connect for SYNC fired the event.\r\n```\r\nthen:\r\n```\r\n* Master replied to PING, replication can continue...\r\n* Trying a partial resynchronization (request 50da9eff70d774f4e6cb723eb4b091440f215772:1).\r\n```\r\nand then hang for 5 seconds:\r\n```\r\n# Timeout connecting to the MASTER...\r\n* Reconnecting to MASTER 127.0.0.1:21112 after failure\r\n```\r\n\r\nbut instead it got this (looks like it disconnected too early, and then tried to re-connect):\r\n```\r\n10890:M 19 Dec 2022 01:32:54.794 * Ready to accept connections tls\r\n10890:M 19 Dec 2022 01:32:54.809 - Accepted 127.0.0.1:41047\r\n10890:M 19 Dec 2022 01:32:54.878 - Reading from client: error:0A000126:SSL routines::unexpected eof while reading\r\n10890:M 19 Dec 2022 01:32:54.925 - Accepted 127.0.0.1:39207\r\n10890:S 19 Dec 2022 01:32:55.463 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.\r\n10890:S 19 Dec 2022 01:32:55.463 * Connecting to MASTER 127.0.0.1:24126\r\n10890:S 19 Dec 2022 01:32:55.463 * MASTER <-> REPLICA sync started\r\n10890:S 19 Dec 2022 01:32:55.463 * REPLICAOF 127.0.0.1:24126 enabled (user request from 'id=4 addr=127.0.0.1:39207 laddr=127.0.0.1:24125 fd=8 name= age=1 idle=0 flags=N db=9 sub=0 psub=0 ssub=0 multi=-1 qbuf=43 qbuf-free=20431 argv-mem=21 multi-mem=0 rbs=1024 rbp=5 obl=0 oll=0 omem=0 tot-mem=22317 events=r cmd=slaveof user=default redir=-1 resp=2')\r\n### Starting test Slave enters handshake in tests/integration/replication.tcl\r\n10890:S 19 Dec 2022 01:32:55.476 * Non blocking connect for SYNC fired the event.\r\n10890:S 19 Dec 2022 01:33:00.701 # Failed to read response from the server: (null)         <- note this!!\r\n10890:S 19 Dec 2022 01:33:00.701 # Master did not respond to command during SYNC handshake\r\n10890:S 19 Dec 2022 01:33:01.002 * Connecting to MASTER 127.0.0.1:24126\r\n10890:S 19 Dec 2022 01:33:01.002 * MASTER <-> REPLICA sync started\r\n### Starting test Slave is able to detect timeout during handshake in tests/integration/replication.tcl\r\n10890:S 19 Dec 2022 01:33:05.497 * Non blocking connect for SYNC fired the event.\r\n10890:S 19 Dec 2022 01:33:05.500 * Master replied to PING, replication can continue...\r\n10890:S 19 Dec 2022 01:33:05.510 * Trying a partial resynchronization (request 947e1956372a0e6c819cfec51c42cc7979b0c221:1).\r\n10890:S 19 Dec 2022 01:34:05.833 # Failed to read response from the server: error:0A000126:SSL routines::unexpected eof while reading\r\n10890:S 19 Dec 2022 01:34:05.833 # Master did not reply to PSYNC, will try later\r\n```\r\n\r\nThis PR sets enables the 5 seconds timeout at a later stage to try and prevent the early disconnection.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-12-18T10:28:55Z",
        "closed_at": "2022-12-18T15:07:46Z",
        "merged_at": "2022-12-18T15:07:46Z",
        "body": "Fix a flaky test that probably fails on overload timing issues.\r\n\r\nThis unit starts with\r\n```\r\n    # Set a threshold high enough to avoid spurious latency events.\r\n    r config set latency-monitor-threshold 200\r\n```\r\n\r\nbut later the test measuring expire event changes the threshold.\r\nthis fix is to revert it to 200 after that test.\r\n\r\nGot this error (ARM+TLS)\r\n```\r\n*** [err]: LATENCY RESET is able to reset events in tests/unit/latency-monitor.tcl\r\nExpected [r latency latest] eq {} (context: type eval line 3 cmd {assert {[r latency latest] eq {}}} proc ::test)\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-12-18T04:48:54Z",
        "closed_at": "2022-12-23T01:37:00Z",
        "merged_at": "2022-12-23T01:37:00Z",
        "body": "Attempt to harden cluster meeting in cluster tests to avoid issues like https://github.com/redis/redis/actions/runs/3708155803/jobs/6285405227.\r\n```\r\nTesting unit: 00-base.tcl\r\n23:20:35> Cluster Join and auto-discovery test: FAILED: Cluster failed to join into a full mesh.\r\n```\r\n\r\nThe best guess I have for the failures is that the cluster meet is timing out. The cluster meet is capped at the node timeout, in this case 3 seconds, which is usually sufficient for a single meet. However, within the span of a couple of seconds we are establishing 760 connections. (20 nodes * 19 partners * 2 links). All of the cluster meets are sent near together, so most of these are happening in a short time span. My guess is that under some scenarios this is stalling the connects and if one initial connection times out, it will never finish.\r\n\r\nTo try to harden it, this PR does 2 things:\r\n1. Retry up to 3 times to join the cluster. Cluster meet is entirely idempotent, so it should stabilize if we missed a node.\r\n2. Validate the connection is actually established, not just exists in the cluster list. Nodes can exist in handshake, but might later get dropped. \r\n\r\nI did some validation dropping the node timeout to 50ms, which was enough to break it before but now it was still able to meet (albeit it's very slow). \r\n\r\nhttps://github.com/redis/redis/actions/runs/3723202714\r\nhttps://github.com/redis/redis/actions/runs/3723202741\r\nhttps://github.com/redis/redis/actions/runs/3723202766",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-12-18T02:14:13Z",
        "closed_at": "2022-12-18T03:36:43Z",
        "merged_at": null,
        "body": "Attempt to harden cluster meeting in cluster tests to avoid issues like https://github.com/redis/redis/actions/runs/3708155803/jobs/6285405227.\r\n\r\nThe best guess I have for the failures is that the cluster meet is timing out. The cluster meet is capped at the node timeout, in this case 3 seconds, which is usually sufficient for a single meet. However, within the span of a couple of seconds we are establishing 760 connections. (20 nodes * 19 partners * 2 links). All of the cluster meets are sent near together, so most of these are happening in a short time span. My guess is that under some scenarios this is stalling the connects and if one initial connection times out, it will never finish.\r\n\r\nTo try to harden it, this PR does 2 things:\r\n1. Validate each node is able to join before progressing, instead of at the end. This limits the number of ongoing connections being established and puts a little bit of back pressure if it's slow.\r\n2. Validate the connection is actually established, not just exists in the cluster list. Nodes can exist in handshake, but might later get dropped. \r\n\r\nI did some validation dropping the node timeout to 50ms, which was enough to break it before but now it was still able to meet (albeit it's very slow). ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 9,
        "changed_files": 4,
        "created_at": "2022-12-15T21:38:40Z",
        "closed_at": "2022-12-16T10:52:58Z",
        "merged_at": "2022-12-16T10:52:58Z",
        "body": "Upgrade urgency: MODERATE, Contains fix for a regression in Geo commands.\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Fix regression from Redis 7.0.6 in distance replies of Geo commands (#11631)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2022-12-15T18:24:58Z",
        "closed_at": "2022-12-15T20:25:39Z",
        "merged_at": "2022-12-15T20:25:39Z",
        "body": "Fixes #11628, a regression introduced by #11552 in 7.0.6.\r\nit causes replies in the GEO commands to contain garbage when the result is a very small distance (less than 1)\r\nIncludes test to confirm indeed with junk in buffer now we properly reply \r\n\r\nConfirmed on unstable the added test fails:\r\n```\r\ntclsh tests/test_helper.tcl --single unit/geo\r\n(...)\r\n[err]: GEOSEARCH with small distance in tests/unit/geo.tcl\r\nExpected '{1 0.0001} {2 9.8182}' to be equal to '{1 0.00v1} {2 9.8182}' (context: type eval line 5 cmd {assert_equal {{1 0.0001} {2 9.8182}} [r GEORADIUS points -122.407107 37.794300 30 mi ASC WITHDIST]} proc ::test)\r\n```\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2022-12-15T12:46:47Z",
        "closed_at": "2022-12-18T15:14:15Z",
        "merged_at": "2022-12-18T15:14:15Z",
        "body": "I've seen the `BRPOPLPUSH with multiple blocked clients` test hang.\r\nthis probably happened because rd2 blocked before rd1 and then it was also released first, and rd1 remained blocked.\r\n\r\n```\r\n        r del blist{t} target1{t} target2{t}\r\n        r set target1{t} nolist\r\n        $rd1 brpoplpush blist{t} target1{t} 0\r\n        $rd2 brpoplpush blist{t} target2{t} 0\r\n        r lpush blist{t} foo\r\n\r\n        assert_error \"WRONGTYPE*\" {$rd1 read}\r\n        assert_equal {foo} [$rd2 read]\r\n        assert_equal {foo} [r lrange target2{t} 0 -1]\r\n```\r\nchanges:\r\n* added all missing calls for wait_for_blocked_client after issuing blocking commands)\r\n* removed some excessive `after 100`\r\n* fix undetected crossslot error in BRPOPLPUSH test\r\n* rollback changes to proto-max-bulk-len so external tests can be rerun\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 19,
        "changed_files": 6,
        "created_at": "2022-12-15T09:59:36Z",
        "closed_at": "2023-05-24T06:40:12Z",
        "merged_at": "2023-05-24T06:40:12Z",
        "body": "This pr can get two performance benefits:\r\n1. Stop redundant initialization when most robj objects are created\r\n2. LRU_CLOCK will no longer be called in io threads, so we can avoid the `atomicGet`\r\n\r\nAnother code optimization:\r\ndeleted the redundant judgment in [dbSetValuea](https://github.com/redis/redis/blob/df327b8bd56023931cd41e233f8703de7bbaa82c/src/db.c#L233), no matter in LFU or LRU, the lru field in old robj is always the freshest (it is always updated in lookupkey), so we don't need to judge if in LFU\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-12-15T07:25:58Z",
        "closed_at": "2022-12-16T18:02:09Z",
        "merged_at": "2022-12-16T18:02:09Z",
        "body": "seems like it was dead since forever.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2022-12-14T20:52:10Z",
        "closed_at": "2023-01-05T00:09:28Z",
        "merged_at": "2023-01-05T00:09:27Z",
        "body": "The logged errors include these on the same format as in CLIENT INFO, e.g. \"addr=127.0.0.1:12345 laddr=127.0.0.1:6379\".\r\n\r\nIssue: #11620",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2022-12-13T18:39:43Z",
        "closed_at": "2023-05-26T00:08:32Z",
        "merged_at": null,
        "body": "I changed http to https, is that correct?",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-12-13T05:14:54Z",
        "closed_at": "2022-12-15T08:26:18Z",
        "merged_at": "2022-12-15T08:26:18Z",
        "body": "\r\nMake sure to flush also_propagate after expiring from expireSlaveKeys to avoid an assertion in beforeSleep.\r\n\r\nNote that this is an edge case that only happens in case volatile keys were created directly on a writable replica, and that anyway nothing is propagated to sub-replicas.\r\n\r\nFor Redis 7.2, We need to honor the post-execution-unit API and call it after each KSN.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-12-12T17:18:04Z",
        "closed_at": "2022-12-13T05:59:43Z",
        "merged_at": "2022-12-13T05:59:43Z",
        "body": "There is a race in the test:\r\n```\r\n*** [err]: Diskless load swapdb (async_loading): new database is exposed after swapping in tests/integration/replication.tcl\r\nExpected 'myvalue' to be equal to '' (context: type eval line 3 cmd {assert_equal [$replica GET mykey] \"\"} proc ::test)\r\n```\r\n\r\nWhen doing `$replica GET mykey`, the replica is using the old database.\r\nThe reason may be that when doing `master client kill type replica`,\r\nthe replica did not yet realize it got disconnected from the master.\r\nSo the check of master_link_status fails, and the replica did not\r\nfinish the swapdb and the loading.\r\n\r\nIn that case, i think the solution is to check the sync_full stat on\r\nthe master and wait for it to get incremented from the previous value.\r\ni.e. the way to know that we're done with the full sync is not to check\r\nthat our state is up (could be up if we check too early), but rather\r\ncheck that the sync_full counter got incremented.\r\n\r\nDuring the reviewing, we found another race, in Aborted testType,\r\nthe `$master config set rdb-key-save-delay 10000` is done after we\r\nalready initiated the disconnection, so there's a chance that the replica\r\nwill attempt to reconnect before that call, in which case if we fork() before\r\nit, the config will not take effect. Move it to above the disconnection.\r\n\r\nThe two fix were suggested by oranagra.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-12-12T07:10:44Z",
        "closed_at": "2022-12-12T16:10:48Z",
        "merged_at": "2022-12-12T16:10:48Z",
        "body": "attach_to_replication_stream already stops pings, but it stops them on the server we connect to, and in this case it's a replica, and we need to stop them on the real master.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 391,
        "deletions": 91,
        "changed_files": 35,
        "created_at": "2022-12-11T07:27:35Z",
        "closed_at": "2022-12-12T15:02:55Z",
        "merged_at": "2022-12-12T15:02:55Z",
        "body": "Upgrade urgency: MODERATE, Contains fixes for a few non-critical or unlikely bugs\r\n\r\nPerformance and resource utilization improvements\r\n=================================================\r\n\r\n* Optimize zset conversion on large ZRANGESTORE (#10789)\r\n\r\nModule API changes\r\n==================\r\n\r\n* Fix crash in CLIENT_CHANGE event, when the selected database is not 0 (#11500)\r\n* Fix RM_SetAbsExpire and RM_GetAbsExpire API registration (#11025, #8564)\r\n\r\nSecurity improvements\r\n=====================\r\n\r\n* Sentinel: avoid logging auth-pass value (#9652)\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Fix a crash when a Lua script returns a meta-table (#11032)\r\n* Fix ZRANGESTORE crash when zset_max_listpack_entries is 0 (#10767)\r\n* Unpause clients after manual failover ends instead of waiting for timed (#9676)\r\n* TLS: Notify clients on connection shutdown (#10931)\r\n* Avoid hang of diskless replication fork child when parent crashes (#11463)\r\n* Fix sentinel function that compares hostnames if failed resolve (#11419)\r\n* Fix a hang when eviction is combined with lazy-free and maxmemory-eviction-tenacity\r\n  is set to 100 (#11237)\r\n* Fix bug with scripts ignoring client tracking NOLOOP (#11052)\r\n* Fix client-side tracking breaking protocol when FLUSHDB / FLUSHALL / SWAPDB is\r\n  used inside MULTI-EXEC (#11038)\r\n* Fix BITFIELD overflow detection on some compilers due to undefined behavior (#9601)\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1701,
        "deletions": 420,
        "changed_files": 60,
        "created_at": "2022-12-11T07:12:10Z",
        "closed_at": "2022-12-12T15:36:35Z",
        "merged_at": "2022-12-12T15:36:35Z",
        "body": "Upgrade urgency: MODERATE, Contains fixes for a few non-critical or unlikely bugs,\r\nand some dramatic optimizations to Geo, EVAL, and Sorted sets commands.\r\n\r\nPotentially Breaking Bug Fixes for new Redis 7.0 features\r\n=======================================================\r\n\r\n* RM_ResetDataset module API should not clear the functions (#11268)\r\n* RM_Call module API used with the \"C\" flag to run scripts, would now cause\r\n  the commands in the script to check ACL with the designated user (#10966)\r\n\r\nPerformance and resource utilization improvements\r\n=================================================\r\n\r\n* Geo commands speedups (#11535, #11522, #11552, #11579)\r\n* Fix EVAL command performance regression from Redis 7.0 (#11521, #11541)\r\n* Reduce EXPIRE commands performance regression from Redis 7.0 (#11602)\r\n* Optimize commands returning double values, mainly affecting zset commands (#11093)\r\n* Optimize Lua parsing of some command responses (#11556)\r\n* Optimize client memory usage tracking operation while client eviction is disabled (#11348)\r\n\r\nPlatform / toolchain support related improvements\r\n=================================================\r\n\r\n* Fix compilation on Solaris (#11327)\r\n\r\nModule API changes\r\n==================\r\n\r\n* RM_SetContextUser, RM_SetModuleUserACLString, RM_GetModuleUserACLString (#10966)\r\n* Fix crash in CLIENT_CHANGE event, when the selected database is not 0 (#11500)\r\n\r\nChanges in CLI tools\r\n====================\r\n\r\n* redis-benchmark avoid aborting on NOPERM from CONFIG GET (#11096)\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Avoid hang of diskless replication fork child when parent crashes (#11463)\r\n* Fix crash with module API of list iterator and RM_ListDelete (#11383)\r\n* Fix TLS error handling to avoid connection drops on timeouts (#11563)\r\n* Fix runtime changes to cluster-announce-*-port to take effect on the local node too (#10745)\r\n* Fix sentinel function that compares hostnames if failed resolve (#11419)\r\n* Fix MIGRATE with AUTH set to \"keys\" is getting wrong key names leading to MOVED or ACL errors (#11253)\r\n\r\nFixes for issues in previous releases of Redis 7.0\r\n--------------------------------------------------\r\n\r\n* Fix command line startup --sentinel problem (#11591)\r\n* Fis missing FCALL commands in monitor (#11510)\r\n* Fix CLUSTER SHARDS showing empty hostname (#11297)\r\n* Replica that asks for rdb-only could have missed the EOF and hang (#11296)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-12-11T07:00:13Z",
        "closed_at": "2022-12-11T16:20:42Z",
        "merged_at": "2022-12-11T16:20:42Z",
        "body": "Clang Address Sanitizer tests started reporting unknown-crash on these tests due to the memcheck, disable the memcheck to avoid that noise.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-12-09T00:50:45Z",
        "closed_at": "2022-12-09T10:06:25Z",
        "merged_at": "2022-12-09T10:06:25Z",
        "body": "As being discussed in https://github.com/redis/redis/issues/10981#issuecomment-1341350192 there is overhead on 7.0 EXPIRE command that is not present on 6.2.7. \r\n\r\nWe can see that on the unstable profile there are around 7% of CPU cycles spent on rewriteClientCommandVector that are not present on 6.2.7. This was introduced in https://github.com/redis/redis/pull/8474.\r\nThis PR reduces the overhead by using 2X rewriteClientCommandArgument instead of rewriteClientCommandVector. In this scenario rewriteClientCommandVector creates 4 arguments. the above usage of rewriteClientCommandArgument reduces the overhead in half. \r\nThis PR should also improve PEXPIREAT performance by avoiding at all rewriteClientCommandArgument usage. \r\n\r\n6.2.7 profile:\r\n\ufeff\ufeff\r\n![image](https://user-images.githubusercontent.com/5832149/206596796-68e99132-8fa5-4d16-980e-ea787a838565.png)\r\n\r\n\r\n\r\nunstable profile:\r\n<img width=\"606\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5832149/206596744-d54ea201-c284-4b66-9fd7-aebf15648ca6.png\">\r\n\r\n\r\nIf we populate data as follows:\r\n```\r\nmemtier_benchmark --port 6379 --server localhost   --key-minimum=1 --key-maximum 100000 \"--data-size\" \"100\" \"--command\" \"HSET __key__ field __value__\" \"--command-key-pattern\" \"P\" \"-c\" \"50\" \"-t\" \"2\" \"--hide-histogram\"\r\n```\r\n\r\nAnd then benchmark:\r\n```\r\nmemtier_benchmark  --pipeline 10 --key-minimum=1 --key-maximum 100000 \"--data-size\" \"100\" --command \"EXPIRE __key__ 3600\" --command-key-pattern=\"R\" -c 50 -t 2 --hide-histogram --test-time 180\r\n```\r\n\r\nwe get the following results on the achievable ops/sec:\r\n\r\ntest | Version 6.2.7 | unstable branch 8th December 2022 ( 10e4f44dc280dd300009366466cd7d5971191e16 ) | % change unstable vs 6.2.7 | this PR | % change this PR vs 6.2.7\r\n-- | -- | -- | -- | -- | --\r\nEXPIRE ( pipeline 10 ) | 876843 | 770614 | -12.1% | 821241 | -6.3%\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-12-08T10:26:04Z",
        "closed_at": "2022-12-09T11:33:38Z",
        "merged_at": "2022-12-09T11:33:38Z",
        "body": "We do defrag during AOF loading, but aim to detect fragmentation only once a second, so this test aims to slow down the AOF loading and mimic loading of a large file.\r\nOn fast machines the sleep, plus the actual work we did was insufficient making it sleep longer so the test won't fail.\r\n\r\nThe error we used to get is this one:\r\nExpected 0 > 100000 (context: type eval line 106 cmd {assert {$hits > 100000}} proc ::test)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2022-12-08T09:32:48Z",
        "closed_at": "2022-12-08T17:29:31Z",
        "merged_at": "2022-12-08T17:29:31Z",
        "body": "From https://en.wikipedia.org/wiki/NaN#Display, it says\r\nthat apart from nan and -nan, we can also get NAN and even\r\nnan(char-sequence) from libc.\r\n\r\nIn #11482, our conclusion was that we wanna normalize it in\r\nRedis to a single nan type, like we already normalized inf.\r\n\r\nFor this, we also reverted the assert_match part of the test\r\nadded in #11506, using assert_equal to validate the changes.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 340,
        "deletions": 168,
        "changed_files": 6,
        "created_at": "2022-12-07T14:11:17Z",
        "closed_at": "2023-01-20T16:45:30Z",
        "merged_at": "2023-01-20T16:45:30Z",
        "body": "If a dict has only keys, and no use of values, then a key can be stored directly in a dict's hashtable. The key replaces the dictEntry. To distinguish between a key and a dictEntry, we only use this optimization if the key is odd, i.e. if the key has the least significant bit set. This is true for sds strings, since the sds header is always an odd number of bytes.\r\n\r\nDict entries are used as a fallback when there is a hash collision. A special dict entry without a value (only key and next) is used so we save one word in this case too.\r\n\r\nThis saves 24 bytes per set element for larges sets, and also gains some speed improvement as a side effect (less allocations and cache misses).\r\n\r\nA quick test adding 1M elements to a set using the command below resulted in memory usage of\r\n28.83M, compared to 46.29M on unstable. That's 18 bytes per set element on average.\r\n\r\n    eval 'for i=1,1000000,1 do redis.call(\"sadd\", \"myset\", \"x\"..i) end' 0\r\n\r\nOther changes:\r\n\r\nAllocations are ensured to have at least 8 bits alignment on all systems. This affects 32-bit builds compiled without HAVE_MALLOC_SIZE (not jemalloc or glibc) in which Redis stores the size of each allocation, after this change in 8 bytes instead of previously 4 bytes per allocation. This is done so we can reliably use the 3 least significant bits in a pointer to encode stuff.\r\n\r\nFixes #11154.",
        "comments": 48
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-12-07T13:03:51Z",
        "closed_at": "2023-01-03T07:37:48Z",
        "merged_at": "2023-01-03T07:37:48Z",
        "body": "This is a small addition to #9640\r\nIt improves performance by avoiding double lookup of the the key.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2022-12-06T08:30:59Z",
        "closed_at": "2022-12-08T17:14:22Z",
        "merged_at": "2022-12-08T17:14:22Z",
        "body": "As Sentinel support dynamic IP only when using hostnames, there\r\nare few leftover addess comparison logic that doesn't take into\r\naccount that the IP might get change. \r\n\r\nI did a limited fix in the past to same issue on another location but\r\nwas too lazy or wasn't sure to fully review the code for othe places.\r\nHope it will cover entirely this time. \r\n\r\nFixes issue: #9998 (read from [here](https://github.com/redis/redis/issues/9998#issuecomment-1298410625)).",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-12-05T15:03:20Z",
        "closed_at": "2022-12-06T08:33:52Z",
        "merged_at": null,
        "body": "Add client `pending_querybuf` len and free size to client info.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-12-05T10:15:42Z",
        "closed_at": "2023-02-02T14:58:17Z",
        "merged_at": "2023-02-02T14:58:16Z",
        "body": "Previously, jemalloc was explicitly configured to build in `gnu99` mode. As a result, `<stdatomic.h>` was presumed to be unavailable and never used.\r\n\r\nThis commit removes explicit build flags configuration and lets `autoconf` to determine the supported build flags. In addition, we also no longer build C++ jemalloc code.",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2022-12-05T10:00:49Z",
        "closed_at": "2022-12-13T07:46:15Z",
        "merged_at": null,
        "body": "When the length of slowlog list reaches server.slowlog_max_len, every time we add a new node to the list we have to delete the last node from the list. This means we have to frequently allcate memory for the new node and its slowlogEntry and at the same time delete the last node and its slowlogEntry.\r\n\r\nIn this PR, we unlink the last node from the list, update its slowlogEntry with new attributes and then link it back to the list head. The new slow command reuse the list node and slowlogEntry of the old command we trimmed, so we don't need the allocation and deletion every time.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 134,
        "deletions": 65,
        "changed_files": 10,
        "created_at": "2022-12-05T03:32:20Z",
        "closed_at": "2022-12-09T15:08:01Z",
        "merged_at": "2022-12-09T15:08:01Z",
        "body": "In #11290, we added listpack encoding for SET object.\r\nBut forgot to support it in zuiFind, causes ZINTER, ZINTERSTORE,\r\nZINTERCARD, ZIDFF, ZDIFFSTORE to crash.\r\nAnd forgot to support it in RM_ScanKey, causes it hang.\r\n\r\nThis PR add support SET listpack in zuiFind, and in RM_ScanKey.\r\nAnd add tests for related commands to cover this case.\r\n\r\nOther changes:\r\n- There is no reason for zuiFind to go into the internals of the SET.\r\n  It can simply use setTypeIsMember and don't care about encoding.\r\n- Remove the `#include \"intset.h\"` from server.h reduce the chance of\r\n  accidental intset API use.\r\n- Move setTypeAddAux, setTypeRemoveAux and setTypeIsMemberAux\r\n  interfaces to the header.\r\n- In scanGenericCommand, use setTypeInitIterator and setTypeNext\r\n  to handle OBJ_SET scan.\r\n- In RM_ScanKey, improve hash scan mode, use lpGetValue like zset,\r\n  they can share code and better performance.\r\n\r\nThe zuiFind part fixes #11578",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2022-12-05T01:21:18Z",
        "closed_at": "2022-12-05T13:45:04Z",
        "merged_at": "2022-12-05T13:45:04Z",
        "body": "This is take 2 of `GEOSEARCH BYBOX` optimizations based on haversine distance formula when longitude diff is 0. The first one was in https://github.com/redis/redis/pull/11535 . \r\n\r\n- Given longitude diff is 0 the asin(sqrt(a)) on the haversine is asin(sin(abs(u))).\r\n- arcsin(sin(x)) equal to x when x \u2208[\u2212\ud835\udf0b/2,\ud835\udf0b/2]. \r\n- Given latitude is between [\u2212\ud835\udf0b/2,\ud835\udf0b/2] we can simplifiy arcsin(sin(x)) to x.\r\n\r\nTo test it we can simply focus on the geo.tcl \r\n\r\n```\r\ntclsh tests/test_helper.tcl --single unit/geo\r\n```\r\n\r\nNotice that this logic is covered/tested in the \"GEOSEARCH box edges fuzzy test\" scenario. \r\n\r\n\r\n**On the sample dataset with 60M datapoints, we've measured 55% increase in the achievable ops/sec.**\r\n\r\nWe can benchmark this improvement using the following dataset with 60M datapoints on the GEO key. https://s3.us-east-2.amazonaws.com/redis.benchmarks.spec/datasets/geopoint/dump.rdb\r\n\r\n\r\nand command: \r\n```\r\nmemtier_benchmark -c 1 -t 1 --pipeline 1 --test-time 60 --command \"GEOSEARCH key FROMLONLAT 7 55 BYBOX 200 200 KM\" --hide-histogram\r\n```\r\n\r\n## unstable ( 61c85a2b2081dffaf45eb8c6b7754b8d9a80c60d )\r\n\r\n```\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nGeosearchs        32.90        30.38723        30.20700        34.04700        35.32700      4922.83 \r\nTotals            32.90        30.38723        30.20700        34.04700        35.32700      4922.83 \r\n```\r\n\r\n\r\n## This PR\r\n\r\n```\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nGeosearchs        51.20        19.51808        19.45500        22.01500        23.03900      7662.12 \r\nTotals            51.20        19.51808        19.45500        22.01500        23.03900      7662.12 \r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 116,
        "deletions": 110,
        "changed_files": 11,
        "created_at": "2022-12-02T12:20:13Z",
        "closed_at": "2022-12-20T07:51:50Z",
        "merged_at": "2022-12-20T07:51:50Z",
        "body": "1. Get rid of server.core_propagates - we can just rely on module/call nesting levels\r\n2. Rename in_nested_call  to execution_nesting and update the comment\r\n3. Remove module_ctx_nesting (redundant, we can use execution_nesting)\r\n4. Modify postExecutionUnitOperations according to the comment (The main purpose of this PR)\r\n5. trackingHandlePendingKeyInvalidations: Check the nesting level inside this function\r\n\r\nfixes #11536",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1025,
        "deletions": 105,
        "changed_files": 21,
        "created_at": "2022-12-01T16:40:33Z",
        "closed_at": "2023-03-16T12:04:32Z",
        "merged_at": "2023-03-16T12:04:32Z",
        "body": "Fix #7992, allow running blocking commands from within a module using `RM_Call`.\r\n\r\nToday, when `RM_Call` is used, the fake client that is used to run command is marked with `CLIENT_DENY_BLOCKING` flag. This flag tells the command that it is not allowed to block the client and in case it needs to block, it must fallback to some alternative (either return error or perform some default behaviour). For example, `BLPOP` fallback to simple `LPOP` if it is not allowed to block.\r\n\r\nAll the commands must respect the `CLIENT_DENY_BLOCKING` flag (including module commands). When the command invocation finished, Redis asserts that the client was not blocked.\r\n\r\nThis PR introduces the ability to call blocking command using `RM_Call` by passing a callback that will be called when the client will get unblocked. In order to do that, the user must explicitly say that he allow to perform blocking command by passing a new format specifier argument, `K`, to the `RM_Call` function. This new flag will tell Redis that it is allow to run blocking command and block the client. In case the command got blocked, Redis will return a new type of call reply (`REDISMODULE_REPLY_PROMISE`). This call reply indicates that the command got blocked and the user can set the on_unblocked handler using `RM_CallReplyPromiseSetUnblockHandler`.\r\n\r\nWhen clients gets unblocked, it eventually reaches `processUnblockedClients` function. This is where we check if the client is a fake module client and if it is, we call the unblock callback instead of performing the usual unblock operations.\r\n\r\n**Notice**: `RM_CallReplyPromiseSetUnblockHandler` must be called atomically along side the command invocation (without releasing the Redis lock in between). In addition, unlike other CallReply types, the promise call reply must be released by the module when the Redis GIL is acquired.\r\n\r\nThe module can abort the execution on the blocking command (if it was not yet executed) using `RM_CallReplyPromiseAbort`. the API will return `REDISMODULE_OK` on success and `REDISMODULE_ERR` if the operation is already executed. **Notice** that in case of misbehave module, Abort might finished successfully but the operation will not really be aborted. This can only happened if the module do not respect the disconnect callback of the blocked client. \r\nFor pure Redis commands this can not happened.\r\n\r\n### Atomicity Guarantees\r\n\r\nThe API promise that the unblock handler will run atomically as an execution unit. This means that all the operation performed on the unblock handler will be wrapped with a multi exec transaction when replicated to the replica and AOF. The API **do not** grantee any other atomicity properties such as when the unblock handler will be called. This gives us the flexibility to strengthen the grantees (or not) in the future if we will decide that we need a better guarantees.\r\n\r\nThat said, the implementation **does** provide a better guarantees when performing pure Redis blocking command like `BLPOP`. In this case the unblock handler will run atomically with the operation that got unblocked (for example, in case of `BLPOP`, the unblock handler will run atomically with the `LPOP` operation that run when the command got unblocked). This is an implementation detail that might be change in the future and the module writer should not count on that.\r\n\r\n### Calling blocking commands while running on script mode (`S`)\r\n\r\n`RM_Call` script mode (`S`) was introduce on https://github.com/redis/redis/pull/10372. It is used for usecases where the command that was invoked on `RM_Call` comes from a user input and we want to make sure the user will not run dangerous commands like `shutdown`. Some command, such as `BLPOP`, are marked with `NO_SCRIPT` flag, which means they will not be allowed on script mode. Those commands are marked with  `NO_SCRIPT` just because they are blocking commands and not because they are dangerous. Now that we can run blocking commands on RM_Call, there is no real reason not to allow such commands on script mode.\r\n\r\nThe underline problem is that the `NO_SCRIPT` flag is abused to also mark some of the blocking commands (notice that those commands know not to block the client if it is not allowed to do so, and have a fallback logic to such cases. So even if those commands were not marked with `NO_SCRIPT` flag, it would not harm Redis, and today we can already run those commands within multi exec).\r\n\r\nIn addition, not all blocking commands are marked with `NO_SCRIPT` flag, for example `blmpop` are not marked and can run from within a script.\r\n\r\nThose facts shows that there are some ambiguity about the meaning of the `NO_SCRIPT` flag, and its not fully clear where it should be use.\r\n\r\nThe PR suggest that blocking commands should not be marked with `NO_SCRIPT` flag, those commands should handle `CLIENT_DENY_BLOCKING` flag and only block when it's safe (like they already does today). To achieve that, the PR removes the `NO_SCRIPT` flag from the following commands:\r\n* `blmove`\r\n* `blpop`\r\n* `brpop`\r\n* `brpoplpush`\r\n* `bzpopmax`\r\n* `bzpopmin`\r\n* `wait`\r\n\r\nThis might be considered a breaking change as now, on scripts, instead of getting `command is not allowed from script` error, the user will get some fallback behaviour base on the command implementation. That said, the change matches the behaviour of scripts and multi exec with respect to those commands and allow running them on `RM_Call` even when script mode is used.\r\n\r\n### Additional RedisModule API and changes\r\n\r\n* `RM_BlockClientSetPrivateData` - Set private data on the blocked client without the need to unblock the client. This allows up to set the promise CallReply as the private data of the blocked client and abort it if the client gets disconnected.\r\n* `RM_BlockClientGetPrivateData` - Return the current private data set on a blocked client. We need it so we will have access to this private data on the disconnect callback.\r\n* On RM_Call, the returned reply will be added to the auto memory context only if auto memory is enabled, this allows us to keep the call reply for longer time then the context lifetime and does not force an unneeded borrow relationship between the CallReply and the RedisModuleContext.\r\n\r\n### TODO\r\n\r\n- [x] Accept/Decline the suggest API (using format specifier argument `K`) - API was changed and top comment was updated.\r\n- [x] Decide if `processUnblockedClients` is the right place to call the unblock callback\r\n- [x] Decide if the changes to `BLPOP` like commands are acceptable or is it a breaking change.\r\n- [x] Test role change while having a blocked RM_Call.\r\n- [x] replication tests\r\n- [x] key space notification tests",
        "comments": 22
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-12-01T03:28:32Z",
        "closed_at": "2022-12-01T07:11:33Z",
        "merged_at": "2022-12-01T07:11:33Z",
        "body": "The test failed with `ERR DUMP payload version or checksum are wrong.`\r\nAnd it only fails on CentOS, this is due to the fact that tcl8.5 does not correctly\r\nparse the hexadecimal abbreviation. And in Ubuntu we are using tcl8.6.\r\n\r\nPayload regenerated using string2printable, introduced in #11099",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 63,
        "deletions": 27,
        "changed_files": 5,
        "created_at": "2022-11-30T18:53:23Z",
        "closed_at": "2022-12-06T09:25:51Z",
        "merged_at": "2022-12-06T09:25:51Z",
        "body": "In most cases when a listpack or intset is converted to a dict, the conversion is trigged when adding an element. The extra element is added after conversion to dict (in all cases except when the conversion is triggered by set-max-intset-entries being reached).\r\n\r\nIf set-max-listpack-entries is set to a power of two, let's say 128, when adding the 129th element, the 128 element listpack is first converted to a dict with a hashtable presized for 128 elements. After converting to dict, the 129th element is added to the dict which immediately triggers incremental rehashing to size 256.\r\n\r\nThis commit instead presizes the dict to one more element, with the assumtion that conversion to dict is followed by adding another element, so the dict doesn't immediately need rehashing.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 57,
        "deletions": 64,
        "changed_files": 2,
        "created_at": "2022-11-30T15:52:36Z",
        "closed_at": "2022-11-30T20:23:00Z",
        "merged_at": "2022-11-30T20:23:00Z",
        "body": "* Remove duplicate code, propagating SSL errors into the connection state.\r\n* Add missing error handling in synchronous IO functions.\r\n* Fix connection error reporting in some replication flows.\r\n\r\nThe background for this work is CI failures on arm64, such as this one:\r\nhttps://github.com/redis/redis-extra-ci/actions/runs/3537044170/jobs/5936665499\r\n\r\nThe test fails as it expects the replica to hit the timeout, but instead, the connection drops and reconnects later. The replica reports connection errors:\r\n\r\n```\r\n10812:S 24 Nov 2022 01:58:00.846 # Failed to read response from the server: Resource temporarily unavailable\r\n10812:S 24 Nov 2022 01:58:00.846 # Master did not respond to command during SYNC handshake\r\n```\r\n\r\nThese errors are potentially misleading because they're derived from `errno` and not from the underlying connection, which, in turn, needs to consult OpenSSL.\r\n\r\nThis PR may fix the root cause or provide more accurate error logging to help track the underlying problem.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-11-29T16:15:36Z",
        "closed_at": "2022-11-30T20:08:13Z",
        "merged_at": "2022-11-30T20:08:12Z",
        "body": "profiling EVALSHA after the merge of https://github.com/redis/redis/pull/11521 ( commit 7dfd7b9197bbe216912049eebecbda3f1684925e ) we can see that luaReplyToRedisReply takes 8.73% out of the 56.90% of luaCallFunction CPU cycles. The new approach drops  luaReplyToRedisReply CPU cycles to 3.77%                                                                                                                                           \r\n\r\n![image](https://user-images.githubusercontent.com/5832149/204578761-105174db-a7c4-4f05-a7b8-6b8f41bf8660.png)\r\n\r\nusing addReplyStatusLength instead of directly composing the protocol to avoid sdscatprintf and addReplySds ( which imply multiple sdslen calls ) we get the following improvement on the overall ops/sec:\r\n\r\n```\r\nredis-cli SCRIPT load \"redis.call('hset', 'h1', 'k', 'v');redis.call('hset', 'h2', 'k', 'v');return redis.call('ping')\"\r\nmemtier_benchmark --command=\"EVALSHA 7cecb99fd9091d8e66d5cccf8979cf3aec5c4951 0\" --hide-histogram --test-time 60 --pipeline 10 -x 3\r\nmemtier_benchmark --command=\"eval \\\"redis.call('hset', 'h1', 'k', 'v');redis.call('hset', 'h2', 'k', 'v');return redis.call('ping')\\\" 0\"  --hide-histogram --test-time 60 --pipeline 10 -x 3\r\n```\r\n\r\n\r\nCOMMAND | 6.2.6 | unstable ( 7dfd7b9197bbe216912049eebecbda3f1684925e ) | this PR ( 20403fa0634b6a4975f98cdd61587012a23ab657 ) | % Improvement vs unstable | % Drop vs 6.2.6\r\n-- | -- | -- | -- | -- | --\r\nEVAL | 223744 | 187298 | 198282 | 5.86% | -11.38%\r\nEVALSHA | 264411 | 207848 | 221750 | 6.69% | -16.13%\r\n\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-11-29T09:55:33Z",
        "closed_at": "2023-01-05T00:26:46Z",
        "merged_at": "2023-01-05T00:26:46Z",
        "body": "In cluster-mode, only DB0 is supported so all data must reside in that database. There is a single check that validates that data loaded from an RDB all resides in DB0. This check is preformed after all the data is loaded which makes it difficult to identify where the non DB0 data resides as well as does a bunch of unnecessary work to load incompatible data. This change override the database config at startup to 1 to throw an error when attempting to add data to a database other than DB0. ",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-11-28T18:59:39Z",
        "closed_at": "2022-11-30T20:03:24Z",
        "merged_at": "2022-11-30T20:03:23Z",
        "body": "This test sets the master ping interval to 1 hour, in order to avoid pings in the replicatoin stream incrementing the replication offset, however, it didn't increase the repl-timeout so on slow machines where the test took more than 60 seconds, the replicas would drop and reconnect.\r\n\r\n```\r\n*** [err]: PSYNC2: Partial resync after restart using RDB aux fields in tests/integration/psync2.tcl\r\nReplica didn't partial sync\r\n```\r\nThe test would detect 4 additional partial syncs where it expects only one.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 167,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2022-11-28T14:46:41Z",
        "closed_at": "2022-12-04T08:11:39Z",
        "merged_at": "2022-12-04T08:11:39Z",
        "body": "GEODIST used snprintf(\"%.4f\") for the reply using addReplyDoubleDistance, which was slow.\r\nThis PR optimizes it without breaking compatibility by following the approach of ll2string with some changes to match the use case of distance and precision.\r\nI.e. we multiply it by 10000 format it as an integer, and then add a decimal point.\r\nThis can achieve about 35% increase in the achievable ops/sec. \r\n\r\nDetails\r\n=====\r\n\r\nOut of the 36.44% geodistCommand CPU cycles we can pinpoint 26.02%  to addReplyDoubleDistance:\r\n\r\n![image](https://user-images.githubusercontent.com/5832149/204306393-9ce6601b-f4dd-49c9-8ea6-f2fcd2b88132.png)\r\n\r\nTo functionally test it:\r\n\r\n```\r\ntclsh tests/test_helper.tcl --single unit/geo\r\n```\r\n\r\nTo benchmark:\r\nUse the following RDB with 60M datapoints on the GEO key. https://s3.us-east-2.amazonaws.com/redis.benchmarks.spec/datasets/geopoint/dump.rdb\r\n\r\n```\r\nmemtier_benchmark --pipeline 10 --test-time 60 --command \"GEODIST key 1 2\" --hide-histogram\r\n```\r\n\r\n\r\n## unstable branch ( 155acef51ac7826ed294a4b61d891f1c7a9a40ac ) \r\n```\r\nALL STATS\r\n===================================================================================================\r\nType          Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n---------------------------------------------------------------------------------------------------\r\nGeodists    642577.89         3.11090         3.05500         6.07900         6.33500     33258.43 \r\nTotals      642577.89         3.11090         3.05500         6.07900         6.33500     33258.43 \r\n```\r\n\r\n## this PR (c7bb2680691e0fec7f03d6c88b558fc4f2622c0b)\r\n\r\nApproximately 35% increase in the achievable ops/sec. \r\n\r\n```\r\nALL STATS\r\n===================================================================================================\r\nType          Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n---------------------------------------------------------------------------------------------------\r\nGeodists    868741.45         2.30079         2.25500         4.51100         4.67100     44964.16 \r\nTotals      868741.45         2.30079         2.25500         4.51100         4.67100     44964.16 \r\n```",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 94,
        "deletions": 109,
        "changed_files": 4,
        "created_at": "2022-11-27T10:43:26Z",
        "closed_at": "2023-07-31T06:38:15Z",
        "merged_at": null,
        "body": "`ZRANGE BYSCORE/BYLEX` with `LIMIT offset count` option would use every level in skiplist to jump to the first/last node in range, but only use `level[0]` in skiplist to locate the node at `offset`:\r\n```\r\nln = ln->level[0].forward;\r\n```\r\nIt could be slow when `offset` is very big. We can also use higher levels in skiplist to jump to the node at `offset`.\r\n\r\n==========[2022-12-12 Update]==========\r\n\r\nI did a simple test using a modified redis benchmark. The test key is generated by the command below, which is a zset having 9000+ random members, and their scores are limited in [1,100000].\r\n```\r\n./redis -p 8 -c 10000 -r 100000 -r2 100000 -command \"zadd test __RAND__ val__RAND2__\"\r\n```\r\nTesting the command below:\r\n```\r\nzrange test 0 +inf byscore limit <offset> 1\r\n```\r\n```\r\nOptimized code:\r\noffset=2000: Qps Max: 69281 Min: 64557 Mean: 67316\r\noffset=5000: Qps Max: 70569 Min: 66916 Mean: 68537\r\n```\r\n```\r\nOriginal code:\r\noffset=2000: Qps Max: 25653 Min: 23571 Mean: 25085\r\noffset=5000: Qps Max: 9894 Min: 9679 Mean: 9796\r\n```\r\nIt is significantly faster when `rev` is not used.\r\n\r\nTesting the command below (with `rev`):\r\n```\r\nzrange test +inf 0 byscore rev limit <offset> 1\r\n```\r\n```\r\nOptimized code:\r\noffset=5: Qps Max: 87820 Min: 83452 Mean: 85119\r\noffset=200: Qps Max: 84450 Min: 83534 Mean: 83998\r\noffset=2000: Qps Max: 87218 Min: 82649 Mean: 84936\r\noffset=5000: Qps Max: 85016 Min: 81380 Mean: 83183\r\n```\r\n```\r\nOriginal code:\r\noffset=5: Qps Max: 88366 Min: 80052 Mean: 85557\r\noffset=200: Qps Max: 87603 Min: 79233 Mean: 85108\r\noffset=2000: Qps Max: 33339 Min: 31288 Mean: 32140\r\noffset=5000: Qps Max: 12169 Min: 12054 Mean: 12125\r\n```\r\nWhen `rev` is used, the optimization will only be used when offset reaches a threshold (currently it is 10), so when offset==5, the results are very close.\r\nWe can see that if `rev` is used, when offset is small (offset==200), the optimized code is a little bit slower than the original code, but when offset is big (offset>=2000), it is a lot faster.\r\nPerhaps we need a better threshhold here.\r\n\r\nSimilar results are generated when using `bylex`.\r\n\r\nWould close #3454\r\n\r\n==========[2022-12-13 Update]==========\r\n\r\nFound an intresting thing: when the nodes of a skiplist is inserted in order of score (loaded from rdb file), adjacent nodes would be placed in the same memory block, which accelerates `ln = ln->level[0].forward;` significantly.\r\nI wonder if this can apply to other data structures, and if there is anything we can do to accelerate some commands.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-11-27T07:19:38Z",
        "closed_at": "2022-11-28T11:03:56Z",
        "merged_at": "2022-11-28T11:03:56Z",
        "body": "In replica, the key expired before master's `INCR` was arrived, so INCR\r\ncreates a new key in the replica and the test failed.\r\n```\r\n*** [err]: Replication of an expired key does not delete the expired key in tests/integration/replication-4.tcl\r\nExpected '0' to be equal to '1' (context: type eval line 13 cmd {assert_equal 0 [$slave exists k]} proc ::test)\r\n```\r\n\r\nThis test is very likely to do a false positive if the `wait_for_ofs_sync`\r\ntakes longer than the expiration time, so give it a few more chances.\r\nGo with 5 retries of increasing timeout, i.e. start with 500ms, then go\r\nto 1000ms, 2000ms, 4000ms, 8000ms.\r\n\r\nThe test was introduced in #9572\r\nAlso it fixes #11153",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-11-27T03:59:30Z",
        "closed_at": "2022-11-27T06:58:44Z",
        "merged_at": "2022-11-27T06:58:44Z",
        "body": "Accidentally introduced when merging unstable in #11199",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 43,
        "changed_files": 1,
        "created_at": "2022-11-26T20:57:13Z",
        "closed_at": "2022-11-27T17:16:16Z",
        "merged_at": "2022-11-27T17:16:16Z",
        "body": "During a different PR I noticed we were doing a bunch of work to convert a dictionary to a dictionary. This wasn't something I was aware of early on with TCL, so I wanted to fix it so others didn't blindly copy the pattern.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-11-26T01:54:23Z",
        "closed_at": "2022-11-27T16:57:50Z",
        "merged_at": "2022-11-27T16:57:50Z",
        "body": "Related to https://github.com/redis/redis/issues/11542.\r\n\r\nAdd an error message when PID file fails to be written. This has historically been considered a best effort failure, but we don't even report the failure.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 88,
        "deletions": 26,
        "changed_files": 4,
        "created_at": "2022-11-24T17:27:03Z",
        "closed_at": "2022-12-05T06:33:54Z",
        "merged_at": "2022-12-05T06:33:54Z",
        "body": "As discussed in https://github.com/redis/redis/issues/10981#issuecomment-1321070278 this PR re-enables argv argument caching.\r\n\r\nThis mechanism aims to reduce calls to malloc and free when preparing the arguments the script sends to redis commands.\r\nThis is a mechanism was originally implemented in 48c49c4 and 4f68655, and was removed in #10220 (thinking it's not needed and that it has no impact), but it now turns out it was wrong, and it indeed provides some 5% performance improvement.\r\n\r\nThe implementation is a little bit too simplistic, it assumes consecutive calls use the same size in the same arg index, but that's arguably sufficient since it's only aimed at caching very small things.\r\n\r\nWe could even consider always pre-allocating args to the full LUA_CMD_OBJCACHE_MAX_LEN (64 bytes) rather than the right size for the argument, that would increase the chance they'll be able to be re-used.\r\nBut in some way this is already happening since we're using sdsalloc, which in turn uses s_malloc_usable and takes ownership of the full side of the allocation, so we are padded to the allocator bucket size.\r\n\r\nTo easily test it:\r\n```\r\ntclsh tests/test_helper.tcl --single unit/scripting\r\n```\r\n\r\nHere's the impact using the following benchmark command:\r\n\r\n```\r\nmemtier_benchmark --command=\"eval \\\"redis.call('hset', 'h1', 'k', 'v');redis.call('hset', 'h2', 'k', 'v');return redis.call('ping')\\\" 0\"  - --hide-histogram --test-time 60 --pipeline 10 -x 1\r\n```\r\n\r\n\r\n### v6.2.6 ~ 222K ops/sec -- 4930d19e70c391750479951022e207e19111eb55 \r\n```\r\nALL STATS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nEvals      223707.29         8.93841         8.83100        17.53500        21.37500     28400.34 \r\nTotals     223707.29         8.93841         8.83100        17.53500        21.37500     28400.34\r\n```\r\n\r\n### Unstable ~ 164K ops/sec -- abc345ad2837cb36ade137982859b6a8666b2735  \r\n```\r\nALL STATS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nEvals      163821.59        12.20666        12.03100        24.06300        24.31900     20797.66 \r\nTotals     163821.59        12.20666        12.03100        24.06300        24.31900     20797.66 \r\n```\r\n\r\n### This PR ~ 184K ops/sec -- 6310ea87d354f8fab35c000237f872f098ff820f  \r\n\r\n```\r\nALL STATS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nEvals      183052.96        10.92378        10.81500        21.37500        25.21500     23239.15 \r\nTotals     183052.96        10.92378        10.81500        21.37500        25.21500     23239.15 \r\n```\r\n",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-11-24T15:30:00Z",
        "closed_at": "2022-11-26T01:35:19Z",
        "merged_at": "2022-11-26T01:35:19Z",
        "body": "When we're shrinking the hash table, we don't need to hash the keys. Since the table sizes are powers of two, we can simply mask the bucket index in the larger table to get the bucket index in the smaller table. We avoid loading the keys into memory and save CPU time.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-11-24T11:44:19Z",
        "closed_at": "2022-11-24T13:27:17Z",
        "merged_at": "2022-11-24T13:27:17Z",
        "body": "Fix compile warning for SHA1Transform() method under alpine with GCC 12.\r\n\r\nWarning:\r\n```\r\nIn function 'SHA1Update',\r\n    inlined from 'SHA1Final' at sha1.c:187:9:\r\nsha1.c:144:13: error: 'SHA1Transform' reading 64 bytes from a region of size 0 [-Werror=stringop-overread]\r\n  144 |             SHA1Transform(context->state, &data[i]);\r\n      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nsha1.c:144:13: note: referencing argument 2 of type 'const unsigned char[64]'\r\nsha1.c: In function 'SHA1Final':\r\nsha1.c:56:6: note: in a call to function 'SHA1Transform'\r\n   56 | void SHA1Transform(uint32_t state[5], const unsigned char buffer[64])\r\n      |      ^~~~~~~~~~~~~\r\n```\r\n\r\nThis warning is a false positive because it has been determined in the loop judgment that there must be 64 chars after position `i`\r\n```c\r\nfor ( ; i + 63 < len; i += 64) {\r\n    SHA1Transform(context->state, &data[i]);\r\n}\r\n```\r\n\r\napline daily CI: https://github.com/sundb/redis/actions/runs/3540293593\r\n\r\nReference: https://github.com/libevent/libevent/commit/e1d7d3e40a7fd50348d849046fbfd9bf976e643c",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-11-24T01:11:54Z",
        "closed_at": "2022-11-24T16:09:56Z",
        "merged_at": "2022-11-24T16:09:56Z",
        "body": "If we run the following benchmark\r\n```\r\nmemtier_benchmark -c 1 -t 1 --pipeline 1 --test-time 60 --command \"GEOSEARCH key FROMLONLAT 7 55 BYBOX 200 200 KM\" --hide-histogram\r\n```\r\nand profile it:\r\n![image](https://user-images.githubusercontent.com/5832149/203671764-2d956f47-f350-4dd2-b44d-c11cc6bfb02f.png)\r\n\r\nWe see that 54.78% of cpu cycles are from geohashGetDistanceIfInRectangle.\r\nWithin it we're calling 3x geohashGetDistance. The first 2 times we call them to produce intermediate results.\r\nThis PR focus on optimizing for those 2 intermediate results.\r\n\r\n- 1 Reduce expensive computation on intermediate geohashGetDistance with same long ( 48895ee2815164749b04a3454c41220c41c6e87c )\r\n- 2 Avoid expensive lon_distance calculation if lat_distance fails beforehand ( 6e2ecc7f45fbc5c7cb2d382174037c7b7366fb3d )\r\n\r\n## Results\r\n\r\nOn pipeline 1, single client benchmark, we move from average latency (including RTT) of 93.59895 ms to 73.04606 ms ( approximately 22% latency drop ).\r\n\r\nFurthermore we can see that the command latency distribution is now more stable ( check avg, p50, p99 and p999 for last result )\r\n\r\n### baseline from unstable branch ( 3b462ce566e577ffcb35822a0a2372f691326cd4 )\r\n```\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nGeosearchs        10.67        93.59895        97.27900        97.79100        97.79100      1596.95 \r\nTotals            10.67        93.59895        97.27900        97.79100        97.79100      1596.95 \r\n```\r\n\r\n### After 1 Reduce expensive computation on intermediate geohashGetDistance with same long ( 48895ee2815164749b04a3454c41220c41c6e87c )\r\n```\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nGeosearchs        11.43        87.40721        87.55100        90.62300        91.13500      1709.95 \r\nTotals            11.43        87.40721        87.55100        90.62300        91.13500      1709.95 \r\n```\r\n\r\n#### This PR at b27590a26f8f9a55683c763ab825d03a18791a89\r\n```\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nGeosearchs        12.67        79.07036        79.35900        79.35900        79.87100      1895.72 \r\nTotals            12.67        79.07036        79.35900        79.35900        79.87100      1895.72 \r\n```\r\n\r\n\r\n### After 2 Avoid expensive lon_distance calculation if lat_distance fails beforehand ( 6e2ecc7f45fbc5c7cb2d382174037c7b7366fb3d )\r\n```\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nGeosearchs        13.67        73.04606        73.21500        73.72700        74.23900      2046.08 \r\nTotals            13.67        73.04606        73.21500        73.72700        74.23900      2046.08 \r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2022-11-21T10:36:32Z",
        "closed_at": "2022-11-22T09:20:24Z",
        "merged_at": "2022-11-22T09:20:24Z",
        "body": "This payload produces a set with duplicate elements (listpack encoding):\r\n```\r\nrestore _key 0 \"\\x14\\x25\\x25\\x00\\x00\\x00\\x0A\\x00\\x06\\x01\\x82\\x5F\\x35\\x03\\x04\\x01\\x82\\x5F\\x31\\x03\\x82\\x5F\\x33\\x03\\x00\\x01\\x82\\x5F\\x39\\x03\\x82\\x5F\\x33\\x03\\x08\\x01\\x02\\x01\\xFF\\x0B\\x00\\x31\\xBE\\x7D\\x41\\x01\\x03\\x5B\\xEC\"\r\n\r\nsmembers key\r\n1) \"6\"\r\n2) \"_5\"\r\n3) \"4\"\r\n4) \"_1\"\r\n5) \"_3\"  ---> dup\r\n6) \"0\"\r\n7) \"_9\"\r\n8) \"_3\"  ---> dup\r\n9) \"8\"\r\n10) \"2\"\r\n```\r\n\r\nThis kind of sets will cause SDIFF to hang, SDIFF generated a broken\r\nprotocol and left the client hung. (Expected ten elements, but only\r\ngot nine elements due to the duplication.)\r\n\r\nIf we set `sanitize-dump-payload` to yes, we will be able to find\r\nthe duplicate elements and report \"ERR Bad data format\".\r\n\r\nDiscovered and discussed in #11290.\r\n\r\nThis PR also improve prints when corrupt-dump-fuzzer hangs, it will\r\nprint the cmds and the payload, an example like:\r\n```\r\nTesting integration/corrupt-dump-fuzzer\r\n[TIMEOUT]: clients state report follows.\r\nsock6 => (SPAWNED SERVER) pid:28884\r\nKilling still running Redis server 28884\r\ncommands caused test to hang:\r\nSDIFF __key \r\npayload that caused test to hang: \"\\x14\\balabala\"\r\n```",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-11-21T03:29:27Z",
        "closed_at": "2022-11-21T20:53:13Z",
        "merged_at": "2022-11-21T20:53:13Z",
        "body": "Apparently we used to set `loglevel debug` for tls in spawn_instance.\r\nI.e. cluster and sentinel tests used to run with debug logging, only when tls mode was enabled.\r\nthis was probably a leftover from when creating the tls mode tests.\r\nit cause a new test created for #11214 to fail in tls mode.\r\n\r\nAt the same time, in order to better distinguish the tests, change the\r\nname of `test-centos7-tls` to `test-centos7-tls-module`, change the name\r\nof `test-centos7-tls-no-tls` to `test-centos7-tls-module-no-tls`.\r\n\r\nNote that in `test-centos7-tls-module`, we did not pass `--tls-module`\r\nin sentinel test because it is not supported, see 4faddf1, added in #9320.\r\nSo only `test-ubuntu-tls` fails in daily CI.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 30,
        "changed_files": 1,
        "created_at": "2022-11-17T23:47:44Z",
        "closed_at": "2022-11-28T08:37:41Z",
        "merged_at": "2022-11-28T08:37:41Z",
        "body": "In scenarios in which we have large datasets and the elements are not contained within the range we do spurious calls do sdsdup and sdsfree.\r\nI.e. instead of pre-creating an sds before we know if we're gonna use it or not, change the role of geoAppendIfWithinShape to just do geoWithinShape, and let the caller create the string only when needed.\r\n\r\n<img width=\"589\" alt=\"Screenshot 2022-11-17 at 23 43 21\" src=\"https://user-images.githubusercontent.com/5832149/202583542-56e55b3d-4ec6-479f-886d-bb6ba1d01b97.png\">\r\n\r\nBenchmarking within a geo key with 23244458 elements and the query returns 90K elements, we noticed an improvement from 20.44 ops/sec to 25.01 ops/sec and a drop in the p50 latency from 48.63900 ms to 41.72700 ms. \r\n\r\nTo benchmark:\r\n\r\n```\r\nmemtier_benchmark -c 1 -t 1 --test-time 60 --command \"GEOSEARCH key FROMLONLAT 7.0 55.0 BYRADIUS 200 KM\" --hide-histogram\r\n``` \r\n\r\n## unstable ( 203b12e41ff7981f0fae5b23819f072d61594813 )\r\n\r\n```\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nGeosearchs        20.44        48.93554        48.63900        50.17500        50.43100     24634.70 \r\nTotals            20.44        48.93554        48.63900        50.17500        50.43100     24634.70 \r\n```\r\n\r\n\r\n## this PR \r\n\r\n```\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nGeosearchs        25.01        39.96836        41.72700        42.23900        43.26300     30147.47 \r\nTotals            25.01        39.96836        41.72700        42.23900        43.26300     30147.47 \r\n\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2022-11-17T18:25:19Z",
        "closed_at": "2022-11-29T12:20:22Z",
        "merged_at": "2022-11-29T12:20:22Z",
        "body": "As being discussed in #10981 we see a degradation in performance between v6.2 and v7.0 of Redis on the EVAL command. \r\n\r\nAfter profiling the current unstable branch ( 203b12e41ff7981f0fae5b23819f072d61594813 ) we can see that we call the expensive function evalCalcFunctionName twice. \r\n![image](https://user-images.githubusercontent.com/5832149/202525956-3f17d658-1888-4ad5-9aaa-b59c5113a6b4.png)\r\n\r\nThe current \"fix\" is to basically avoid calling evalCalcFunctionName and even dictFind(lua_scripts) twice for the same command.\r\nInstead we cache the current script's dictEntry (for both Eval and Functions) in the current client so we don't have to repeat these calls.\r\nThe exception would be when doing an EVAL on a new script that's not yet in the script cache. in that case we will call evalCalcFunctionName (and even evalExtractShebangFlags) twice.\r\n\r\nNotice that the current \"fix\" is still not enough to reach v6.2 numbers. \r\nWe need to improve 10% further to reach the v6.2 results ( even though we have new logic on 7.0 )\r\n\r\nTo benchmark the improvement we can do as follows with memtier:\r\n\r\nEVALSHA\r\n```\r\nredis-cli SCRIPT load \"redis.call('hset', 'h1', 'k', 'v');redis.call('hset', 'h2', 'k', 'v');return redis.call('ping')\"\r\nmemtier_benchmark --command=\"EVALSHA 7cecb99fd9091d8e66d5cccf8979cf3aec5c4951 0\"  - --hide-histogram --test-time 60 --pipeline 10 -x 1\r\n```\r\n\r\nEVAL\r\n```\r\nmemtier_benchmark --command=\"eval \\\"redis.call('hset', 'h1', 'k', 'v');redis.call('hset', 'h2', 'k', 'v');return redis.call('ping')\\\" 0\"  - --hide-histogram --test-time 60 --pipeline 10 -x 1\r\n``` \r\n\r\nResults based on OSS standalone achievable ops/sec\r\n\r\n\r\n\r\nCOMMAND | 6.2.6 | unstable | this PR | % Improvement vs unstable | % Drop vs 6.2.6\r\n-- | -- | -- | -- | -- | --\r\nEVAL | 223744 | 169767 | 187298 | 10.33% | -16.29%\r\nEVALSHA | 264411 | 204867 | 207763 | 1.41% | -21.42%\r\n\r\n \r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2022-11-17T11:10:08Z",
        "closed_at": "2022-11-20T10:12:15Z",
        "merged_at": "2022-11-20T10:12:15Z",
        "body": "The following example will create an empty set (listpack encoding):\r\n```\r\n> RESTORE key 0\r\n\"\\x14\\x25\\x25\\x00\\x00\\x00\\x00\\x00\\x02\\x01\\x82\\x5F\\x37\\x03\\x06\\x01\\x82\\x5F\\x35\\x03\\x82\\x5F\\x33\\x03\\x00\\x01\\x82\\x5F\\x31\\x03\\x82\\x5F\\x39\\x03\\x04\\xA9\\x08\\x01\\xFF\\x0B\\x00\\xA3\\x26\\x49\\xB4\\x86\\xB0\\x0F\\x41\"\r\nOK\r\n> SCARD key\r\n(integer) 0\r\n> SRANDMEMBER key\r\nError: Server closed the connection\r\n```\r\n\r\nIn the spirit of #9297, skip empty set when loading RDB_TYPE_SET_LISTPACK.\r\nIntroduced in #11290",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-11-17T09:07:00Z",
        "closed_at": "2023-03-02T15:22:02Z",
        "merged_at": null,
        "body": "As I see it, currently there is no very convenient way to enable debugging for the Lua repo (`deps/lua`). The repo is skipped by both `make` and `make noopt`. So one has to look into Makefiles and learn about the relationship between flags, prerequisites and targets in order to figure it out. So I think it is better to have a single option for this. So far I simply add `LUA_DEBUG=yes` to the `noopt` target (which is mentioned in the doc: https://redis.io/docs/management/debugging/#compiling-redis-without-optimizations), so that everything including the Lua repo is enabled for debugging under this target. Of course it can be done in other ways (e.g. use another target for Lua debugging and leave `noopt` unchanged), so let me know which you prefer. Thanks!",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 61,
        "deletions": 73,
        "changed_files": 1,
        "created_at": "2022-11-16T16:00:11Z",
        "closed_at": "2022-11-25T15:03:39Z",
        "merged_at": null,
        "body": "Move the structure outside the function for initialization",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2022-11-16T14:22:38Z",
        "closed_at": "2023-09-20T12:19:54Z",
        "merged_at": null,
        "body": "last_bgsave_failure_reason variable is presented in INFO it could be helpful when looking through crash reports.\r\n\r\nAlso I noticed in server.c/serverCron, that we trigger bgsave for dirty server. Currently this does not change the bgsave schedule flag to 0, which can result in starting two bgsaves in the same iteration, where the second one is redundant. If we want, I can fix this in this PR.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2022-11-15T21:04:40Z",
        "closed_at": "2022-11-24T11:10:41Z",
        "merged_at": "2022-11-24T11:10:41Z",
        "body": "Command SENTINEL DEBUG could be no arguments, which display all configurable arguments and their values.\r\nUpdate the command arguments in the docs (json file) to indicate that arguments are optional.\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/202026455-b47411a7-fc03-43e1-8ea5-ec2be69da502.png)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2022-11-15T15:08:00Z",
        "closed_at": "2022-11-22T16:10:22Z",
        "merged_at": "2022-11-22T16:10:22Z",
        "body": "Technically, these commands were deprecated as of 2.6.12, with the introduction of the respective arguments to `SET`.\nIn reality, the deprecation note will only be added in 7.2.0 if this gets merged.\n\nThis PR is technically correct, but we should consider changing it to be realistic.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 56,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2022-11-15T13:15:37Z",
        "closed_at": "2022-11-23T15:39:08Z",
        "merged_at": "2022-11-23T15:39:08Z",
        "body": "In unwatchAllKeys() function, we traverse all the keys watched by the client, and for each key we need to remove the client from the list of clients watching that key. This is implemented by listSearchKey which traverses the list of clients.\r\n\r\nIf we can reach the node of the list of clients from watchedKey in O(1) time, then we do not need to call listSearchKey anymore.\r\n\r\nChanges in this PR: put the node of the list of clients of each watched key in the db inside the watchedKey structure.\r\nIn this way, for every key watched by the client, we can get the watchedKey structure and then reach the node in the list of clients in db->watched_keys to remove it from that list.\r\nFrom the perspective of the list of clients watching the key, the list node is inside a watchedKey structure, so we can get to the watchedKey struct from the listnode by struct member offset math. And because of this, node->value is not used, we can point node->value to the list itself, so that we don't need to fetch the list of clients from the dict.\r\n\r\nIf a client is watching at key0 and key1, and these two keys are also watched by some other clients, then the structure looks like this:\r\n![](https://wx2.sinaimg.cn/mw690/006XXwaCgy1h85y8vs32fj30ny0o7tbg.jpg)\r\n\r\nIn this picture if we call unwatchAllKeys() for the client, then for each node in the BLUE list we need to traverse the corresponding GREEN list to remove the node of the list of clients.\r\n\r\nAnd after this PR the structure looks like this:\r\n![](https://wx1.sinaimg.cn/mw690/006XXwaCgy1h85y8vlhh3j30n70pe0wd.jpg)\r\n\r\nWhen we call unwatchAllKeys() in this picture, for each node in the BLUE list, we can reach the RED watchedKey structure and then get the node in the GREEN list in O(1) time.\r\n\r\n\r\n\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-11-15T04:15:10Z",
        "closed_at": "2022-11-16T01:21:27Z",
        "merged_at": "2022-11-16T01:21:27Z",
        "body": "Resolves https://github.com/redis/redis/issues/11507.\r\n\r\nBoth functions and eval are marked as \"no-monitor\", since we want to explicitly feed in the script command before the commands generated by the script. Note that we want this behavior generally, so that commands can redact arguments before being added to the monitor.\r\n\r\nEval solves this problem by explicitly replicating commands, but it seems like functions missed it? I didn't find any explicit conversation here https://github.com/redis/redis/pull/10004. \r\n\r\n```\r\nRelease notes\r\nFix a bug where FCALL and FCALL_RO where not being reported in monitor.\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 13,
        "changed_files": 2,
        "created_at": "2022-11-14T20:57:47Z",
        "closed_at": "2022-11-27T08:18:19Z",
        "merged_at": null,
        "body": "Call `trimStringObjectIfNeeded` only when using the `querybuf` as the object's ptr.\r\n\r\nCurrently, we call `trimStringObjectIfNeeded` from `tryObjectEncoding` which is called for all set-commands.\r\n\r\nHowever, the only place where we might allocate a larger than necessary string object is when we use the `querybuf` itself as the object's value.\r\n\r\n`trimStringObjectIfNeeded` might be expensive as it tries to remove the internal fragmentation (a result of [#7875](https://github.com/redis/redis/pull/7875) change).\r\n ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 89,
        "deletions": 69,
        "changed_files": 10,
        "created_at": "2022-11-14T19:34:37Z",
        "closed_at": "2023-02-20T07:07:46Z",
        "merged_at": null,
        "body": "In [#7875](https://github.com/redis/redis/pull/7875), we change the sds alloc to be the usable allocation size in order to:\r\n>reduce the need for realloc calls by making the sds implicitly take over\r\nthe internal fragmentation\r\n\r\nThis change was not done in `sdsRemoveFreeSpace`, resulting in inconsistency and unnecessary reallocations.\r\n\r\n",
        "comments": 46
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-11-14T09:45:55Z",
        "closed_at": "2022-11-15T15:18:21Z",
        "merged_at": "2022-11-15T15:18:21Z",
        "body": "The test introduced in #11482 fail on ARM (extra CI):\r\n```\r\n*** [err]: RESP2: RM_ReplyWithDouble: NaN in tests/unit/moduleapi/reply.tcl\r\nExpected '-nan' to be equal to 'nan' (context: type eval line 3 cmd\r\n{assert_equal \"-nan\" [r rw.double 0 0]} proc ::test)\r\n\r\n*** [err]: RESP3: RM_ReplyWithDouble: NaN in tests/unit/moduleapi/reply.tcl\r\nExpected ',-nan' to be equal to ',nan' (context: type eval line 8 cmd\r\n{assert_equal \",-nan\" [r rw.double 0 0]} proc ::test)\r\n```\r\n\r\nIt looks like there is no nagative nan on ARM. ",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-11-14T02:47:16Z",
        "closed_at": "2022-11-14T09:07:10Z",
        "merged_at": "2022-11-14T09:07:10Z",
        "body": "The test introduced in #11482 fail on mac:\r\n```\r\n*** [err]: RESP3: RM_ReplyWithDouble: inf in tests/unit/moduleapi/reply.tcl\r\nExpected 'Inf' to be equal to 'inf'\r\n(context: type eval line 6 cmd {assert_equal Inf [r rw.double inf]} proc ::test)\r\n```\r\n\r\nLooks like the mac platform returns inf instead of Inf in this case, this PR uses readraw to verify the protocol.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2022-11-13T13:26:59Z",
        "closed_at": "2022-11-14T22:40:35Z",
        "merged_at": "2022-11-14T22:40:35Z",
        "body": "In `moduleFireServerEvent` we change the real client DB to 0 on `freeClient` in case the event is `REDISMODULE_EVENT_CLIENT_CHANGE`.\r\nIt results in a crash if the client is blocked on a key on other than DB 0.\r\n\r\nThe DB change is not necessary even for module-client, as we set its DB to 0 on either `createClient` or `moduleReleaseTempClient`.\r\n\r\n\r\nBacktrace:\r\n```\r\nBacktrace:\r\nsrc/redis-server 127.0.0.1:21111(_serverAssertWithInfo+0x5d)[0x50f9bd]\r\nsrc/redis-server 127.0.0.1:21111(unblockClientWaitingData+0x150)[0x4ad150]\r\nsrc/redis-server 127.0.0.1:21111(unblockClient+0xa0)[0x4ad270]\r\nsrc/redis-server 127.0.0.1:21111(freeClient+0x45a)[0x50446a]\r\nsrc/redis-server 127.0.0.1:21111(clientCommand+0x899)[0x5097b9]\r\nsrc/redis-server 127.0.0.1:21111(call+0xb0)[0x4fc680]\r\nsrc/redis-server 127.0.0.1:21111(processCommand+0xa5b)[0x4fd64b]\r\n```\r\n\r\n```\r\nRelease notes:\r\nFix a crash when a client is disconnected while blocking on a key outside of DB 0 while a module is listening for client change events.\r\nFix bug in which module listening on CLIENT_CHANGE event would always see DB 0 being selected, even if the client had another db.\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2022-11-12T17:01:14Z",
        "closed_at": "2022-11-12T18:35:34Z",
        "merged_at": "2022-11-12T18:35:34Z",
        "body": "Fix a few issues with the recent #11463\r\n* use exitFromChild instead of exit\r\n* test should ignore defunct process since that's what we expect to happen for thees child processes when the parent dies.\r\n* fix typo",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 17,
        "changed_files": 3,
        "created_at": "2022-11-11T16:26:26Z",
        "closed_at": "2023-08-17T22:06:25Z",
        "merged_at": null,
        "body": "For `SETEX` commands and `SET` commands that specify and expiration time (e.g. `SET key val EX 60`), when the command vector is re-written for propagation in `setGenericCommand()`, the expiration time will usually be represented as an integer-type robj instead of a string-type robj.\r\n\r\nThere are two reasons to avoid an integer robj in the command vector:\r\n1. It breaks the assumption that command vectors will always consist of string objects. For example, if `setGetKeys()` is called in command post-processing to get the number of keys in a `SETEX` command, it will segfault.\r\n2. It is unnecessary from a performance perspective. Before the command is fed to the replication buffer, the expiration time will need to be converted back to a string anyways.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-11-10T03:24:29Z",
        "closed_at": "2022-11-10T08:27:38Z",
        "merged_at": "2022-11-10T08:27:38Z",
        "body": "Add missing lpFree, introduced in #11290",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 11,
        "changed_files": 4,
        "created_at": "2022-11-08T17:13:56Z",
        "closed_at": "2022-11-22T14:38:28Z",
        "merged_at": "2022-11-22T14:38:28Z",
        "body": "This PR add `assert_refcount_morethan`, and modify `assert_refcount` to skip\r\nthe `OBJECT REFCOUNT` check with `needs:debug` flag. Use them to modify all\r\n`OBJECT REFCOUNT` calls and also update the tests/README to be more specific.\r\n\r\nThe reasoning is that some of these tests could be testing something important,\r\nand along the way also add a check for the refcount, and it could be a shame to skip\r\nthe whole test just because the refcount functionality is missing or blocked.\r\nbut much like the fact that some redis variants may not support DEBUG,\r\nand still we want to run the majority of the test for coverage, and just skip the digest match.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-11-08T09:36:38Z",
        "closed_at": "2022-11-08T12:58:38Z",
        "merged_at": "2022-11-08T12:58:38Z",
        "body": "Add `needs:save` tag for the test introduced by https://github.com/redis/redis/pull/11376",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-11-07T20:36:38Z",
        "closed_at": "2022-11-22T14:18:37Z",
        "merged_at": "2022-11-22T14:18:37Z",
        "body": "Now, according to the comments, if the truncated file is not the last file, it will be considered as a fatal error. \r\nAnd the return code will updated to AOF_FAILED, then server will exit without any error message to the client.\r\n\r\nSimilar to other error situations, this PR add an explicit error message for this case and make the client know clearly what happens.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 104,
        "deletions": 3,
        "changed_files": 6,
        "created_at": "2022-11-07T12:18:57Z",
        "closed_at": "2023-02-23T07:07:49Z",
        "merged_at": "2023-02-23T07:07:49Z",
        "body": "To resolve #11409\r\n\r\nWhen no-touch mode is enabled, the client will not touch LRU/LFU of the keys it accesses, except when executing command `TOUCH`.\r\nThis allows inspecting or modifying the key-space without affecting their eviction.\r\n\r\nChanges:\r\n- A command `CLIENT NO-TOUCH ON|OFF` to switch on and off this mode.\r\n- A client flag `#define CLIENT_NOTOUCH (1ULL<<45)`, which can be shown with `CLIENT INFO`, by the letter \"T\" in the \"flags\" field.\r\n- Clear `NO-TOUCH` flag in `clearClientConnectionState`, which is used by `RESET` command and resetting temp clients used by modules.\r\n- Also clear `NO-EVICT` flag in `clearClientConnectionState`, this might have been an oversight, spotted by @madolson.\r\n- A test using `DEBUG OBJECT` command to verify that LRU stat is not touched when no-touch mode is on.\r\n ",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-11-07T08:39:00Z",
        "closed_at": "2022-11-13T11:12:22Z",
        "merged_at": "2022-11-13T11:12:22Z",
        "body": "Adding a test to cover the already existing behavior of NAN replies, to accompany the PR that adds them to the RESP3 spec: https://github.com/redis/redis-specifications/pull/10\r\n\r\nThis PR also covers Inf replies that are already in the spec, as well as RESP2 coverage.",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-11-07T03:35:20Z",
        "closed_at": "2022-11-07T22:41:03Z",
        "merged_at": null,
        "body": "KEEPTTL and PERSIST flags only exists in SET and GET respectively. There's no need to check for that flag because we already checked for the command_type. I also took the liberty of reordering the checks a bit to make it more clear that they are mutually exclusive.\r\n\r\nSigned-off-by: Hanif Bin Ariffin <hanif.ariffin.4326@gmail.com>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-11-06T05:15:58Z",
        "closed_at": "2022-11-06T06:33:29Z",
        "merged_at": "2022-11-06T06:33:29Z",
        "body": "AFAIK, these are never mutated anyways.\r\n\r\n```\r\nhbina@akarin ~/g/redis (unstable)> rg \"shared.integers\\[\" --after-context 5\r\nsrc/server.c\r\n1846:        shared.integers[j] =\r\n1847-            makeObjectShared(createObject(OBJ_STRING,(void*)(long)j));\r\n1848:        shared.integers[j]->encoding = OBJ_ENCODING_INT;\r\n1849-    }\r\n1850-    for (j = 0; j < OBJ_SHARED_BULKHDR_LEN; j++) {\r\n1851-        shared.mbulkhdr[j] = createObject(OBJ_STRING,\r\n1852-            sdscatprintf(sdsempty(),\"*%d\\r\\n\",j));\r\n1853-        shared.bulkhdr[j] = createObject(OBJ_STRING,\r\n\r\nsrc/t_stream.c\r\n1563:    argv[4] = shared.integers[0];\r\n1564-    argv[5] = id;\r\n1565-    argv[6] = shared.time;\r\n1566-    argv[7] = createStringObjectFromLongLong(nack->delivery_time);\r\n1567-    argv[8] = shared.retrycount;\r\n1568-    argv[9] = createStringObjectFromLongLong(nack->delivery_count);\r\n\r\nsrc/object.c\r\n161:        incrRefCount(shared.integers[value]);\r\n162:        o = shared.integers[value];\r\n163-    } else {\r\n164-        if (value >= LONG_MIN && value <= LONG_MAX) {\r\n165-            o = createObject(OBJ_STRING, NULL);\r\n166-            o->encoding = OBJ_ENCODING_INT;\r\n167-            o->ptr = (void*)((long)value);\r\n--\r\n639:            incrRefCount(shared.integers[value]);\r\n640:            return shared.integers[value];\r\n641-        } else {\r\n642-            if (o->encoding == OBJ_ENCODING_RAW) {\r\n643-                sdsfree(o->ptr);\r\n644-                o->encoding = OBJ_ENCODING_INT;\r\n645-                o->ptr = (void*) value;\r\n\r\nsrc/redis-check-rdb.c\r\n432:    if (shared.integers[0] == NULL)\r\n433-        createSharedObjects();\r\n434-    server.loading_process_events_interval_bytes = 0;\r\n435-    server.sanitize_dump_payload = SANITIZE_DUMP_YES;\r\n436-    rdbCheckMode = 1;\r\n437-    rdbCheckInfo(\"Checking RDB file %s\", argv[1]);\r\n```\r\n\r\nSigned-off-by: Hanif Bin Ariffin <hanif.ariffin.4326@gmail.com>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 47,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2022-11-04T21:53:53Z",
        "closed_at": "2023-02-02T20:12:18Z",
        "merged_at": "2023-02-02T20:12:18Z",
        "body": "Added 3 fields to the ACL LOG - adds entry_id, timestamp_created and timestamp_last_updated, which updates similar existing log error entries. The pair - entry_id, timestamp_created is a unique identifier of this entry, in case the node dies and is restarted, it can detect that if it's a new series.\r\n\r\nThe primary use case of Unique id is to uniquely identify the error messages and not to detect if the server has restarted.\r\n\r\n1. entry-id is the sequence number of the entry (starting at 0) since the server process started. Can also be used to check if items were \"lost\" if they fell between periods.\r\n2. timestamp-created is the unix-time in ms at the time the entry was first created.\r\n3. timestamp-last-updated is the unix-time in ms at the time the entry was last updated\r\n\r\nTime_created gives the absolute time which better accounts for network time as compared to time since. It can also be older than 60 secs and presently there is no field that can display the original time of creation once the error entry is updated.\r\nThe reason of timestamp_last_updated field is that it provides a more precise value for the \u201clast time\u201d an error was seen where as, presently it is only in the 60 second period.\r\n\r\nChanges in acl.c: adds entry_id, timestamp_created and timestamp_last_updated to ACL LOG.\r\n\r\nChanges in acl.tcl: adds test that verify that all of the additional fields are available in the reply when the ACL LOG commands and verifies that timestamp_created does not change when log entry is updated and that timestamp-last-updated is updated when entry is updated. Also checks if a new unique id is assigned to a new task.\r\n\r\n```\r\nRelease notes\r\nAdd entry id, timestamp created, and timestamp last updated time to ACL log entries.\r\n```",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-11-04T17:03:10Z",
        "closed_at": "2022-11-04T18:28:27Z",
        "merged_at": "2022-11-04T18:28:27Z",
        "body": "Our FreeBSD daily has been failing recently:\r\n```\r\n  Config file: freebsd-13.1.conf\r\n  cd: /Users/runner/work/redis/redis: No such file or directory\r\n  gmake: *** No targets specified and no makefile found.  Stop.\r\n```\r\n\r\nUpgrade vmactions/freebsd-vm to the latest version (0.3.0) can work.\r\nI've tested it, but don't know why, but first let's fix it.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 52,
        "deletions": 23,
        "changed_files": 1,
        "created_at": "2022-11-03T19:27:05Z",
        "closed_at": "2022-11-05T08:44:04Z",
        "merged_at": null,
        "body": "https://github.com/redis/redis/commit/0bf90d944313919eb8e63d3588bf63a367f020a3\r\nBack port the above commit which fixes #11298 from unstable branch to Redis 6.2 branch.\r\n\r\nQualification: make test, smoke testing on Linux x86_64",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 492,
        "deletions": 512,
        "changed_files": 18,
        "created_at": "2022-11-03T10:47:14Z",
        "closed_at": "2023-01-11T12:27:59Z",
        "merged_at": "2023-01-11T12:27:59Z",
        "body": "This PR refactors the abstraction of the dictEntry by making it opaque. This enables future optimizations of the dict implementation without affecting the code using it.\r\n\r\nThe PR contains 5 commits. More detailed commit messages are found in each commit.\r\n\r\n* Make dictEntry opaque\r\n* Let active expire cycle use dictScan instead of messing with internals\r\n* activeDefragSdsDict use scan instead of iterator and drop dictSetNext\r\n* Remove the bucket-cb from dictScan and move dictEntry defrag to dictScanDefrag\r\n* Move stat_active_defrag_hits increment to activeDefragAlloc",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 59,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2022-11-03T09:39:11Z",
        "closed_at": "2022-11-09T08:02:19Z",
        "merged_at": "2022-11-09T08:02:19Z",
        "body": "During a diskless sync, if the master main process crashes, the child would\r\nhave hung in `write`. This fix closes the read fd on the child side, so that if the\r\nparent crashes, the child will get a write error and exit.\r\n\r\nThis change also fixes disk-based replication, BGSAVE and AOFRW.\r\nIn that case the child wouldn't have been hang, it would have just kept\r\nrunning until done which may be pointless.\r\n\r\nThere is a certain degree of risk here. in case there's a BGSAVE child that could\r\nmaybe succeed and the parent dies for some reason, the old code would have let\r\nthe child keep running and maybe succeed and avoid data loss.\r\nOn the other hand, if the parent is restarted, it would have loaded an old rdb file\r\n(or none), and then the child could reach the end and rename the rdb file (data\r\nconflicting with what the parent has), or also have a race with another BGSAVE\r\nchild that the new parent started.\r\n\r\nNote that i removed a comment saying a write error will be ignored in the child\r\nand handled by the parent (this comment was very old and i don't think relevant).",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-11-01T16:06:14Z",
        "closed_at": "2022-12-07T13:45:21Z",
        "merged_at": "2022-12-07T13:45:21Z",
        "body": "It is not possible to reliably select the target server instance for TLS replication in case that the target sits behind a TLS (terminating or pass-through) proxy, as there's no SNI extension in the ClientHello message sent via the connection created for the sake of replication.\r\n\r\nThis is a simple attempt at setting the SNI to the value of the master server's address, in case that address is not actually an IP",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-11-01T12:46:49Z",
        "closed_at": "2022-11-14T00:47:03Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 30,
        "changed_files": 1,
        "created_at": "2022-11-01T11:30:27Z",
        "closed_at": "2022-11-06T12:49:56Z",
        "merged_at": "2022-11-06T12:49:55Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-10-31T20:10:29Z",
        "closed_at": "2022-11-01T05:54:03Z",
        "merged_at": "2022-11-01T05:54:03Z",
        "body": "This PR add test case for PR https://github.com/redis/redis/pull/11417, with only key as arugment for GEOHASH and GEOPOS",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-10-30T17:20:04Z",
        "closed_at": "2022-10-31T08:51:13Z",
        "merged_at": null,
        "body": "### redis-benchmark: Support client connection duration limit and jitter.\r\n### Motivation\r\nSimulate real-life scenario of clients that reestablishing the connection after certain time in the air, in order to take into account the reconnection overhead.\r\n\r\n### Changes\r\n1. Client connection duration sets the time the client is alive, when the duration reached, the client will be freed and a new client will start instead.\r\n2. Configurable client connection duration by `--connection-duration`, time in `msec`. Default 0 (no duration limit).\r\n3. Client connection duration support adding jitter, calculated by randomize the jitter by +-(jitter/2).\r\n4. Jitter for connection duration is configurable by `--connection-duration-jitter`, time in `msec`. Default 0 (no jitter).\r\n5. Added descriptions in help.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-10-30T11:56:15Z",
        "closed_at": "2022-11-01T16:15:52Z",
        "merged_at": null,
        "body": "This is a sample implementation of the proposal in https://github.com/redis/redis/issues/11448.\r\n\r\nIt issues `PAUSE` instructions during the IO threads spin loop, in order to give the CPU and other hyperthreads some room to breathe.\r\n\r\nThis seems particularly relevant for multi tenant environments (e.g. kubernetes), but interference could even manifest itself on the main thread.\r\n\r\nI have not yet performed any extensive benchmarks. Assistance in that regard would be greatly appreciated.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2022-10-27T15:17:42Z",
        "closed_at": "2022-11-02T13:15:13Z",
        "merged_at": "2022-11-02T13:15:13Z",
        "body": "It is similar to the pr https://github.com/redis/redis/pull/11417,\r\n\r\nAccording to the source code, the commands can be executed with only key name,\r\nand no GET/SET/INCR operation arguments.\r\nchange the docs to reflect that by marking these arguments as optional.\r\nalso add tests.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-10-26T19:03:25Z",
        "closed_at": "2022-11-02T14:16:16Z",
        "merged_at": "2022-11-02T14:16:16Z",
        "body": "Resolve an edge case where the ID of a stream is updated retroactively\r\nto an ID lower than the already set max_deleted_entry_id.\r\n\r\nCurrently, if we have command as below:\r\n**xsetid mystream 1-1 MAXDELETEDID 1-2**\r\nThen we will get the following error:\r\n**(error) ERR The ID specified in XSETID is smaller than the provided max_deleted_entry_id**\r\nBecuase the provided MAXDELETEDID 1-2 is greated than input last-id: 1-1\r\n\r\nThen we could assume there is a similar situation:\r\nstep 1: we add three items in the mystream\r\n\r\n**127.0.0.1:6381> xadd mystream 1-1 a 1\r\n\"1-1\"\r\n127.0.0.1:6381> xadd mystream 1-2 b 2\r\n\"1-2\"\r\n127.0.0.1:6381> xadd mystream 1-3 c 3\r\n\"1-3\"**\r\n\r\nstep 2: we could check the mystream infomation as below:\r\n**127.0.0.1:6381> xinfo stream mystream\r\n 1) \"length\"\r\n 2) (integer) 3\r\n 7) \"last-generated-id\"\r\n 8) \"1-3\"\r\n 9) \"max-deleted-entry-id\"\r\n10) \"0-0\"\r\n\r\nstep 3: we delete the item id 1-2 and 1-3 as below:\r\n**127.0.0.1:6381> xdel mystream 1-2\r\n(integer) 1\r\n127.0.0.1:6381> xdel mystream 1-3\r\n(integer) 1**\r\n\r\nstep 4: we check the mystream information:\r\n127.0.0.1:6381> xinfo stream mystream\r\n 1) \"length\"\r\n 2) (integer) 1\r\n 7) \"last-generated-id\"\r\n 8) \"1-3\"\r\n 9) \"max-deleted-entry-id\"\r\n10) \"1-3\"\r\n\r\nwe could notice that the **max-deleted-entry-id update to 1-3**, so right now, if we just run:\r\n**xsetid mystream 1-2** \r\nthe above command has the same effect with **xsetid mystream 1-2  MAXDELETEDID 1-3**\r\n\r\nSo we should return an error to the client that **(error) ERR The ID specified in XSETID is smaller than current max_deleted_entry_id**",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-10-26T16:13:49Z",
        "closed_at": "2022-11-02T02:27:30Z",
        "merged_at": "2022-11-02T02:27:30Z",
        "body": "Add a print statement to indicate which IP/port is sending\r\nthe error messages. That way we can at least check to see\r\nif it is a node in the cluster or some other nefarious nodes.\r\n\r\nIt is proposed in #11339.\r\n\r\nUnrelated changes: the return check for connAddrPeerName should\r\nbe -1 instead of C_ERR, although the value of C_ERR is also -1.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-10-26T07:05:58Z",
        "closed_at": "2022-11-25T19:58:19Z",
        "merged_at": "2022-11-25T19:58:19Z",
        "body": "Two minor fix for redis cluster:\r\n1. `clusterNodeClearSlotBit()/clusterNodeSetSlotBit()`, only set bit when slot does not exist and clear bit when slot does exist.\r\n 2. ~~`clusterNodeCronHandleReconnect()`, when try to connect a new node and create new link, add link memory usage~~. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-10-25T19:30:04Z",
        "closed_at": "2022-11-09T12:41:01Z",
        "merged_at": "2022-11-09T12:41:01Z",
        "body": "Fixes #11420.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 58,
        "deletions": 17,
        "changed_files": 2,
        "created_at": "2022-10-25T17:10:23Z",
        "closed_at": "2022-11-28T15:35:32Z",
        "merged_at": "2022-11-28T15:35:32Z",
        "body": "Before this PR, we use sdsMakeRoomFor() to expand the size of hyperloglog string (sparse representation). And because sdsMakeRoomFor() uses a greedy strategy (allocate about twice what we need), the memory we allocated for the hyperloglog may be more than `server.hll_sparse_max_bytes` bytes. The memory more than` server.hll_sparse_max_bytes` will be wasted.\r\n\r\nIn this pull request, tone down the greediness of the allocation growth, and also make sure it'll never request more than `server.hll_sparse_max_bytes`.\r\n\r\nThis could in theory mean the size of the hyperloglog string is insufficient for the increment we need, should be ok since in this case we promote the hyperloglog to dense representation, an assertion was added to make sure.\r\n\r\nThis PR also add some tests and fixes some typo and indentation issues.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-10-25T15:13:42Z",
        "closed_at": "2022-10-26T15:45:20Z",
        "merged_at": "2022-10-26T15:45:20Z",
        "body": "Reducing duplicate header file definitions <sys/resource.h>\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-10-25T07:22:53Z",
        "closed_at": "2022-11-03T11:19:50Z",
        "merged_at": "2022-11-03T11:19:49Z",
        "body": "Today we don't place any specific restrictions on module command names.\r\nThis can cause ambiguous scenarios. For example, someone might name a\r\ncommand like \"module|feature\" which would be incorrectly parsed by the\r\nACL system as a subcommand.\r\n\r\nIn this PR, we will block some chars that we know can mess things up.\r\nSpecifically ones that can appear ok at first and cause problems in some\r\ncases (we rather surface the issue right away).\r\n\r\nThere are these characters:\r\n * ` ` (space) - issues with old inline protocol.\r\n * `\\r`, `\\n` (newline) - can mess up the protocol on acl error replies.\r\n * `|` - sub-commands.\r\n * `@` - ACL categories\r\n * `=`, `,` - info and client list fields.\r\n\r\nnote that we decided to leave `:` out as it's handled by `getSafeInfoString` and is more likely to already been used by existing modules.\r\n\r\nThis is a breaking change, closes #11390.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 29,
        "changed_files": 2,
        "created_at": "2022-10-24T23:45:11Z",
        "closed_at": "2022-10-25T21:36:49Z",
        "merged_at": "2022-10-25T21:36:49Z",
        "body": "This test passes accidentally (it checks memory usage). ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-10-24T23:39:59Z",
        "closed_at": "2022-10-29T23:24:13Z",
        "merged_at": "2022-10-29T23:24:13Z",
        "body": "Currently neither the problem nor the fix has an impact. File events aren't filtered out in any scenario, but this can change in the future.\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 9,
        "changed_files": 4,
        "created_at": "2022-10-24T13:24:08Z",
        "closed_at": "2022-10-27T06:29:44Z",
        "merged_at": "2022-10-27T06:29:44Z",
        "body": "RM_Call is designed to let modules call redis commands disregarding the OOM state (the module is responsible to declare its command flags to redis, or perform the necessary checks).\r\nThe other (new) alternative is to pass the \"M\" flag to RM_Call so that redis can OOM reject commands implicitly.\r\n\r\nHowever, Currently, RM_Call enforces OOM on scripts (excluding scripts that declared `allow-oom`) in all cases, regardless of the RM_Call \"M\" flag being present.\r\n\r\nThis PR fixes scripts to be consistent with other commands being executed by RM_Call.\r\nIt modifies the flow in effect treats scripts as if they if they have the ALLOW_OOM script flag, if the \"M\" flag is not passed (i.e. no OOM checking is being performed by RM_Call, so no OOM checking should be done on script).\r\n\r\nThis replaced https://github.com/redis/redis/pull/11161 instead of requiring a new flag, it redefines how RM_call treats the M flag (or the lack of M flag).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-10-24T08:58:17Z",
        "closed_at": "2022-10-24T15:27:56Z",
        "merged_at": "2022-10-24T15:27:56Z",
        "body": "This is a rare failure mode of a new feature of redis 7 introduced in #9217 (when the incremental part of the ID overflows).\r\ntill now, the outcome of that error was undetermined (could easily result in `Elements are too large to be stored` wrongly, due to unset `errno`).\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-10-23T07:33:37Z",
        "closed_at": "2022-10-23T13:00:47Z",
        "merged_at": "2022-10-23T13:00:47Z",
        "body": "Funcion sentinelAddrEqualsHostname() of sentinel makes DNS resolve\r\nand based on it determines if two IP addresses are equal. Now, If the \r\nDNS resolve command fails, the function simply returns 0, even if the\r\nhostnames are identical. \r\n\r\nThis might become an issue in case of failover such that sentinel might \r\nreceives from Redis instance, response to regular INFO query it sent,\r\nand wrongly decide that the instance is pointing to is different leader \r\nthan the one recorded because of this function, yet hostnames are \r\nidentical. In turn sentinel disconnects the connection between sentinel\r\nand valid slave which leads to `-failover-abort-no-good-slave`. \r\nSee issue #11241.\r\n\r\nI managed to reproduce only part of the flow in which the function \r\nreturn wrong result and trigger `+fix-slave-config`.\r\n\r\nThe fix is even if the function failed to resolve then compare based on\r\nhostnames. That is our best effort as long as the server is unavailable\r\nfor some reason. It is fine since Redis instance cannot have multiple \r\nhostnames for a given setup",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2022-10-21T18:38:59Z",
        "closed_at": "2022-10-25T11:26:08Z",
        "merged_at": "2022-10-25T11:26:08Z",
        "body": "These commands take a list of members, which can be empty (i.e. running the command with just a key name).\r\nthis always results in an empty array reply, so it doesn't make much sense, but changing it is a breaking change.\r\n\r\nThis PR fixes the documentation, making the member field as optional, just makes sure the command format documentation is consistent with the command behavior.\r\n\r\nThe command format will be:\r\n\r\n127.0.0.1:6381> GEOPOS key [member [member ...]]\r\n127.0.0.1:6381> GEOHASH key [member [member ...]]\r\n\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-10-21T09:08:23Z",
        "closed_at": "2022-10-23T12:06:58Z",
        "merged_at": "2022-10-23T12:06:58Z",
        "body": "redis.set_repl() needs one arg, but the tips says two.\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-10-20T13:24:12Z",
        "closed_at": "2022-11-14T00:46:09Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2022-10-19T09:38:57Z",
        "closed_at": "2022-11-07T17:11:13Z",
        "merged_at": "2022-11-07T17:11:13Z",
        "body": "Till now Redis attempted to avoid using jemalloc on ARM, but didn't do that properly (missing armv8l and aarch64), so in fact we did you jemalloc on these without a problem.\r\n\r\nSide notes:\r\n\r\nSome ARM platforms, which share instruction set and can share binaries (docker images), may have different page size, and apparently jemalloc uses the page size of the build machine as the maximum page size to be supported by the build.\r\nsee https://github.com/redis-stack/redis-stack/issues/187\r\n\r\nTo work around that, when building for ARM, one can change the maximum page size to 64k (or greater if present on the build machine) In recent versions of jemalloc, this should not have any severe side effects (like VM map fragmentation), see:\r\nhttps://github.com/jemalloc/jemalloc/issues/467\r\nhttps://github.com/redis/redis/pull/11170#issuecomment-1236265230\r\n\r\nTo do that, one can use:\r\n```\r\nJEMALLOC_CONFIGURE_OPTS=\"--with-lg-page=16\" make\r\n```\r\n\r\nBesides that, this PR fixes a messy makefile condition that was created\r\nhere: f30b18f4de",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-10-18T04:09:32Z",
        "closed_at": "2022-10-18T05:24:31Z",
        "merged_at": "2022-10-18T05:24:31Z",
        "body": "And fix a few newly detected typo.\r\nCloses #11394",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2022-10-18T02:15:23Z",
        "closed_at": "2022-10-18T05:28:25Z",
        "merged_at": "2022-10-18T05:28:25Z",
        "body": "fix some malloc macros in `listpack.c`.\r\nlistpack has it's own malloc aliases, but in some places normal redis malloc calls have slipped in.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-10-17T15:38:33Z",
        "closed_at": "2022-10-18T04:09:54Z",
        "merged_at": null,
        "body": "Bumps [codespell](https://github.com/codespell-project/codespell) from 2.2.1 to 2.2.2.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/codespell-project/codespell/releases\">codespell's releases</a>.</em></p>\n<blockquote>\n<h2>v2.2.2</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>MAINT: Bump to dev by <a href=\"https://github.com/larsoner\"><code>@\u200blarsoner</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2463\">codespell-project/codespell#2463</a></li>\n<li>Capitalize LessTif by <a href=\"https://github.com/dforsi\"><code>@\u200bdforsi</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2471\">codespell-project/codespell#2471</a></li>\n<li>Add several spelling corrections by <a href=\"https://github.com/ydah\"><code>@\u200bydah</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2467\">codespell-project/codespell#2467</a></li>\n<li>aexs-&gt;axes by <a href=\"https://github.com/tkoyama010\"><code>@\u200btkoyama010</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2475\">codespell-project/codespell#2475</a></li>\n<li>MAINT: Add tests as submodule by <a href=\"https://github.com/larsoner\"><code>@\u200blarsoner</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2477\">codespell-project/codespell#2477</a></li>\n<li>diffues-&gt;diffuse, defuse, by <a href=\"https://github.com/tkoyama010\"><code>@\u200btkoyama010</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2476\">codespell-project/codespell#2476</a></li>\n<li>conviencece-&gt;convenience by <a href=\"https://github.com/tkoyama010\"><code>@\u200btkoyama010</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2474\">codespell-project/codespell#2474</a></li>\n<li>htose-&gt;those, these, by <a href=\"https://github.com/tkoyama010\"><code>@\u200btkoyama010</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2480\">codespell-project/codespell#2480</a></li>\n<li>Add several spelling corrections by <a href=\"https://github.com/adrien-berchet\"><code>@\u200badrien-berchet</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2461\">codespell-project/codespell#2461</a></li>\n<li>move <code>ro</code> to code dictionary by <a href=\"https://github.com/robin-wayve\"><code>@\u200brobin-wayve</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2468\">codespell-project/codespell#2468</a></li>\n<li>Add ascconciated-&gt;associated by <a href=\"https://github.com/peternewman\"><code>@\u200bpeternewman</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2482\">codespell-project/codespell#2482</a></li>\n<li>Fix various spelling corrections by <a href=\"https://github.com/luzpaz\"><code>@\u200bluzpaz</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2456\">codespell-project/codespell#2456</a></li>\n<li>Add several spelling corrections by <a href=\"https://github.com/luzpaz\"><code>@\u200bluzpaz</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2481\">codespell-project/codespell#2481</a></li>\n<li>Add sorkflow-&gt;workflow by <a href=\"https://github.com/peternewman\"><code>@\u200bpeternewman</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2485\">codespell-project/codespell#2485</a></li>\n<li>Add spelling corrections for install and variants. by <a href=\"https://github.com/cfi-gb\"><code>@\u200bcfi-gb</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2486\">codespell-project/codespell#2486</a></li>\n<li>Capitalize all suggested fixes by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2223\">codespell-project/codespell#2223</a></li>\n<li>Add several spelling corrections and refinements by <a href=\"https://github.com/luzpaz\"><code>@\u200bluzpaz</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2484\">codespell-project/codespell#2484</a></li>\n<li>Add <code>knowladge-&gt;knowledge</code> by <a href=\"https://github.com/danielhoherd\"><code>@\u200bdanielhoherd</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2493\">codespell-project/codespell#2493</a></li>\n<li>Add several spelling corrections by <a href=\"https://github.com/ydah\"><code>@\u200bydah</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2492\">codespell-project/codespell#2492</a></li>\n<li>Change &quot;circularly&quot; to &quot;circulary&quot;. by <a href=\"https://github.com/vinc17fr\"><code>@\u200bvinc17fr</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2422\">codespell-project/codespell#2422</a></li>\n<li>Use <code>grep -E</code> instead of <code>egrep</code> by <a href=\"https://github.com/a1346054\"><code>@\u200ba1346054</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2496\">codespell-project/codespell#2496</a></li>\n<li>move <code>warmup</code> to code dictionary by <a href=\"https://github.com/robin-wayve\"><code>@\u200brobin-wayve</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2495\">codespell-project/codespell#2495</a></li>\n<li>downoload-&gt;download and friends by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2494\">codespell-project/codespell#2494</a></li>\n<li>Add some alternative line endings and whitespace checks by <a href=\"https://github.com/peternewman\"><code>@\u200bpeternewman</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2490\">codespell-project/codespell#2490</a></li>\n<li>occulusion-&gt;occlusion by <a href=\"https://github.com/tkoyama010\"><code>@\u200btkoyama010</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2502\">codespell-project/codespell#2502</a></li>\n<li>Add another suggestion for relected by <a href=\"https://github.com/vinc17fr\"><code>@\u200bvinc17fr</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2501\">codespell-project/codespell#2501</a></li>\n<li>add dependendenc(y|ies)-&gt;dependenc(y|ies) by <a href=\"https://github.com/robin-wayve\"><code>@\u200brobin-wayve</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2499\">codespell-project/codespell#2499</a></li>\n<li>Add application(s) spelling corrections. by <a href=\"https://github.com/cfi-gb\"><code>@\u200bcfi-gb</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2503\">codespell-project/codespell#2503</a></li>\n<li>Add spelling correction for place by <a href=\"https://github.com/adrien-berchet\"><code>@\u200badrien-berchet</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2500\">codespell-project/codespell#2500</a></li>\n<li>Add spelling correction by <a href=\"https://github.com/adrien-berchet\"><code>@\u200badrien-berchet</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2498\">codespell-project/codespell#2498</a></li>\n<li>'analyses' is the plural of 'analysis' by <a href=\"https://github.com/quyykk\"><code>@\u200bquyykk</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2401\">codespell-project/codespell#2401</a></li>\n<li>Add spelling corrections for dictionar(y|ies) by <a href=\"https://github.com/cfi-gb\"><code>@\u200bcfi-gb</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2509\">codespell-project/codespell#2509</a></li>\n<li>Add spelling correction for export by <a href=\"https://github.com/adrien-berchet\"><code>@\u200badrien-berchet</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2506\">codespell-project/codespell#2506</a></li>\n<li>Fix <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2055\">#2055</a>: Add support for PEP 518 by <a href=\"https://github.com/Freed-Wu\"><code>@\u200bFreed-Wu</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2290\">codespell-project/codespell#2290</a></li>\n<li>Add regresison-&gt;regression by <a href=\"https://github.com/janosh\"><code>@\u200bjanosh</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2518\">codespell-project/codespell#2518</a></li>\n<li>DOC: Better docs by <a href=\"https://github.com/larsoner\"><code>@\u200blarsoner</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2515\">codespell-project/codespell#2515</a></li>\n<li>Fix build_exclude_hashes fails: UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 2244: character maps to <!-- raw HTML omitted --> by <a href=\"https://github.com/JoergAtGithub\"><code>@\u200bJoergAtGithub</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2520\">codespell-project/codespell#2520</a></li>\n<li>Add alternative for 'bu' by <a href=\"https://github.com/flyingdutchman23\"><code>@\u200bflyingdutchman23</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2514\">codespell-project/codespell#2514</a></li>\n<li>Add alternative for 'tooo' by <a href=\"https://github.com/flyingdutchman23\"><code>@\u200bflyingdutchman23</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2513\">codespell-project/codespell#2513</a></li>\n<li>Fix uncaught exception on empty files by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2195\">codespell-project/codespell#2195</a></li>\n<li>Add combintaion(s)-&gt;combination(s) by <a href=\"https://github.com/janosh\"><code>@\u200bjanosh</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2524\">codespell-project/codespell#2524</a></li>\n<li>MAINT: Use pyproject and setuptools_scm by <a href=\"https://github.com/larsoner\"><code>@\u200blarsoner</code></a> in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2523\">codespell-project/codespell#2523</a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/ydah\"><code>@\u200bydah</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2467\">codespell-project/codespell#2467</a></li>\n<li><a href=\"https://github.com/adrien-berchet\"><code>@\u200badrien-berchet</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2461\">codespell-project/codespell#2461</a></li>\n<li><a href=\"https://github.com/danielhoherd\"><code>@\u200bdanielhoherd</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2493\">codespell-project/codespell#2493</a></li>\n<li><a href=\"https://github.com/quyykk\"><code>@\u200bquyykk</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2401\">codespell-project/codespell#2401</a></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/055cb2a9bc1e98a9800a352c53c38a50faf62dc3\"><code>055cb2a</code></a> FIX: No TestPyPI</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/aa50a992c52911219a7c0b71a856bd1c6c2b2857\"><code>aa50a99</code></a> FIX: Trailing slash</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/4bb2bcffa004609ac10f9c75d9c10032611a8c04\"><code>4bb2bcf</code></a> FIX: Need path</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/b07b69314df0a7c972fd0b25838cc9d27733ba17\"><code>b07b693</code></a> FIX: Artifact</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/754b9a52f7bb83d8d356050a14caebc6fbaccc85\"><code>754b9a5</code></a> MAINT: Use pyproject and setuptools_scm (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2523\">#2523</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/3f06fb0b080c334cda9e0918d53b34d29b2f3e4b\"><code>3f06fb0</code></a> Add combintaion(s)-&gt;combination(s) (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2524\">#2524</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/900f18654b570df28365d07a8908e7cf01917ac9\"><code>900f186</code></a> Fix encoding detection and exception on empty files (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2195\">#2195</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/ad644529d235bfe19974750d03f7a511d7d08262\"><code>ad64452</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2513\">#2513</a> from flyingdutchman23/topic/tooo</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/8ab9f5cd68c7f335b9ab6582a7a2b3b27048ddf0\"><code>8ab9f5c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2514\">#2514</a> from flyingdutchman23/topic/bu</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/5aa40824e6659a35cf2072ecfa9f78392664cc64\"><code>5aa4082</code></a> Provide more possible corrections</li>\n<li>Additional commits viewable in <a href=\"https://github.com/codespell-project/codespell/compare/v2.2.1...v2.2.2\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=codespell&package-manager=pip&previous-version=2.2.1&new-version=2.2.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-10-17T13:12:32Z",
        "closed_at": "2022-10-18T10:55:49Z",
        "merged_at": "2022-10-18T10:55:49Z",
        "body": "In function `keyIsExpired`,  it's designed to not expire any key while loading. But I'm confused why check if server is loading after `getExpire`, which seems not necessary to getExpire in advance.\r\n\r\nso I adjust the sequence of it and pls let me know if there's anything missing. thanks!",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-10-15T06:54:06Z",
        "closed_at": "2022-10-16T04:51:44Z",
        "merged_at": "2022-10-16T04:51:44Z",
        "body": "* Fixes build warning when CACHE_LINE_SIZE is already defined\r\n* Fixes wrong CACHE_LINE_SIZE on some FreeBSD systems where it could be set to 128 (e.g. on MIPS)\r\n* Fixes wrong CACHE_LINE_SIZE on Apple M1 (use 128 instead of 64)\r\n\r\nWrong cache line size in that case can some false sharing of array elements between threads, see #10892",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 102,
        "deletions": 23,
        "changed_files": 3,
        "created_at": "2022-10-13T07:00:22Z",
        "closed_at": "2022-10-22T17:36:50Z",
        "merged_at": "2022-10-22T17:36:50Z",
        "body": "In the module, we will reuse the list iterator entry for RM_ListDelete, but `listTypeDelete` will only update\r\n`quicklistEntry->zi` but not `quicklistEntry->node`, which will result in `quicklistEntry->node` pointing to\r\na freed memory address if the quicklist node is deleted. \r\n\r\nThis PR sync `key->u.list.index` and `key->u.list.entry` to list iterator after `RM_ListDelete`.\r\n\r\nThis PR also optimizes the release code of the original list iterator.\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 92,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2022-10-12T07:15:59Z",
        "closed_at": "2022-10-16T05:30:01Z",
        "merged_at": "2022-10-16T05:30:01Z",
        "body": "As discussed on https://github.com/redis/redis/pull/11084#discussion_r948972838 `propagatePendingCommands` should happened after the del notification is fired so that the notification effect and the `del` will be replicated inside MULTI EXEC.\r\n\r\nTest was added to verify the fix.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 75,
        "deletions": 4,
        "changed_files": 6,
        "created_at": "2022-10-12T02:42:46Z",
        "closed_at": "2022-11-04T16:46:38Z",
        "merged_at": "2022-11-04T16:46:38Z",
        "body": "Introduce socket `shutdown()` into connection type, and use it\r\non normal socket if a fork is active. This allows us to close\r\nclient connections when there are child processes sharing the\r\nfile descriptors.\r\n\r\nFixes #10077. The reason is that since the `fork()` child is holding\r\nthe file descriptors, the `close` in `unlinkClient -> connClose`\r\nisn't sufficient. The client will not realize that the connection is\r\ndisconnected until the child process ends.\r\n\r\nLet's try to be conservative and only use shutdown when the fork is active.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 185,
        "deletions": 31,
        "changed_files": 7,
        "created_at": "2022-10-11T12:11:39Z",
        "closed_at": "2022-10-18T16:45:46Z",
        "merged_at": "2022-10-18T16:45:46Z",
        "body": "### Background\r\n\r\nAs describe here: https://github.com/redis/redis/pull/11056\r\n\r\nThe issue is that when saving an RDB with module AUX data, the module AUX metadata (moduleid, when, ...) is saved to the RDB even though the module did not saved any actual data. This prevent loading the RDB in the absence of the module (although there is no actual data in the RDB that requires the module to be loaded).\r\n\r\nOn https://github.com/redis/redis/pull/11056, the solution suggested was the ignore AUX data if the module is not loaded. This solution is wrong because we have data that we ignore and allow the server to start without it.\r\n\r\nOther solution that was suggested is to provide a new module API to save optional AUX fields (new RDB format). This solution will solve the problem but it is probably more then we actually need.\r\n\r\n### Solution\r\n\r\nThe solution suggested in this PR is that module AUX will be saved on the RDB only if the module actually saved something during `aux_save` function.\r\n\r\nTo support backward compatibility, we introduce `aux_save2` callback that acts the same as `aux_save` with the tiny change of avoid saving the aux field if no data was actually saved by the module. Modules can use the new API to make sure that if they have no data to save, then it will be possible to load the created RDB even without the module.\r\n\r\n### Concerns\r\n\r\nA module may register for the aux load and save hooks just in order to be notified when saving or loading starts or completed (there are better ways to do that, but it still possible that someone used it).\r\n\r\nHowever, if a module didn't save a single field in the save callback, it means it's not allowed to read in the read callback, since it has no way to distinguish between empty and non-empty payloads. furthermore, it means that if the module did that, it must never change it, since it'll break compatibility with it's old RDB files, so this is really not a valid use case.\r\n\r\nSince some modules (ones who currently save one field indicating an empty payload), need to know if saving an empty payload is valid, and if Redis is gonna ignore an empty payload or store it, we opted to add a new API (rather than change behavior of an existing API and expect modules to check the redis version)\r\n\r\n### Technical Details\r\n\r\nTo avoid saving AUX data on RDB, we change the code to first save the AUX metadata (moduleid, when, ...) into a temporary buffer. The buffer is then flushed to the rio at the first time the module makes a write operation inside the `aux_save` function. If the module saves nothing (and `aux_save2` was used), the entire temporary buffer is simply dropped and no data about this AUX field is saved to the RDB. This make it possible to load the RDB even in the absence of the module.\r\n\r\nTest was added to verify the fix.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 23,
        "changed_files": 1,
        "created_at": "2022-10-11T04:09:40Z",
        "closed_at": "2022-10-11T08:39:38Z",
        "merged_at": "2022-10-11T08:39:38Z",
        "body": "Redis commands has been significantly refactored in 7.0.\r\nThis PR updates the outdated README.md to reflect it.\r\n\r\nBased on #10864, doing some additional cleanups.\r\n\r\nCo-authored-by: theoboldalex <theoboldalex@gmail.com>",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2022-10-10T14:13:39Z",
        "closed_at": "2022-10-11T06:17:56Z",
        "merged_at": "2022-10-11T06:17:56Z",
        "body": "remove double period at the end of sentence.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-10-09T11:42:17Z",
        "closed_at": "2022-11-14T00:45:55Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-10-08T11:34:36Z",
        "closed_at": "2022-10-09T09:17:36Z",
        "merged_at": "2022-10-09T09:17:36Z",
        "body": "Forgot to start redis-server when testing performance. When opening the benchmark for testing, it will always be stuck, and the process cpu will reach 100%.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-10-08T04:09:45Z",
        "closed_at": "2022-10-08T07:16:48Z",
        "merged_at": "2022-10-08T07:16:48Z",
        "body": "```\r\ns/cluster-tls/tls-cluster/g\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-10-07T15:50:09Z",
        "closed_at": "2022-10-07T18:24:54Z",
        "merged_at": "2022-10-07T18:24:54Z",
        "body": "Build fails with warnings in ARM CI after adding more aggressive optimizations (#11350)\r\nprobably a result of more aggressive inlining\r\n\r\n```\r\nziplist.c: In function \u2018pop.constprop\u2019:\r\nziplist.c:1770:13: error: \u2018vlong\u2019 may be used uninitialized in this function [-Werror=maybe-uninitialized]\r\n             printf(\"%lld\", vlong);\r\n             ^~~~~~~~~~~~~~~~~~~~~\r\n```\r\n\r\n```\r\nlistpack.c: In function \u2018lpInsert.constprop\u2019:\r\nlistpack.c:406:9: error: argument 2 null where non-null expected [-Werror=nonnull]\r\n         memcpy(buf+1,s,len);\r\n         ^~~~~~~~~~~~~~~~~~~\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-10-07T08:42:59Z",
        "closed_at": "2022-10-09T11:17:57Z",
        "merged_at": null,
        "body": "Context returned from `RM_GetContextFromIO` doesn't have a client, so when I call `RM_CreateTimer` in `rdb_load` callback, the new timer's `dbid` will default to 0.\r\n\r\nThis PR inits `ctx` with a temp client and proper `dbid`.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-10-06T15:04:15Z",
        "closed_at": "2022-10-13T10:05:21Z",
        "merged_at": "2022-10-13T10:05:21Z",
        "body": "1. show the overcommit warning when overcommit is disabled (2), not just when it is set to heuristic (0).\r\n2. improve warning text to mention the issue with jemalloc causing VM mapping fragmentation when set to 2.\r\n\r\ncloses #10234, closes #10319",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-10-04T04:45:49Z",
        "closed_at": "2022-10-09T09:26:12Z",
        "merged_at": "2022-10-09T09:26:12Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-10-03T15:15:17Z",
        "closed_at": "2022-11-04T17:04:40Z",
        "merged_at": null,
        "body": "Bumps [vmactions/freebsd-vm](https://github.com/vmactions/freebsd-vm) from 0.2.4 to 0.2.9.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vmactions/freebsd-vm/releases\">vmactions/freebsd-vm's releases</a>.</em></p>\n<blockquote>\n<p>Improve rsync</p>\n<p>Add reconnect for sshfs</p>\n<p>Show files tree for debug</p>\n<p>Fix for 13.0 bugs</p>\n<h2>v0.2.5</h2>\n<p>Minor, just using vbox v0.0.1</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/08e4bc422b24209fb6d631c5755cdb4ec2e0b9f3\"><code>08e4bc4</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/5fbf2ff07a68fe3d3e85822f061053c90099026e\"><code>5fbf2ff</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/b5bb1a17d851acf72d40ae764526d8a3a050c782\"><code>b5bb1a1</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/42248a0ea95768c2a32de3adf02c9d2a2e86f3ee\"><code>42248a0</code></a> Update version to v0.2.8</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/4612c18e63b11e07d47168650b7f89f0b0f6764f\"><code>4612c18</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/4715280408f75faf8eb50167dde1310189c214b3\"><code>4715280</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/46442ed0aabd08742997b455ae3aba35bf6659ab\"><code>46442ed</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/56761d53f4feaa71686812212687237ea372c1c3\"><code>56761d5</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/7a96c46abd95558e9c012c222dac7d33736341bb\"><code>7a96c46</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/8aafaca8b1f560cc281dc5456177c6655c8aa80d\"><code>8aafaca</code></a> Update version to v0.2.7</li>\n<li>Additional commits viewable in <a href=\"https://github.com/vmactions/freebsd-vm/compare/v0.2.4...v0.2.9\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vmactions/freebsd-vm&package-manager=github_actions&previous-version=0.2.4&new-version=0.2.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2022-10-03T05:32:19Z",
        "closed_at": "2022-10-06T08:26:20Z",
        "merged_at": "2022-10-06T08:26:20Z",
        "body": "Currently, we add `-flto` to the compile flags only. We are supposed\r\nto add it to the linker flags as well. Clang build fails because of this. \r\n\r\nAdded a change to add `-flto` to  `REDIS_CFLAGS` and `REDIS_LDFLAGS` \r\nif the build optimization flag is `-O3`. (`noopt` build will not use `-flto`)\r\n\r\nIntroduced by https://github.com/redis/redis/pull/11207",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-10-03T04:18:16Z",
        "closed_at": "2022-10-03T06:21:42Z",
        "merged_at": "2022-10-03T06:21:42Z",
        "body": "There is a race condition in the test:\r\n```\r\n*** [err]: redis-cli --cluster add-node with cluster-port in tests/unit/cluster/cli.tcl\r\nExpected '5' to be equal to '4' {assert_equal 5 [CI 0 cluster_known_nodes]} proc ::test)\r\n```\r\n\r\nWhen using cli to add node, there can potentially be a race condition\r\nin which all nodes presenting cluster state o.k even though the added\r\nnode did not yet meet all cluster nodes.\r\n\r\nThis comment and the fix were taken from #11221. Also apply it in several\r\nother similar places.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 196,
        "deletions": 61,
        "changed_files": 10,
        "created_at": "2022-10-03T03:40:29Z",
        "closed_at": "2022-12-07T06:26:56Z",
        "merged_at": "2022-12-07T06:26:56Z",
        "body": "## Issue\r\nDuring the client input/output buffer processing, the memory usage is incrementally updated to keep track of clients going beyond a certain threshold `maxmemory-clients` to be evicted. However, this additional tracking activity leads to unnecessary CPU cycles wasted when no client-eviction is required. It is applicable in two cases.\r\n\r\n* `maxmemory-clients` is set to `0` which equates to no client eviction (applicable to all clients)\r\n* `CLIENT NO-EVICT` flag is set to `ON` which equates to a particular client not applicable for eviction.  \r\n\r\n## Solution\r\n* Disable client memory usage tracking during the read/write flow when `maxmemory-clients` is set to `0` or `client no-evict` is `on`. The memory usage is tracked only during the `clientCron` i.e. it gets periodically updated.\r\n* Cleanup the clients from the memory usage bucket when client eviction is disabled.\r\n* When the maxmemory-clients config is enabled or disabled at runtime, we immediately update the memory usage buckets for all clients (tested scanning 80000 took some 20ms)\r\n\r\nBenchmark shown that this can improve performance by about 5% in certain situations.\r\n\r\n## Testing\r\n### Redis Benchmark\r\n```\r\n./redis-benchmark -t set -r 100000 -n 10000000\r\n```\r\n\r\nDuring the above benchmark operation, ran the following CPU profiling command.\r\n```\r\nperf record -g --pid $(pgrep redis-server) -F 999 -- sleep 60\r\n```\r\n\r\n### Machine\r\nc5.4xlarge\r\n\r\n### Observation\r\nThe CPU utilization for `updateClientMemoryUsage` dropped from `2.39%` during write and `1.07%` during read to `0`.\r\n\r\n### FlameGraph on the current branch\r\n![FlameGraph on the current branch](https://gist.githubusercontent.com/hpatro/08d97e3b64d4679a661deb85d477be1c/raw/15af02da86cbdc641b795f98cccdd72bfbc09475/redis-opt.svg)\r\n\r\n### FlameGraph on the unstable branch\r\n![FlameGraph on the unstable branch](https://gist.githubusercontent.com/hpatro/08d97e3b64d4679a661deb85d477be1c/raw/15af02da86cbdc641b795f98cccdd72bfbc09475/redis-unstable.svg)",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 62,
        "deletions": 19,
        "changed_files": 6,
        "created_at": "2022-10-02T08:04:46Z",
        "closed_at": "2022-10-12T10:09:51Z",
        "merged_at": "2022-10-12T10:09:51Z",
        "body": "PR #9320 introduces initialization order changes. Now cluster is initialized after modules.\r\nThis changes causes a crash if the module uses RM_Call inside the load function\r\non cluster mode (the code will try to access `server.cluster` which at this point is NULL).\r\n\r\nTo solve it, separate cluster initialization into 2 phases:\r\n1. Structure initialization that happened before the modules initialization\r\n2. Listener initialization that happened after.\r\n\r\nTest was added to verify the fix.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2022-10-01T18:46:15Z",
        "closed_at": "2022-10-02T06:29:24Z",
        "merged_at": null,
        "body": "1. I removed some unused variable from code.\r\n2. According to one stack overflow post it is better to use sys.exit(1) insted of exit(1) which working with programs and exit(1) is helper for the interactive shell.\r\n\r\nhttps://stackoverflow.com/questions/6501121/difference-between-exit-and-sys-exit-in-python\r\n\r\nHope This Help ..!!\r\nThank You.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 35,
        "changed_files": 2,
        "created_at": "2022-10-01T07:53:13Z",
        "closed_at": "2022-10-03T09:43:23Z",
        "merged_at": null,
        "body": "Expire the watched key instead of marking it as `expired`.  This PR try to make the `watch` logic simpler and clearer.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 467,
        "deletions": 361,
        "changed_files": 9,
        "created_at": "2022-09-30T22:04:32Z",
        "closed_at": "2022-11-02T02:26:44Z",
        "merged_at": "2022-11-02T02:26:44Z",
        "body": "Resolves https://github.com/redis/redis/issues/10863\r\n\r\nThis change is a performance improvement that replaces the sds cluster link send buffer with a linked list to prevent high pub/sub traffic from leading to large `memmove` operations that takeover the CPU. The overall memory management is also improved for broadcasted cluster bus messages which are now shared amongst cluster link send buffers rather than each one having a separate copy.\r\n\r\nThe only functionality change is around `cluster-link-sendbuf-limit`. Because broadcasted messages are now shared memory amongst cluster link send buffers, hitting this limit and freeing the link will lead to a reduction in memory anywhere between nearly 0 to `cluster-link-sendbuf-limit` bytes. However, it still puts the same upper bound on the node's overall cluster link send buffer memory.\r\n\r\nAnother side effect is that the `send-buffer-allolcated` and `send-buffer-used` fields accessed with `CLUSTER LINKS` will always be equivalent. We've left them as is for now to avoid a backwards breaking change.\r\n\r\nCPU profiling over 5 minutes during high pub/sub traffic with the old sds send buffer:\r\n![sds-cluster-link-snd-buf](https://user-images.githubusercontent.com/31714723/193153483-36da341f-53e2-4ae9-934b-9850589797bc.svg)\r\n\r\nCPU profiling during the same traffic with the new linked list send buffer:\r\n![linked-list-cluster-link-snd-buf](https://user-images.githubusercontent.com/31714723/193154870-93006b6e-b52f-4d42-900e-6150a5faccf6.jpeg)\r\n\r\nNote that in both cases the `cluster-link-sendbuf-limit` was 1GB which was repeatedly hit.\r\n\r\n```\r\nRelease notes\r\nImprove memory management of clusterbus links when there is a large number of pending messages.\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-09-30T09:55:50Z",
        "closed_at": "2022-09-30T13:24:46Z",
        "merged_at": "2022-09-30T13:24:46Z",
        "body": "We will always show the bus-port, and if the node hostname exists, it will also show it.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2022-09-29T08:44:20Z",
        "closed_at": "2022-10-06T09:12:05Z",
        "merged_at": "2022-10-06T09:12:05Z",
        "body": "Refine getTimeoutFromObjectOrReply() out-of-range check.\r\n\r\nTimeout is parsed (and verifies out of range) as double and \r\nmultiplied by 1000, added mstime() and stored in long-long \r\nwhich might lead to out-of-range value of long-long.\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2022-09-29T01:25:29Z",
        "closed_at": "2022-09-29T07:44:49Z",
        "merged_at": null,
        "body": "Some modules internally use asynchronous threads to exec some tasks, and these asynchronous threads may use some locks to solve concurrency problems. We hope that these modules can be notified before redis do the fork, so that they can block waiting for the asynchronous task to complete and release the lock, otherwise there may be deadlock in the child process.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2022-09-28T21:00:28Z",
        "closed_at": "2022-10-02T13:36:31Z",
        "merged_at": "2022-10-02T13:36:31Z",
        "body": "Adds stack trace and register dump support in crash report for illumos/solaris",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-09-28T20:11:28Z",
        "closed_at": "2022-09-29T06:33:37Z",
        "merged_at": "2022-09-29T06:33:37Z",
        "body": "Now commands:\r\n\r\n**sentinel config set parameter value\r\nsentinel config get parameter**\r\n\r\ndo not support multiply parameters, so this PR changes remove the **multiply** keywords in order to keep consistent with \r\ncurrent feature.\r\n\r\nHowever, @moticless do you think we should support multiply parameters later? ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-09-27T09:40:15Z",
        "closed_at": "2022-09-27T13:20:13Z",
        "merged_at": "2022-09-27T13:20:13Z",
        "body": "SunOS/Solaris and its relatives don't support the flock() function. While \"redis\" has been excluding setting up the lock using flock() on the cluster configuration file in https://github.com/redis/redis/blob/unstable/src/cluster.c#L502 when compiling under Solaris, it was still using flock() in the unlock call while shutting down. \r\n\r\nThis pull request eliminates the flock() call also in the unlocking stage for Oracle Solaris and its relatives.\r\n\r\nFix compilation regression from #10912",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 38,
        "changed_files": 3,
        "created_at": "2022-09-27T08:06:41Z",
        "closed_at": "2022-09-28T18:07:39Z",
        "merged_at": "2022-09-28T18:07:39Z",
        "body": "Mainly fix two minor bug\r\n1. When handle BL*POP/BLMOVE commands with blocked clients, we should increment server.dirty.\r\n2.  `listPopRangeAndReplyWithKey()` in `serveClientBlockedOnList()` should not repeat calling `signalModifiedKey()` which has been called when an element was pushed on the list.\r\n\r\nOther optimization\r\nadd `signal` param for `listElementsRemoved` to skip `signalModifiedKey()` to unify all pop operation.\r\n\r\nUnifying all pop operations also prepares us for #11303, so that we can avoid having to deal with the conversion from quicklist to listpack() in various places when the list shrinks.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 118,
        "deletions": 152,
        "changed_files": 10,
        "created_at": "2022-09-26T13:46:38Z",
        "closed_at": "2022-09-28T09:40:33Z",
        "merged_at": null,
        "body": "The json files already provide basic arity information",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2022-09-26T12:22:25Z",
        "closed_at": "2022-09-26T13:48:08Z",
        "merged_at": null,
        "body": "We should not use lpFind, there's no point in\r\ncomparing the score elements against the element\r\nname",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2022-09-26T08:07:37Z",
        "closed_at": "2022-09-28T11:15:08Z",
        "merged_at": "2022-09-28T11:15:08Z",
        "body": "The original idea behind auto-setting the default (first,last,step) spec was to use the most \"open\" flags when the user didn't provide any key-spec flags information.\r\n\r\nWhile the above idea is a good approach, it really makes no sense to set CMD_KEY_VARIABLE_FLAGS if the user didn't provide the getkeys-api flag: in this case there's not way to retrieve these variable flags, so what's the point?\r\n\r\nInternally in redis there was code to ignore this already, so this fix doesn't change redis's behavior, it only affects the output of COMMAND command.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-09-25T20:36:24Z",
        "closed_at": "2022-09-26T07:03:45Z",
        "merged_at": "2022-09-26T07:03:45Z",
        "body": "If a command gets an OOM response and then if we set maxmemory to zero\r\nto disable the limit, `server.pre_command_oom_state` never gets updated\r\nand it stays true. As `RM_Call()` calls with \"respect deny-oom\" flag checks \r\n`server.pre_command_oom_state`, all calls will fail with OOM.\r\n\r\nAdded `server.maxmemory` check in `RM_Call()` to process deny-oom flag\r\nonly if maxmemory is configured.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 301,
        "deletions": 133,
        "changed_files": 11,
        "created_at": "2022-09-22T17:30:55Z",
        "closed_at": "2022-10-18T16:50:03Z",
        "merged_at": "2022-10-18T16:50:03Z",
        "body": "The use case is a module that wants to implement a blocking command on a key that necessarily exists and wants to unblock the client in case the key is deleted (much like what we implemented for XREADGROUP in #10306)\r\n\r\nNew module API:\r\n* RedisModule_BlockClientOnKeysWithFlags\r\n\r\nFlags:\r\n* REDISMODULE_BLOCK_UNBLOCK_NONE\r\n* REDISMODULE_BLOCK_UNBLOCK_DELETED\r\n\r\n### Detailed description of code changes\r\n\r\nblocked.c:\r\n1. Both module and stream functions are called whether the key exists or not, regardless of its type. We do that in order to allow modules/stream to unblock the client in case the key is no longer present or has changed type (the behavior for streams didn't change, just code that moved into serveClientsBlockedOnStreamKey)\r\n2. Make sure afterCommand is called in serveClientsBlockedOnKeyByModule, in order to propagate actions from moduleTryServeClientBlockedOnKey.\r\n3. handleClientsBlockedOnKeys: call propagatePendingCommands directly after lookupKeyReadWithFlags to prevent a possible lazy-expire DEL from being mixed with any command propagated by the preceding functions.\r\n4. blockForKeys: Caller can specifiy that it wants to be awakened if key is deleted.\r\n   Minor optimizations (use dictAddRaw).\r\n5. signalKeyAsReady became signalKeyAsReadyLogic which can take a boolean in case the key is deleted. It will only signal if there's at least one client that awaits key deletion (to save calls to handleClientsBlockedOnKeys).\r\n   Minor optimizations (use dictAddRaw)\r\n\r\ndb.c:\r\n1. scanDatabaseForDeletedStreams is now scanDatabaseForDeletedKeys and will signalKeyAsReady for any key that was removed from the database or changed type. It is the responsibility of the code in blocked.c to ignore or act on deleted/type-changed keys.\r\n2. Use the new signalDeletedKeyAsReady where needed\r\n\r\nblockedonkey.c + tcl:\r\n1. Added test of new capabilities (FSL.BPOPGT now requires the key to exist in order to work)",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 10,
        "changed_files": 6,
        "created_at": "2022-09-22T10:27:50Z",
        "closed_at": "2022-10-03T06:25:16Z",
        "merged_at": "2022-10-03T06:25:16Z",
        "body": "This PR introduces a couple of changes to improve cluster test stability:\r\n1. Increase the cluster node timeout to 3 seconds, which is similar to the normal cluster tests, but introduce a new mechanism to increase the ping period so that the tests are still fast. This new config is a debug config.\r\n2. Set `cluster-replica-no-failover yes` on a wider array of tests which are sensitive to failovers. This was occurring on the ARM CI.\r\n\r\nShould hopefully resolve failures like: https://github.com/redis/redis-extra-ci/actions/runs/2896719317/jobs/4607730641.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-09-22T09:58:34Z",
        "closed_at": "2023-04-18T13:19:36Z",
        "merged_at": null,
        "body": "Fix how the child process exists inside `checkLinuxMadvFreeForkBug` since we do not want the forked child to delete any threads that some module might use etc. Hence  we do not want it to call `exit()` that will do additional cleanups before terminating the process and we better use` _exit()` that does not perform any such cleaning operations",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 834,
        "deletions": 280,
        "changed_files": 20,
        "created_at": "2022-09-22T08:31:48Z",
        "closed_at": "2022-11-16T18:29:46Z",
        "merged_at": "2022-11-16T18:29:46Z",
        "body": "Implement #11156\r\n\r\n## Description of the feature\r\nThe new listpack encoding uses the old `list-max-listpack-size` config to perform the conversion, which we can think it of as a node inside a quicklist, but without 80 bytes overhead (internal fragmentation included) of quicklist and quicklistNode structs.\r\nFor example, a list key with 5 items of 10 chars each, now takes 128 bytes instead of 208 it used to take.\r\n\r\n## Conversion rules\r\n* Convert listpack to quicklist\r\n  When the listpack length or size reaches the `list-max-listpack-size` limit, it will be converted to a quicklist.\r\n* Convert quicklist to listpack\r\n  When a quicklist has only one node, and its length or size is reduced to half of the `list-max-listpack-size` limit, it will be converted to a listpack.\r\n    This is done to avoid frequent conversions when we add or remove at the bounding size or length.\r\n    \r\n## Interface changes\r\n1. add list entry param to listTypeSetIteratorDirection\r\n    When list encoding is listpack, `listTypeIterator->lpi` points to the next entry of current entry,\r\n    so when changing the direction, we need to use the current node (listTypeEntry->p) to \r\n    update `listTypeIterator->lpi` to the next node in the reverse direction.\r\n\r\n## Benchmark\r\nHW: Cloud server, Intel Xeon Cooper Lake, 8cores, 16G mem, Ubuntu 22, GCC 11.3\r\n\r\n### Listpack VS Quicklist with one node\r\n* LPUSH\r\nUse `redis-benchmark -n 1000000 -r 10000000 -q eval 'for i=1,N,1 do redis.call(\"rpush\", \"lst:\"..ARGV[1], \"hellohello\") end' 0 __rand_int__` to create 1M lists with N entries of 10 chars.\r\n\r\n|  N  |  Unstable (quicklist)| This PR (listpack) |  Difference|\r\n|  ----| ----| ----| ----|\r\n|    10  | 71515.41  (p50=0.663) | 71694.87 (p50=0.655) | +0.25% |\r\n|    30  | 39987.20 (p50=1.303) | 40109.10 (p50=1.303) | +0.3% |\r\n|     50  | 27913.47  (p50=1.951) | 27824.15 (p50=1.959) | -0.3% |\r\n\r\n* LRANGE\r\n\r\nDataset Preparation: Use `redis-benchmark -n 600 -q RPUSH mylist hello` to create a list with 600 entries of 10 chars.\r\n\r\n`memtier_benchmark --hide-histogram -n 50000 --pipeline=10 --command=<COMMAND>`\r\n|  Command  |  Unstable (listpack) | This PR (listpack) |  Difference|\r\n|  ----| ----| ----| ----|\r\n|  LRANGE mylist 0 100 | 164007.37 (p50=12.67100) | 186012.67 (p50=10.94300) | +13.4%|\r\n|  LRANGE mylist 0 300 | 65296.66 (p50=32.12700) | 74293.38 (p50=28.79900) | +13.7% |\r\n|  LRANGE mylist 0 500 | 41626.12 (p50=50.94300) | 47235.58 (p50=45.82300) | +13.4% |\r\n|  LRANGE mylist 0 600 | 34868.03 (p50=60.92700) | 39477.82 (p50=55.03900) | +13.2% |\r\n\r\n### Both are quicklist\r\n\r\n* LRANGE\r\n\r\nDataset Preparation: Use `redis-benchmark -q -n 1000000 RPUSH mylist hello` to create a list with 1M entries.\r\n\r\nWithout pipeline\r\n`memtier_benchmark --hide-histogram -n 50000 --command=<COMMAND>`\r\n|  Command  |  Unstable (Quicklist) | This PR (Quicklist) |  Difference|\r\n|  ----| ----| ----| ----|\r\n|  LRANGE mylist 0 100 | 89279.56 (p50=2.20700) | 93242.59 (p50=2.04700) | +4.4% |\r\n|  LRANGE mylist 0 300 | 47908.40 (p50=3.99900) | 50459.88 (p50=3.80700) | +5.3% |\r\n|  LRANGE mylist 0 500 | 34696.12 (p50=5.69500) | 35462.93 (p50=5.50300) | +2.2% |\r\n|  LRANGE mylist 0 600 | 29447.42 (p50=6.65500) | 30381.95 (p50=6.36700) | +3.1% |\r\n\r\nWith pipeline\r\n`memtier_benchmark --hide-histogram -n 50000 --pipeline=10 --command=<COMMAND>`\r\n|  Command  |  Unstable (Quicklist) | This PR (Quicklist) |  Difference|\r\n|  ----| ----| ----| ----|\r\n|  LRANGE mylist 0 100 | 167135.03 (p50=12.67100) | 172968.38 (p50=11.96700) | +3.4%|\r\n|  LRANGE mylist 0 300 | 65079.61 (p50=32.38300) | 68443.57 (p50=31.10300) | +5.1% |\r\n|  LRANGE mylist 0 500 | 41846.02 (p50=51.19900) | 42757.76 (p50=49.66300) | +2.1% |\r\n|  LRANGE mylist 0 600 | 34865.95 (p50=60.92700) | 36438.67 (p50=59.39100) | +4.5% |\r\n\r\n* Other commands\r\n* `redis-benchmark -q -n 1000000 -t LRANGE COMMAND`\r\n\r\n|  Command  |  Unstable (Quicklist) | This PR (Quicklist) |  Difference|\r\n|  ----| ----| ----| ----|\r\n|  LPOP mylist | 126519.82 (p50=0.199) | 127554.27 (p50=0.199) | +0.8% |\r\n|  RPOP mylist | 127140.73 (p50=0.199) | 126363.14 (p50=0.199) | -0.6% |\r\n| LSET mylist 250000 a  | 126984.12 (p50=0.327) | 126454.23 (p50=0.327) | -0.4% |\r\n\r\nFrom the benchmark, as we can see from the results\r\n1. When list is quicklist encoding, LRANGE improves performance by <5%.\r\n2. When list is listpack encoding, LRANGE improves performance by ~13%, the main enhancement is brought by `addListListpackRangeReply()`.\r\n\r\n## Memory usage\r\nQuote from @zuiderkwast\r\nUse a modified version of `debug populate` to create 1M lists(key:0~key:1000000) with 5 items of 10 chars (\"hellohello\") each.\r\nChecked using `./redis-cli info memory | grep ^used_memory_human` shows memory usage down by 35.49%.\r\n|  dataset   | Unstable(quicklist)  |  This PR (listpack) |  Difference|\r\n|  ----         | ----          | ----        | ----           |\r\n|    1M lists with 5 items of 10 chars  | 214.86M | 138.59M | -35.49% |\r\n\r\n## Note\r\n1. Add conversion callback to support doing some work before conversion\r\n    Since the quicklist iterator decompresses the current node when it is released, we can \r\n    no longer decompress the quicklist after we convert the list.",
        "comments": 25
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-09-22T05:17:24Z",
        "closed_at": "2022-09-22T08:55:54Z",
        "merged_at": "2022-09-22T08:55:53Z",
        "body": "Executing an XAUTOCLAIM command on a stream key in a specific state, with a specially crafted COUNT argument may cause an integer overflow, a subsequent heap overflow, and potentially lead to remote code execution. The problem affects Redis versions 7.0.0 or newer.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 782,
        "deletions": 332,
        "changed_files": 67,
        "created_at": "2022-09-21T15:08:50Z",
        "closed_at": "2022-09-21T19:42:01Z",
        "merged_at": "2022-09-21T19:42:01Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-35951) Executing a XAUTOCLAIM command on a stream key in a specific\r\n  state, with a specially crafted COUNT argument, may cause an integer overflow,\r\n  a subsequent heap overflow, and potentially lead to remote code execution.\r\n  The problem affects Redis versions 7.0.0 or newer\r\n  [reported by Xion (SeungHyun Lee) of KAIST GoN].\r\n\r\nModule API changes\r\n==================\r\n\r\n* Fix RM_Call execution of scripts when used with M/W/S flags to properly\r\n  handle script flags (#11159)\r\n* Fix RM_SetAbsExpire and RM_GetAbsExpire API registration (#11025, #8564)\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Fix a hang when eviction is combined with lazy-free and maxmemory-eviction-tenacity is set to 100 (#11237)\r\n* Fix a crash when a replica may attempt to set itself as its master as a result of a manual failover (#11263)\r\n* Fix a bug where a cluster-enabled replica node may permanently set its master's hostname to '?' (#10696)\r\n* Fix a crash when a Lua script returns a meta-table (#11032)\r\n\r\nFixes for issues in previous releases of Redis 7.0\r\n--------------------------------------------------\r\n\r\n* Fix redis-cli to do DNS lookup before sending CLUSTER MEET (#11151)\r\n* Fix crash when a key is lazy expired during cluster key migration (#11176)\r\n* Fix AOF rewrite to fsync the old AOF file when a new one is created (#11004)\r\n* Fix some crashes involving a list containing entries larger than 1GB (#11242)\r\n* Correctly handle scripts with a non-read-only shebang on a cluster replica (#11223)\r\n* Fix memory leak when unloading a module (#11147)\r\n* Fix bug with scripts ignoring client tracking NOLOOP (#11052)\r\n* Fix client-side tracking breaking protocol when FLUSHDB / FLUSHALL / SWAPDB is used inside MULTI-EXEC (#11038)\r\n* Fix ACL: BITFIELD with GET and also SET / INCRBY can be executed with read-only key permission (#11086)\r\n* Fix missing sections for INFO ALL when also requesting a module info section (#11291)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 58,
        "deletions": 22,
        "changed_files": 1,
        "created_at": "2022-09-21T13:39:33Z",
        "closed_at": "2022-09-29T05:58:58Z",
        "merged_at": "2022-09-29T05:58:58Z",
        "body": "If Redis crashes due to calling an invalid function pointer, the `backtrace` function will try to de-reference this invalid pointer which will cause a crash inside the crash report and will kill the processes without having all the crash report information.\r\n\r\nExample:\r\n\r\n```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n198672:M 19 Sep 2022 18:06:12.936 # Redis 255.255.255 crashed by signal: 11, si_code: 1\r\n198672:M 19 Sep 2022 18:06:12.936 # Accessing address: 0x1\r\n198672:M 19 Sep 2022 18:06:12.936 # Crashed running the instruction at: 0x1\r\n// here the processes is crashing\r\n```\r\n\r\nThis PR tries to fix this crash by:\r\n1. Identify the issue when it happened.\r\n2. Replace the invalid pointer with a pointer to some dummy function so that `backtrace` will not crash.\r\n\r\nThe identification is done by comparing `eip` to `info->si_addr`, if they are the same we know that the crash happened on the same address it tries to accesses and we can conclude that it tries to call and invalid function pointer.\r\n\r\nTo replace the invalid pointer we introduce a new function, `setAndGetMcontextEip`, which replaces `getMcontextEip` and allow to set the Eip for the different supported OS's. After printing the trace we retrieve the old `Eip` value.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-09-21T11:02:04Z",
        "closed_at": "2022-09-22T18:39:34Z",
        "merged_at": "2022-09-22T18:39:34Z",
        "body": "In #10290, we changed clusterNode hostname from `char*` to `sds`, and the old `node->hostname` was changed to `sdslen(node->hostname)!=0`.\r\n\r\nBut in `addNodeDetailsToShardReply` it is missing. It results in the return of an empty string hostname in CLUSTER SHARDS command if it unavailable.\r\n\r\nLike this (note that we listed it as optional in the doc):\r\n```\r\n 9) \"hostname\"\r\n10) \"\"\r\n```\r\n\r\n```\r\nRelease notes:\r\nFix a bug where an empty hostname was displayed in `CLUSTER SHARDS` when no hostname was configured.\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2022-09-21T09:57:30Z",
        "closed_at": "2022-09-22T08:22:05Z",
        "merged_at": "2022-09-22T08:22:05Z",
        "body": "The bug is that the the server keeps on sending newlines to the client. As a result, the receiver might not find the EOF marker since it searches for it only on the end of each payload it reads from the socket. The but only affects `CLIENT_REPL_RDBONLY`.\r\nThis affects `redis-cli --rdb` (depending on timing)\r\n\r\nThe fixed consist of two steps:\r\n1. The `CLIENT_REPL_RDBONLY` should be closed ASAP (we cannot always call to `freeClient` so we use `freeClientAsync`)\r\n2. Add new replication state `SLAVE_STATE_RDB_TRANSMITTED`\r\n\r\nfix #11261\r\n\r\nRelease notes:\r\n> Fix a race which may cause redis-cli --rdb and alike to hang.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-09-21T03:22:40Z",
        "closed_at": "2022-11-15T19:42:17Z",
        "merged_at": null,
        "body": "In Redis 7, there is a new type of module API that supports adding module config and register callback function to handle it.\r\n\r\nFor example:\r\nhttps://redis.io/docs/reference/modules/modules-api-ref/#RedisModule_RegisterBoolConfig\r\n\r\nThe issue is if a module introduces a new config, and the config handler tries to replicate the config, by passing \"CONFIG SET\" to \"RM_Replicate\", the config command will appear twice in the replication stream.\r\n\r\nTo prevent this, the existing function \"preventCommandPropagation\" is called inside \"RM_Replicate\" so the second call of \"alsoPropagate\" in \"call()\" is skipped.\r\n\r\n(This is a straightforward fix; the problem behind is that some improvements are needed for module command handling. Any comments or suggestions would be appreciated.)",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-09-20T15:16:08Z",
        "closed_at": "2022-09-21T05:10:03Z",
        "merged_at": "2022-09-21T05:10:03Z",
        "body": "### Description\r\nWhen using `INFO ALL <section>`, when `section` is a specific module section. \r\nRedis will not print the additional section(s).\r\n\r\nThe fix in this case, will search the modules info sections if the user provided additional sections to `ALL`.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1132,
        "deletions": 344,
        "changed_files": 19,
        "created_at": "2022-09-20T13:33:14Z",
        "closed_at": "2022-11-09T17:50:08Z",
        "merged_at": "2022-11-09T17:50:08Z",
        "body": "Small sets with not only integer elements are listpack encoded, by default up to 128 elements, max 64 bytes per element, new config `set-max-listpack-entries` and `set-max-listpack-value`. This saves memory for small sets compared to using a hashtable.\r\n\r\nSets with only integers, even very small sets, are still intset encoded (up to 1G limit, etc.). Larger sets are hashtable encoded.\r\n\r\nPossible conversions when elements are added:\r\n\r\n    intset -> listpack\r\n    listpack -> hashtable\r\n    intset -> hashtable\r\n\r\nNote: No conversion happens when elements are deleted. If all elements are deleted and then added again, the set is deleted and recreated, thus implicitly converted to a smaller encoding.\r\n\r\nThis PR increments the RDB version, and has an effect on OBJECT ENCODING\r\n\r\nFixes #11155.\r\n\r\n## Benchmark\r\n\r\n`redis-server` started with default config on my laptop.\r\n\r\n`redis-benchmark -P 10 -q --threads 2 -n 10000000 -r 200000 COMMAND ARGS...` was\r\nused for benchmarking, where command and args are given as in the table below.\r\n\r\n### Throughput (requests per second)\r\n\r\n| Command                               | Unstable  | This PR   | Difference  |\r\n|---------------------------------------|-----------|-----------|-------------|\r\n| `sadd __rand_int__ __rand_int__`      | 579407.81 | 655479.81 | +13%        |\r\n| `smembers __rand_int__`               | 83266.02  | 220735.92 | +165%       |\r\n| `sismember __rand_int__ __rand_int__` | 676956.38 | 570580.88 | -16%        |\r\n| `spop __rand_int__`                   | 412014.34 | 563031.38 | +37%        |\r\n\r\n### Memory usage\r\n\r\nMemory usage after the 10M `SADD` commands, resulting in 200k keys with ~50 set\r\nelements per key, checked using `./redis-cli info memory\r\n| grep ^used_memory_human` shows memory usage down by 70%.\r\n\r\n| Command                                  | Unstable  | This PR   | Difference  |\r\n|------------------------------------------|-----------|-----------|-------------|\r\n| Memory usage 200k sets with ~50 elements | 539.98M   | 160.19M   | -70%        |\r\n\r\n### Remarks\r\n\r\nThe keys and the set elements given as `__rand_int__` which results in strings\r\non the form \"000000129245\". Since they have leading zeroes, these strings are\r\nnot integer-encoded in a listpack. With numeric values without leading zeroes,\r\nyou can expect even greater memory savings. Huge memory saving is expected,\r\nsince listpack is a compact representation designed to save memory.\r\n\r\nThe huge speed improvement for `SMEMBERS` is expected, since a listpack is a\r\nsingle allocation and traversing it means very few cache misses. Traversing the\r\nkeys in a hash table OTOH means several memory lookups.\r\n\r\nThe degradation for `SISMEMBER` can be expected, since a hashtable-lookup is\r\nnaturally faster than a linear search through a listpack of ~50 elements.\r\n",
        "comments": 20
    },
    {
        "merged": true,
        "additions": 114,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2022-09-20T09:25:22Z",
        "closed_at": "2022-10-07T17:19:35Z",
        "merged_at": "2022-10-07T17:19:35Z",
        "body": "Description\r\nAdding new ACL metrics to the server, that tracking :\r\n\r\n- user invalid command access\r\n- user invaild key access\r\n- user authentication failure\r\n\r\nThese metrics can give more information about the clients' usage and see if there are server authentication problems.\r\n\r\nThe metrics will be shown when using the command INFO with sections ALL/ACL/\"\"\r\n\r\nAdded 4 new info fields:\r\n```\r\nacl_access_denied_auth: Number of AUTH and HELLO commands rejected from invalid username and passwords.\r\nacl_access_denied_cmd: Number of commands rejected because of insufficient command permissions. \r\nacl_access_denied_key: Number of commands rejected because of insufficient key permissions.\r\nacl_access_denied_channel: Number of commands rejected because of insufficient channel permissions.\r\n```\r\n\r\n```\r\nrelease notes\r\nAdded 4 new info fields for authentication errors and commands denied access for keys, channels and commands.\r\n```",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-09-19T15:15:50Z",
        "closed_at": "2022-10-03T15:15:23Z",
        "merged_at": null,
        "body": "Bumps [vmactions/freebsd-vm](https://github.com/vmactions/freebsd-vm) from 0.2.4 to 0.2.8.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vmactions/freebsd-vm/releases\">vmactions/freebsd-vm's releases</a>.</em></p>\n<blockquote>\n<p>Add reconnect for sshfs</p>\n<p>Show files tree for debug</p>\n<p>Fix for 13.0 bugs</p>\n<h2>v0.2.5</h2>\n<p>Minor, just using vbox v0.0.1</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/4612c18e63b11e07d47168650b7f89f0b0f6764f\"><code>4612c18</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/4715280408f75faf8eb50167dde1310189c214b3\"><code>4715280</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/46442ed0aabd08742997b455ae3aba35bf6659ab\"><code>46442ed</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/56761d53f4feaa71686812212687237ea372c1c3\"><code>56761d5</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/7a96c46abd95558e9c012c222dac7d33736341bb\"><code>7a96c46</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/8aafaca8b1f560cc281dc5456177c6655c8aa80d\"><code>8aafaca</code></a> Update version to v0.2.7</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/db6439863b91bd51e51cd8838d21184fce657f00\"><code>db64398</code></a> more debug info</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/6d531dfbe0ca13dc04f8f3af7b954d24a9ac15f6\"><code>6d531df</code></a> upgrade for ssh keep alive</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/38dbd88aefa49e31752f6e06845f3c1a44c65c07\"><code>38dbd88</code></a> upgrade for &quot;tree&quot;</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/cfc756b9cd8b2d5552b4cfbd8ed691da2f353245\"><code>cfc756b</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li>Additional commits viewable in <a href=\"https://github.com/vmactions/freebsd-vm/compare/v0.2.4...v0.2.8\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vmactions/freebsd-vm&package-manager=github_actions&previous-version=0.2.4&new-version=0.2.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-09-19T14:25:06Z",
        "closed_at": "2022-10-19T12:11:28Z",
        "merged_at": "2022-10-19T12:11:28Z",
        "body": "11 was the size of header/trailer in the old structure Ziplist, but now the size of header/trailer in the new structure Listpack should be 7.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-09-19T05:40:52Z",
        "closed_at": "2022-09-26T09:21:58Z",
        "merged_at": null,
        "body": "Avoid dbAsyncDelete() in performEvictions(). Even if lazy_eviction is configured, it\r\nis inefficient to lazy-free and let the main thread just to wait to memory reduction",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 59,
        "deletions": 0,
        "changed_files": 6,
        "created_at": "2022-09-19T03:05:58Z",
        "closed_at": "2022-09-28T05:35:10Z",
        "merged_at": null,
        "body": "This PR adds `RedisModule_ReplyWithUnsignedLongLong` to the MODULE_API.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 9,
        "changed_files": 7,
        "created_at": "2022-09-18T04:20:12Z",
        "closed_at": "2022-10-02T10:56:45Z",
        "merged_at": "2022-10-02T10:56:45Z",
        "body": "- fix `the the` typo\r\n- `LPOPRPUSH` does not exist, should be `RPOPLPUSH`\r\n- `CLUSTER GETKEYINSLOT` 's time complexity should be O(N)\r\n- `there bytes` should be `three bytes`, this closes #11266\r\n- `slave` word to `replica` in log, modified the front and missed the back\r\n- remove useless aofReadDiffFromParent in server.h\r\n- `trackingHandlePendingKeyInvalidations` method adds a void parameter",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2022-09-18T03:34:41Z",
        "closed_at": "2022-09-22T06:13:39Z",
        "merged_at": "2022-09-22T06:13:39Z",
        "body": "Starting from 6.2, after ACL SETUSER user reset, the user\r\nwill carry the sanitize-payload flag. It was added in #7807,\r\nand then ACL SETUSER reset is inconsistent with default\r\nnewly created user which missing sanitize-payload flag.\r\n\r\nSame as `off` and `on` these two bits are mutually exclusive,\r\nthe default created user needs to have sanitize-payload flag.\r\nAdds USER_FLAG_SANITIZE_PAYLOAD flag to ACLCreateUser.\r\n\r\nNote that the bug don't have any real implications,\r\nsince the code in rdb.c (rdbLoadObject) checks for\r\n`USER_FLAG_SANITIZE_PAYLOAD_SKIP`, so the fact that\r\n`USER_FLAG_SANITIZE_PAYLOAD` is missing doesn't really matters.\r\n\r\nAdded tests to make sure it won't be broken in the future,\r\nand updated the comment in ACLSetUser and redis.conf\r\n\r\nFixes #11278",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-09-16T15:40:24Z",
        "closed_at": "2022-09-16T17:35:44Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2022-09-16T13:01:13Z",
        "closed_at": "2022-09-21T07:11:08Z",
        "merged_at": null,
        "body": "In this PR, `lpAssertValidEntry` is remove from `lpNext`, `lpPrev`, 'lpFirst' and 'lpFind'.\r\nThe element valid is now checked in `lpInsert`.\r\n\r\nA difference can be seen for both HSET and HGET commands.\r\n\r\n----------------------------------\r\nHGETALL a hash with 50 fields of numbers '0' to '50'.\r\n**Unstable** \r\n* hashTypeNext - `19.13%`\r\n* lpAssertValidEntry - `10.2%`\r\n![hgetall_50hash_with](https://user-images.githubusercontent.com/44112901/190641449-04f22f9b-94c9-406a-959d-70a7d613dcb6.svg)\r\n**Check validity on insert**\r\n* hashTypeNext - `10.86%`\r\n* lpAssertValidEntry -`0%`\r\n![hgetall_50hash_wo](https://user-images.githubusercontent.com/44112901/190641478-aa5f6853-e882-4389-9b3b-b0cedd65b6e6.svg)\r\n--------------------------------\r\n\r\nHSET a hash with 50 fields of numbers '0' to '50'.\r\n**Unstable** \r\n* hashTypeSet - `57.72%`\r\n* lpAssertValidEntry - `7.5%`\r\n![hset](https://user-images.githubusercontent.com/44112901/190643661-3885063b-a660-42f9-a8bf-c9968d53ad95.svg)\r\n**Check validity on insert**\r\n* hashTypeSet - `53.61%`\r\n* lpAssertValidEntry -`3.9%`\r\n![hset_assert_on_insert](https://user-images.githubusercontent.com/44112901/190644007-21e0bd81-7794-4654-8a12-d0e9c24702d2.svg)\r\n",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-09-15T06:40:03Z",
        "closed_at": "2022-09-28T10:38:20Z",
        "merged_at": "2022-09-28T10:38:20Z",
        "body": "doing redis-cli --memkeys-samples without any additional arguments would have lead to a crash of the cli.\r\nPR for issue [11267](https://github.com/redis/redis/issues/11267#issuecomment-1247629420)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-09-15T05:56:26Z",
        "closed_at": "2022-10-09T04:42:22Z",
        "merged_at": "2022-10-09T04:42:22Z",
        "body": "Discussed on: https://github.com/redis/redis/pull/9953#issuecomment-1120432030\r\n\r\nAs mentioned on docs, `RM_ResetDataset` Performs similar operation to FLUSHALL. As FLUSHALL do not clean the function, `RM_ResetDataset` should not clean the functions as well.\r\n\r\nThis raise the question, do we want to add a RedisModuleAPI to clean functions or do we think that RM_Call is good enough for this?\r\n\r\nAlso we need to decide if this change is considered a breaking change.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 46,
        "changed_files": 8,
        "created_at": "2022-09-14T07:32:26Z",
        "closed_at": "2022-10-13T11:54:25Z",
        "merged_at": null,
        "body": "In Redis command key specifications, there is an \"INCOMPLETE\" flag. Quote from redis.io:\r\n\r\n> Some commands feature exotic approaches when it comes to specifying their keys, which makes extraction difficult. Consider, for example, what would happen with a call to [MIGRATE](https://redis.io/commands/migrate) that includes the literal string \"KEYS\" as an argument to its AUTH clause. Our key specifications would miss the mark, and extraction would begin at the wrong index.\r\n> \r\n> Thus, we recognize that key specifications are incomplete and may fail to extract all keys. However, we assure that even incomplete specifications never yield the wrong names of keys, providing that the command is syntactically correct.\r\n>\r\n> \u2026\u2026\r\n>\r\n> Note: the only commands with incomplete key specifications are [SORT](https://redis.io/commands/sort) and [MIGRATE](https://redis.io/commands/migrate). We don't expect the addition of such commands in the future.\r\n\r\nFor most commands, the function `getKeysFromCommandWithSpecs` tries to use `key_specs` to extract keys. However, since any command with the \"INCOMPLETE\" flag finally has to use a `get_keys_function` to extract all keys mentioned in the command, we can say this flag is useless.\r\n\r\nAnd since there is only one command having \"INCOMPLETE\" flag, which is `migrate` and \"we don't expect the addition of such commands in the future\", we can totally get rid of this flag, and mark the `begin_search` and `find_keys` in migrate.json as \"unknown\", just like what is written in the sort.json. Then leave the keys extracting to function `migrateGetKeys`, just like the `sort` command.\r\n\r\nAlso, `REDISMODULE_CMD_KEY_INCOMPLETE` is removed to make sure that no more \"INCOMPLETE\" commands from module will be added.\r\n\r\nBTW, both key specifications of command `migrate` and `migrateGetKeys` have bugs in extracting keys. See PR #11253, where this PR is originated from.",
        "comments": 30
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-09-13T16:06:51Z",
        "closed_at": "2022-09-14T00:48:49Z",
        "merged_at": "2022-09-14T00:48:49Z",
        "body": "In a configuration with ~100 cluster nodes, where a machine containing 6 nodes is taken down and our internal recovery machinery kicks in, we have observed a crash.\r\n\r\nOur internal machinery sends CLUSTER FAILOVER TAKOVER to replicas on other machines who lost their masters (TAKEOVER because we need to handle a scenario when we have only two machines and no majority if one goes down btw) and at the same time the cluster is rebalanced. Whether this is a good idea or not can be debated, but we observed a crash which happens when a node detects that it is a replica of a replica of itself.\r\n\r\nThe attempt to avoid being a sub-replica was introduced in #10489. Now, if the node is a sub-replica of itself, I suggest we simply skip this attempt and let the node remain a sub-replica of itself and hope that the cluster will be repaired later, either by itself or by an admin.\r\n\r\nThis is the crash log:\r\n\r\n```\r\n27186:M 29 Aug 2022 12:23:41.784 * No cluster configuration found, I'm 1ecbffc6dbe0eb9428eb53e46e66170fd1af8e22\r\n27186:M 29 Aug 2022 12:23:41.788 * Running mode=cluster, port=7103.\r\n\r\n...\r\n\r\n27186:M 29 Aug 2022 13:15:14.012 # Connection with replica 127.3.6.1:7203 lost.\r\n27186:M 29 Aug 2022 13:15:14.115 # Configuration change detected. Reconfiguring myself as a replica of 0a5070b5b7124bc88f2dd36acc86a6923340a6bf\r\n27186:S 29 Aug 2022 13:15:14.126 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.\r\n27186:S 29 Aug 2022 13:15:14.126 * Connecting to MASTER 127.3.6.1:7203\r\n27186:S 29 Aug 2022 13:15:14.126 * MASTER <-> REPLICA sync started\r\n27186:S 29 Aug 2022 13:15:14.163 * Non blocking connect for SYNC fired the event.\r\n27186:S 29 Aug 2022 13:15:14.336 # I'm a sub-replica! Reconfiguring myself as a replica of grandmaster 1ecbffc6dbe0eb9428eb53e46e66170fd1af8e22\r\n\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n27186:S 29 Aug 2022 13:15:14.336 # === ASSERTION FAILED ===\r\n27186:S 29 Aug 2022 13:15:14.336 # ==> cluster.c:4546 'n != myself' is not true\r\n\r\n------ STACK TRACE ------\r\n\r\nBacktrace:\r\n/opt/services/epg/bin/redis-server 127.3.0.1:7103 [cluster][0x4bd3b6]\r\n/opt/services/epg/bin/redis-server 127.3.0.1:7103 [cluster](clusterUpdateSlotsConfigWith+0x282)[0x4c3192]\r\n/opt/services/epg/bin/redis-server 127.3.0.1:7103 [cluster](clusterProcessPacket+0x881)[0x4c48e1]\r\n/opt/services/epg/bin/redis-server 127.3.0.1:7103 [cluster](clusterReadHandler+0xfa)[0x4c53da]\r\n/opt/services/epg/bin/redis-server 127.3.0.1:7103 [cluster][0x518ba1]\r\n/opt/services/epg/bin/redis-server 127.3.0.1:7103 [cluster](aeProcessEvents+0x1d9)[0x44c169]\r\n/opt/services/epg/bin/redis-server 127.3.0.1:7103 [cluster](aeMain+0x25)[0x44c535]\r\n/opt/services/epg/bin/redis-server 127.3.0.1:7103 [cluster](main+0x342)[0x448252]\r\n/lib64/libc.so.6(__libc_start_main+0xf0)[0x15555468d6a0]\r\n/opt/services/epg/bin/redis-server 127.3.0.1:7103 [cluster](_start+0x29)[0x448799]\r\n```\r\n\r\nExplanation: The assert `'n != myself' is not true` happens in `clusterSetMaster()` when the node attempts turn itself into a replica of itself.\r\n\r\n```\r\nRelease notes:\r\nFix a crash when a replica may attempt to set itself as its master as a result of a manual failover. \r\n```",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-09-12T20:35:28Z",
        "closed_at": "2022-09-23T20:36:37Z",
        "merged_at": null,
        "body": "This is a backport 6.2 fix for redis compilation issues on Raspberry Pi, original issue https://github.com/redis/redis/issues/6275.\r\n\r\nThe 6.2 branch has the `ifneq (,$(findstring armv,$(uname_M)))` conditional, but is missing the needed `(,$(filter aarch64 armv,$(uname_M)))` conditional present in `upstream` and `5.0` branches.\r\n\r\nThis change was already applied to unstable branch: https://github.com/redis/redis/commit/f5d48537f1aa76e5ce4f14953517bd25c9ad4673 and 5.0 branch: https://github.com/redis/redis/pull/6975.\r\n\r\nRelated to https://github.com/redis/redis/pull/11230.\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-09-12T15:19:21Z",
        "closed_at": "2022-09-19T15:15:56Z",
        "merged_at": null,
        "body": "Bumps [vmactions/freebsd-vm](https://github.com/vmactions/freebsd-vm) from 0.2.4 to 0.2.6.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vmactions/freebsd-vm/releases\">vmactions/freebsd-vm's releases</a>.</em></p>\n<blockquote>\n<p>Fix for 13.0 bugs</p>\n<h2>v0.2.5</h2>\n<p>Minor, just using vbox v0.0.1</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/6b6db0efd32ec6ff6a71817489ad40581f41bba4\"><code>6b6db0e</code></a> fix bug for 13.0</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/e6a78f5a8f77e2b00ee50533ce76e6bdf101e12b\"><code>e6a78f5</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/d7adb414feecb23399b3c0641e3ef515d0bb1af2\"><code>d7adb41</code></a> fix for 13.0</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/9f55fb291ad4476177ab86c2992c213999ea9669\"><code>9f55fb2</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/6d62c3ddbec832d6f4cf5ad1399beb576545e45c\"><code>6d62c3d</code></a> fix for random timing bug for 13.0</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/a34d5fdbd1bd18db54810a555a302c5e7f719ac1\"><code>a34d5fd</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/469785c553acd8953c5f79698f19a3a6dc8a1629\"><code>469785c</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/dda15cde6dff12dcedfa84862353bd419af6e01b\"><code>dda15cd</code></a> Minor, just update builder</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/cc881f08ac4e7cf45ef57d4dbb67e155577b57f8\"><code>cc881f0</code></a> Update version to v0.2.5</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/912abaef321fa22392803ffb4f648b49a23b635a\"><code>912abae</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li>Additional commits viewable in <a href=\"https://github.com/vmactions/freebsd-vm/compare/v0.2.4...v0.2.6\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vmactions/freebsd-vm&package-manager=github_actions&previous-version=0.2.4&new-version=0.2.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-09-09T03:11:13Z",
        "closed_at": "2022-09-13T21:19:29Z",
        "merged_at": "2022-09-13T21:19:29Z",
        "body": "As part of https://github.com/redis/redis/pull/9774, we introduced a change so that we keep track of both the inbound and outbound links for cluster nodes. However, it's possible to have multiple inbound links at the same time when the remote node is attempting to reconnect. The current code replaces the `inbound_link` with the new link. This leaves a dangling reference to the node on the older cluster link Since nodes and links have mutual references. The solution here is just to drop the old link.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-09-08T10:08:21Z",
        "closed_at": "2022-10-13T12:03:54Z",
        "merged_at": "2022-10-13T12:03:54Z",
        "body": "When using the MIGRATE, with a destination Redis that has the user name or password set to the string \"keys\",\r\nRedis would have determine the wrong set of key names the command is gonna access.\r\nThis can lead to getting MOVED error during resharding, and also to ACL errors.\r\n\r\nDestination instance:\r\n```\r\n127.0.0.1:6380> acl setuser default >keys\r\nOK\r\n127.0.0.1:6380> acl setuser keys on nopass ~* &* +@all\r\nOK\r\n```\r\n\r\nSource instance:\r\n```\r\n127.0.0.1:6379> set a 123\r\nOK\r\n127.0.0.1:6379> acl setuser cc on nopass ~a* +@all\r\nOK\r\n127.0.0.1:6379> auth cc 1\r\nOK\r\n127.0.0.1:6379> migrate 127.0.0.1 6380 \"\" 0 1000 auth keys keys a\r\n(error) NOPERM this user has no permissions to access one of the keys used as arguments\r\n127.0.0.1:6379> migrate 127.0.0.1 6380 \"\" 0 1000 auth2 keys pswd keys a\r\n(error) NOPERM this user has no permissions to access one of the keys used as arguments\r\n```\r\n\r\nUsing `acl dryrun` we know that the parameters of `auth` and `auth2` are mistaken for the `keys` option.\r\n```\r\n127.0.0.1:6379> acl dryrun cc migrate whatever whatever \"\" 0 1000 auth keys keys a\r\n\"This user has no permissions to access the 'keys' key\"\r\n127.0.0.1:6379> acl dryrun cc migrate whatever whatever \"\" 0 1000 auth2 keys pswd keys a\r\n\"This user has no permissions to access the 'pswd' key\"\r\n```\r\n\r\nFix the bug by editing db.c/migrateGetKeys function, which finds the `keys` option and all the keys following.\r\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 234,
        "deletions": 27,
        "changed_files": 15,
        "created_at": "2022-09-07T11:36:04Z",
        "closed_at": "2023-02-12T07:23:30Z",
        "merged_at": "2023-02-12T07:23:30Z",
        "body": "# Background\r\nThe RDB file is usually generated and used once and seldom used again, but the content would reside in page cache until OS evicts it. A potential problem is that once the free memory exhausts, the OS have to reclaim some memory from page cache or swap anonymous page out, which may result in a jitters to the Redis service.\r\n\r\nSupposing an exact scenario, a high-capacity machine hosts many redis instances, and we're upgrading the Redis together. The page cache in host machine increases as RDBs are generated. Once the free memory drop into low watermark(which is more likely to happen in older Linux kernel like 3.10, before [watermark_scale_factor](https://lore.kernel.org/lkml/1455813719-2395-1-git-send-email-hannes@cmpxchg.org/) is introduced, the `low watermark` is linear to `min watermark`, and there'is not too much buffer space for `kswapd` to be wake up to reclaim memory), a `direct reclaim` happens, which means the process would stall to wait for memory allocation.\r\n\r\n# What the PR does\r\nThe PR introduces a capability to reclaim the cache when the RDB is operated. Generally there're two cases, read and write the RDB. For read it's a little messy to address the incremental reclaim, so the reclaim is done in one go in background after the load is finished to avoid blocking the work thread. For write, incremental reclaim amortizes the work of reclaim so no need to put it into background, and the peak watermark of cache can be reduced in this way.\r\n\r\nTwo cases are addresses specially, replication and restart, for both of which the cache is leveraged to speed up the processing, so the reclaim is postponed to a right time. To do this, a flag is added to`rdbSave` and `rdbLoad` to control whether the cache need to be kept, with the default value false.\r\n\r\n# Something deserve noting\r\n1. Though `posix_fadvise` is the POSIX standard, but only few platform support it, e.g. Linux, FreeBSD 10.0.\r\n2. In Linux `posix_fadvise` only take effect on writeback-ed pages, so a `sync`(or `fsync`, `fdatasync`) is needed to flush the dirty page before `posix_fadvise` if we reclaim write cache.\r\n\r\n# About test\r\nA unit test is added to verify the effect of `posix_fadvise`.\r\nIn integration test overall cache increase is checked, as well as the cache backed by RDB as a specific TCL test is executed in isolated Github action job.\r\n",
        "comments": 22
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2022-09-07T08:06:58Z",
        "closed_at": "2022-09-08T01:57:43Z",
        "merged_at": "2022-09-08T01:57:43Z",
        "body": "This PR follows issue #10978, and provides an API to avoid unnecessary memory allocations for dictionary iterators.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-09-07T07:01:45Z",
        "closed_at": "2022-09-08T03:35:25Z",
        "merged_at": "2022-09-08T03:35:25Z",
        "body": "In setGenericComannd() function, we remove the expiration entry from db->expires. However, I think we don't need to do this when 'expire' is not NULL, because in this case, we will call setExpire() later, which will overwrite the expiration entry with a new TTL. So we don't need to remove the entry now. We can keep it by setting the 'SETKEY_KEEPTTL' flag.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 85,
        "deletions": 17,
        "changed_files": 5,
        "created_at": "2022-09-06T11:03:48Z",
        "closed_at": "2022-09-19T06:47:52Z",
        "merged_at": "2022-09-19T06:47:52Z",
        "body": "This PR mainly deals with 2 crashes introduced in #9357, and fix the QUICKLIST-PACKED-THRESHOLD mess in extern \r\n test mode.\r\n\r\n1. Fix crash due to deleting an entry from a compress quicklistNode\r\n   When inserting a large element, we need to create a new quicklistNode first, and then delete its previous element, if the node where the deleted element is located is compressed, it will cause a crash.\r\n   Now add `dont_compress` to quicklistNode, if we want to use a quicklistNode after some operation, we can use this flag like following:\r\n    \r\n\r\n    ```c\r\n    node->dont_compress = 1; /* Prevent to be compressed */\r\n    some_operation(node); /* This operation might try to compress this node */\r\n    some_other_operation(node); /* We can use this node without decompress it */\r\n    node->dont_compress = 0; /* Re-able compression */\r\n    quicklistCompressNode(node);\r\n    ```\r\n\r\n    Perhaps in the future, we could just disable the current entry from being compressed during the iterator loop, but that would require more work.\r\n\r\n2. Fix crash due to wrongly split quicklist\r\n    before #9357, the offset param of _quicklistSplitNode() will not negative.\r\n\tFor now, when offset is negative, the split extent will be wrong.\r\n    following example:\r\n    ```c\r\n    int orig_start = after ? offset + 1 : 0;\r\n    int orig_extent = after ? -1 : offset;\r\n    int new_start = after ? 0 : offset;\r\n    int new_extent = after ? offset + 1 : -1;\r\n    # offset: -2, after: 1, node->count: 2\r\n    # current wrong range: [-1,-1] [0,-1]\r\n    # correct range: [1,-1] [0, 1]\r\n    ```\r\n\r\n     Because only `_quicklistInsert()` splits the quicklistNode and only `quicklistInsertAfter()`, `quicklistInsertBefore()` call _quicklistInsert(), \r\n     so `quicklistReplaceEntry()` and `listTypeInsert()` might occur this crash.\r\n     But the iterator of `listTypeInsert()` is alway from head to tail(iter->offset is always positive), so it is not affected.\r\n     The final conclusion is this crash only occur when we insert a large element with negative index into a list, that affects `LSET` command and `RM_ListSet` module api.\r\n     \r\n3. In extern test mode, we need to restore quicklist packed threshold after when the end of test.\r\n4. Show `node->count` in quicklistRepr().\r\n5. Add new tcl proc `config_get_set` to support restoring config in tests.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-09-06T09:42:39Z",
        "closed_at": "2022-09-07T13:25:10Z",
        "merged_at": null,
        "body": "1.Optimize multiple lua_pop\r\n2.Optimize lua_pop count , one less pointer move\uff1bfor example:lua_pop(lua,1);lua_pop(lua,1) --->lua_pop(lua,2) ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 606,
        "deletions": 165,
        "changed_files": 7,
        "created_at": "2022-09-06T01:06:10Z",
        "closed_at": "2022-09-06T03:05:06Z",
        "merged_at": null,
        "body": "1. Added support for auxiliary fields in CLUSTER NODES and nodes.conf as follows:\r\n    823ca5eb86404530a2cd1f6beee7ed9c00e786fb 127.0.0.1:30001@40001,host-name,aux1=val1,aux2=val2,aux3=val3,... master - 0 1656441238093 1 connected 0-5460\r\n   where aux/val can contain alnum or '_' only and the order of aux/val\r\n   pairs is insignificant.\r\n2. Added a new \"shard_id\" aux field to CLUSTER NODES and nodes.conf\r\n3. Added a new entry \"shard_id\" to CLUSTER SHARDS at the beginning of every shard\r\n4. Added a new PING extension to propagate \"shard_id\"\r\n5. Handled upgrade from pre-7.0 releases automatically\r\n6. Refactored PING extension assembling/parsing logic\r\n7. Maintain shard membership explicitly using a hashtable",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-09-05T17:05:06Z",
        "closed_at": "2022-09-06T05:40:38Z",
        "merged_at": "2022-09-06T05:40:37Z",
        "body": "The current description about `REPLYBUFFER RESIZING` which we get from `DEBUG HELP` is as follows.\r\n\r\n```\r\n101) REPLYBUFFER RESIZING <0|1>\r\n102)     Enable or disable the replay buffer resize cron job\r\n```\r\n\r\nThis command is related with reply buffer, not replay buffer. This PR is for fixing this word.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-09-05T11:07:41Z",
        "closed_at": "2022-09-18T14:46:25Z",
        "merged_at": "2022-09-18T14:46:25Z",
        "body": "This bug is introduced in #7653. (Redis 6.2.0)\r\n\r\nWhen `server.maxmemory_eviction_tenacity` is 100, `eviction_time_limit_us` is `ULONG_MAX`, and if we cannot find the best key to delete (e.g. maxmemory-policy is `volatile-lru` and all keys with ttl have been evicted), in `cant_free` redis will sleep forever if some items are being freed in the lazyfree thread.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-09-05T06:54:27Z",
        "closed_at": "2022-09-06T06:04:33Z",
        "merged_at": "2022-09-06T06:04:33Z",
        "body": "When Redis is built without TLS support, connectionTypeTls() function \r\nkeeps searching connection type as cached connection type is NULL. \r\n\r\nAdded another variable to track if we cached the connection type to \r\nprevent search after the first time. \r\n\r\nNoticed a log warning message is printed repeatedly by connectionTypeTls.\r\n\r\nIntroduced by https://github.com/redis/redis/pull/9320",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 96,
        "deletions": 18,
        "changed_files": 6,
        "created_at": "2022-09-05T05:16:44Z",
        "closed_at": "2022-11-28T09:57:11Z",
        "merged_at": "2022-11-28T09:57:11Z",
        "body": "Add an option \"withscore\" to ZRANK and ZREVRANK.\r\n\r\nAdd `[withscore]` option to both `zrank` and `zrevrank`, like this:\r\n```\r\nz[rev]rank key member [withscore]\r\n```\r\n\r\ndiscussion\r\n------\r\n\r\nTo close #11111 \r\nThe conclusion in #11111 is adding 2 options `[withscore]` and `[rev]` to `zrank` like this:\r\n```\r\nzrank key member [withscore] [rev]\r\n```\r\nHowever, since we already have `zrevrank` and it shares the same code with `zrank`, it would be quite inconvenient to add option `[rev]` to `zrank`, as this requires some ugly judgements in the code. Also it breaks the symmetry between `zrank` and `zrevrank`, in the aspects of both code implementation and command calling.\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 8,
        "changed_files": 5,
        "created_at": "2022-09-04T11:13:01Z",
        "closed_at": "2022-09-05T03:41:13Z",
        "merged_at": null,
        "body": "close #11111 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-09-04T08:28:05Z",
        "closed_at": "2022-09-29T06:49:53Z",
        "merged_at": "2022-09-29T06:49:53Z",
        "body": "Better redis-cli hints for commands that take other commands as arguments.\r\n\r\n```\r\ncommand getkeysandflags hello [protover [AUTH username password]]\r\nacl dryrun user hello [protover [AUTH username password]]\r\n```",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 28,
        "changed_files": 4,
        "created_at": "2022-09-02T09:36:27Z",
        "closed_at": "2022-09-15T07:09:04Z",
        "merged_at": null,
        "body": "This PR might not have a significant impact on performance (though the first run, I had a huge gap), but it clears up some code and feels unnecessary.\r\n\r\n![geohashGetCoordRange](https://user-images.githubusercontent.com/44112901/188111820-886a993f-a3ad-4e1a-b983-0b2dcad9a6e0.png)",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-09-02T06:09:00Z",
        "closed_at": "2022-09-07T07:02:43Z",
        "merged_at": null,
        "body": "Before this change, when we call setGenericCommand() with expire set, the expiration information(key and its TTL) will be removed in setKey() function, and then later be set back in the setExpire() function. \r\n\r\nActually, there is no need to remove the expiration information; the setExpire() function can overwrite the key with its new TTL. \r\n\r\nAn optimization is to set the SETKEY_KEEPTTL flag when expire is not NULL so that the TTL will not be removed in setKey() before we use setExpire() to overwrite it.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 154,
        "deletions": 129,
        "changed_files": 2,
        "created_at": "2022-09-01T19:34:19Z",
        "closed_at": "2022-11-03T17:14:56Z",
        "merged_at": "2022-11-03T17:14:56Z",
        "body": "A suggestion for how to resolve part of https://github.com/redis/redis/issues/11083. The idea is that we should retain categories and commands so we can correctly produce ACL strings that represent the original intention of the user. \r\n\r\nThe new strategy is as follow:\r\n1. Retain the exact order of categories and commands provided from the user, stored in a string. \r\n2. Completely compact commands and categories when +@all or -@all are used.\r\n3. Remove duplicate command, categories, or subcommands. Note: That deduping commands will remove extra subcommands. \r\n\r\nNote, this does provide a slightly different output. Rules which don't \"do\" anything will be kept and printed back out. Some examples:\r\n1. \"+@all +hget\": Hget can't do anything since all commands were previously added.\r\n2. \"+@all -@hash -hget\": -Hget is implicitly covered as part of -@hash. Since something may change in the future with modules, we leave the hget.\r\n\r\nSome of these, like 1, could be resolved but it would add complexity that isn't really needed.\r\n\r\n```\r\nRelease notes\r\nThe way ACL users are stored internally no longer removes redundant command and category rules, which may alter the way those rules are displayed as part of `ACL SAVE`, `ACL GETUSER` and `ACL LIST`. \r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-09-01T08:30:02Z",
        "closed_at": "2022-09-05T13:59:14Z",
        "merged_at": "2022-09-05T13:59:14Z",
        "body": "EVAL scripts are by default not considered `write` commands, so they were allowed on a replica.\r\nBut when adding a shebang, they become `write` command (unless the `no-writes` flag is added).\r\nWith this change we'll handle them as write commands, and reply with MOVED instead of READONLY when executed on a redis cluster replica.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-09-01T06:09:46Z",
        "closed_at": "2022-09-06T23:54:24Z",
        "merged_at": "2022-09-06T23:54:24Z",
        "body": "When using cli to add node, there can potentially be a race condition in\r\nwhich all nodes presenting cluster state o.k even though the added node\r\ndid not yet meet all cluster nodes.\r\nthis adds another utility function to wait untill all cluster nodes\r\nsee the same cluster size",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 47,
        "deletions": 12,
        "changed_files": 5,
        "created_at": "2022-08-31T13:38:58Z",
        "closed_at": "2022-09-15T03:39:47Z",
        "merged_at": "2022-09-15T03:39:47Z",
        "body": "### Intro\r\nWhen a client has a pending write the corresponding client struct is added to the global server.clients_pending_write list. When a pending write is handled the corresponding client struct is removed from the clients_pending_write list.\r\n\r\n### Current Behavior:\r\nWhen a client struct is added to the clients_pending_write list, a new listnode is malloced and its ptr field is set to point to the client struct. When a client struct is removed from cleints_pending_write the listnode pointing to that client is freed.\r\n \r\n \r\nWe observed that in an 800 client test scenario, redis spends about 3% of its time mallocing and freeing listnodes for the clients_pending_write list.\r\n\r\n### Proposed Behavior:\r\nWe take inspiration from the linked list implementation of the linux kernel and suggest adding a listnode field to the client struct to be used in the clients_pending_write list. The clients \"point to each other\" instead of being pointed to by external list nodes. This spares a malloc call when adding a client struct to the list, and spares a free call when removing a client from the list.\r\n\r\n### Numbers we observed:\r\nOur benchmarks showed a 3% improvement on the m5.xlarge series of AWS EC2 instances.\r\nWe used 20% set 80% get with 512 bytes data size and 100% hit ratio\r\n\r\nRedis 7 before the change: Average of 212612 RPS\r\nRedis 7 after the chang: Average of 220331 RPS\r\n3.6% improvement\r\n\r\nRedis 6.2 before the change: Average of 227134 RPS\r\nRedis 6.2 after the change 231979 RPS\r\n2% improvement \r\n\r\nAlso note that it is not expected to affect the memory footprint as it is expected that each client struct will be added to the clients_pending_write list (since clients expect a response)\r\n\r\n```\r\nRelease notes\r\nMinor performance improvement for workloads that use commands without pipelining.\r\n```",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2022-08-31T11:10:15Z",
        "closed_at": "2022-09-08T09:16:35Z",
        "merged_at": null,
        "body": "Currently, if one adds a script to Redis without shbang flags, it will get the SCRIPT_FLAG_EVAL_COMPAT_MODE flag instead.\r\n\r\nFor OOM situations this means, that even if we call RM_Call with 'M' to prevent execution of \"deny oom\" commands (which this script would be without the compat mode), it won't be considered deny-oom and will just fail with an OOM during script execution.\r\n\r\nThis flag allows RM_Call to treat such scripts as if they had no flags at all and be derived from there.  For 'M' this means that the scripts will be treated as DENY_OOM, and if a user wanted to allow them to run in an OOM situation, would have to use the appropriate shbang flags.",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 5191,
        "deletions": 970,
        "changed_files": 129,
        "created_at": "2022-08-31T08:15:13Z",
        "closed_at": "2022-08-31T13:37:09Z",
        "merged_at": null,
        "body": "## Adds listnode to client struct to be used in clients_pending_write list\r\n\r\nWhen a client has a pending write the corresponding client struct is added to the global server.clients_pending_write list. When a pending write is handled the corresponding client struct is removed from the clients_pending_write list.\r\n\r\n### Current Behavior:\r\nWhen a client struct is added to the clients_pending_write list, a new listnode is malloced and its ptr field is set to point to the client struct. When a client struct is removed from cleints_pending_write the listnode pointing to that client is freed.\r\n\r\nWe observed that in an 800 client test scenario, redis spends about 3% of its time mallocing and freeing listnodes for the clients_pending_write list.\r\n\r\n### Proposed Behavior:\r\nWe take inspiration from the linked list implementation of the linux kernel and suggest adding a listnode field to the client struct to be used in the clients_pending_write list. The clients \u201cpoint to each other\u201d instead of being pointed to by external list nodes. This spares a malloc call when adding a client struct to the list, and spares a free call when removing a client from the list.\r\n\r\nOur benchmarks showed a 3% improvement on the m5 series of AWS EC2 instances.\r\n\r\nAlso note that this change is not expected to affect the memory footprint significantly\r\nas it is expected that each client struct will be added to the clients_pending_write list (since clients expect a response)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2022-08-30T18:59:09Z",
        "closed_at": "2022-11-20T10:03:00Z",
        "merged_at": "2022-11-20T10:03:00Z",
        "body": "Till now Sentinel allowed modifying the log level in the config file, but not at runtime.\r\nthis makes it possible to tune the log level at runtime",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2022-08-30T18:16:10Z",
        "closed_at": "2022-09-11T08:22:59Z",
        "merged_at": "2022-09-11T08:22:59Z",
        "body": "Just noticed that there are some inaccurate, or at least confusing information about `repl-diskless-load` in `redis.conf`\r\nIt shouldn't scare away users willing to spend the extra memory.\r\n\r\n`may mean that we have to flush the contents of the current database before the full rdb was received.`: this is likely related to the time when there was an option `always`, where content on replica was flushed before loading from master.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-08-30T14:20:05Z",
        "closed_at": "2022-08-31T15:48:20Z",
        "merged_at": null,
        "body": "It makes redis-cli more portable out-of-the-box for ipv6 and customized interfaces.\r\nIt also gives the option in the future to optimize and connect by default, where possible,\r\nvia sockets (and not to be bound to TCPIP stack. mysql client is doing exactly that!).\r\n\r\nExtra system calls price (vs. 127.0.0.1):\r\n```\r\nopenat(AT_FDCWD, \"/etc/host.conf\", O_RDONLY|O_CLOEXEC) = 3\r\nopenat(AT_FDCWD, \"/etc/resolv.conf\", O_RDONLY|O_CLOEXEC) = 3\r\nopenat(AT_FDCWD, \"/etc/nsswitch.conf\", O_RDONLY|O_CLOEXEC) = 3\r\nopenat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3\r\nopenat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libnss_files.so.2\", O_RDONLY|O_CLOEXEC) = 3\r\nopenat(AT_FDCWD, \"/etc/hosts\", O_RDONLY|O_CLOEXEC) = 3\r\nopenat(AT_FDCWD, \"/etc/gai.conf\", O_RDONLY|O_CLOEXEC) = 3\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2022-08-29T17:46:24Z",
        "closed_at": "2022-10-02T12:15:14Z",
        "merged_at": "2022-10-02T12:15:14Z",
        "body": "Optimization update from -O2 to -O3 -flto gives up to 5% performance gain in 'redis-benchmarks-spec-client-runner' tests geomean where GCC 9.4.0 is used for build",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2022-08-29T16:55:36Z",
        "closed_at": "2022-10-22T17:41:17Z",
        "merged_at": "2022-10-22T17:41:17Z",
        "body": "The following two cases will create an empty destkey HLL:\r\n1. called with no source keys, like `pfmerge destkey`\r\n2. called with non-existing source keys, like `pfmerge destkey non-existing-source-key`\r\n\r\nIn the first case, in `PFMERGE`, the dest key is actually one of the source keys too.\r\nSo `PFMERGE k1 k2` is equivalent to `SUNIONSTORE k1 k1 k2`,\r\nand `PFMERGE k1` is equivalent to `SUNIONSTORE k1 k1`.\r\nSo the first case is reasonable, the source key is actually optional.\r\n\r\nAnd the second case, `PFMERGE` on missing keys should succeed and create an empty dest.\r\nThis is consistent with `PFCOUNT`, and also with `SUNIONSTORE`, no need to change.\r\n\r\nCloses #11202",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-08-29T15:19:52Z",
        "closed_at": "2022-08-31T07:13:48Z",
        "merged_at": "2022-08-31T07:13:48Z",
        "body": "Bumps [vmactions/freebsd-vm](https://github.com/vmactions/freebsd-vm) from 0.2.3 to 0.2.4.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vmactions/freebsd-vm/releases\">vmactions/freebsd-vm's releases</a>.</em></p>\n<blockquote>\n<p>Fix sshfs to work in MacOS 12</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/1f933cf652838e2f86425fc50625c8cf6f39335b\"><code>1f933cf</code></a> Update version to v0.2.3</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/4a95aaffbda9bc1844e0d69dbc7fd8903fa9378b\"><code>4a95aaf</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/e9bf3d28d20b9c664fea908f774e474c302e0394\"><code>e9bf3d2</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/e25921b71d3447e9d92ccc63b7cbc34dc530b494\"><code>e25921b</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/86d7207ed4e3e4476b5247e20c06e41760d653b7\"><code>86d7207</code></a> Update version to v0.2.3</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/d6f3dc080e7195cbbf87573eaa88173ae40077f8\"><code>d6f3dc0</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/617667f4d95afd35037bc5930b3f0a4d21fb029d\"><code>617667f</code></a> delete dependabot.yml</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/1d13e83feeb8d48e7bc755db5779fb746e87b8b8\"><code>1d13e83</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/22cfe0aff36a151aa4eb61076b1f094ea30e5aa8\"><code>22cfe0a</code></a> Sync from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/865e4d5e401b312b3b3b1a48fad5a469cd600731\"><code>865e4d5</code></a> Sync from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li>Additional commits viewable in <a href=\"https://github.com/vmactions/freebsd-vm/compare/v0.2.3...v0.2.4\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vmactions/freebsd-vm&package-manager=github_actions&previous-version=0.2.3&new-version=0.2.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 99,
        "deletions": 12,
        "changed_files": 6,
        "created_at": "2022-08-29T11:29:14Z",
        "closed_at": "2022-10-02T07:52:14Z",
        "merged_at": "2022-10-02T07:52:14Z",
        "body": "- BITOP: turn argument `operation` from string to oneof\r\n- CLIENT KILL: turn argument `skipme` from string to oneof\r\n- COMMAND GETKEYS / GETKEYSANDFLAGS: change arguments to optional, and change arity to -3 (fixes regression in redis 7.0)\r\n- CLIENT PAUSE: this command was added in v3.0.0\r\n\r\n> release notes: the GETKEYS fix mentioned above.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-08-29T05:24:34Z",
        "closed_at": "2022-08-29T07:25:25Z",
        "merged_at": "2022-08-29T07:25:25Z",
        "body": "New test fails on valgrind because strtold(\"+inf\") with valgrind returns a non-inf result\r\nsame thing is done in incr.tcl.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 651,
        "deletions": 14,
        "changed_files": 14,
        "created_at": "2022-08-28T16:00:39Z",
        "closed_at": "2022-11-24T17:00:04Z",
        "merged_at": "2022-11-24T17:00:04Z",
        "body": "### Summary of API additions\r\n\r\n* `RedisModule_AddPostNotificationJob` - new API to call inside a key space notification (and on more locations in the future) and allow to add a post job as describe above.\r\n* New module option, `REDISMODULE_OPTIONS_ALLOW_NESTED_KEYSPACE_NOTIFICATIONS`, allows to disable Redis protection of nested key-space notifications.\r\n* `RedisModule_GetModuleOptionsAll` - gets the mask of all supported module options so a module will be able to check if a given option is supported by the current running Redis instance.\r\n\r\n### Background\r\n\r\nRelated PRs: #10969, #11178\r\n\r\nThe following PR is a proposal of handling write operations inside module key space notifications.\r\n\r\nAfter a lot of discussions we came to a conclusion that module should not perform any write operations on key space notification.\r\n\r\nSome examples of issues that such write operation can cause are describe on the following links:\r\n\r\n* Bad replication oreder - https://github.com/redis/redis/pull/10969\r\n* Used after free - https://github.com/redis/redis/pull/10969#issuecomment-1223771006\r\n* Used after free - https://github.com/redis/redis/pull/9406#issuecomment-1221684054\r\n\r\nThere are probably more issues that are yet to be discovered. The underline problem with writing inside key space notification is that the notification runs synchronously, this means that the notification code will be executed in the middle on Redis logic\r\n(commands logic, eviction, expire). Redis **do not assume** that the data might change while running the logic and such changes can crash Redis or cause unexpected behaviour.\r\n\r\nOne solution is to go over all those places (that assumes no data changes) and fix them such that they will be tolerable to data changes. This approach will probably require a lot of changes all around the code and it will be very hard to know whether or not we got all those issues. Moreover, it will be very hard to maintain such code and be sure that we are not adding more places that assume no data changes.\r\n\r\nAnother approach (which presented in this PR) is to state that modules **should not** perform any write command inside key space notification (we can chose whether or not we want to force it). To still cover the use-case where module wants to perform a write operation as a reaction to key space notifications, we introduce a new API , `RedisModule_AddPostNotificationJob`, that allows to register a callback that will be called by Redis when the following conditions hold:\r\n\r\n* It is safe to perform any write operation.\r\n* The job will be called atomically along side the operation that triggers it (in our case, key space notification).\r\n\r\nModule can use this new API to safely perform any write operation and still achieve atomicity between the notification and the write.\r\n\r\nNotice that another approach that was suggested was to collect all the key space notifications and fire them at the end (where it is safe to write). This solution might be simpler to the module but it has some limitations:\r\n1. It can be considered a breaking change because the notifications will be fired at the end and not synchronously when they happened\r\n2. If the key space notification will be fire at the end, the module might miss data that it has before, one example is this [PR](https://github.com/redis/redis/pull/9406#issuecomment-1221684054) that adds unlink notification that allows the module to read the data just before it is deleted.\r\n\r\nWe believe that the suggested API gives a good abstraction (module do not need to know the internal for Redis and decide when it safe to write, Redis calls the module when such conditions hold) with good flexibility (module can still perform logic synchronously with the notification and read the state of the database as it was at the time of the notification).\r\n\r\nThough currently the API is supported on key space notifications, the API is written in a generic way so that in the future we will be able to use it on other places (server events for example).\r\n\r\n### Technical Details\r\n\r\nWhenever a module uses `RedisModule_AddPostNotificationJob` the callback is added to a list of callbacks (called `modulePostExecUnitJobs`) that need to be invoke after the current execution unit ends (whether its a command, eviction, or active expire). In order to trigger those callback atomically with the notification effect, we call those callbacks on `postExecutionUnitOperations` (which was `propagatePendingCommands` before this PR). The new function fires the post jobs and then calls `propagatePendingCommands`.\r\n\r\nIf the callback perform more operations that triggers more key space notifications. Those keys space notifications might register more callbacks. Those callbacks will be added to the end of `modulePostExecUnitJobs` list and will be invoke atomically after the current callback ends. This raises a concerns of entering an infinite loops, we consider infinite loops as a logical bug that need to be fixed in the module, an attempt to protect against infinite loops by halting the execution could result in violation of the feature correctness and so **Redis will make no attempt to protect the module from infinite loops**\r\n\r\nIn addition, currently key space notifications are not nested. Some modules might want to allow nesting key-space notifications. To allow that and keep backward compatibility, we introduce a new module option called `REDISMODULE_OPTIONS_ALLOW_NESTED_KEYSPACE_NOTIFICATIONS`. Setting this option will disable the Redis key-space notifications nesting protection and will pass this responsibility to the module.\r\n\r\n### Redis infrastructure\r\n\r\nThis PR promotes the existing `propagatePendingCommands` to an \"Execution Unit\" concept, which is called after each atomic unit of execution,\r\n\r\n### Cherry pick considerations and followup PR\r\n\r\nOne of the goal of this PR was to make it as safe as possible to cherry-pick to 7.0. While working on the PR we realize the code has some state duplication when it comes to command propagation to replica and AOF. We realize that `server.core_propagation` is probably redundant and we probably can also unify `server.module_ctx_nesting` and `server.in_nested_call`. Such changes are probably much more risky so we avoid doing them on this PR. A followup PR will cover those topics and will try to rearrange this part of the code.\r\n\r\n### TODO\r\n- [x] Decide whether the approach taken on this PR makes sense and if there are more edge cases we might have missed\r\n- [x] Decide whether or not we want to force modules not to write inside key space notifications - no such validation is require. Document this limitation is good enough.\r\n- [x] Decide whether or not we want to force modules to use the new API only on key space notifications - no\r\n- [x] Handle `RM_StringDMA` which is an example of read operation that might change the data (see issue describe in this comment https://github.com/redis/redis/pull/9406#issuecomment-1221684054) - after checking the issue we see that there is no really a problem using `RM_StringDMA` and its better not to change it to avoid breaking change. Test was added to verify it.\r\n- [x] Consider whether or not `propagatePendingCommands` is the right place to fire the post notifications callbacks.\r\n- [x] Move max action limitation to be a parameter that can be set by the module itself - we decided we will not have any limits\r\n- [x] Improve testing to the new API",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-08-27T18:17:50Z",
        "closed_at": "2022-09-08T10:43:21Z",
        "merged_at": null,
        "body": "Closes #11197",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2022-08-27T17:49:26Z",
        "closed_at": "2022-09-05T13:09:28Z",
        "merged_at": "2022-09-05T13:09:28Z",
        "body": "micro optimizations, giving the hints that the returned addresses\r\n are guaranteed to be unique. The alloc_size attribute gives an extra hint\r\n about the source of the size, useful mostly for calloc-like calls or when there\r\n are extra arguments.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-08-25T08:57:40Z",
        "closed_at": "2022-08-26T16:06:56Z",
        "merged_at": "2022-08-26T16:06:56Z",
        "body": "Change \u201cSetup a an \u2026\u201c to \u201cSet up an \u2026\u201d.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 16,
        "changed_files": 7,
        "created_at": "2022-08-24T19:30:50Z",
        "closed_at": "2022-09-08T06:37:49Z",
        "merged_at": "2022-09-08T06:37:49Z",
        "body": "For the stream data type, some commands, such as **XGROUP CREATE, XGROUP DESTROY, XGROUP CREATECONSUMER, \r\nXGROUP DELCONSUMER and XINFO CONSUMERS** use groupname and consumername in the command description;\r\n\r\nHowever, for the commands **XREADGROUP GROUP, XPENDING, XACK , XCLAIM  and XAUTOCLAIM**  use term \"group and consumer\", clients could be confused.\r\n\r\nThis PR goal is to unify all the commands to groupname and consumername.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 110,
        "deletions": 185,
        "changed_files": 7,
        "created_at": "2022-08-23T14:21:02Z",
        "closed_at": "2022-08-24T09:51:36Z",
        "merged_at": "2022-08-24T09:51:36Z",
        "body": "The PR reverts the changes made on #10969. The reason for revert was trigger because of occasional test failure that started after the PR was merged.\r\n\r\nThe issue is that if there is a lazy expire during the command invocation, the `del` command is added to the replication stream after the command placeholder. So the logical order on the primary is:\r\n\r\n* Delete the key (lazy expiration)\r\n* Command invocation\r\n\r\nBut the replication stream gets it the other way around:\r\n\r\n* Command invocation (because the command is written into the placeholder)\r\n* Delete the key (lazy expiration)\r\n\r\nSo if the command write to the key that was just lazy expired we will get inconsistency between primary and replica.\r\n\r\nOne solution we considered is to add another lazy expire replication stream and write all the lazy expire there. Then when replicating, we will replicate the lazy expire replication stream first. This will solve this specific test failure but we realise that the issues does not ends here and the more we dig the more problems we find.One of the example we thought about (that can actually crashes Redis) is as follow:\r\n\r\n* User perform SINTERSTORE\r\n* When Redis tries to fetch the second input key it triggers lazy expire\r\n* The lazy expire trigger a module logic that deletes the first input key\r\n* Now Redis hold the robj of the first input key that was actually freed\r\n\r\nWe believe we took the wrong approach and we will come up with another PR that solve the problem differently, for now we revert the changes so we will not have the tests failure.\r\n\r\nNotice that not the entire code was revert, some parts of the PR are changes that we would like to keep. The changes that **was** reverted are:\r\n\r\n* Saving a placeholder for replication at the beginning of the command (`call` function)\r\n* Order of the replication stream on active expire and eviction (we will decide how to handle it correctly on follow up PR)\r\n* `Spop` changes are no longer needed (because we reverted the placeholder code)\r\n\r\nChanges that **was not** reverted:\r\n\r\n* On expire/eviction, wrap the `del` and the notification effect in a multi exec.\r\n* `PropagateNow` function can still accept a special dbid, -1, indicating not to replicate select.\r\n* Keep optimisation for reusing the `alsoPropagate` array instead of allocating it each time.\r\n\r\nTests:\r\n\r\n* All tests was kept and only few tests was modify to work correctly with the changes\r\n* Test was added to verify that the revert fixes the issues.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-08-23T13:52:44Z",
        "closed_at": "2022-08-25T06:46:12Z",
        "merged_at": null,
        "body": "fix int32_t type precision overflow",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 45,
        "deletions": 8,
        "changed_files": 5,
        "created_at": "2022-08-23T11:44:44Z",
        "closed_at": "2022-08-24T16:39:16Z",
        "merged_at": "2022-08-24T16:39:16Z",
        "body": "Redis 7.0 has #9890 which added an assertion when the propagation queue\r\nwas not flushed and we got to beforeSleep.\r\nBut it turns out that when processCommands calls getNodeByQuery and\r\ndecides to reject the command, it can lead to a key that was lazy\r\nexpired and is deleted without later flushing the propagation queue.\r\n\r\nThis change prevents lazy expiry from deleting the key at this stage\r\n(not as part of a command being processed in `call`)\r\n\r\nCloses #11014",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-08-23T02:26:41Z",
        "closed_at": "2022-08-24T11:11:04Z",
        "merged_at": "2022-08-24T11:11:04Z",
        "body": "The GETDEL command only operates on strings, and strings are never freed lazily, so there's no need to use `dbAsyncDelete` or `shared.unlink`.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-08-22T15:17:36Z",
        "closed_at": "2022-08-24T11:25:01Z",
        "merged_at": null,
        "body": "Bumps [codespell](https://github.com/codespell-project/codespell) from 2.1.0 to 2.2.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/codespell-project/codespell/releases\">codespell's releases</a>.</em></p>\n<blockquote>\n<h2>v2.2.1</h2>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/mdeweerd\"><code>@\u200bmdeweerd</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2457\">codespell-project/codespell#2457</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/codespell-project/codespell/compare/v2.2.0...v2.2.1\">https://github.com/codespell-project/codespell/compare/v2.2.0...v2.2.1</a></p>\n<h2>v2.2.0</h2>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/jzinn\"><code>@\u200bjzinn</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/1886\">codespell-project/codespell#1886</a></li>\n<li><a href=\"https://github.com/enjoy-binbin\"><code>@\u200benjoy-binbin</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/1942\">codespell-project/codespell#1942</a></li>\n<li><a href=\"https://github.com/tir38\"><code>@\u200btir38</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/1535\">codespell-project/codespell#1535</a></li>\n<li><a href=\"https://github.com/dkg\"><code>@\u200bdkg</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/1891\">codespell-project/codespell#1891</a></li>\n<li><a href=\"https://github.com/ryo-tagami\"><code>@\u200bryo-tagami</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2003\">codespell-project/codespell#2003</a></li>\n<li><a href=\"https://github.com/timgates42\"><code>@\u200btimgates42</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2020\">codespell-project/codespell#2020</a></li>\n<li><a href=\"https://github.com/gijs-pennings\"><code>@\u200bgijs-pennings</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2021\">codespell-project/codespell#2021</a></li>\n<li><a href=\"https://github.com/apoclyps\"><code>@\u200bapoclyps</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/1999\">codespell-project/codespell#1999</a></li>\n<li><a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2059\">codespell-project/codespell#2059</a></li>\n<li><a href=\"https://github.com/a1346054\"><code>@\u200ba1346054</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2077\">codespell-project/codespell#2077</a></li>\n<li><a href=\"https://github.com/clickthisnick\"><code>@\u200bclickthisnick</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2082\">codespell-project/codespell#2082</a></li>\n<li><a href=\"https://github.com/robin-wayve\"><code>@\u200brobin-wayve</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2118\">codespell-project/codespell#2118</a></li>\n<li><a href=\"https://github.com/matkoniecz\"><code>@\u200bmatkoniecz</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2143\">codespell-project/codespell#2143</a></li>\n<li><a href=\"https://github.com/Akuli\"><code>@\u200bAkuli</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2174\">codespell-project/codespell#2174</a></li>\n<li><a href=\"https://github.com/jtroup\"><code>@\u200bjtroup</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2169\">codespell-project/codespell#2169</a></li>\n<li><a href=\"https://github.com/sed-i\"><code>@\u200bsed-i</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2246\">codespell-project/codespell#2246</a></li>\n<li><a href=\"https://github.com/josh11b\"><code>@\u200bjosh11b</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2240\">codespell-project/codespell#2240</a></li>\n<li><a href=\"https://github.com/isaak654\"><code>@\u200bisaak654</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2241\">codespell-project/codespell#2241</a></li>\n<li><a href=\"https://github.com/akien-mga\"><code>@\u200bakien-mga</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2274\">codespell-project/codespell#2274</a></li>\n<li><a href=\"https://github.com/FloEdelmann\"><code>@\u200bFloEdelmann</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2287\">codespell-project/codespell#2287</a></li>\n<li><a href=\"https://github.com/matthewfeickert\"><code>@\u200bmatthewfeickert</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2341\">codespell-project/codespell#2341</a></li>\n<li><a href=\"https://github.com/david-marchand\"><code>@\u200bdavid-marchand</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2346\">codespell-project/codespell#2346</a></li>\n<li><a href=\"https://github.com/jdufresne\"><code>@\u200bjdufresne</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2363\">codespell-project/codespell#2363</a></li>\n<li><a href=\"https://github.com/mar1ad\"><code>@\u200bmar1ad</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2367\">codespell-project/codespell#2367</a></li>\n<li><a href=\"https://github.com/tkoyama010\"><code>@\u200btkoyama010</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2369\">codespell-project/codespell#2369</a></li>\n<li><a href=\"https://github.com/penguin42\"><code>@\u200bpenguin42</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2388\">codespell-project/codespell#2388</a></li>\n<li><a href=\"https://github.com/fxlb\"><code>@\u200bfxlb</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2390\">codespell-project/codespell#2390</a></li>\n<li><a href=\"https://github.com/kianmeng\"><code>@\u200bkianmeng</code></a> made their first contribution in <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/pull/2396\">codespell-project/codespell#2396</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/codespell-project/codespell/compare/v2.1.0...v2.2.0\">https://github.com/codespell-project/codespell/compare/v2.1.0...v2.2.0</a></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/c6ecb9fc51571a77bc92e6c265c358aef7cb6c38\"><code>c6ecb9f</code></a> MAINT: 2.2.1 (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2462\">#2462</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/a322ee3c2023386ef454c3795ed906f3af8f5aa2\"><code>a322ee3</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2457\">#2457</a> from mdeweerd/patch-1</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/5d50d032a7ddad689444b0846f5b9e168b75dc82\"><code>5d50d03</code></a> 'which' -&gt; 'that' because the clause is essential to the meaning of the sente...</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/10943b0345bc6d63c84b857726569f6e520ccda1\"><code>10943b0</code></a> MAINT: Bump to next dev (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2455\">#2455</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/291a64940601c5f99ac3136c49ad405f636f48c2\"><code>291a649</code></a> MAINT: Stable version (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2454\">#2454</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/101498daa5ebd2108518c4f7ff7c6d0d2520c9ec\"><code>101498d</code></a> Add several spelling corrections (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2453\">#2453</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/68dd613024fef205482a01e3c01c9ea829c0e0ae\"><code>68dd613</code></a> Add several spelling corrections (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2449\">#2449</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/a51101a137037c11212d8d7262af716d81142659\"><code>a51101a</code></a> Fix several spelling corrections (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2445\">#2445</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/559a1c99302741087ffbc9bf1ac459c119aae053\"><code>559a1c9</code></a> Fix several spelling corrections (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2446\">#2446</a>)</li>\n<li><a href=\"https://github.com/codespell-project/codespell/commit/6adff2257455769eefeac68730bdfff67f4fb943\"><code>6adff22</code></a> Add several spelling corrections and refinements (<a href=\"https://github-redirect.dependabot.com/codespell-project/codespell/issues/2447\">#2447</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/codespell-project/codespell/compare/v2.1.0...v2.2.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=codespell&package-manager=pip&previous-version=2.1.0&new-version=2.2.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-08-22T11:30:45Z",
        "closed_at": "2022-10-19T09:49:03Z",
        "merged_at": null,
        "body": "The makefile in [src](https://github.com/redis/redis/blob/a5349832fe5d6a663096525a20dc237fa8ba3503/src/Makefile#L63) currently states that on ARM, jemalloc is to avoid being used. However, this only applies to armv6l and armv7l.\n\nThis PR extends the arm platforms to include arm8l and aarch64.  Realistically, we could instead, probably check if the arm platform exists within a list, and use that - if that's the desired approach.\n",
        "comments": 19
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2022-08-21T13:47:53Z",
        "closed_at": "2022-10-27T06:16:22Z",
        "merged_at": null,
        "body": "previously, while RM_Call ignores OOM for regular commands, it would execute scripts normally which could cause the script (depending on how they are configured) to OOM on their own in the\r\nmiddle of execution.\r\n\r\nThis allows the caller of RM_Call to specify a flag that will allow the module to ensure that scripts it executes will run completely even if redis is > maxmemory limit.",
        "comments": 20
    },
    {
        "merged": true,
        "additions": 126,
        "deletions": 145,
        "changed_files": 12,
        "created_at": "2022-08-21T13:27:04Z",
        "closed_at": "2022-10-16T06:01:37Z",
        "merged_at": "2022-10-16T06:01:37Z",
        "body": "Motivation: for applications that use RM ACL verification functions, they would want to return errors back to the user, in ways that are consistent with Redis. While investigating how we should return ACL errors to the user,  we realized that Redis isn't consistent, and currently returns ACL error strings in 3 primary ways.\r\n\r\n[For the actual implications of this change, see the \"Impact\" section at the bottom]\r\n\r\n1. how it returns an error when calling a command normally\r\n   ACL_DENIED_CMD -> \"this user has no permissions to run the '%s' command\"\r\n   ACL_DENIED_KEY -> \"this user has no permissions to access one of the keys used as arguments\"\r\n   ACL_DENIED_CHANNEL -> \"this user has no permissions to access one of the channels used as arguments\"\r\n\r\n2. how it returns an error when calling via 'acl dryrun' command\r\n   ACL_DENIED_CMD ->  \"This user has no permissions to run the '%s' command\"\r\n   ACL_DENIED_KEY -> \"This user has no permissions to access the '%s' key\"\r\n   ACL_DENIED_CHANNEL -> \"This user has no permissions to access the '%s' channel\"\r\n\r\n3. how it returns an error via RM_Call (and scripting is similar).\r\n   ACL_DENIED_CMD -> \"can't run this command or subcommand\";\r\n   ACL_DENIED_KEY -> \"can't access at least one of the keys mentioned in the command arguments\";\r\n   ACL_DENIED_CHANNEL -> \"can't publish to the channel mentioned in the command\";\r\n   \r\n   In addition, if one wants to use RM_Call's \"dry run\" capability instead of the RM ACL functions directly, one also sees a different problem than it returns ACL errors with a -ERR, not a -PERM, so it can't be returned directly to the caller.\r\n\r\nThis PR modifies the code to generate a base message in a common manner with the ability to set verbose flag for acl dry run errors, and keep it unset for normal/rm_call/script cases\r\n\r\n```c\r\nsds getAclErrorMessage(int acl_res, user *user, struct redisCommand *cmd, sds errored_val, int verbose) {\r\n    switch (acl_res) {\r\n    case ACL_DENIED_CMD:\r\n        return sdscatfmt(sdsempty(), \"User %S has no permissions to run \"\r\n                                     \"the '%S' command\", user->name, cmd->fullname);\r\n    case ACL_DENIED_KEY:\r\n        if (verbose) {\r\n            return sdscatfmt(sdsempty(), \"User %S has no permissions to access \"\r\n                                         \"the '%S' key\", user->name, errored_val);\r\n        } else {\r\n            return sdsnew(\"No permissions to access a key\");\r\n        }\r\n    case ACL_DENIED_CHANNEL:\r\n        if (verbose) {\r\n            return sdscatfmt(sdsempty(), \"User %S has no permissions to access \"\r\n                                         \"the '%S' channel\", user->name, errored_val);\r\n        } else {\r\n            return sdsnew(\"No permissions to access a channel\");\r\n        }\r\n    }\r\n```\r\n\r\nThe caller can append/prepend the message (adding NOPERM for normal/RM_Call or indicating it's within a script).\r\n\r\nImpact:\r\n- Plain commands, as well as scripts and RM_Call now include the user name.\r\n- ACL DRYRUN remains the only one that's verbose (mentions the offending channel or key name)\r\n- Changes RM_Call ACL errors from being a `-ERR` to being `-NOPERM` (besides for textual changes)\r\n  **This somewhat a breaking change, but it only affects the RM_Call with both `C` and `E`, or `D`**\r\n- Changes ACL errors in scripts textually from being\r\n  `The user executing the script <old non unified text>`\r\n  to\r\n  `ACL failure in script: <new unified text>`\r\n",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 20,
        "changed_files": 4,
        "created_at": "2022-08-21T13:24:20Z",
        "closed_at": "2022-08-28T10:10:11Z",
        "merged_at": "2022-08-28T10:10:11Z",
        "body": "When RM_Call was used with `M` (reject OOM), `W` (reject writes), as well as `S` (rejecting stale or write commands in \"Script mode\"), it would have only checked the command flags, but not the declared [script flag](https://redis.io/topics/lua-api#script_flags) in case it's a command that runs a script.\r\n\r\nRefactoring: extracts out similar code in server.c's processCommand to be usable in RM_Call as well.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-08-21T12:42:43Z",
        "closed_at": "2022-09-05T13:19:32Z",
        "merged_at": "2022-09-05T13:19:32Z",
        "body": "Add a new \"D\" flag to RM_Call which runs whatever verification the user requests,\r\nbut returns before the actual execution of the command.\r\n\r\nIt automatically enables returning error messages as CallReply objects to distinguish\r\nsuccess (NULL) from failure (CallReply returned).\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 112,
        "changed_files": 17,
        "created_at": "2022-08-21T09:11:00Z",
        "closed_at": "2022-08-28T08:24:47Z",
        "merged_at": "2022-08-28T08:24:47Z",
        "body": "TLDR: the CLUSTER command originally had the `random` flag,\r\nso all the sub-commands initially got that new flag, but in fact many\r\nof them don't need it.\r\nThe only effect of this change is on the output of COMMAND INFO.\r\n\r\nOriginal text:\r\n\r\n`cluster keyslot` is deterministic.\r\nI doubt whether other cluster commands is `NONDETERMINISTIC_OUTPUT`.\r\n`cluster myid`\r\n`cluster flushslots`\r\n\r\nWhat definition of `NONDETERMINISTIC_OUTPUT` really is?",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-08-19T08:12:38Z",
        "closed_at": "2022-09-19T10:59:37Z",
        "merged_at": "2022-09-19T10:59:37Z",
        "body": "This bug was introduced in #10344 (7.0.3), and it breaks the\r\nredis-cli --cluster create usage in #10436 (7.0 RC3).\r\n\r\nAt the same time, the cluster-port support introduced in #10344\r\ncannot use the DNS lookup brought by #10436.\r\n\r\nFixes #11148\r\n\r\n> Release notes:\r\nRegression in 7.0.3 resulting redis-cli doesn't do DNS lookup before sending CLUSTER MEET",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-08-19T07:28:20Z",
        "closed_at": "2022-08-28T08:33:41Z",
        "merged_at": "2022-08-28T08:33:41Z",
        "body": "Check the validity of the value before performing the create operation, prevents new data from being generated even if the request fails to execute.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-08-17T10:40:16Z",
        "closed_at": "2022-09-08T02:48:11Z",
        "merged_at": "2022-09-08T02:48:11Z",
        "body": " #11140  Thanks for approval",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-08-17T03:59:30Z",
        "closed_at": "2022-08-18T16:18:18Z",
        "merged_at": "2022-08-18T16:18:18Z",
        "body": "A timing issue like this was reported in freebsd daily CI:\r\n```\r\n*** [err]: Verify command got unblocked after resharding in tests/unit/cluster/cli.tcl\r\nExpected 'CLUSTERDOWN The cluster is down' to match '*MOVED*'\r\n```\r\n\r\nThe reason is, in clusterRedirectBlockedClientIfNeeded (which is\r\ncalled by clientsCron), it is possible for the client to unblock\r\nbecause of CLUSTERDOWN, depending on the cluster state when clientsCron\r\nis called.\r\n\r\nThe cluster-down might be because cluster-node-timeout is set to 1.\r\nIn this fix, we change the cluster-node-timeout from 1 to 1000.\r\nIt will stabilize the test a bit. And additionally fix some typos.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-08-16T18:16:07Z",
        "closed_at": "2022-08-21T04:52:57Z",
        "merged_at": "2022-08-21T04:52:57Z",
        "body": "This PR includes 2 missed test cases of XDEL and XGROUP CREATE command\r\n\r\n1. one test case: XDEL delete multiply id once\r\n2. 3 test cases:  XGROUP CREATE has ENTRIESREAD parameter, which equal 0 (special positive number), 3 and negative value.\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 42,
        "changed_files": 3,
        "created_at": "2022-08-16T14:10:27Z",
        "closed_at": "2022-08-26T16:09:24Z",
        "merged_at": "2022-08-26T16:09:24Z",
        "body": "- Remove redundant array bio_pending[]. Value at index i in array identically reflects \r\nthe length of array list bio_jobs[i]. Better to use directly list length getter instead of\r\nhaving another array. No critical section issues to concern about.\r\n\r\n- Changed returned value of bioPendingJobsOfType() from \"long long\" to \"long\". \r\n(type of array length is only 'long') \r\n\r\n- Masked unused API. Maybe we will use this API later. Let's keep it for now.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 24,
        "changed_files": 2,
        "created_at": "2022-08-16T11:19:06Z",
        "closed_at": "2022-08-16T19:04:22Z",
        "merged_at": "2022-08-16T19:04:22Z",
        "body": "Make sure the script calls in the tests declare the keys they intend to use.\r\nDo that with minimal changes to existing lines (so many scripts still have a hard coded key names)",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2022-08-16T04:49:14Z",
        "closed_at": "2022-10-02T11:18:43Z",
        "merged_at": null,
        "body": "As title.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-08-15T16:24:57Z",
        "closed_at": "2022-08-16T05:14:59Z",
        "merged_at": null,
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-08-12T20:52:21Z",
        "closed_at": "2022-08-14T11:29:06Z",
        "merged_at": "2022-08-14T11:29:06Z",
        "body": "Fix Lua compile warning on GCC 12.1\r\n\r\nGCC 12.1 prints a warning on compile: \r\n```\r\nldump.c: In function \u2018DumpString\u2019:\r\nldump.c:63:26: warning: the comparison will always evaluate as \u2018false\u2019 for the pointer operand in \u2018s + 24\u2019 must not be NULL [-Waddress]\r\n   63 |  if (s==NULL || getstr(s)==NULL)\r\n\r\n```\r\n\r\nIt seems correct, `getstr(s)` can't be `NULL`.  \r\nAlso, I see Lua v5.2 does not have that check: https://github.com/lua/lua/blob/v5-2/ldump.c#L63\r\nI assume it is safe to delete this check just to silence warning. \r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2022-08-12T04:35:42Z",
        "closed_at": "2022-08-14T11:53:40Z",
        "merged_at": "2022-08-14T11:53:40Z",
        "body": "Before this change in hashTypeSet() function, we first use dictFind() to look for the field and if it does not exist, we use dictAdd() to add it. In dictAdd() function the dictionary will look for the field again and I think this is meaningless as we already know that the field does not exist.\r\n\r\nAn optimization is to use dictAddRaw() instead of dictFind() and dictAdd(). If we use dictAddRaw(), a new entry will be added when the field does not exist, and what we should do then is just set the value of that entry, and set its key to 'sdsdup(field)' in the case that 'HASH_SET_TAKE_FIELD' flag wasn't set.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-08-11T15:40:04Z",
        "closed_at": "2022-08-14T07:50:31Z",
        "merged_at": "2022-08-14T07:50:31Z",
        "body": "The divided by two and less <= 10 logics were changed in 06ca9d683920da19ad53532f8cd55b54584027bc.\r\nNow we just decrement the counter by num_periods.\r\n\r\nThe lfu-decay-time special value of 0 's meaning was actually changed in 06ca9d683920da19ad53532f8cd55b54584027bc.\r\nNow we won't do anything on counter if lfu-decay-time is 0.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-08-11T09:41:19Z",
        "closed_at": "2022-08-14T15:25:01Z",
        "merged_at": "2022-08-14T15:25:01Z",
        "body": "There's really no point in having dedicated flags to test these features (why shouldn't all commands/features get their own tag?)",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2022-08-11T09:23:21Z",
        "closed_at": "2022-08-11T12:28:17Z",
        "merged_at": "2022-08-11T12:28:17Z",
        "body": "Fix overflow in redis-benchmark affecting latency measurements on 32bit builds.\r\n\r\nIf `long` is 4 bytes (typical on 32 bit systems), multiplication overflows. Using `long long` will fix the issue as it is guaranteed to be at least 8 bytes. \r\n\r\nAlso, I've added a change to reuse `ustime()` for `mstime()`. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 144,
        "deletions": 57,
        "changed_files": 6,
        "created_at": "2022-08-10T14:54:56Z",
        "closed_at": "2022-11-30T12:21:31Z",
        "merged_at": "2022-11-30T12:21:31Z",
        "body": "Fix #9996\r\n\r\n1. \"Fixed\" the current code so that seen-time/idle actually refers to interaction attempts (as documented; breaking change)\r\n2. Added active-time/inactive to refer to successful interaction (what seen-time/idle used to be)\r\n\r\nAt first, I tried to avoid changing the behavior of seen-time/idle but then realized that, in this case, the odds are the people read the docs and implemented their code based on the docs (which didn't match the behavior).\r\nFor the most part, that would work fine, except that issue #9996 was found.\r\n\r\nI was working under the assumption that people relied on the docs, and for the most part, it could have worked well enough. so instead of fixing the docs, as I would usually do, I fixed the code to match the docs in this particular case.\r\n\r\nNote that, in case the consumer has never read any entries, the values\r\nfor both \"active-time\" (XINFO FULL) and \"inactive\" (XINFO CONSUMERS) will\r\nbe -1, meaning here that the consumer was never active.\r\n\r\nNote that seen/active time is only affected by XREADGROUP / X[AUTO]CLAIM, not\r\nby XPENDING, XINFO, and other \"read-only\" stream CG commands (always has been,\r\neven before this PR)\r\n\r\nOther changes:\r\n* Another behavioral change (arguably a bugfix) is that XREADGROUP and X[AUTO]CLAIM create the consumer regardless of whether it was able to perform some reading/claiming\r\n* RDB format change to save the `active_time`, and set it to the same value of `seen_time` in old rdb files.\r\n\r\nTodo:\r\n- [ ] update docs",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 161,
        "deletions": 115,
        "changed_files": 10,
        "created_at": "2022-08-09T14:18:33Z",
        "closed_at": "2022-10-27T08:57:05Z",
        "merged_at": "2022-10-27T08:57:05Z",
        "body": "Renamed from \"Pause Clients\" to \"Pause Actions\" since the mechanism can pause\r\nseveral actions in redis, not just clients (e.g. eviction, expiration).\r\n\r\nPreviously each pause purpose (which has a timeout that's tracked separately from others purposes),\r\nalso implicitly dictated what it pauses (reads, writes, eviction, etc). Now it is explicit, and\r\nthe actions that are paused (bit flags) are defined separately from the purpose. \r\n\r\n- Previously, when using feature pause-client it also implicitly means to make the server static:\r\n  - Pause replica traffic\r\n  - Pauses eviction processing\r\n  - Pauses expire processing\r\n\r\nMaking the server static is used also for failover and shutdown. This PR internally rebrand \r\npause-client API to become pause-action API. It also Simplifies pauseClients structure \r\nby replacing pointers array with static array.\r\n\r\nThe context of this PR is to add another trigger to pause-client which will activated in case\r\nof OOM as throttling mechanism ([see here](https://github.com/redis/redis/issues/10907)).\r\nIn this case we want only to pause client, and eviction actions.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 25,
        "changed_files": 4,
        "created_at": "2022-08-09T06:30:36Z",
        "closed_at": "2022-08-14T08:50:19Z",
        "merged_at": "2022-08-14T08:50:19Z",
        "body": "This pr mainly has the following four changes:\r\n1. Add missing lua_pop in `luaGetFromRegistry`.\r\n    This bug affects `redis.register_function`, where `luaGetFromRegistry` in `luaRegisterFunction` will return null when we call `redis.register_function` nested.\r\n    .e.g\r\n    ```\r\n    FUNCTION LOAD \"#!lua name=mylib \\n local lib=redis \\n lib.register_function('f2', function(keys, args) lib.register_function('f1', function () end) end)\"\r\n    fcall f2 0\r\n    ````\r\n    But since we exit when luaGetFromRegistry returns null, it does not cause the stack to grow indefinitely.\r\n\r\n3. When getting `REGISTRY_RUN_CTX_NAME` from the registry, use `serverAssert` instead of error return.\r\n    Since none of these lua functions are registered at the time of function load, scriptRunCtx will never be NULL.\r\n4. Add `serverAssert` for `luaLdbLineHook`, `luaEngineLoadHook`.\r\n5. Remove `luaGetFromRegistry` from `redis_math_random` and `redis_math_randomseed`, it looks like they are redundant.\r\n    ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2022-08-09T03:46:33Z",
        "closed_at": "2022-11-28T12:51:26Z",
        "merged_at": "2022-11-28T12:51:26Z",
        "body": "redis-benchmark: when trying to get the CONFIG before benchmark,\r\navoid printing any warning on most errors (e.g. NOPERM error).\r\navoid aborting the benchmark on NOPERM.\r\nkeep the warning only when we abort the benchmark on a NOAUTH error",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-08-08T21:12:35Z",
        "closed_at": "2022-08-10T06:00:16Z",
        "merged_at": null,
        "body": "When I looked at the usage of parameter ENTRIESADDED  of xSetID internal command,\r\nI found the condition is **entries_added < 0**, but the error message is **entries_added must be positive**\r\nThus they have contradiction.\r\n\r\nI check the test cases of command xSetID, i think the 0 is acceptable, so in this PR I update the error message to\r\n**entries_added can only be 0 or positive**",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-08-08T19:43:46Z",
        "closed_at": "2022-08-23T06:38:00Z",
        "merged_at": "2022-08-23T06:37:59Z",
        "body": "The previous implementation calls `snprintf` twice, the second time used to\r\n'memcpy' the output of the first, which could be a very large string.\r\nThe new implementation reserves space for the protocol header ahead\r\nof the formatted double, and then prepends the string length ahead of it.\r\n\r\nMeasured improvement of simple ZADD of some 25%.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2022-08-08T15:00:22Z",
        "closed_at": "2022-08-09T05:50:26Z",
        "merged_at": "2022-08-09T05:50:26Z",
        "body": "replace \"dist\" with \"dict\"",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-08-07T12:17:56Z",
        "closed_at": "2022-08-07T13:32:32Z",
        "merged_at": "2022-08-07T13:32:32Z",
        "body": "some skip tags were missing on some tests",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-08-05T12:42:03Z",
        "closed_at": "2022-08-07T06:21:19Z",
        "merged_at": "2022-08-07T06:21:19Z",
        "body": "`bitfield` with `get` may not be readonly.\r\n\r\n```\r\n127.0.0.1:6384> acl setuser hello on nopass %R~* +@all\r\nOK\r\n127.0.0.1:6384> auth hello 1\r\nOK\r\n127.0.0.1:6384> bitfield hello set i8 0 1\r\n(error) NOPERM this user has no permissions to access one of the keys used as arguments\r\n127.0.0.1:6384> bitfield hello set i8 0 1 get i8 0\r\n1) (integer) 0\r\n2) (integer) 1\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 67,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2022-08-05T06:46:52Z",
        "closed_at": "2022-08-28T08:37:26Z",
        "merged_at": "2022-08-28T08:37:26Z",
        "body": "Bugfix:\r\nwith the scenario if we force assigned a slot to other master,  old master will lose the slot ownership, then old master will call the function delKeysInSlot() to delete all keys which in the slot. These delete operations should replicate to replicas, avoid the data divergence issue in master and replicas.\r\n\r\nAdditionally, in this case, we now call:\r\n* signalModifiedKey (to invalidate WATCH)\r\n* moduleNotifyKeyspaceEvent (key space notification for modules)\r\n* dirty++ (to signal that the persistence file may be outdated)\r\n\r\nFixes #10967\r\n\r\n> Release notes: Fix a bug where nodes in a cluster may not replicate or handle internal events for keys deleted when another node in the cluster claimed a slot.",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 21,
        "changed_files": 9,
        "created_at": "2022-08-03T14:44:43Z",
        "closed_at": "2022-08-03T16:38:08Z",
        "merged_at": "2022-08-03T16:38:08Z",
        "body": "Fixing few macros that doesn't follows most basic safety conventions\r\nwhich is wrapping any usage of passed variable\r\nwith parentheses and if written more than one command, then wrap\r\nit with do-while(0) (or parentheses).\r\n\r\nNeedless to say, whenever possible we should prefer inline functions\r\nover using macros.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2022-08-03T12:52:17Z",
        "closed_at": "2022-08-03T14:35:56Z",
        "merged_at": null,
        "body": "Fixing some macros that doesn't follows most basic conventions (of \r\nwriting macros) which is wrapping any usage of passed variable \r\nwith parentheses and if written more than one command, then wrap\r\nit with do-while(0) (or parentheses). \r\n\r\nNeedless to say, whenever possible we should prefer inline functions\r\nover using macros.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2022-08-03T10:27:09Z",
        "closed_at": "2022-08-04T07:10:42Z",
        "merged_at": "2022-08-04T07:10:42Z",
        "body": "Small refactoring done to reuse `checkGoodReplicasStatus` in `script.c` when checking for status of good replicas.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-08-02T16:22:04Z",
        "closed_at": "2022-08-03T05:03:13Z",
        "merged_at": "2022-08-03T05:03:13Z",
        "body": "This is a harmless optimization/bug.\r\nWhen the top of the lua stack is a string, we should not continue to use `lua_getfield` to get the table fields.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-08-01T15:20:26Z",
        "closed_at": "2022-08-02T18:56:48Z",
        "merged_at": "2022-08-02T18:56:48Z",
        "body": "Bumps [vmactions/freebsd-vm](https://github.com/vmactions/freebsd-vm) from 0.2.0 to 0.2.3.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vmactions/freebsd-vm/releases\">vmactions/freebsd-vm's releases</a>.</em></p>\n<blockquote>\n<p>Fix ntpd random issue</p>\n<h2>Update Major release version</h2>\n<p>Please use the major release version <code>v0</code> instead.</p>\n<h2>Just polish the output</h2>\n<p>Minor, Just polish the output</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/b85d3b6abf80e0b58cc0f2b6e4414dc0523a861a\"><code>b85d3b6</code></a> minor</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/7bdf527f9527db5cb781f6d03db2d9a1c2b5474e\"><code>7bdf527</code></a> enabled ntpd</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/7293c5b370d924b192661360c2adc28d4a6bf35e\"><code>7293c5b</code></a> speedup</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/acda5b0241674f35af4b7776dc2bf492c64e4233\"><code>acda5b0</code></a> fix ntpdate</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/5ac199dccae859f635402f56d5a4de201a515b3e\"><code>5ac199d</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/394d00a24b2cf69154caa62c1ac0fa21f4893262\"><code>394d00a</code></a> fix ntpdate</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/4d2f252c69194bb52f533a22595dd323426c1465\"><code>4d2f252</code></a> fix ntpdate</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/13911cbfa1c47de2c72d587b0c79fbcd0bebd81f\"><code>13911cb</code></a> Update version to v0.2.2</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/8ce7329a90423d6b25cdd7e59ed80c0fe309731b\"><code>8ce7329</code></a> Generated from <a href=\"https://github.com/vmactions/base-vm\">https://github.com/vmactions/base-vm</a></li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/89bfd648fae0c29e0a27f3fb17dc93bc566fa325\"><code>89bfd64</code></a> Update version to v0.2.1</li>\n<li>Additional commits viewable in <a href=\"https://github.com/vmactions/freebsd-vm/compare/v0.2.0...v0.2.3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vmactions/freebsd-vm&package-manager=github_actions&previous-version=0.2.0&new-version=0.2.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-08-01T04:44:03Z",
        "closed_at": "2022-11-26T04:36:34Z",
        "merged_at": "2022-11-26T04:36:34Z",
        "body": "A deleted timer still would be selected by `usUntilEarliestTimer` now, and result in an unnecessary wakeup(if the timer is deleted by `AE_NOMORE` returned by `aeTimeProc`, an immediate wakeup follows) . Here an untouchable time is assigned to the deleted timer to avoid the case.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-07-31T07:43:39Z",
        "closed_at": "2022-07-31T13:14:39Z",
        "merged_at": "2022-07-31T13:14:39Z",
        "body": "tracking pending invalidation message of flushdb sent by trackingHandlePendingKeyInvalidations should use proto.\r\nThis is caused by https://github.com/redis/redis/pull/11038.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-07-30T14:35:28Z",
        "closed_at": "2022-07-31T01:35:53Z",
        "merged_at": null,
        "body": "The condition `type == HI_SDS_TYPE_5` seems to be unnecessary, so it would be better to remove it.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 114,
        "deletions": 8,
        "changed_files": 3,
        "created_at": "2022-07-29T03:31:56Z",
        "closed_at": "2022-07-31T14:09:32Z",
        "merged_at": null,
        "body": "When updating SENTINEL with master\u2019s new password (command:\r\nSENTINEL SET mymaster auth-pass some-new-password),\r\nsentinel might still keep the old connection and avoid reconnecting\r\nwith the new password. This is because of wrong logic that traces\r\nthe last ping (pong) time to servers. In fact it worked fine until https://github.com/redis/redis/commit/8631e6477904f3d8f87662fd93a1ba294615654a\r\nchanged the condition to send ping. To resolve it with minimal risk,\r\nlet\u2019s disconnect master and replicas once changing password/user.\r\n\r\nBased on earlier work of: [yz1509](https://github.com/yz1509)\r\n\r\nmerge from 590bcece95ea6fa582a7c4d9ccf6f380899a2633\r\n\r\n\r\nPS: this merge is copy from #10400,just cherry-pick to redis 5.0.x",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2022-07-29T03:13:48Z",
        "closed_at": "2022-08-21T14:55:46Z",
        "merged_at": "2022-08-21T14:55:46Z",
        "body": "Till now Redis officially supported tuning it via environment variable see #1074.\r\nBut we had other requests to allow changing it at runtime, see #799, and #11041.\r\n\r\nNote that `strcoll()` is used as Lua comparison function and also for comparison of certain string objects in Redis, which leads to a problem that, in different regions, for some characters, the result may be different. Below is an example.\r\n```\r\n127.0.0.1:6333> SORT test alpha\r\n1) \"<\"\r\n2) \">\"\r\n3) \",\"\r\n4) \"*\"\r\n127.0.0.1:6333> CONFIG GET locale-collate\r\n1) \"locale-collate\"\r\n2) \"\"\r\n127.0.0.1:6333> CONFIG SET locale-collate 1\r\n(error) ERR CONFIG SET failed (possibly related to argument 'locale')\r\n127.0.0.1:6333> CONFIG SET locale-collate C\r\nOK\r\n127.0.0.1:6333> SORT test alpha\r\n1) \"*\"\r\n2) \",\"\r\n3) \"<\"\r\n4) \">\"\r\n```\r\nThat will cause accidental code compatibility issues for Lua scripts and some Redis commands. This commit creates a new config parameter to control the local environment which only affects `Collate` category. Above shows how it affects `SORT` command, and below shows the influence on Lua scripts.\r\n```\r\n127.0.0.1:6333> CONFIG GET locale-collate\r\n1) \" locale-collate\"\r\n2) \"C\"\r\n127.0.0.1:6333> EVAL \"return ',' < '*'\" 0\r\n(nil)\r\n127.0.0.1:6333> CONFIG SET locale-collate \"\"\r\nOK\r\n127.0.0.1:6333> EVAL \"return ',' < '*'\" 0\r\n(integer) 1\r\n```\r\n",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 102,
        "changed_files": 6,
        "created_at": "2022-07-28T20:43:24Z",
        "closed_at": "2022-08-15T18:41:44Z",
        "merged_at": "2022-08-15T18:41:44Z",
        "body": "The initial module format introduced in 4.0  RC1 and was changed in RC2\r\nThe initial function format introduced in 7.0 RC1 and changed in RC3",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 10,
        "changed_files": 3,
        "created_at": "2022-07-28T15:03:14Z",
        "closed_at": "2022-08-15T07:20:24Z",
        "merged_at": null,
        "body": "Array `unsigned long long bio_pending[]` holds the number of pending\r\njobs for every OP type. The basic operations on its entries are increment, \r\ndecrement and read. There is no real need to such big counter. Reduced\r\nto `unsigned long`. There is also no need use mutex to read a primitive. ",
        "comments": 16
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 14,
        "changed_files": 3,
        "created_at": "2022-07-28T13:27:34Z",
        "closed_at": "2022-10-11T12:12:15Z",
        "merged_at": null,
        "body": "An issue that we encounter recently is a user that loaded a module that saves AUX data. The user ended up not using the module, but the RDB that was created did contain the module AUX data. The user ended up not been able to load his data without the module (even though he is not using the module).\r\n\r\nNotice that even if the module saves nothing on the module AUX save callback, the module AUX load will still be called when loading the RDB and will failed if the module is missing. So the problem can not be solved on the module level and requires changes in Redis.\r\n\r\n## The Solution\r\n\r\nWe believe the AUX data is used to save module metadata and it is OK to simply skip it if the module isn't loaded. This is what the PR does, if module is not loaded we just skip its AUX data. To conclude:\r\n\r\n* Module not loaded -> skip module aux data\r\n* Module loaded -> call the module load callback\r\n\r\n## Other Solutions to Consider\r\n\r\nThe other solutions attempt to recognize the case where the module wrote nothing on the AUX save callback and allow loading the RDB (even if the module is missing) in this case only.\r\nNote that we'll need some mechanism to read-head the next module type opcode so we can detect the EOF marker without damaging the parsing.\r\n\r\n1. Skip module AUX data only if **no data was actually written to the RDB by the module AUX save** callback and the **module is not loaded**. If the module is loaded we can not skip calling the load callback because otherwise it can be considered a breaking change. So we have to pass the responsibility of **not** reading any data to the module, but the module can not know that he has nothing to read without reading something :). So we need to give the module an ability to check that there is nothing to read. We can do it with another module API, `RM_LoadHasData`, that will tell the module if something was written to the AUX field. To conclude:\r\n    * Aux field is empty ->\r\n      * Module not loaded -> skip\r\n      * Module loaded -> call the module load callback, module will need to use `RM_LoadHasData` to know if there is a data to read.\r\n    * Aux field not empty ->\r\n      * Module not loaded -> fail\r\n      * Module loaded -> call the module load callback.\r\n\r\n2. Break the existing API and decide that if nothing was written on the save callback then we will not call the load callback (even if the module is exists). This can be considered a breaking change but it is easier solution then (1). To conclude:\r\n    * Aux field is empty ->\r\n      * Module not loaded -> skip\r\n      * Module loaded -> skip\r\n    * Aux field not empty ->\r\n      * Module not loaded -> fail\r\n      * Module loaded -> call the module load callback.\r\n\r\n3. Same as 2 but with some datatype flag to avoid breaking change. To conclude:\r\n    * Aux field is empty ->\r\n      * Module not loaded -> skip\r\n      * Module loaded -> check datatype flag to decide whether or not to call the module load callback\r\n    * Aux field not empty ->\r\n      * Module not loaded -> fail\r\n      * Module loaded -> call the module load callback.\r\n\r\nWe do believe the solution in the PR is good enough but we want to hear other opinions.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-07-28T03:30:48Z",
        "closed_at": "2022-07-28T22:14:18Z",
        "merged_at": "2022-07-28T22:14:18Z",
        "body": "In clusterMsgPingExtForgottenNode, sizeof(name) is CLUSTER_NAMELEN,\r\nand sizeof(clusterMsgPingExtForgottenNode) is > CLUSTER_NAMELEN.\r\nDoing a (name + sizeof(clusterMsgPingExtForgottenNode)) sanitizer\r\ngenerates an out-of-bounds error which is a false positive in here.\r\n\r\nReported in test-sanitizer-undefined (clang):\r\n```\r\nruntime error: index 48 out of bounds for type 'char [40]'\r\nSUMMARY: UndefinedBehaviorSanitizer: undefined-behavior\r\n```\r\n\r\nIn this fix, we are using ext+1 instead of ext->name+sizeof(clusterMsgPingExtForgottenNode)\r\nto avoid false positive sanitizer out-of-bounds error. Also convert writeHostnamePingExt.\r\n\r\nintroduced in #10869",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-07-27T15:17:54Z",
        "closed_at": "2022-08-10T08:58:55Z",
        "merged_at": "2022-08-10T08:58:55Z",
        "body": "> Release notes: Fix bug with scripts ignoring client tracking NOLOOP\r\n\r\n## Reproduce the bug\r\n```\r\n127.0.0.1:6379> hello 3\r\n1# \"server\" => \"redis\"\r\n2# \"version\" => \"255.255.255\"\r\n3# \"proto\" => (integer) 3\r\n4# \"id\" => (integer) 3\r\n5# \"mode\" => \"standalone\"\r\n6# \"role\" => \"master\"\r\n7# \"modules\" => (empty array)\r\n127.0.0.1:6379> CLIENT TRACKING ON OPTOUT NOLOOP\r\nOK\r\n127.0.0.1:6379> get k\r\n\"2\"\r\n127.0.0.1:6379> eval \"return redis.call('set', 'k', '2')\" 0\r\nOK\r\n127.0.0.1:6379> get k\r\n-> invalidate: 'k'\r\n\"2\"\r\n127.0.0.1:6379>\r\n```\r\nI got unexpected `invalidate: 'k'` when set client tracking with OPTOUT NOLOOP,  fix the client type.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 385,
        "deletions": 373,
        "changed_files": 62,
        "created_at": "2022-07-27T15:15:12Z",
        "closed_at": "2022-08-18T12:09:36Z",
        "merged_at": "2022-08-18T12:09:36Z",
        "body": "Partial fix of #10948\r\n\r\nThis PR makes sure that \"name\" is unique for all arguments in the same\r\nlevel (i.e. all args of a command and all args within a block/oneof).\r\nThis means several argument with identical meaning can be referred to together,\r\nbut also if someone needs to refer to a specific one, they can use its full path.\r\n\r\nIn addition, the \"display_text\" field has been added, to be used by redis.io\r\nin order to render the syntax of the command (for the vast majority it is\r\nidentical to \"name\" but sometimes we want to use a different string\r\nthat is not \"name\")\r\nThe \"display\" field is exposed via COMMAND DOCS and will be present\r\nfor every argument, except \"oneof\" and \"block\" (which are container\r\narguments)\r\n\r\nOther changes:\r\n1. Make sure we do not have any container arguments (\"oneof\" or \"block\")\r\n   that contain less than two sub-args (otherwise it doesn't make sense)\r\n2. migrate.json: both AUTH and AUTH2 should not be \"optional\"\r\n3. arg names cannot contain underscores, and force the usage of hyphens (most of these were a result of the script that generated the initial json files from redis.io commands.json). \r\n\r\n- [x] go over all \"name\"s: make sure they make sense because soon we won't be able to modify them (people may rely on their value)\r\n- [x] validate no underscore in \"name\"",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-07-27T13:35:48Z",
        "closed_at": "2022-07-31T14:29:59Z",
        "merged_at": "2022-07-31T14:29:59Z",
        "body": "Before this change, if the module has an embedded string, then uses RedisModule_SaveString and RedisModule_LoadString, the result would be a raw string instead of an embedded string.\r\n\r\nNow the `RDB_LOAD_ENC` flag to `moduleLoadString` only affects integer encoding, but not embedded strings (which still hold an sds in the robj ptr, so they're actually still raw strings for anyone who reads them).\r\n\r\n> Release notes: Reduce memory usage on strings loaded by a module from an RDB file",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-07-27T10:53:02Z",
        "closed_at": "2023-03-12T16:08:03Z",
        "merged_at": "2023-03-12T16:08:03Z",
        "body": "In unsubscribe related commands, we need to read the specified\r\nnumber of replies according to the number of parameters.\r\n\r\nThese commands may return multiple RESP replies, and currently\r\nredis-cli only tries to read only one reply.\r\n\r\nFixes #11046, this redis-cli bug seems to be there forever.\r\nNote that the [UN]SUBSCRIBE command response is a bit awkward\r\nsee: https://github.com/redis/redis-doc/pull/2327",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2022-07-27T03:49:21Z",
        "closed_at": "2022-08-05T03:42:56Z",
        "merged_at": "2022-08-05T03:42:56Z",
        "body": "optimize the performance of multi-key commands in cluster mode by avoid `equalStringObjects` calls.\r\n\r\n`multiple_keys` flag  be only used  in importing slot, we can avoid comparing string objects in most scenarios.\r\nin my bench test, mset command with 10 keys improved by 16%.\r\n\r\nbench command:\r\n`memtier_benchmark -p 6385  --key-prefix=\"{asdfghjkl}\" --command=\"mset __key__ 1 __key__ 2 __key__ 3 __key__ 4 __key__ 5 __key__ 6 __key__ 7 __key__ 8 __key__ 9 __key__ 10 \"`\r\n\r\n\r\nOptimized result\r\n\r\n```\r\nALL STATS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec\r\n--------------------------------------------------------------------------------------------------\r\nMsets       26018.15         7.68145         6.84700        19.96700        39.93500      8635.69\r\nTotals      26018.15         7.68145         6.84700        19.96700        39.93500      8635.69\r\n\r\n```\r\n\r\nBefore optimization\r\n```\r\n ALL STATS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec\r\n--------------------------------------------------------------------------------------------------\r\nMsets       22383.81         8.92822         7.42300        33.27900        88.57500      7429.42\r\nTotals      22383.81         8.92822         7.42300        33.27900        88.57500      7429.42\r\n```\r\n\r\n```\r\nRelease notes:\r\nOptimize the performance of commands with multiple keys in cluster mode\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-07-26T16:49:59Z",
        "closed_at": "2022-07-27T11:58:25Z",
        "merged_at": "2022-07-27T11:58:25Z",
        "body": "The kill above is sometimes successful and sometimes already too late.\r\nThe PING in pysnc wrong offset test got rejected by bgsaveerr because\r\nlastbgsave_status is C_ERR.\r\n\r\nIn theory, using diskless can avoid PING being affected, because when\r\nthe replica is dropped, we will kill the child with SIGUSR1, and this\r\nwill not affect lastbgsave_status.\r\n\r\nAnyway, this kill is not particularly needed here, dropping the kill\r\nis the best one, since we do have the waitForBgsave, so just let it\r\ntake care of the bgsave. No need for fast termination.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 5,
        "created_at": "2022-07-26T15:53:38Z",
        "closed_at": "2022-08-04T07:47:37Z",
        "merged_at": "2022-08-04T07:47:37Z",
        "body": "This is an addition to #11039, which cleans up rdbLoad* related errno. Remove the\r\nerrno print from the outer message (may be invalid since errno may have been overwritten).\r\n\r\nOur aim should be the code that detects the error and knows which system call\r\ntriggered it, is the one to print errno, and not the code way up above (in some cases\r\na result of a logical error and not a system one).\r\n\r\nRemove the code to update errno in rdbLoadRioWithLoadingCtx, signature check\r\nand the rdb version check, in these cases, we do print the error message.\r\nThe caller dose not have the specific logic for handling EINVAL.\r\n\r\nSmall fix around rdb-preamble AOF: A truncated RDB is considered a failure,\r\nnot handled the same as a truncated AOF file.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 38,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2022-07-26T14:30:29Z",
        "closed_at": "2022-08-03T07:24:50Z",
        "merged_at": null,
        "body": "It is noted that **Lua** basically uses _strcoll()_ as comparison function, which leads to a problem that, in different regions, for some characters, the  comparison result may be different.  Below is an example.\r\n<img width=\"412\" alt=\"16578518801507(1)\" src=\"https://user-images.githubusercontent.com/56780191/181020766-ab0517ff-4021-4ebd-b0c3-119570ea9aa7.png\"> \r\nThis will cause accidental code compatibility issues for Lua scripts. And here provides a solution.\r\nThis commit allows the use of _setlocale()_ function of Lua os library. To avoid hidden systematic risk, the parameter _category_  is set to **collate** and is unchangeable. The packaged function accepts only one parameter and the other functions of os library are unreachable. Allow setting locale offers an opportunity to uniform standards. Below shows how it works.\r\n```\r\n127.0.0.1:6379> EVAL \"return os.setlocale('C')\" 0\r\n\"C\"\r\n127.0.0.1:6379> EVAL \"return os.clock()\" 0\r\n(error) ERR user_script:1: attempt to call field 'clock' (a nil value) script: ea58cfad299460ea863f576834104a675e48c28c, on @user_script:1.\r\n127.0.0.1:6379> EVAL \"return os.setlocale('C', 'numeric')\" 0\r\n(error) ERR wrong number of arguments script: 4393e830ac85509505f24894333f199e9a5e3eb6, on @user_script:1.\r\n127.0.0.1:6379> EVAL \"return os.setlocale('C')\" 0\r\n\"C\"\r\n127.0.0.1:6379> EVAL \"return '*' < ','\" 0\r\n(integer) 1\r\n127.0.0.1:6379> EVAL \"return '*' > ','\" 0\r\n(nil)\r\n127.0.0.1:6379> EVAL \"return os.setlocale('fr_FR')\" 0\r\n\"fr_FR\"\r\n127.0.0.1:6379> EVAL \"return '*' > ','\" 0\r\n(integer) 1\r\n127.0.0.1:6379> EVAL \"return '*' < ','\" 0\r\n(nil)\r\n```\r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 6,
        "changed_files": 5,
        "created_at": "2022-07-26T10:44:56Z",
        "closed_at": "2022-07-26T12:13:13Z",
        "merged_at": "2022-07-26T12:13:13Z",
        "body": "The reason we do this is because in #11036, we added error\r\nlog message when failing to open RDB file for reading.\r\nIn loadDdataFromDisk we call rdbLoad and also check errno,\r\nnow the logging corrupts errno (reported in alpine daily).\r\n\r\nIt is not safe to rely on errno as we do today, so we change\r\nthe return value of rdbLoad function to enums, like we have\r\nwhen loading an AOF.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-07-26T06:22:37Z",
        "closed_at": "2022-07-26T10:28:37Z",
        "merged_at": "2022-07-26T10:28:37Z",
        "body": "When FLUSHDB / FLUSHALL / SWAPDB is inside MULTI / EXEC, the client side tracking invalidation message was interleaved with transaction response causing broken protocol.\r\n\r\nFix https://github.com/redis/redis/issues/8935",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-07-24T19:27:33Z",
        "closed_at": "2022-07-25T05:06:51Z",
        "merged_at": null,
        "body": "GitHub Action is sunsetting the macOS 10.15 Actions runner. It will stop working intermittently until being completely removed by 2022-8-30: https://github.blog/changelog/2022-07-20-github-actions-the-macos-10-15-actions-runner-image-is-being-deprecated-and-will-be-removed-by-8-30-22/\r\n\r\nI have checked [`vmactions/freebsd-vm`](https://github.com/vmactions/freebsd-vm) does support `macos-12` (which requires `vmactions/freebsd-vm@0.2.0`).\r\n\r\nI have updated the `runs-on` to `macos-12` and bumped `vmactions/freebsd-vm` to `0.2.0` in the PR.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-07-24T11:10:58Z",
        "closed_at": "2022-08-14T13:13:02Z",
        "merged_at": null,
        "body": "Previously RM_FreeString did not handle NULL pointers, which is inconvenient\r\nand inconsistent with other existing APIs like free() and RM_Free().\r\n\r\nAn informational message will be logged each time the function is called with a\r\nNULL pointer. This is to help prevent backwards compatibility issues, where\r\nmodules developed and tested on the latest Redis could crash when loaded into\r\nan older version if they implicitly relied on this behaviour.\r\n\r\nThis log message may be removed when non-supporting versions of Redis are old\r\nenough.\r\n\r\nResolves #10887",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-07-24T10:58:42Z",
        "closed_at": "2022-07-25T06:30:12Z",
        "merged_at": null,
        "body": "Prevent forgetting to register API, adding assertions, see #11025\r\nAlso remove `ReplySetPushLength`, it is a dead code.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2022-07-24T07:51:29Z",
        "closed_at": "2022-07-26T07:33:51Z",
        "merged_at": "2022-07-26T07:33:51Z",
        "body": "Fix #11030, use lua_rawget to avoid triggering metatables.\r\n\r\n#11030 shows how return `_G` from the Lua script (either function or eval), cause the Lua interpreter to Panic and the Redis processes to exit with error code 1. Though return `_G` only panic on Redis 7 and 6.2.7, the underline issue exists on older versions as well (6.0 and 6.2). The underline issue is returning a table with a metatable such that the metatable raises an error.\r\n\r\nThe following example demonstrate the issue:\r\n```\r\n127.0.0.1:6379> eval \"local a = {}; setmetatable(a,{__index=function() foo() end}) return a\" 0\r\nError: Server closed the connection\r\n```\r\n```\r\nPANIC: unprotected error in call to Lua API (user_script:1: Script attempted to access nonexistent global variable 'foo')\r\n```\r\n\r\nThe Lua panic happened because when returning the result to the client, Redis needs to introspect the returning table and transform the table into a resp. In order to scan the table, Redis uses `lua_gettable` api which might trigger the metatable (if exists) and might raise an error. This code is not running inside `pcall` (Lua protected call), so raising an error causes the Lua to panic and exit. Notice that this is not a crash, its a Lua panic that exit with error code 1.\r\n\r\nReturning `_G` panics on Redis 7 and 6.2.7 because on those versions `_G` has a metatable that raises error when trying to fetch a none existing key.\r\n\r\n### Solution\r\n\r\nInstead of using `lua_gettable` that might raise error and cause the issue, use `lua_rawget` that simply return the value from the table without triggering any metatable logic. This is promised not to raise and error.\r\n\r\nThe downside of this solution is that it might be considered as breaking change, if someone rely on metatable in the returned value. An alternative solution is to wrap this entire logic with `pcall` (Lua protected call), this alternative require a much bigger refactoring. We need to decide if we believe its actually a breaking change or is it something that we can accept (@yossigo @oranagra @madolson FYI).\r\n\r\n### Back Porting\r\n\r\nThe same fix will work on older versions as well (6.2, 6.0). Notice that on those version, the issue can cause Redis to crash if inside the metatable logic there is an attempt to accesses Redis (`redis.call`). On 7.0, there is not crash and the `redis.call` is executed as if it was done from inside the script itself.\r\n\r\n### Tests\r\n\r\nTests was added the verify the fix\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2022-07-22T07:37:20Z",
        "closed_at": "2022-07-24T05:38:04Z",
        "merged_at": "2022-07-24T05:38:04Z",
        "body": "Swap M and N in the complexity formula of [B]ZMPOP\r\nhttps://github.com/redis/redis/issues/11022",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-07-22T06:33:47Z",
        "closed_at": "2022-07-24T05:50:21Z",
        "merged_at": "2022-07-24T05:50:21Z",
        "body": "RM_SetAbsExpire and RM_GetAbsExpire were not actually operational since they were introduced, due to omission in API registration.\r\nSorry this was an omission in the previous PR #8564",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2022-07-20T06:53:17Z",
        "closed_at": "2022-07-27T11:40:06Z",
        "merged_at": "2022-07-27T11:40:06Z",
        "body": "RM_Microseconds\r\nReturn the wall-clock Unix time, in microseconds\r\n\r\nRM_CachedMicroseconds\r\nReturns a cached copy of the Unix time, in microseconds.\r\nIt is updated in the server cron job and before executing a command.\r\nIt is useful for complex call stacks, such as a command causing a\r\nkey space notification, causing a module to execute a RedisModule_Call,\r\ncausing another notification, etc.\r\nIt makes sense that all these callbacks would use the same clock.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 805,
        "deletions": 772,
        "changed_files": 22,
        "created_at": "2022-07-19T11:07:59Z",
        "closed_at": "2023-01-01T21:35:42Z",
        "merged_at": "2023-01-01T21:35:42Z",
        "body": "*TL;DR*\r\n---------------------------------------\r\nFollowing the discussion over the issue [#7551](https://github.com/redis/redis/issues/7551)\r\nWe decided to refactor the client blocking code to eliminate some of the code duplications\r\nand to rebuild the infrastructure better for future key blocking cases.\r\n\r\n\r\n*In this PR*\r\n---------------------------------------\r\n1. reprocess the command once a client becomes unblocked on key (instead of running custom code for the unblocked path that's different than the one that would have run if blocking wasn't needed)\r\n2. eliminate some (now) irrelevant code for handling unblocking lists/zsets/streams etc...\r\n3. modify some tests to intercept the error in cases of error on reprocess after unblock (see details in the notes section below)\r\n4. replace '$' on the client argv with current stream id. Since once we reprocess the stream XREAD we need to read from the last msg and not wait for new msg  in order to prevent endless block loop. \r\n5. Added statistics to the info \"Clients\" section to report the:\r\n   * `total_blocking_keys` - number of blocking keys\r\n   * `total_blocking_keys_on_nokey` - number of blocking keys which have at least 1 client which would like\r\n   to be unblocked on when the key is deleted.\r\n6. Avoid expiring unblocked key during unblock. Previously we used to lookup the unblocked key which might have been expired during the lookup. Now we lookup the key using NOTOUCH and NOEXPIRE to avoid deleting it at this point, so propagating commands in blocked.c is no longer needed.\r\n7. deprecated command flags. We decided to remove the CMD_CALL_STATS and CMD_CALL_SLOWLOG and make an explicit verification in the call() function in order to decide if stats update should take place.\r\nThis should simplify the logic and also mitigate existing issues: for example module calls which are triggered as part of AOF loading might still report stats even though they are called during AOF loading.\r\n\r\n*Behavior changes*\r\n---------------------------------------------------\r\n\r\n1. As this implementation prevents writing dedicated code handling unblocked streams/lists/zsets,\r\nsince we now re-process the command once the client is unblocked some errors will be reported differently.\r\nThe old implementation used to issue\r\n``UNBLOCKED the stream key no longer exists``\r\nin the following cases:\r\n   - The stream key has been deleted (ie. calling DEL)\r\n   - The stream and group existed but the key type was changed by overriding it (ie. with set command)\r\n   - The key not longer exists after we swapdb with a db which does not contains this key\r\n   - After swapdb when the new db has this key but with different type.\r\n   \r\n  In the new implementation the reported errors will be the same as if the command was processed after effect:\r\n  **NOGROUP** - in case key no longer exists, or **WRONGTYPE** in case the key was overridden with a different type.\r\n\r\n2. Reprocessing the command means that some checks will be reevaluated once the client is unblocked.\r\nFor example, ACL rules might change since the command originally was executed and will fail once the client is unblocked.\r\nAnother example is OOM condition checks which might enable the command to run and block but fail the command reprocess once the client is unblocked.\r\n\r\n3. One of the changes in this PR is that no command stats are being updated once the command is blocked (all stats will be updated once the client is unblocked). This implies that when we have many clients blocked, users will no longer be able to get that information from the command stats. However the information can still be gathered from the client list. This also means that clients that get killed or timeout while blocked, will not increment the command stats. \r\n\r\n**Client blocking**\r\n---------------------------------------------------\r\n\r\nthe blocking on key will still be triggered the same way as it is done today.\r\nin order to block the current client on list of keys, the call to\r\nblockForKeys will still need to be made which will perform the same as it is today:\r\n\r\n*  add the client to the list of blocked clients on each key\r\n*  keep the key with a matching list node (position in the global blocking clients list for that key) in the client private blocking key dict.\r\n*  flag the client with CLIENT_BLOCKED\r\n*  update blocking statistics\r\n*  register the client on the timeout table\r\n\r\n**Key Unblock**\r\n---------------------------------------------------\r\n\r\nUnblocking a specific key will be triggered (same as today) by calling signalKeyAsReady.\r\nthe implementation in that part will stay the same as today - adding the key to the global readyList.\r\nThe reason to maintain the readyList (as apposed to iterating over all clients blocked on the specific key)\r\nis in order to keep the signal operation as short as possible, since it is called during the command processing.\r\nThe main change is that instead of going through a dedicated code path that operates the blocked command\r\nwe will just call processPendingCommandsAndResetClient.\r\n\r\n**ClientUnblock (keys)**\r\n---------------------------------------------------\r\n\r\n1. Unblocking clients on keys will be triggered after command is\r\n   processed and during the beforeSleep\r\n8. the general schema is:\r\n9. For each key *k* in the readyList:\r\n```            \r\nFor each client *c* which is blocked on *k*:\r\n            in case either:\r\n\t          1. *k* exists AND the *k* type matches the current client blocking type\r\n\t  \t      OR\r\n\t          2. *k* exists and *c* is blocked on module command\r\n\t    \t      OR\r\n\t          3. *k* does not exists and *c* was blocked with the flag\r\n\t             unblock_on_deleted_key\r\n                 do:\r\n                                  1. remove the client from the list of clients blocked on this key\r\n                                  2. remove the blocking list node from the client blocking key dict\r\n                                  3. remove the client from the timeout list\r\n                                  10. queue the client on the unblocked_clients list\r\n                                  11. *NEW*: call processCommandAndResetClient(c);\r\n```\r\n*NOTE:* for module blocked clients we will still call the moduleUnblockClientByHandle\r\n              which will queue the client for processing in moduleUnblockedClients list.\r\n\r\n**Process Unblocked clients**\r\n---------------------------------------------------\r\n\r\nThe process of all unblocked clients is done in the beforeSleep and no change is planned in that part.\r\nThe general schema will be:\r\nFor each client *c* in server.unblocked_clients:\r\n\r\n        * remove client from the server.unblocked_clients\r\n        * set back the client readHandler\r\n        * continue processing the pending command and input buffer.\r\n\r\n*Some notes regarding the new implementation*\r\n---------------------------------------------------\r\n\r\n1. Although it was proposed, it is currently difficult to remove the\r\n   read handler from the client while it is blocked.\r\n   The reason is that a blocked client should be unblocked when it is\r\n   disconnected, or we might consume data into void.\r\n\r\n2. While this PR mainly keep the current blocking logic as-is, there\r\n   might be some future additions to the infrastructure that we would\r\n   like to have:\r\n   - allow non-preemptive blocking of client - sometimes we can think\r\n     that a new kind of blocking can be expected to not be preempt. for\r\n     example lets imagine we hold some keys on disk and when a command\r\n     needs to process them it will block until the keys are uploaded.\r\n     in this case we will want the client to not disconnect or be\r\n     unblocked until the process is completed (remove the client read\r\n     handler, prevent client timeout, disable unblock via debug command etc...).\r\n   - allow generic blocking based on command declared keys - we might\r\n     want to add a hook before command processing to check if any of the\r\n     declared keys require the command to block. this way it would be\r\n     easier to add new kinds of key-based blocking mechanisms.\r\n\r\nSigned-off-by: Ran Shidlansik <ranshid@amazon.com>",
        "comments": 46
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2022-07-19T04:15:47Z",
        "closed_at": "2022-07-19T05:59:39Z",
        "merged_at": "2022-07-19T05:59:39Z",
        "body": "Following #10996, it forgot to modify RM_StringCompare in module.c\r\n\r\nModified RM_StringCompare, compareStringObjectsWithFlags,\r\ncompareStringObjects and collateStringObjects.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2022-07-18T17:58:53Z",
        "closed_at": "2022-07-20T01:15:38Z",
        "merged_at": null,
        "body": "Hello, I saw that _dictNextExp in dict needs to be optimized, so I added it.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-07-18T15:19:22Z",
        "closed_at": "2022-07-19T12:30:30Z",
        "merged_at": null,
        "body": "[//]: # (dependabot-start)\n\u26a0\ufe0f  **Dependabot is rebasing this PR** \u26a0\ufe0f \n\nRebasing might not happen immediately, so don't worry if this takes some time.\n\nNote: if you make any changes to this PR yourself, they will take precedence over the rebase.\n\n---\n\n[//]: # (dependabot-end)\n\nBumps [vmactions/freebsd-vm](https://github.com/vmactions/freebsd-vm) from 0.1.6 to 0.1.9.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vmactions/freebsd-vm/releases\">vmactions/freebsd-vm's releases</a>.</em></p>\n<blockquote>\n<h2>Fix bug for execSSH</h2>\n<p>fix bug <a href=\"https://github-redirect.dependabot.com/vmactions/freebsd-vm/issues/56\">vmactions/freebsd-vm#56</a></p>\n<h2>Support &quot;release&quot; config to select the OS version</h2>\n<p>Support 12.3, 13.0 and 13.1</p>\n<p>Upgrade to FreeBSD 13.1</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/fbc82e7d4b43cc377ec771ac616a168459f779f9\"><code>fbc82e7</code></a> fix exec</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/fb76e6fe988d372bee5579806e5987e40d8911e2\"><code>fb76e6f</code></a> minor</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/f9bf9cfeb244549811c5cd2361809ec295f81327\"><code>f9bf9cf</code></a> fix path</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/25d3ef16f4f24643560ef1c19d36ecb282fc8a0e\"><code>25d3ef1</code></a> fix doc</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/77cf0b6296b738429f8f9dc3533234ec0d9cc895\"><code>77cf0b6</code></a> Support release config to select FreeBSD version</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/70ee03a9843e3a620140403c82abc927954a4458\"><code>70ee03a</code></a> fix error code</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/4cd729015c49e724a717aeab306684f2ee7f65ff\"><code>4cd7290</code></a> test debug</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/3e82e669c490f048c6425c71ef34e86b61894add\"><code>3e82e66</code></a> s</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/8b171fc83fcf902d7417e7c8751ab56c8e2afd19\"><code>8b171fc</code></a> add</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/78931dff941e4ecd63eb3cf8f8ea45be7bf43823\"><code>78931df</code></a> remove t</li>\n<li>Additional commits viewable in <a href=\"https://github.com/vmactions/freebsd-vm/compare/v0.1.6...v0.1.9\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vmactions/freebsd-vm&package-manager=github_actions&previous-version=0.1.6&new-version=0.1.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 23,
        "changed_files": 4,
        "created_at": "2022-07-18T10:32:39Z",
        "closed_at": "2022-07-25T06:16:36Z",
        "merged_at": "2022-07-25T06:16:36Z",
        "body": "In rewriteAppendOnlyFileBackground, after flushAppendOnlyFile(1),\r\nand before openNewIncrAofForAppend, we should call redis_fsync\r\nto fsync the aof file.\r\n\r\nBecause we may open a new INCR AOF in openNewIncrAofForAppend,\r\nin the case of using everysec policy, the old AOF file may not\r\nbe fsynced in time (or even at all).\r\n\r\nWhen using everysec, we don't want to pay the disk latency from\r\nthe main thread, so we will do a background fsync.\r\n\r\nAdding a argument for bioCreateCloseJob, a `need_fsync` flag to\r\nindicate that a fsync is required before the file is closed. So we will\r\nfsync the old AOF file before we close it.\r\n\r\nA cleanup, we make bio_job become a union, since the free_* args and\r\nthe fd / fsync args are never used together.",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 6,
        "changed_files": 5,
        "created_at": "2022-07-18T08:38:37Z",
        "closed_at": "2022-07-18T13:04:07Z",
        "merged_at": "2022-07-18T13:04:07Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-31144) A specially crafted XAUTOCLAIM command on a stream\r\n  key in a specific state may result with heap overflow, and potentially\r\n  remote code execution. The problem affects Redis versions 7.0.0 or newer.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 217,
        "deletions": 15,
        "changed_files": 10,
        "created_at": "2022-07-18T03:07:30Z",
        "closed_at": "2023-08-25T03:23:44Z",
        "merged_at": null,
        "body": "ref https://github.com/redis/redis/issues/9925",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2022-07-16T15:58:40Z",
        "closed_at": "2023-01-05T00:41:50Z",
        "merged_at": null,
        "body": "Github -> GitHub",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-07-16T07:18:36Z",
        "closed_at": "2022-07-17T06:57:34Z",
        "merged_at": "2022-07-17T06:57:34Z",
        "body": "In the newly added cluster hostnames test, the primary is failing over during the reboot for valgrind so we are validating the wrong node. This change just sets the replica to prevent taking over, which seems to fix the test.\r\n\r\nWe could have also set the timeout higher, but it slows down the test.\r\n\r\n```\r\ndev-dsk-matolson-2c-96119f9f % ./runtest --valgrind --no-latency --verbose --clients 1 --timeout 2400 --dump-logs --single unit/cluster/hostnames\r\nCleanup: may take some time... OK\r\nStarting test server at port 21079\r\n[ready]: 22670\r\nTesting unit/cluster/hostnames\r\n=== (external:skip cluster) Starting server 127.0.0.1:21111 ok\r\n=== (external:skip cluster) Starting server 127.0.0.1:21112 ok\r\n=== (external:skip cluster) Starting server 127.0.0.1:21113 ok\r\n=== (external:skip cluster) Starting server 127.0.0.1:21114 ok\r\n=== (external:skip cluster) Starting server 127.0.0.1:21115 ok\r\n=== (external:skip cluster) Starting server 127.0.0.1:21116 ok\r\n=== (external:skip cluster) Starting server 127.0.0.1:21117 ok\r\n[ok]: Set cluster hostnames and verify they are propagated (1108 ms)\r\n[ok]: Update hostnames and make sure they are all eventually propagated (862 ms)\r\n[ok]: Remove hostnames and make sure they are all eventually propagated (963 ms)\r\n[ok]: Verify cluster-preferred-endpoint-type behavior for redirects and info (993 ms)\r\n[ok]: Verify the nodes configured with prefer hostname only show hostname for new nodes (3161 ms)\r\n[ok]: Test restart will keep hostname information (2562 ms)\r\n[ok]: Test hostname validation (17 ms)\r\n[1/1 done]: unit/cluster/hostnames (27 seconds)\r\n\r\n                   The End\r\n\r\nExecution time of different units:\r\n  27 seconds - unit/cluster/hostnames\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2022-07-15T15:27:41Z",
        "closed_at": "2022-07-17T06:40:07Z",
        "merged_at": "2022-07-17T06:40:07Z",
        "body": "According to the Redis functions documentation,  FCALL command format could be\r\nFCALL function_name  numberOfKeys  [key1, key2, key3.....]  [arg1, arg2,  arg3.....]\r\n\r\nSo in the json file of fcall and fcall_ro, we should add optional for  key and arg part.\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/179255520-40855982-87d2-4072-8857-e789899676ce.png)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2022-07-15T12:55:44Z",
        "closed_at": "2022-08-04T07:38:20Z",
        "merged_at": "2022-08-04T07:38:20Z",
        "body": "Fixes https://github.com/redis/redis/issues/10986\r\n\r\nProblem description:\r\nUsing the lrange benchmarks of redis benchmark ( I was investigating it due to #10981 ) we can see that we're wasting a large ammount of cpu cycles on sdslen() computing the sizes we already know:\r\n\r\n```\r\n                           - 77.55% lrangeCommand                                                                                                                                                          \u2592\r\n                              - 77.31% addListRangeReply                                                                                                                                                   \u2592\r\n                                 + 35.46% addReplyBulkCBuffer                                                                                                                                              \u2592\r\n                                 + 21.34% listTypeNext (inlined)                                                                                                                                           \u2592\r\n                                 - 11.32% addReply (inlined)                                                                                                                                               \u2592\r\n                                      6.55% sdslen (inlined)                                                                                                                                               \u2592\r\n                                      0.41% _addReplyToBufferOrList (inlined)                                                                                                                              \u2592\r\n                                 + 6.88% _addReplyToBufferOrList (inlined)                                                                                                                                 \u2592\r\n                                 + 0.60% addReplyLongLongWithPrefix                                                                                                                                        \u2592\r\n                                   0.50% prepareClientToWrite                                                                                                                                              \u2592\r\n                                   0.15% listTypeReleaseIterator (inlined)  \r\n```\r\n\r\nFix: \r\n\r\nThe fix is a simple replace of using addReply with addReplyProto for the `shared.crlf` related replies. \r\n\r\n-------\r\n\r\nOn the LRANGE_600 benchmark we get:\r\n\r\nbenchmark command:\r\n```\r\nredis-benchmark  -n 1000000 --threads 3 \"LRANGE\" \"mylist\" \"0\" \"599\"\r\n```\r\n\r\nUnstable\r\n```\r\nSummary:\r\n  throughput summary: 22055.58 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n        1.585     0.080     1.607     2.391     3.015     4.575\r\n```\r\nNew\r\n```\r\nSummary:\r\n  throughput summary: 22821.67 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n        1.623     0.056     1.639     2.375     2.959     7.935\r\n```",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-07-14T09:39:00Z",
        "closed_at": "2022-07-17T05:47:36Z",
        "merged_at": "2022-07-17T05:47:36Z",
        "body": "`--skipfile` can be repeated.\r\nFor example: ./runtests --skipfile file1.txt --skipfile file2.txt",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2022-07-14T06:03:29Z",
        "closed_at": "2022-08-04T08:13:29Z",
        "merged_at": "2022-08-04T08:13:29Z",
        "body": "This is the history of aof-race related changes:\r\n1. added in 3aa4b0097062b13031506b6b52fc8fc4bfec6dfc\r\n2. disabled in dcdfd005a0133a347cc0aae54c690cd8c845fed7\r\n3. enabled in 5c63922691ed8a821bd7f9f2837a8270f1154268\r\n4. disabled in 53a2af3941b0c6cd8057983ee92775916f1490ab\r\n\r\nThis PR refreshes the aof-race test, re-enable it.\r\nCloses #10971",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 95,
        "deletions": 70,
        "changed_files": 2,
        "created_at": "2022-07-13T14:10:59Z",
        "closed_at": "2022-08-01T12:52:40Z",
        "merged_at": "2022-08-01T12:52:40Z",
        "body": "This PR originates from #10948\r\n\r\nThe docs state that there is a new and an old argument format.\r\nThe current state of the arguments allows mixing the old and new format, thus the need for two additional `oneof` blocks.\r\nOne for differentiating the new from the old format and then a inner one to allow setting multiple filters using the new format.\r\n\r\nI also changed the name of the old format in accordance with the new format `oneof` to make that clearer in the docs.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 375,
        "deletions": 125,
        "changed_files": 15,
        "created_at": "2022-07-13T07:50:56Z",
        "closed_at": "2022-08-18T07:16:32Z",
        "merged_at": "2022-08-18T07:16:32Z",
        "body": "NOTICE! The main part of this change has been later reverted in #11178, and isn't included in 7.2 RC1, despite what the release notes state\r\nsee https://github.com/redis/redis/pull/10969#issuecomment-1223771006 as to why\r\n\r\n-----\r\n\r\nFix replication inconsistency on modules that uses key space notifications.\r\n\r\n### The Problem\r\n\r\nIn general, key space notifications are invoked after the command logic was executed (this is not always the case, we will discuss later about specific command that do not follow this rules). For example, the `set x 1` will trigger a `set` notification that will be invoked after the `set` logic was performed, so if the notification logic will try to fetch `x`, it will see the new data that was written.\r\nConsider the scenario on which the notification logic performs some write commands. for example, the notification logic increase some counter, `incr x{counter}`, indicating how many times `x` was changed. The logical order by which the logic was executed is has follow:\r\n\r\n```\r\nset x 1\r\nincr x{counter}\r\n```\r\n\r\nThe issue is that the `set x 1` command is added to the replication buffer at the end of the command invocation (specifically after the key space notification logic was invoked and performed the `incr` command). The replication/aof sees the commands in the wrong order:\r\n\r\n```\r\nincr x{counter}\r\nset x 1\r\n```\r\n\r\nIn this specific example the order is less important. But if, for example, the notification would have deleted `x` then we would end up with primary-replica inconsistency.\r\n\r\n### The Solution\r\n\r\nPut the command that cause the notification in its rightful place. In the above example, the `set x 1` command logic was executed before the notification logic, so it should be added to the replication buffer before the commands that is invoked by the notification logic. To achieve this, without a major code refactoring, we save a placeholder in the replication buffer, when finishing invoking the command logic we check if the command need to be replicated, and if it does, we use the placeholder to add it to the replication buffer instead of appending it to the end.\r\n\r\nTo be efficient and not allocating memory on each command to save the placeholder, the replication buffer array was modified to reuse memory (instead of allocating it each time we want to replicate commands). Also, to avoid saving a placeholder when not needed, we do it only for WRITE or MAY_REPLICATE commands.\r\n\r\n#### Additional Fixes\r\n\r\n* Expire and Eviction notifications:\r\n  * Expire/Eviction logical order was to first perform the Expire/Eviction and then the notification logic. The replication buffer got this in the other way around (first notification effect and then the `del` command). The PR fixes this issue.\r\n  * The notification effect and the `del` command was not wrap with `multi-exec` (if needed). The PR also fix this issue.\r\n* SPOP command:\r\n  * On spop, the `spop` notification was fired before the command logic was executed. The change in this PR would have cause the replication order to be change (first `spop` command and then notification `logic`) although the logical order is first the notification logic and then the `spop` logic. The right fix would have been to move the notification to be fired after the command was executed (like all the other commands), but this can be considered a breaking change. To overcome this, the PR keeps the current behaviour and changes the `spop` code to keep the right logical order when pushing commands to the replication buffer. Another PR will follow to fix the SPOP properly and match it to the other command (we split it to 2 separate PR's so it will be easy to cherry-pick this PR to 7.0 if we chose to).\r\n\r\n#### Unhanded Known Limitations\r\n\r\n* key miss event:\r\n   *  On key miss event, if a module performed some write command on the event (using `RM_Call`), the `dirty` counter would increase and the read command that cause the key miss event would be replicated to the replication and aof. This problem can also happened on a write command that open some keys but eventually decides not to perform any action. We decided not to handle this problem on this PR because the solution is complex and will cause additional risks in case we will want to cherry-pick this PR. We should decide if we want to handle it in future PR's (@oranagra @yossigo @madolson FYI). For now, modules writers is advice not to perform any write commands on key miss event.\r\n\r\n#### Testing\r\n\r\n* We already have tests to cover cases where a notification is invoking write commands that are also added to the replication buffer, the tests was modified to verify that the replica gets the command in the correct logical order.\r\n* Test was added to verify that `spop` behaviour was kept unchanged.\r\n* Test was added to verify key miss event behave as expected.\r\n* Test was added to verify the changes do not break lazy expiration.\r\n\r\n#### Additional Changes\r\n\r\n* Follow @guybe7 suggestion, `propagateNow` function can accept a special dbid, -1, indicating not to replicate `select`. We use this to replicate `multi/exec` on `propagatePendingCommands` function. The side effect of this change is that now the `select` command will appear inside the `multi/exec` block on the replication stream (instead of outside of the `multi/exec` block). Tests was modified to match this new behaviour.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 449,
        "deletions": 75,
        "changed_files": 10,
        "created_at": "2022-07-12T15:34:09Z",
        "closed_at": "2022-09-22T13:29:00Z",
        "merged_at": "2022-09-22T13:29:00Z",
        "body": "Adds a number of user management/ACL validaiton/command execution functions to improve a Redis module's ability to enforce ACLs correctly and easily.\r\n\r\n* RM_SetContextUser - sets a RedisModuleUser on the context, which RM_Call will use to both validate ACLs (if requested and set) as well as assign to the client so that scripts executed via RM_Call will have proper ACL validation.\r\n* RM_SetModuleUserACLString - Enables one to pass an entire ACL string, not just a single OP and have it applied to the user\r\n* RM_GetModuleUserACLString - returns a stringified version of the user's ACL (same format as dump and list).  Contains an optimization to cache the stringified version until the underlying ACL is modified.\r\n* Slightly re-purpose the \"C\" flag to RM_Call from just being about ACL check before calling the command, to actually running the command with the right user, so that it also affects commands inside EVAL scripts. see #11231\r\n",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-07-11T19:31:58Z",
        "closed_at": "2022-07-12T16:56:02Z",
        "merged_at": "2022-07-12T16:56:02Z",
        "body": "For any command with supporting XXX help command,  we could see it alwasy return:  **Prints this help**, it should be return\r\n**Print this help**\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/178343599-119487df-8fb5-4d00-a968-78c0fba02e04.png)\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2022-07-11T19:12:22Z",
        "closed_at": "2022-08-03T01:19:53Z",
        "merged_at": "2022-08-03T01:19:53Z",
        "body": "In this PR, we fix the following issues of the Function Load command:\r\n\r\n1. According to the source code, Library name can include letters, numbers and underscore(_), but in our current error message, we missed the underscore part.\r\n  \r\n![image](https://user-images.githubusercontent.com/51993843/178339843-8c64b639-f619-4cd2-b059-5e98680e28a3.png)\r\n\r\n2. in the message of command function help\r\nNow it displays the following infomation for Redis Function Load command:\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/178340585-a80b1b44-9d60-4a7e-b963-177d3f4152c2.png)\r\n\r\nBut this is just a description of the function load command, it is not command format\r\nThus, in this pr, we update the command format as below:\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/178340711-2c250a42-cb32-45a4-84a5-eb94cad0717e.png)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1160,
        "deletions": 295,
        "changed_files": 86,
        "created_at": "2022-07-11T09:38:59Z",
        "closed_at": "2022-07-11T14:44:20Z",
        "merged_at": "2022-07-11T14:44:20Z",
        "body": "Upgrade urgency: MODERATE, specifically if you're using a previous release of\r\nRedis 7.0, contains fixes for bugs in previous 7.0 releases.\r\n\r\n\r\nPerformance and resource utilization improvements\r\n=================================================\r\n\r\n* Optimize zset conversion on large ZRANGESTORE (#10789)\r\n* Optimize the performance of sending PING on large clusters (#10624)\r\n* Allow for faster restart of Redis in cluster mode (#10912)\r\n\r\nINFO fields and introspection changes\r\n=====================================\r\n\r\n* Add missing sharded pubsub keychannel count to CLIENT LIST (#10895)\r\n* Add missing pubsubshard_channels field in INFO STATS (#10929)\r\n\r\nModule API changes\r\n==================\r\n\r\n* Add RM_StringToULongLong and RM_CreateStringFromULongLong (#10889)\r\n* Add RM_SetClientNameById and RM_GetClientNameById (#10839)\r\n\r\nChanges in CLI tools\r\n====================\r\n\r\n* Add missing cluster-port support to redis-cli --cluster (#10344)\r\n\r\nOther General Improvements\r\n==========================\r\n\r\n* Account sharded pubsub channels memory consumption (#10925)\r\n* Allow ECHO in loading and stale modes (#10853)\r\n* Cluster: Throw -TRYAGAIN instead of -ASK on migrating nodes for multi-key\r\n  commands when the node only has some of the keys (#9526)\r\n\r\nBug Fixes\r\n=========\r\n\r\n* TLS: Notify clients on connection shutdown (#10931)\r\n* Fsync directory while persisting AOF manifest, RDB file, and config file (#10737)\r\n* Script that made modification will not break with unexpected NOREPLICAS error (#10855)\r\n* Cluster: Fix a bug where nodes may not acknowledge a CLUSTER FAILOVER TAKEOVER\r\n  after a replica reboots (#10798)\r\n* Cluster: Fix crash during handshake and cluster shards call (#10942)\r\n\r\nFixes for issues in previous releases of Redis 7.0\r\n--------------------------------------------------\r\n\r\n* TLS: Fix issues with large replies (#10909)\r\n* Correctly report the startup warning for vm.overcommit_memory (#10841)\r\n* redis-server command line allow passing config name and value in the same argument (#10866)\r\n* Support --save command line argument with no value for backwards compatibility (#10866)\r\n* Fix CLUSTER RESET command regression requiring an argument (#10898)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 17,
        "changed_files": 6,
        "created_at": "2022-07-06T16:51:21Z",
        "closed_at": "2022-07-07T03:31:59Z",
        "merged_at": "2022-07-07T03:31:59Z",
        "body": "Here are some small fixes I've noticed over a while, rolled up to clean up",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 259,
        "deletions": 35,
        "changed_files": 7,
        "created_at": "2022-07-06T13:24:27Z",
        "closed_at": "2022-10-24T22:04:55Z",
        "merged_at": null,
        "body": "A while ago #9891 was proposed and approved, but the following PR #9929 became stale and was abandoned.\r\n\r\n### Changes\r\n\r\nTwo new commands. See madolsons comment https://github.com/redis/redis/issues/9891#issuecomment-987697073\r\n\r\n`LMMOVE destination numkeys [srckey1 ...] LEFT|RIGHT LEFT|RIGHT`\r\n`BLMMOVE destination numkeys [srckey1 ...] LEFT|RIGHT LEFT|RIGHT timeout`\r\n\r\nLMMOVE moves the first value it finds in the first non-empty list. \r\nBLMMOVE blocks only if all lists are empty. It moves the first value that was added to any of the lists.\r\n\r\nSo they work exactly like LMOVE and BLMOVE, but support multiple source keys. \r\n\r\n### Implementation\r\n\r\n- Instead of creating a new function, I generalized `lmoveGenericCommand` to work with multiple keys.\r\n- `blmmoveGenericCommand` doesn't introduce any additional overhead, `lmmoveGenericCommand` gets fast-forwared to the key that was found (i.e they are not traversed for a second time)\r\n\r\n### Tests\r\n- plain LMMOVE test\r\n- chained BLMMOVE test\r\n- multiple replica tests for BLMMOVE and LMMOVE (both get converted to LMOVE)\r\n\r\nI didn't add that many tests because those commands now share the same implementation with LMOVE/BLMOVE which there are plenty of tests for. I can add more, If needed\r\n\r\nBesides that, I created a simple fuzzy tester https://gist.github.com/dranikpg/7a44d761331fd261fe85f943883922cc\r\n\r\n### Comments\r\n\r\nIf there is more clarity on whether new optional arguments can be added, I'd much rather just extend LMOVE, like suggested by zuiderkwast https://github.com/redis/redis/issues/9891#issuecomment-986760305.\r\nHowever one such potential argument for LMMOVE could be the count. If we already move from multiple lists, why not handle multiple values at once \ud83e\udd14 ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-07-06T02:49:10Z",
        "closed_at": "2022-07-11T05:00:44Z",
        "merged_at": "2022-07-11T05:00:44Z",
        "body": "A node that is in handshaking is neither a MASTER nor a REPLICA until we learn more about the node. During cluster shards, we want to loop over all the masters to determine the \"shards\". The current code checks to see if a node is a replica, which will return false when we don't know what the node is yet. Later on we will check the replicas of the node, and crash because it doesn't have any.\r\n\r\nThis change does two things:\r\n1. Update the code to bail out if the node is not a primary as far as the current node is concerned.\r\n2. Add a defensive check that asserts that the primary is not NULL.\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2022-07-06T02:06:59Z",
        "closed_at": "2022-07-06T08:58:44Z",
        "merged_at": "2022-07-06T08:58:44Z",
        "body": "* CONTRIBUTING to get better formatting\r\n* CONDUCT also because github doesn't seem recognize the code of conduct page",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-07-05T08:31:11Z",
        "closed_at": "2022-07-13T06:14:38Z",
        "merged_at": "2022-07-13T06:14:38Z",
        "body": "The corrupt dump fuzzer uncovered a valgrind warning saying:\r\n```\r\n==76370== Argument 'size' of function malloc has a fishy (possibly negative) value: -3744781444216323815\r\n```\r\nThis allocation would have failed (returning NULL) and being handled properly by redis (even before this change), but we also want to silence the valgrind warnings (which are checking that casting to ssize_t produces a non-negative value).\r\n\r\nThe solution i opted for is to explicitly fail these allocations (returning NULL), before even reaching `malloc` (which would have failed and return NULL too).\r\n\r\nThe implication is that we will not be able to support a single allocation of more than 2GB on a 32bit system (which i don't think is a realistic scenario).\r\ni.e. i do think we could be facing cases were redis consumes more than 2gb on a 32bit system, but not in a single allocation.\r\n\r\nThe byproduct of this, is that i dropped the overflow assertions, since these will now lead to the same OOM panic we have for failed allocations.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 227,
        "deletions": 113,
        "changed_files": 19,
        "created_at": "2022-07-04T14:51:18Z",
        "closed_at": "2022-07-18T07:56:27Z",
        "merged_at": "2022-07-18T07:56:27Z",
        "body": "replace use of:\r\nsprintf --> snprintf\r\nstrcpy/strncpy  --> redis_strlcpy\r\nstrcat/strncat  --> redis_strlcat\r\n\r\n**why are we making this change?**\r\nMuch of the code uses some unsafe variants or deprecated buffer handling\r\nfunctions.\r\nWhile most cases are probably not presenting any issue on the known path\r\nprogramming errors and unterminated strings might lead to potential\r\nbuffer overflows which are not covered by tests.\r\n\r\n**As part of this PR we change**\r\n1. added implementation for redis_strlcpy and redis_strlcat based on the strl implementation: https://linux.die.net/man/3/strl\r\n2. change all occurrences of use of sprintf with use of snprintf\r\n3. change occurrences of use of  strcpy/strncpy with redis_strlcpy\r\n4. change occurrences of use of strcat/strncat with redis_strlcat\r\n5. change the behavior of ll2string/ull2string/ld2string so that it will always place null termination ('\\0') on the output buffer in the first index. this was done in order to make the use of these functions more safe in cases were the user will not check the output returned by them (for example in rdbRemoveTempFile)\r\n6. we added a compiler directive to issue a deprecation error in case a use of sprintf/strcpy/strcat is found during compilation which will result in error during compile time. However keep in mind that since the deprecation attribute is not supported on all compilers, this is expected to fail during push workflows.\r\n\r\n\r\n**NOTE:** while this is only an initial milestone. We might also consider\r\nusing the *_s implementation provided by the C11 Extensions (however not\r\nyet widly supported). I would also suggest to start\r\nlooking at static code analyzers to track unsafe use cases.\r\nFor example LLVM clang checker supports security.insecureAPI.DeprecatedOrUnsafeBufferHandling\r\nwhich can help locate unsafe function usage.\r\nhttps://clang.llvm.org/docs/analyzer/checkers.html#security-insecureapi-deprecatedorunsafebufferhandling-c\r\nThe main reason not to onboard it at this stage is that the alternative\r\nexcepted by clang is to use the C11 extensions which are not always\r\nsupported by stdlib.\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-07-04T08:55:53Z",
        "closed_at": "2022-07-05T05:41:18Z",
        "merged_at": "2022-07-05T05:41:17Z",
        "body": "Use SSL_shutdown(), in a best-effort manner, when closing a TLS\r\nconnection. This change better supports OpenSSL 3.x clients that will\r\nnot silently ignore the socket-level EOF.\r\n\r\nFixes #10915 ",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-07-04T06:57:43Z",
        "closed_at": "2022-07-06T06:50:08Z",
        "merged_at": "2022-07-06T06:50:08Z",
        "body": "We already have `pubsub_channels` and `pubsub_patterns`\r\nin INFO stats, now add `pubsubshard_channels` (symmetry).\r\n\r\nSharded pubsub was added in #8621",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2022-07-03T21:56:58Z",
        "closed_at": "2022-07-04T06:18:57Z",
        "merged_at": "2022-07-04T06:18:57Z",
        "body": "Account sharded pubsub channels memory consumption in client memory usage computation to accurately evict client based on the set threshold for `maxmemory-clients`.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 60,
        "deletions": 27,
        "changed_files": 2,
        "created_at": "2022-07-03T17:08:26Z",
        "closed_at": "2022-07-20T06:11:01Z",
        "merged_at": "2022-07-20T06:11:01Z",
        "body": "As an outstanding part mentioned in #10737, we could just make the cluster config file and ACL file saving done with a more safe and atomic pattern (write to temp file, fsync, rename, fsync dir).\r\n\r\nThe cluster config file uses an in-place overwrite and truncation (which was also used by the main config file before #7824).\r\nThe ACL file is using the temp file and rename approach, but was missing an fsync.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-07-03T11:31:01Z",
        "closed_at": "2022-07-03T12:36:43Z",
        "merged_at": "2022-07-03T12:36:43Z",
        "body": "We should also set aof_lastbgrewrite_status to C_ERR on these\r\nerrors. Because aof rewrite did fail, and we did not finish the\r\nmanifest update. Also maintain the stat_aofrw_consecutive_failures.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-07-03T02:03:43Z",
        "closed_at": "2022-07-10T04:02:22Z",
        "merged_at": "2022-07-10T04:02:22Z",
        "body": "When you load an ACL file that has an error, it prints out the log with the error. If the line has multiple errors, it prints out multiple errors. This just changes the code so that we only print out the line once in all cases.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-07-03T01:52:19Z",
        "closed_at": "2022-07-04T06:47:34Z",
        "merged_at": "2022-07-04T06:47:34Z",
        "body": "## Issue\r\nDuring the MULTI/EXEC flow, each command gets queued until the `EXEC` command is received and during this phase on every command queue, a `realloc` is being invoked. This could be expensive based on the realloc behaviour (if copy to a new memory location). \r\n\r\n\r\n## Solution\r\nIn order to reduce the no. of syscall, couple of optimization I've used.\r\n\r\n1. By default, reserve memory for atleast two commands. `MULTI/EXEC` for a single command doesn't have any significance. Hence, I believe customer wouldn't use it.\r\n2. For further reservation, increase the memory allocation in exponent growth (power of 2). This reduces the no. of `realloc` call from `N` to `log(N)` times.\r\n\r\n## Alternative(s)\r\n\r\n@madolson had a nice suggestion to have static memory allocation of a fixed size (16) and use dynamic allocation for further increase. This would completely remove the memory allocation/deallocation for majority of the cases however I didn't want to have two separate code path hence avoided this approach.\r\n\r\n## Testing\r\n### Script\r\n\r\n```\r\nimport redis\r\n\r\nr=redis.Redis()\r\np = r.pipeline()\r\nfor j in range(100000000):\r\n    print(\"Step #\", j)\r\n    for i in range(100):\r\n        p.mset({\"Croatia\" + str(i) + str(j): \"Zagreb\", \"Bahamas\" + str(i) + str(j): \"Nassau\"})\r\n    p.execute()\r\n```\r\n\r\n### Machine\r\nc5.4xlarge\r\n\r\n### Observation\r\nRan the above script for 3 mins with new key generation on each iteration, there was a drop in sample collection and CPU utilization from `260 samples,1.49%` to `78 samples, 0.46%`. Here are the flamegraph images:\r\n\r\n### FlameGraph on the current branch\r\n![FlameGraph on the current branch](https://gist.githubusercontent.com/hpatro/8b9434055dc241827737f7870402dbb5/raw/fe96bb965d2d04e0a4573f8444fb2a56538d940e/realloc_opt%2520(depth%2520100)%2520(3%2520mins).svg)\r\n\r\n### FlameGraph on the unstable branch\r\n![FlameGraph on the unstable branch](https://gist.githubusercontent.com/hpatro/ee0e147624e7fc9a2c603fc76a410cac/raw/449911afb9f88c87887f0d40f1f4c8a060e445c8/unstable%2520(depth%2520100)%2520(3%2520mins).svg)\r\n\r\n## Other changes:\r\n\r\n* Include multi exec queued command array in client memory consumption calculation (affects client eviction too)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 235,
        "deletions": 210,
        "changed_files": 10,
        "created_at": "2022-07-03T00:37:05Z",
        "closed_at": "2022-07-12T17:41:30Z",
        "merged_at": "2022-07-12T17:41:30Z",
        "body": "The goal of this PR is provided a better foundation to implement cluster based tests in our normal testing framework. We have a lot of cluster tests that could be executed more quickly, independently and with the CI, but there wasn't a good framework to work on it. There was some previous work done in the cluster.tcl test.  This PR builds ontop of that as well as porting some tests as examples:\r\n\r\nExactly what is implemented:\r\n1. Implemented a new `start_cluster` which will start a cluster, assigned slots, meet the nodes, and wait for the cluster to be up and running. All of this code is in cluster_helper.tcl. \r\n2.  Implement a couple of primitive cluster functions: `R` and `CI` so that can be ported almost as is. Also migrated csi -> CI. either are very clear and CI is used in the cluster test framework. Also restructured the code a bit to rely on more common functions.\r\n3. Allow the `loadmodule` directive to be used multiple times, so that you don't have to load modules on each and every node.\r\n4. Ported the moduleapi/cluster.tcl, cluster-scripting.tcl, cluster-endpoints.tcl, and cluster-multi-slots.tcl to the new functions. \r\n\r\nIn the fullness of time I expect we want to fully merge the cluster tests into the normal framework, but I suppose that won't happen for a bit. Some of the less CPU intensive tests, basically that isn't the failover test, can be ported as it's useful. \r\n",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-07-02T04:26:54Z",
        "closed_at": "2022-07-06T08:04:51Z",
        "merged_at": null,
        "body": "Contributing.md get better formatting",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-07-02T01:31:18Z",
        "closed_at": "2022-07-03T05:34:39Z",
        "merged_at": "2022-07-03T05:34:39Z",
        "body": "The `can_log` variable prevents us from outputting too\r\nmany error logs. But it should not include the modification\r\nof server.aof_last_write_errno.\r\n\r\nWe are doing this because:\r\n1. In the short write case, we always set aof_last_write_errno\r\nto ENOSPC, we don't care the `can_log` flag.\r\n\r\n2. And we always set aof_last_write_status to C_ERR in aof write\r\nerror (except for FSYNC_ALWAYS, we exit). So there may be a chance\r\nthat `aof_last_write_errno` is not right.\r\n\r\nan innocent bug or just a code cleanup.\r\nit affects the errno on the MISCONF reply",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2022-07-01T01:18:53Z",
        "closed_at": "2022-07-04T06:38:20Z",
        "merged_at": "2022-07-04T06:38:20Z",
        "body": "Currently in cluster mode, Redis process locks the cluster config file when starting up and holds the lock for the entire lifetime of the process. When the server shuts down, it doesn't explicitly release the lock on the cluster config file. We noticed a problem with restart testing that if you shut down a very large redis-server process (i.e. with several hundred GB of data stored), it takes the OS a while to free the resources and unlock the cluster config file. So if we immediately try to restart the redis server process, it might fail to acquire the lock on the cluster config file and fail to come up.\r\n\r\nThis fix explicitly releases the lock on the cluster config file upon a shutdown rather than relying on the OS to release the lock, which is a cleaner and safer approach to free up resources acquired. ",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2022-06-30T18:04:43Z",
        "closed_at": "2022-07-03T10:34:14Z",
        "merged_at": "2022-07-03T10:34:14Z",
        "body": "Before this commit, TLS tests on Ubuntu 22.04 would fail as dropped\r\nconnections result with an ECONNABORTED error thrown instead of an empty\r\nread.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 39,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2022-06-30T18:03:40Z",
        "closed_at": "2022-07-03T10:35:58Z",
        "merged_at": "2022-07-03T10:35:58Z",
        "body": "This problem was introduced by 496375fc36134c72461e6fb97f314be3adfd8b68 and seems to reproduce on macOS since OpenSSL writes more frequently return with `EAGAIN`.\r\n\r\nReproduced easily by the following test (from [this repository](https://github.com/michael-grunder/hiredis-tls-error-demonstrator) by @michael-grunder):\r\n\r\n```c\r\n#include <hiredis/hiredis.h>\r\n#include <hiredis/hiredis_ssl.h>\r\n#include <assert.h>\r\n#include <stdlib.h>\r\n\r\n#define REDIS_HOST \"127.0.0.1\"\r\n#define REDIS_PORT 56443\r\n\r\n#define CA_CERT \"redis/ca.crt\"\r\n#define SSL_CERT \"redis/redis.crt\"\r\n#define SSL_KEY \"redis/redis.key\"\r\n\r\n#define N 10000\r\n\r\nint main(void) {\r\n    redisSSLContext *sctx;\r\n    redisContext *c;\r\n    redisReply *r;\r\n\r\n    sctx = redisCreateSSLContext(CA_CERT, NULL, SSL_CERT, SSL_KEY, NULL, NULL);\r\n    assert(sctx != NULL);\r\n\r\n    c = redisConnect(REDIS_HOST, REDIS_PORT);\r\n    assert(c != NULL && c->err == 0);\r\n\r\n    assert(redisInitiateSSLWithContext(c, sctx) == REDIS_OK);\r\n\r\n    for (size_t i = 0; i < 500; i++) {\r\n        assert((r = redisCommand(c, \"RPUSH mylist element:%d\", i)) != NULL);\r\n        freeReplyObject(r);\r\n    }\r\n\r\n    for (size_t i = 0; i < N; i++) {\r\n        assert(redisAppendCommand(c, \"LRANGE mylist 0 499\") == REDIS_OK);\r\n    }\r\n\r\n    for (size_t i = 0; i < N; i++) {\r\n        /* Bug will happen here at some point in the loop */\r\n        if (redisGetReply(c, (void**)&r) != REDIS_OK || c->err != 0 ||\r\n            r->type != REDIS_REPLY_ARRAY || r->elements != 500)\r\n        {\r\n            fprintf(stderr, \"[%zu] redisContext error: %s (%d)\\n\", i, c->errstr, c->err);\r\n            exit(1);\r\n        }\r\n\r\n        freeReplyObject(r);\r\n    }\r\n}\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 103,
        "deletions": 63,
        "changed_files": 27,
        "created_at": "2022-06-29T08:09:50Z",
        "closed_at": "2022-06-30T13:32:41Z",
        "merged_at": "2022-06-30T13:32:41Z",
        "body": "This was harmless because we marked the parent command\r\nwith SENTINEL flag. So the populateCommandTable was ok.\r\nAnd we also don't show the flag (SENTINEL and ONLY-SENTNEL)\r\nin COMMAND INFO.\r\n\r\nIn this PR, we also add the same CMD_SENTINEL and CMD_ONLY_SENTINEL\r\nflags check when populating the sub-commands.\r\nso that in the future it'll be possible to add some sub-commands to sentinel or sentinel-only but not others.\r\n\r\nI noticed it because all ACL subcommands are marked with\r\nthe SENTINEL flag. The subcommands should also have the flag.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-06-27T12:08:28Z",
        "closed_at": "2022-06-28T03:42:55Z",
        "merged_at": "2022-06-28T03:42:55Z",
        "body": "`sizeof(clusterMsgPingExt)` here seems needless. The original discussion is at https://github.com/redis/redis/pull/9530#discussion_r907189733.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-06-26T05:51:49Z",
        "closed_at": "2022-06-26T09:32:35Z",
        "merged_at": "2022-06-26T09:32:35Z",
        "body": "This is harmless, we only restore mstate to make sure we\r\nfree the right pointer in freeClientMultiState, but it'll\r\nbe nicer to also sync that argv_len var back.\r\n\r\nmention in https://github.com/redis/redis/pull/9602#pullrequestreview-1019372509",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2022-06-24T20:10:46Z",
        "closed_at": "2022-06-29T05:17:01Z",
        "merged_at": "2022-06-29T05:17:01Z",
        "body": "Fix regression of CLUSTER RESET command in redis 7.0.\r\n\r\ncluster reset command format is:\r\nCLUSTER RESET [ HARD | SOFT]  \r\n\r\nAccording to the cluster reset command doc and codes, the third argument is optional, so\r\nthe arity in json file should be -2 instead of 3.\r\n\r\nAdd test to verify future regressions with RESET and RESET SOFT that were not covered.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-06-23T13:03:39Z",
        "closed_at": "2022-06-23T15:19:36Z",
        "merged_at": "2022-06-23T15:19:36Z",
        "body": "The new test added in #10891 fails with TLS with this error:\r\n```\r\n[err]: benchmark: clients idle mode should return error when reached maxclients limit in tests/integration/redis-benchmark.tcl\r\nExpected 'Creating 2 idle connections and waiting forever (Ctrl+C when done)\r\nERROR: failed to fetch CONFIG from 127.0.0.1:26613\r\nWARNING: Could not fetch server CONFIG\r\nError: SSL_connect failed: Success' to match '*ERR max number of clients reached*' (context: type eval line 6 cmd {assert_match \"*ERR max number of clients reached*\" $error} proc ::test) \r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-06-23T07:05:02Z",
        "closed_at": "2022-06-28T07:11:17Z",
        "merged_at": "2022-06-28T07:11:17Z",
        "body": "When calling CLIENT INFO/LIST, and in various debug prints, Redis is printing the number of pubsub channels / patterns the client is subscribed to.\r\nWith the addition of sharded pubsub, it would be useful to print the number of keychannels the client is subscribed to as well.\r\n\r\nBefore the fix:\r\n```\r\n~/ $ telnet localhost 6379\r\n...\r\nhello 3\r\n...\r\nssubscribe xyz\r\n>3\r\n$10\r\nssubscribe\r\n$3\r\nxyz\r\n:1\r\nclient info\r\n=268\r\ntxt:id=7 addr=127.0.0.1:43326 laddr=127.0.0.1:6379 fd=8 name= age=16 idle=0 flags=P db=0 sub=0 psub=0 multi=-1 qbuf=13 qbuf-free=20461 argv-mem=10 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=22298 events=r cmd=client|info user=default redir=-1 resp=3\r\n```\r\nAfter the fix:\r\n```\r\n~/ $ telnet localhost 6379\r\n...\r\nhello 3\r\n...\r\nssubscribe xyz\r\n>3\r\n$10\r\nssubscribe\r\n$3\r\nxyz\r\n:1\r\nclient info\r\n=268\r\ntxt:id=7 addr=127.0.0.1:43326 laddr=127.0.0.1:6379 fd=8 name= age=16 idle=0 flags=P db=0 sub=0 psub=0 **ssub=1** multi=-1 qbuf=13 qbuf-free=20461 argv-mem=10 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=22298 events=r cmd=client|info user=default redir=-1 resp=3\r\n```\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-06-22T19:34:29Z",
        "closed_at": "2022-06-23T04:55:34Z",
        "merged_at": null,
        "body": "cluster meet command format as below:\r\n\r\ncluster meet ip port [cport], cport as an optional argument is missing.\r\n\r\nThe doc pr is https://github.com/redis/redis-doc/pull/2006",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-06-22T17:53:49Z",
        "closed_at": "2022-06-26T05:39:30Z",
        "merged_at": "2022-06-26T05:39:30Z",
        "body": "There are a lot of false sharing cache misses in line 4013 inside getIOPendingCount function.\r\nThe reason is that elements of io_threads_pending array access the same cache line from different threads, so it is better to represent it as an array of structures with fields aligned to cache line size.\r\nThis change should improve performance (in particular, it affects the latency metric, we saw up to 3% improvement).\r\nBasic Info:\r\n    io-threads is enabled and io-threads-do-reads is set to yes in redis.conf\r\n    X86 platform\r\n\r\nElvina Yakubova,\r\nAdvanced Software Technology Lab, Huawei\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-06-21T13:26:07Z",
        "closed_at": "2022-06-22T16:30:23Z",
        "merged_at": "2022-06-22T16:30:23Z",
        "body": "my maxclients config:\r\n```\r\nredis-cli config get maxclients\r\n1) \"maxclients\"\r\n2) \"4064\"\r\n```\r\n\r\nBefore this bug was fixed, creating 4065 clients appeared to be successful, but only 4064 were actually created```\r\n```\r\n./redis-benchmark -c 4065 -I\r\nCreating 4065 idle connections and waiting forever (Ctrl+C when done)\r\ncients: 4065\r\n```\r\n\r\nnow :\r\n```\r\n./redis-benchmark -c 4065 -I\r\nCreating 4065 idle connections and waiting forever (Ctrl+C when done)\r\nError from server: ERR max number of clients reached\r\n\r\n./redis-benchmark -c 4064 -I\r\nCreating 4064 idle connections and waiting forever (Ctrl+C when done)\r\nclients: 4064\r\n\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2022-06-21T12:54:01Z",
        "closed_at": "2022-06-21T14:00:25Z",
        "merged_at": "2022-06-21T14:00:25Z",
        "body": "* Fix typo `RedisModule_CreatString` -> `RedisModule_CreateString` (multiple occurrences)\r\n* Make the markdown gen script change all `RM_` to `RedisModule_` even in code examples, etc.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 98,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2022-06-21T01:43:46Z",
        "closed_at": "2022-06-26T12:02:53Z",
        "merged_at": "2022-06-26T12:02:53Z",
        "body": "Signed-off-by: RinChanNOWWW <hzy427@gmail.com>\r\n\r\nRelated issue: #10884\r\n\r\nSince the ranges of `unsigned long long` and `long long` are different, we cannot read an `unsigned long long` integer from a `RedisModuleString` by `RedisModule_StringToLongLong` . \r\n\r\nSo I added two new Redis Module APIs to support the conversion between these two types:\r\n* `RedisModule_StringToULongLong`\r\n* `RedisModule_CreateStringFromULongLong`",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2022-06-20T21:01:20Z",
        "closed_at": "2022-06-21T05:14:31Z",
        "merged_at": "2022-06-21T05:14:31Z",
        "body": "`COUNTER_INIT_VAL` doesn't exist in the code anymore so we can replace it with `LFU_INIT_VAL` entries",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2022-06-20T15:52:35Z",
        "closed_at": "2022-06-21T07:01:14Z",
        "merged_at": "2022-06-21T07:01:14Z",
        "body": "The PR fixes 2 issues:\r\n\r\n### RM_Call crash on script mode\r\n\r\n`RM_Call` can potentially be called from a background thread where `server.current_client`\r\nare not set. In such case we get a crash on `NULL` dereference.\r\nThe fix is to check first if `server.current_client` is `NULL`, if it does we should\r\nverify disc errors and readonly replica as we do to any normal clients (no masters nor AOF).\r\n\r\n### RM_Call block OOM commands when not needed\r\n\r\nAgain `RM_Call` can be executed on a background thread using a `ThreadSafeCtx`. In such case `server.pre_command_oom_state` can be irrelevant and should not be considered when check OOM state. This cause OOM commands to be blocked when not necessarily needed.\r\n\r\nIn such case, check the actual used memory (and not the cached value). Notice that in order to know if the cached value can be used, we check that the ctx that was used on the `RM_Call` is a ThreadSafeCtx. Module writer can potentially abuse the API and use ThreadSafeCtx on the main thread. We consider this as a API miss used.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2022-06-20T13:25:23Z",
        "closed_at": "2022-06-27T05:29:05Z",
        "merged_at": "2022-06-27T05:29:05Z",
        "body": "The module API docs mentions this macro, but it was not defined (so no one could have used it).\r\n\r\nInstead of adding it as is, we decided to add a _V1 macro, so that if / when we some day extend this struct,\r\nmodules that use this API and don't need the extra fields, will still use the old version\r\nand still be compatible with older redis version (despite being compiled with newer redismodule.h)\r\n\r\nSee #10839 and #9987",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-06-17T20:11:07Z",
        "closed_at": "2022-06-19T05:18:47Z",
        "merged_at": "2022-06-19T05:18:47Z",
        "body": "Clients could use this config in sentinel only when they already add rename-command in the redis conf file,\r\nbut becuase in Redis instance, we already DEPRECATED rename-command config,\r\nthus we should deprecate this in the sentinel part as well.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2022-06-16T15:40:02Z",
        "closed_at": "2022-06-17T07:08:15Z",
        "merged_at": null,
        "body": "#3037 (feature request)\r\nLIMIT option for sorted set operations, speeding up cases where fewer result elements are required than the number of total results, similarly to ZINTERCARD, lowering the N argument of the time complexity to the limit instead of the cardinality of the smallest set.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-06-16T13:28:47Z",
        "closed_at": "2022-06-19T05:42:12Z",
        "merged_at": "2022-06-19T05:42:12Z",
        "body": "https://github.com/redis/redis/pull/5476#issuecomment-1157159541\r\n\r\nhttps://github.com/redis/redis/blob/7.0.2/src/redisassert.c#L42 uses `const char *` \r\n```c\r\nvoid _serverAssert(const char *estr, const char *file, int line) {\r\n```\r\n\r\nhttps://github.com/redis/redis/blob/7.0.2/src/server.h#L3532 also uses `const char *`\r\n```c\r\nvoid _serverAssert(const char *estr, const char *file, int line);\r\n```\r\n\r\nbut https://github.com/redis/redis/blob/7.0.2/src/redisassert.h#L46 uses `char *`\r\n```c\r\nvoid _serverAssert(char *estr, char *file, int line);\r\n```\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 79,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2022-06-16T12:10:52Z",
        "closed_at": "2022-07-26T07:28:13Z",
        "merged_at": "2022-07-26T07:28:13Z",
        "body": "Gossip the cluster node blacklist in ping and pong messages. This means that CLUSTER FORGET doesn't need to be sent to all nodes in a cluster. It can be sent to one or more nodes and then be propagated to the rest of them.\r\n\r\nFor each blacklisted node, its node id and its remaining blacklist TTL is gossiped in a cluster bus ping extension (introduced in #9530).\r\n\r\n```\r\nRelease notes:\r\nAutomatically propagate node deletion to other nodes in a cluster when `CLUSTER FORGET` is called, allowing nodes to be deleted with a single call in most cases.\r\n```\r\n\r\nFixes #10861.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2022-06-16T11:54:26Z",
        "closed_at": "2022-06-19T06:45:17Z",
        "merged_at": "2022-06-19T06:45:17Z",
        "body": null,
        "comments": 4
    },
    {
        "merged": true,
        "additions": 76,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2022-06-15T10:10:11Z",
        "closed_at": "2022-06-26T11:36:40Z",
        "merged_at": "2022-06-26T11:36:39Z",
        "body": "This PR has two topics.\r\n\r\n## Passing config name and value in the same arg\r\nIn #10660 (Redis 7.0.1), when we supported the config values that can start with `--` prefix (one of the two topics of that PR),\r\nwe broke another pattern: `redis-server redis.config \"name value\"`, passing both config name\r\nand it's value in the same arg, see #10865\r\n\r\nThis wasn't a intended change (i.e we didn't realize this pattern used to work).\r\nAlthough this is a wrong usage, we still like to fix it.\r\n\r\nNow we support something like:\r\n```\r\nsrc/redis-server redis.conf \"--maxmemory '700mb'\" \"--maxmemory-policy volatile-lru\" --proc-title-template --my--title--template --loglevel verbose\r\n```\r\n\r\n## Changes around --save\r\nAlso in this PR, we undo the breaking change we made in #10660 on purpose.\r\n1. `redis-server redis.conf --save --loglevel verbose` (missing `save` argument before anotehr argument).\r\n    In 7.0.1, it was throwing an wrong arg error.\r\n    Now it will work and reset the save, similar to how it used to be in 7.0.0 and 6.2.x.\r\n3. `redis-server redis.conf --loglevel verbose --save` (missing `save` argument as last argument).\r\n    In 6.2, it did not reset the save, which was a bug (inconsistent with the previous bullet).\r\n    Now we will make it work and reset the save as well (a bug fix).",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2022-06-14T21:35:08Z",
        "closed_at": "2022-10-11T04:11:06Z",
        "merged_at": null,
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-06-13T15:44:35Z",
        "closed_at": "2022-06-14T05:09:08Z",
        "merged_at": "2022-06-14T05:09:08Z",
        "body": "Fix 2 comments in dict.c & redis-cli.c",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-06-13T09:21:00Z",
        "closed_at": "2022-06-14T18:09:50Z",
        "merged_at": "2022-06-14T18:09:50Z",
        "body": "If a script made a modification and then was interrupted for taking too long.\r\nthere's a chance redis will detect that a replica dropped and would like to reject\r\nwrite commands with NOREPLICAS due to insufficient good replicas.\r\nreturning an error on a command in this case breaks the script atomicity.\r\n\r\nThe same could in theory happen with READONLY, MISCONF, but i don't think\r\nthese state changes can happen during script execution.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2022-06-13T09:10:00Z",
        "closed_at": "2022-06-14T05:48:08Z",
        "merged_at": "2022-06-14T05:48:08Z",
        "body": "I noticed that scripting.tcl uses INFO from within a script and thought it's an overkill and concluded it's nicer to use another CMD_STALE command, decided to use ECHO, and then noticed it's not at all allowed in stale mode.\r\nprobably overlooked at #6843",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 173,
        "deletions": 72,
        "changed_files": 12,
        "created_at": "2022-06-12T10:07:15Z",
        "closed_at": "2022-06-12T13:00:17Z",
        "merged_at": "2022-06-12T13:00:17Z",
        "body": "Upgrade urgency: MODERATE, specifically if you're using a previous release of\r\nRedis 7.0, contains fixes for bugs in previous 7.0 releases.\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Fixed SET and BITFIELD commands being wrongly marked movablekeys (#10837)\r\n  Regression in 7.0 possibly resulting in excessive roundtrip from cluster clients.\r\n* Fix crash when /proc/sys/vm/overcommit_memory is inaccessible (#10848)\r\n  Regression in 7.0.1 resulting in crash on startup on some configurations.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 66,
        "deletions": 26,
        "changed_files": 6,
        "created_at": "2022-06-11T01:19:22Z",
        "closed_at": "2022-07-13T02:44:39Z",
        "merged_at": null,
        "body": "This is a follow up for the issue here - https://github.com/redis/redis/issues/10709 - with which Modules can set the client name and protocol.\r\n\r\n## The problem/use-case that the feature addresses\r\nIt is very difficult to implement a custom AUTH command, since we also need to support the HELLO case.\r\nCurrently, there is no way for a Redis Module to directly reply to the client with the HELLO command\u2019s response.\r\n\r\n## Description of the feature\r\nCreate a new Module API - `RM_ReplyWithHelloResponse`\r\nWhen this API is used on a `RedisModuleContext`, it will reply to the client with the HELLO command\u2019s response in the same RESP format that the client has set in its `resp` field.\r\nWith this API (and the other two APIs from the earlier issue to set the client name / protocol), Module developers can now create a custom HELLO command.\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2022-06-10T20:30:21Z",
        "closed_at": "2022-06-11T19:27:45Z",
        "merged_at": "2022-06-11T19:27:45Z",
        "body": "When `/proc/sys/vm/overcommit_memory` is inaccessible, fp is NULL.\r\n`checkOvercommit` will return -1 without setting err_msg, and then\r\nthe err_msg is used to print the log, crash the server.\r\nSet the err_msg variables to Null when declaring it, seems safer.\r\n\r\nAnd the overcommit_memory error log will print two \"WARNING\",\r\nlike `WARNING WARNING overcommit_memory is set to 0!`, this PR\r\nalso removes the second WARNING in `checkOvercommit`.\r\n\r\nReported in #10846. Fixes #10846. Introduced in #10636 (7.0.1)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-06-10T16:14:46Z",
        "closed_at": "2022-06-12T06:18:28Z",
        "merged_at": null,
        "body": "I was once asked that if sdshrd5 is really not used.\r\nOne thing i can confirm is that the \"key\" with string length\r\nless than 32 are using sdshdr5. Like sdsnew(\"abc\") is using sdshdr5.\r\n\r\nThese key names like \"a\", \"abc\" below stored in `db->dict` will use sdshdr5:\r\n```\r\nset a 1\r\nset abc 3\r\nset 1234567890123456789012345678901 31\r\n```\r\n\r\nThis commit also adds some type of judgement in sdsTest.\r\nIt should be easy for people to find it, if searching for type references.\r\nPreviously when sdsTest failed, it also returned 0, also fixed it.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-06-10T09:33:03Z",
        "closed_at": "2022-06-12T05:44:44Z",
        "merged_at": "2022-06-12T05:44:44Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-06-09T15:22:49Z",
        "closed_at": "2022-06-12T17:53:12Z",
        "merged_at": "2022-06-12T17:53:12Z",
        "body": "A regression caused by #10636 (released in 7.0.1) causes Redis startup warning about overcommit to be missing when needed and printed when not. \r\n\r\nAlso, using atoi() to convert the string's value to an integer cannot discern\r\nbetween an actual 0 (zero) having been read, or a conversion error. \r\n\r\n### Behavior after fix (warns correctly):\r\n```bash\r\n$ sudo sysctl -w vm.overcommit_memory=0\r\nvm.overcommit_memory = 0\r\n$ ./src/redis-server 2>&1 | grep -F vm.overcommit\r\n69889:M 09 Jun 2022 17:19:22.922 # WARNING WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\r\n^C\r\n$ sudo sysctl -w vm.overcommit_memory=1\r\nvm.overcommit_memory = 1\r\n$ ./src/redis-server 2>&1 | grep -F vm.overcommit\r\n^C\r\n```\r\n\r\n### Behavior as of 7.0.1 (bogus warning):\r\n```bash\r\n$ sudo sysctl -w vm.overcommit_memory=0\r\nvm.overcommit_memory = 0\r\n$ ./src/redis-server 2>&1 | grep -F vm.overcommit\r\n^C\r\n$ sudo sysctl -w vm.overcommit_memory=1\r\nvm.overcommit_memory = 1\r\n$ ./src/redis-server 2>&1 | grep -F vm.overcommit\r\n70145:M 09 Jun 2022 17:20:51.930 # WARNING WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\r\n^C\r\n```",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 101,
        "deletions": 15,
        "changed_files": 6,
        "created_at": "2022-06-09T13:32:33Z",
        "closed_at": "2022-06-26T11:34:59Z",
        "merged_at": "2022-06-26T11:34:59Z",
        "body": "Adding Module APIs to let the module read and set the client name of an arbitrary connection.\r\n\r\nRelated discussion in #10709.",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 130,
        "deletions": 48,
        "changed_files": 6,
        "created_at": "2022-06-09T04:21:01Z",
        "closed_at": "2022-06-12T05:22:18Z",
        "merged_at": "2022-06-12T05:22:18Z",
        "body": "The SET and BITFIELD command were added `get_keys_function` in #10148, causing\r\nthem to be wrongly marked movablekeys in `populateCommandMovableKeys`.\r\n\r\nThis was an unintended side effect introduced in #10148 (7.0 RC1)\r\nwhich could cause some clients an extra round trip for these commands in cluster mode.\r\n\r\nSince we define movablekeys as a way to determine if the legacy range [first, last, step]\r\ndoesn't find all keys, then we need a completely different approach.\r\n\r\nThe right approach should be to check if the legacy range covers all key-specs,\r\nand if none of the key-specs have the INCOMPLETE flag. \r\nThis way, we don't need to look at getkeys_proc of VARIABLE_FLAG at all.\r\nProbably with the exception of modules, who may still not be using key-specs.\r\n\r\nIn this PR, we removed `populateCommandMovableKeys` and put its logic in\r\n`populateCommandLegacyRangeSpec`.\r\nIn order to properly serve both old and new modules, we must probably keep relying\r\nCMD_MODULE_GETKEYS, but do that only for modules that don't declare key-specs. \r\nFor ones that do, we need to take the same approach we take with native redis commands.\r\n\r\nThis approach was proposed by Oran. Fixes #10833",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-06-08T16:54:37Z",
        "closed_at": "2022-06-09T09:59:33Z",
        "merged_at": "2022-06-09T09:59:33Z",
        "body": "The test calls `ldd` on `redis-server` in order to find out whether the binary\r\nwas linked against `libmusl`; However, `ldd` returns a value different from `0`\r\nwhen statically linking the binaries agains libc-musl, because `redis-server` is\r\nnot a dynamic executable (as given by the exception thrown by the failing test),\r\nand `make test` terminates with an error:\r\n\r\n    $ ldd src/redis-server\r\n        not a dynamic executable\r\n    $ echo $?\r\n    1\r\n\r\nThis commit fixes the test by adding another check to find out whether `Redis`\r\nwas statically linked, and only call `ldd` if this is not the case.\r\n\r\nWith that fix, all tests pass without errors, when building statically linked\r\nbinaries.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2639,
        "deletions": 806,
        "changed_files": 110,
        "created_at": "2022-06-08T05:44:19Z",
        "closed_at": "2022-06-08T09:56:25Z",
        "merged_at": "2022-06-08T09:56:24Z",
        "body": "Upgrade urgency: MODERATE, specifically if you're using a previous release of\r\nRedis 7.0, contains some behavior changes for new 7.0 features and important\r\nfixes for bugs in previous 7.0 releases.\r\n\r\nImprovements\r\n============\r\n\r\n* Add warning for suspected slow system clocksource setting\r\n  Add --check-system command line option. (#10636)\r\n* Allow read-only scripts (*_RO commands, and ones with `no-writes` flag)\r\n  during CLIENT PAUSE WRITE (#10744)\r\n* Add `readonly` flag in COMMAND command for EVAL_RO, EVALSHA_RO and FCALL_RO (#10728)\r\n* redis-server command line arguments now accept one string with spaces\r\n  for multi-arg configs (#10660)\r\n\r\nPotentially Breaking Changes\r\n============================\r\n\r\n* Omitting a config option value in command line argument no longer works (#10660)\r\n* Hide the `may_replicate` flag from the COMMAND command response (#10744)\r\n\r\nPotentially Breaking Changes for new Redis 7.0 features\r\n-------------------------------------------------------\r\n\r\n* Protocol: Sharded pubsub publish emits `smessage` instead of `message` (#10792)\r\n* CLUSTER SHARDS returns slots as RESP integers, not strings (#10683)\r\n* Block PFCOUNT and PUBLISH in read-only scripts (*_RO commands, and no-writes) (#10744)\r\n* Scripts that declare the `no-writes` flag are implicitly `allow-oom` too (#10699)\r\n\r\nChanges in CLI tools\r\n====================\r\n\r\n* redis-cli --bigkeys, --memkeys, --hotkeys, --scan. Finish nicely after Ctrl+C (#10736)\r\n\r\nPlatform / toolchain support related improvements\r\n=================================================\r\n\r\n* Support tcp-keepalive config interval on MacOs (#10667)\r\n* Support RSS metrics on Haiku OS (#10687)\r\n\r\nINFO fields and introspection changes\r\n=====================================\r\n\r\n* Add isolated network metrics for replication. (#10062, #10810)\r\n\r\nModule API changes\r\n==================\r\n\r\n* Add two more new checks to RM_Call script mode (#10786)\r\n* Add new RM_Call flag to let Redis automatically refuse `deny-oom` commands (#10786)\r\n* Add module API RM_MallocUsableSize (#10795)\r\n* Add missing REDISMODULE_NOTIFY_NEW (#10688)\r\n* Fix cursor type in RedisModuleScanCursor to handle more than 2^31 elements (#10698)\r\n* Fix RM_Yield bugs and RM_Call(\"EVAL\") OOM check bug (#10786)\r\n* Fix bugs in enum configs with overlapping bit flags (#10661)\r\n\r\nBug Fixes\r\n=========\r\n\r\n* FLUSHALL correctly resets rdb_changes_since_last_save INFO field (#10691)\r\n* FLUSHDB is now propagated to replicas / AOF, even if the db is empty (#10691)\r\n* Replica fail and retry the PSYNC if the master is unresponsive (#10726)\r\n* Fix ZRANGESTORE crash when zset_max_listpack_entries is 0 (#10767)\r\n\r\nFixes for issues in previous release candidates of Redis 7.0\r\n------------------------------------------------------------\r\n\r\n* CONFIG REWRITE could cause a config change to be dropped for aliased configs (#10811)\r\n* CONFIG REWRITE would omit rename-command and include lines (#10761)\r\n  NOTE: Affected users who used Redis 7.0.0 to rewrite their configuration file\r\n  should review and fix the file.\r\n* Fix broken protocol after MISCONF (persistence) error (#10786)\r\n* Fix --save command line regression (#10690)\r\n* Fix possible regression around TLS config changes. re-load files even if the\r\n  file name didn't change. (#10713)\r\n* Re-add SENTINEL SLAVES command, missing in redis 7.0 (#10723)\r\n* BZMPOP gets unblocked by non-key args and returns them (#10764)\r\n* Fix possible memory leak in XADD and XTRIM (#10753)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-06-07T13:46:25Z",
        "closed_at": "2022-06-07T15:47:01Z",
        "merged_at": "2022-06-07T15:47:01Z",
        "body": "This change fixes failing `integration/logging.tcl` test in Gentoo with musl libc, where `ldd` returns\r\n```\r\nlibc.so => /lib/ld-musl-x86_64.so.1 (0x7f9d5f171000)\r\n```\r\nunlike Alpine's\r\n```\r\nlibc.musl-x86_64.so.1 => /lib/ld-musl-x86_64.so.1 (0x7f82cfa16000)\r\n```\r\nThe solution is to extend matching pattern introduced in #8532 by @yossigo.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2022-06-07T08:50:07Z",
        "closed_at": "2022-06-07T10:26:47Z",
        "merged_at": "2022-06-07T10:26:47Z",
        "body": "Correcting the introduction version of the command `BITFIELD_RO`\r\nCommand added by commit: b3e4abf06e \r\n\r\nAdd history info of the `FULL` modifier in `XINFO STREAM`\r\nModifier was added by commit: 1e2aee3919\r\n\r\n(Includes output from `./utils/generate-command-code.py`)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2022-06-06T18:52:47Z",
        "closed_at": "2022-06-09T07:16:56Z",
        "merged_at": "2022-06-09T07:16:56Z",
        "body": "When I take a look the XStream data type, I found some comments does not match with our current documentation or command format, thus I update them to latest version.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-06-06T17:56:43Z",
        "closed_at": "2022-06-07T01:11:59Z",
        "merged_at": "2022-06-07T01:11:59Z",
        "body": "Current documentation only states a single GET token is needed which is not true. Marking the command with multiple_token",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2022-06-06T06:16:52Z",
        "closed_at": "2022-06-06T08:39:53Z",
        "merged_at": "2022-06-06T08:39:53Z",
        "body": "before releasing 7.0.1",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-06-03T11:10:02Z",
        "closed_at": "2022-06-06T08:39:23Z",
        "merged_at": "2022-06-06T08:39:23Z",
        "body": "Trying to avoid people opening crash report issues about module crashes and ARM QEMU bugs.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2022-06-02T13:35:43Z",
        "closed_at": "2022-06-03T22:36:36Z",
        "merged_at": "2022-06-03T22:36:36Z",
        "body": "@itamarhaber - opening this to address some of the spelling/grammar issues that came up in redis/redis-doc#1971 - looks the actual spelling errors that were in there have probably already been remediated. There were some other issues though, weird use of indefinite articles, duplicated words, and a markdown issue that are definitely worth fixing.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2022-06-02T04:10:26Z",
        "closed_at": "2022-06-02T11:03:47Z",
        "merged_at": "2022-06-02T11:03:47Z",
        "body": "Redis 7 adds some new alias config like `hash-max-listpack-entries` alias `hash-max-ziplist-entries`.\r\n\r\nIf a config file contains both real name and alias like this:\r\n```\r\nhash-max-listpack-entries 20\r\nhash-max-ziplist-entries 20\r\n```\r\n\r\nafter set `hash-max-listpack-entries` to 100 and `config rewrite`, the config file becomes to:\r\n```\r\nhash-max-listpack-entries 100\r\nhash-max-ziplist-entries 20\r\n```\r\n\r\nwe can see that the alias config is not modified, and users will get wrong config after restart.\r\n\r\n6.0 and 6.2 doesn't have this bug, since they only have the `slave` word alias.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-06-02T04:07:20Z",
        "closed_at": "2022-06-06T05:29:24Z",
        "merged_at": "2022-06-06T05:29:24Z",
        "body": "A supplement to https://github.com/redis/redis/pull/10062\r\nSplit `instantaneous_repl_total_kbps` to `instantaneous_input_repl_kbps` and `instantaneous_output_repl_kbps`. \r\n## Work:\r\nThis PR:\r\n- delete 1 info field:\r\n    - `instantaneous_repl_total_kbps`\r\n- add 2 info fields:\r\n    - `instantaneous_input_repl_kbps / instantaneous_output_repl_kbps`\r\n## Result:\r\n- master\r\n```\r\ntotal_net_input_bytes:26633673\r\ntotal_net_output_bytes:21716596\r\ntotal_net_repl_input_bytes:0\r\ntotal_net_repl_output_bytes:18433052\r\ninstantaneous_input_kbps:0.02\r\ninstantaneous_output_kbps:0.00\r\ninstantaneous_input_repl_kbps:0.00\r\ninstantaneous_output_repl_kbps:0.00\r\n```\r\n- slave\r\n```\r\ntotal_net_input_bytes:18433212\r\ntotal_net_output_bytes:94790\r\ntotal_net_repl_input_bytes:18433052\r\ntotal_net_repl_output_bytes:0\r\ninstantaneous_input_kbps:0.00\r\ninstantaneous_output_kbps:0.05\r\ninstantaneous_input_repl_kbps:0.00\r\ninstantaneous_output_repl_kbps:0.00\r\n```",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-06-02T03:01:11Z",
        "closed_at": "2022-06-02T06:00:13Z",
        "merged_at": "2022-06-02T06:00:12Z",
        "body": "Skip the print on AOF_NOT_EXIST status.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 25,
        "changed_files": 2,
        "created_at": "2022-06-01T21:25:19Z",
        "closed_at": "2022-06-03T18:35:56Z",
        "merged_at": null,
        "body": "A small change to wrap intset and encoding details when making a change to a set.\r\nIt does not cover reading from a set. Those code paths are more numerous.\r\n\r\nI removed the setTypeRandomElements forward declaration because that function has been long removed from the code base. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-06-01T08:39:50Z",
        "closed_at": "2022-06-01T13:21:46Z",
        "merged_at": "2022-06-01T13:21:46Z",
        "body": "Apparently, GCC 11.2.0 has a new fancy warning for misleading indentations. It prints a warning when `BRET(b)` is on the same line as the loop.\r\n\r\n```\r\nlua_bit.c: In function \u2018bit_band\u2019:\r\nlua_bit.c:101:5: warning: this \u2018for\u2019 clause does not guard... [-Wmisleading-indentation]\r\n  101 |     for (i = lua_gettop(L); i > 1; i--) b opr barg(L, i); BRET(b) }\r\n      |     ^~~\r\nlua_bit.c:102:1: note: in expansion of macro \u2018BIT_OP\u2019\r\n  102 | BIT_OP(bit_band, &=)\r\n      | ^~~~~~\r\nlua_bit.c:94:18: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the \u2018for\u2019\r\n   94 | #define BRET(b)  lua_pushnumber(L, (lua_Number)(SBits)(b)); return 1;\r\n      |                  ^~~~~~~~~~~~~~\r\nlua_bit.c:101:59: note: in expansion of macro \u2018BRET\u2019\r\n  101 |     for (i = lua_gettop(L); i > 1; i--) b opr barg(L, i); BRET(b) }\r\n      |                                                           ^~~~\r\n.....\r\n```",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-05-30T18:13:37Z",
        "closed_at": "2022-06-21T04:40:48Z",
        "merged_at": "2022-06-21T04:40:48Z",
        "body": "tl;dr: Prevent replicas from incorrectly loading a configEpoch that is the highest in the cluster, preventing `CLUSTER FAILOVER TAKEOVER` from being acknowledged within the cluster.\r\n\r\n1. The configEpoch shown by the 'cluster nodes/info' command (cluster_my_epoch in 'cluster info', or index 6 in the node line in 'cluster nodes') is of the primary of the shard, and there is no indication of the config epoch of any of the replicas, as stored in myself->configEpoch.\r\n```\r\n        myepoch = (nodeIsSlave(myself) && myself->slaveof) ?\r\n                  myself->slaveof->configEpoch : myself->configEpoch;\r\n            ...\r\n        sds info = sdscatprintf(sdsempty(),\r\n            ...\r\n            \"cluster_my_epoch:%llu\\r\\n\"\r\n            (unsigned long long) myepoch,\r\n```\r\n\r\n2. When Redis is being started with cluster configuration file (nodes.conf) it loads the configEpoch from the file, which contains the output of 'cluster nodes' command. That means, that if Redis is being restarted on a replica and a cluster configuration file is being loaded, the replica will be configured with the primary's config epoch. That is because the cluster nodes output contains only the primaries configuration epochs, and as explained in (1), we don't have indication for the real config epoch of the replica.\r\nHere's a short example:\r\n(*I added cluster_my_replica_epoch to the 'cluster info' command to show the actual configEpoch of the replica.*) \r\n - Before restarting replica node 16379:\r\n```\r\nec2-user@ip-172-31-113-41 ~/redis (unstable) $ redis-cli -p 16379 cluster info\r\n...\r\ncluster_my_epoch:11\r\ncluster_my_replica_epoch:1\r\n```\r\n -  Restart Redis on replica 16379 with this cluster configuration:\r\n```\r\na55b0db29d7326891f58e07f0ff62a00eb2077e7 127.0.0.1:16383@26383 slave 63fef265453ad4c7853a3d3c7852f3b20fff02d4 0 1653928756000 3 connected\r\n4c771033854e5b345b05fbf599b46969e76b2110 127.0.0.1:16387@26387 slave cfdc5605449628edd73fd2734a20fc1c2d1be25b 0 1653928757379 2 connected\r\ne379c456fc7851e22d3092dd4c3e5e3c320ce2aa 127.0.0.1:16386@26386 slave e6fa3c8db3fb522596d63322569af478aa0d5eab 0 1653928756000 11 connected\r\nd02e89ef37de64bde93e3eaf60eefd1cf3dc51c0 127.0.0.1:16379@26379 myself,slave e6fa3c8db3fb522596d63322569af478aa0d5eab 0 1653928748000 11 connected\r\ne6fa3c8db3fb522596d63322569af478aa0d5eab 127.0.0.1:16388@26388 master - 0 1653928754000 11 connected 0-5460\r\ncfdc5605449628edd73fd2734a20fc1c2d1be25b 127.0.0.1:16380@26380 master - 0 1653928756377 2 connected 5461-10922\r\n881114f5c6fb875cb1d2b0af20592be2773fa216 127.0.0.1:16385@26385 slave 63fef265453ad4c7853a3d3c7852f3b20fff02d4 0 1653928756000 3 connected\r\n63fef265453ad4c7853a3d3c7852f3b20fff02d4 127.0.0.1:16381@26381 master - 0 1653928755000 3 connected 10923-16383\r\na35e7cac6de26444fb53d520292d8dddf323bbed 127.0.0.1:16384@26384 slave e6fa3c8db3fb522596d63322569af478aa0d5eab 0 1653928754000 11 connected\r\nb45422e62de3b69532a70197404c3a0ca8b6fa66 127.0.0.1:16382@26382 slave cfdc5605449628edd73fd2734a20fc1c2d1be25b 0 1653928756578 2 connected\r\n```\r\n\r\n - After the replica was restated and the configuration file is loaded:\r\n```\r\nec2-user@ip-172-31-113-41 ~/redis (unstable) $ redis-cli -p 16379 cluster info\r\n...\r\ncluster_my_epoch:11\r\ncluster_my_replica_epoch:11\r\n```\r\n\r\n3. How can it be a problem? If we'll now initiate 'cluster failover takeover' on this replica, it will have the greatest epoch in the cluster, and therefore the cluster epoch won't be bumped (see [cluster failover takeover](https://redis.io/commands/cluster-failover/#takeover-option-manual-failover-without-cluster-consensus)). If the epoch isn't bumped, other nodes in the cluster won't recognize the rebinding of slots to the new primary.\r\n4. As the configEpoch of the primary, when the replica is just being started, does not reflect the real status of the replica, I propose that it be set to zero when loading configuration files. \r\n\r\nChanges summary:\r\n- Changed clusterLoadConfig to set the config epoch of replica nodes to 0 when loaded.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-05-30T04:28:52Z",
        "closed_at": "2022-05-31T07:29:17Z",
        "merged_at": "2022-05-31T07:29:17Z",
        "body": "This allows the module to know the usable size of an allocation it made, rather than the consumed size.\r\n\r\nA module I recently developed uses the data structure `listpack`, but `listpack` depends on `zmalloc_usable_size()` which is not provided by the redis module.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 20,
        "changed_files": 7,
        "created_at": "2022-05-28T19:33:04Z",
        "closed_at": "2022-05-31T05:04:00Z",
        "merged_at": "2022-05-31T05:04:00Z",
        "body": "To easily distinguish between sharded channel message and a global channel message, introducing `smessage` (instead of `message`) as message bulk for sharded channel publish message.\r\n\r\nThis is gonna be a breaking change in 7.0.1!\r\n\r\nBackground:\r\nSharded pubsub introduced in redis 7.0, but after the release we quickly realized that the fact that it's problematic that the client can't distinguish between normal (global) pubsub messages and sharded ones.\r\nThis is important because the same connection can subscribe to both, but messages sent to one pubsub system are not propagated to the other (they're completely separate), so if one connection is used to subscribe to both, we need to assist the client library to know which message it got so it can forward it to the correct callback.\r\n\r\nBefore this PR:\r\n\r\n```\r\n> SPUBLISH ch1 hello\r\n(integer) 1\r\n\r\n> SSUBSCRIBE ch1\r\nReading messages... (press Ctrl-C to quit)\r\n1) \"ssubscribe\"\r\n2) \"ch1\"\r\n3) (integer) 1\r\n1) \"message\"\r\n2) \"ch1\"\r\n3) \"hello\"\r\n```\r\n\r\nAfter this PR:\r\n\r\n```\r\n> SPUBLISH ch1 hello\r\n(integer) 1\r\n\r\n> SSUBSCRIBE ch1\r\nReading messages... (press Ctrl-C to quit)\r\n1) \"ssubscribe\"\r\n2) \"ch1\"\r\n3) (integer) 1\r\n1) \"smessage\"\r\n2) \"ch1\"\r\n3) \"hello\"\r\n```\r\n\r\n\r\nRef #10748 ",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-28T06:03:43Z",
        "closed_at": "2022-05-29T19:45:56Z",
        "merged_at": "2022-05-29T19:45:56Z",
        "body": "This bug resulted in wrong memory usage statistics after a redis function library is removed.\r\n\r\nSigned-off-by: skygragon <skygragon@gmail.com>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2022-05-28T04:45:38Z",
        "closed_at": "2022-05-29T10:33:16Z",
        "merged_at": "2022-05-29T10:33:16Z",
        "body": "The ret value should be AOF_OK instead of C_OK.\r\nAOF_OK and C_OK are both 0, so this is just a cleanup.\r\nAlso updated some outdated comments.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-05-27T19:54:49Z",
        "closed_at": "2022-06-14T18:12:45Z",
        "merged_at": "2022-06-14T18:12:45Z",
        "body": "when we know the size of the zset we're gonna store in advance, we can check if it's greater than the listpack encoding threshold, in which case we can create a skiplist from the get go, and avoid converting the listpack to skiplist later after it was already populated.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 416,
        "deletions": 29,
        "changed_files": 12,
        "created_at": "2022-05-27T14:28:06Z",
        "closed_at": "2022-06-01T10:04:23Z",
        "merged_at": "2022-06-01T10:04:22Z",
        "body": "* Fix broken protocol when redis can't persist to RDB (general commands, not\r\n  modules), excessive newline. regression of #10372 (7.0 RC3)\r\n* Fix broken protocol when Redis can't persist to AOF (modules and\r\n  scripts), missing newline.\r\n* Fix bug in OOM check of EVAL scripts called from RM_Call.\r\n  set the cached OOM state for scripts before executing module commands too,\r\n  so that it can serve scripts that are executed by modules.\r\n  i.e. in the past EVAL executed by RM_Call could have either falsely\r\n  fail or falsely succeeded because of a wrong cached OOM state flag.\r\n* Fix bugs with RM_Yield:\r\n  1. SHUTDOWN should only accept the NOSAVE mode\r\n  2. Avoid eviction during yield command processing.\r\n  3. Avoid processing master client commands while yielding from another client\r\n* Add new two more checks to RM_Call script mode.\r\n  1. READONLY You can't write against a read only replica\r\n  2. MASTERDOWN Link with MASTER is down and `replica-serve-stale-data` is set to `no`\r\n* Add new RM_Call flag to let redis automatically refuse `deny-oom` commands\r\n  while over the memory limit. \r\n* Add tests to cover various errors from Scripts, Modules, Modules\r\n  calling scripts, and Modules calling commands in script mode.\r\n\r\nAdd tests:\r\n* Looks like the MISCONF error was completely uncovered by the tests,\r\n  add tests for it, including from scripts, and modules\r\n* Add tests for NOREPLICAS from scripts\r\n* Add tests for the various errors in module RM_Call, including RM_Call that\r\n  calls EVAL, and RM_call in \"eval mode\". that includes:\r\n  NOREPLICAS, READONLY, MASTERDOWN, MISCONF",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-27T13:18:57Z",
        "closed_at": "2022-05-29T05:51:34Z",
        "merged_at": "2022-05-29T05:51:33Z",
        "body": "fix typo ` the largest possible limit is 16k` -> ` the largest possible limit is 64k`.\r\nThe count field is 16 bits so the largest possible limit is 64k(2**16).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-05-27T02:56:07Z",
        "closed_at": "2022-05-29T05:38:38Z",
        "merged_at": "2022-05-29T05:38:38Z",
        "body": "There is a timing issue reported in test-sanitizer-address (gcc):\r\n```\r\nSentinels (re)connection following SENTINEL SET mymaster auth-pass:\r\nFAILED: Expected to be disconnected from master due to wrong password\r\n```\r\n\r\nThe reason we reach it, is because the test is fast enough to modify\r\nauth-pass and test sentinel connection status with the server,\r\nbefore its scheduled operation got the chance to update connection\r\nstatus with the server.\r\n\r\nWe need to wait for `sentinelTimer` to kick in, and then update the\r\nconnection status. Replace condition with wait_for_condition on the check.\r\n\r\nFix just like #10480 did",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-26T20:00:36Z",
        "closed_at": "2022-05-31T04:57:59Z",
        "merged_at": "2022-05-31T04:57:59Z",
        "body": "When I read the source codes, I have no idea where the option \"age\" come from.\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-05-26T17:47:41Z",
        "closed_at": "2022-05-30T17:38:43Z",
        "merged_at": null,
        "body": "Assume in the server, we have only one key: **mystream** as below:\r\n\r\nIf the the stream has reached the last possible ID,  we will meet the following error:\r\n(error) ERR The stream has exhausted the last possible ID, unable to add more items\r\n\r\n127.0.0.1:6379> xadd maxStream * aaa bbb ccc ddd eee fff\r\n**(error) ERR The stream has exhausted the last possible ID, unable to add more items**\r\n127.0.0.1:6379> keys *\r\n1) \"mystream\"\r\n2) \"maxStream\"\r\n\r\n127.0.0.1:6379> info Persistence\r\n# Persistence\r\nloading:0\r\nasync_loading:0\r\ncurrent_cow_peak:0\r\ncurrent_cow_size:0\r\ncurrent_cow_size_age:0\r\ncurrent_fork_perc:0.00\r\ncurrent_save_keys_processed:0\r\ncurrent_save_keys_total:0\r\n**rdb_changes_since_last_save:0**\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1653587494\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:-1\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_saves:0\r\nrdb_last_cow_size:0\r\nrdb_last_load_keys_expired:0\r\nrdb_last_load_keys_loaded:1\r\naof_enabled:0\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_rewrites:0\r\naof_rewrites_consecutive_failures:0\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\n\r\n\r\nAfter this fix, the message will change as below:\r\nrdb_changes_since_last_save will be 1 instead of 0\r\n\r\n**127.0.0.1:6379> xadd maxStream * aaa bbb ccc ddd eee fff\r\n(error) ERR The stream has exhausted the last possible ID, unable to add more items**\r\n127.0.0.1:6379> keys *\r\n1) \"mystream\"\r\n2) \"maxStream\"\r\n127.0.0.1:6379> info Persistence\r\n# Persistence\r\nloading:0\r\nasync_loading:0\r\ncurrent_cow_peak:0\r\ncurrent_cow_size:0\r\ncurrent_cow_size_age:0\r\ncurrent_fork_perc:0.00\r\ncurrent_save_keys_processed:0\r\ncurrent_save_keys_total:0\r\n**rdb_changes_since_last_save:1**\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1653588851\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:-1\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_saves:0\r\nrdb_last_cow_size:0\r\nrdb_last_load_keys_expired:0\r\nrdb_last_load_keys_loaded:1\r\naof_enabled:0\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_rewrites:0\r\naof_rewrites_consecutive_failures:0\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\n\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 181,
        "deletions": 19,
        "changed_files": 10,
        "created_at": "2022-05-26T08:18:18Z",
        "closed_at": "2022-05-27T09:58:00Z",
        "merged_at": "2022-05-27T09:58:00Z",
        "body": "There are some commands that has the wrong key specs.\r\nThis PR adds a key-spec related check in generate-command-code.py.\r\nCheck if the index is valid, or if there is an unused index.\r\n\r\nThe check result will look like:\r\n```\r\n[root]# python utils/generate-command-code.py\r\nProcessing json files...\r\nLinking container command to subcommands...\r\nChecking all commands...\r\ncommand: RESTORE_ASKING may have unused key_spec\r\ncommand: RENAME may have unused key_spec\r\ncommand: PFDEBUG may have unused key_spec\r\ncommand: WATCH key_specs missing flags\r\ncommand: LCS arg: key2 key_spec_index error\r\ncommand: RENAMENX may have unused key_spec\r\nError: There are errors in the commands check, please check the above logs.\r\n```\r\n\r\nThe following commands have been fixed according to the check results:\r\n- RESTORE ASKING: add missing arguments section (and history section)\r\n- RENAME: newkey's key_spec_index should be 1\r\n- PFDEBUG: add missing arguments (and change the arity from -3 to 3)\r\n- WATCH: add missing key_specs flags: RO, like EXIST (it allow you to know the key exists, or is modified, but doesn't \"leak\" the data)\r\n- LCS: key2 key_spec_index error, there is only one key-spec\r\n- RENAMENX: newkey's key_spec_index should be 1\r\n\r\nThis was mention in https://github.com/redis/redis/pull/9656#pullrequestreview-951194532",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-25T16:49:40Z",
        "closed_at": "2022-06-23T15:40:20Z",
        "merged_at": "2022-06-23T15:40:20Z",
        "body": "It will be displayed in the `aof_last_bgrewrite_status`\r\nfield of the INFO command.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-25T09:38:07Z",
        "closed_at": "2022-06-06T05:17:18Z",
        "merged_at": "2022-06-06T05:17:18Z",
        "body": "On line 4068, redis has a logical nodeIsSlave(myself) on the outer if layer, which you can delete without having to repeat the decision",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-25T03:17:43Z",
        "closed_at": "2022-05-25T09:36:43Z",
        "merged_at": null,
        "body": "On line 4068, redis has a logical nodeIsSlave(myself) on the outer if layer, which you can delete without having to repeat the decision",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-05-24T23:50:17Z",
        "closed_at": "2022-05-27T19:34:01Z",
        "merged_at": "2022-05-27T19:34:01Z",
        "body": "When `zrangestore` is called container destination object is created. \r\nBefore this PR we used to create a listpack based object even if `zset-max-ziplist-entries` or equivalent`zset-max-listpack-entries` was set to 0.\r\nThis triggered immediate conversion of the listpack into a skiplist in `zrangestore`, which hits an assertion [here](https://github.com/redis/redis/blob/unstable/src/t_zset.c#L1189) resulting in an engine crash.\r\n\r\n[EDIT] This crash also happens when the first entry being inserted to the zset by ZRANGESTORE is bigger than `zset-max-ziplist-value` (default to 64).\r\nThis makes it even more severe.\r\n\r\nAdded a TCL test that reproduces this issue.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-05-23T07:36:18Z",
        "closed_at": "2022-05-23T11:15:55Z",
        "merged_at": "2022-05-23T11:15:55Z",
        "body": "This bug was introduced in #9484 (7.0.0).\r\nIt result that BZMPOP blocked on non-key arguments.\r\n\r\nLike `bzmpop 0 1 myzset min count 10`, this command will additionally\r\nblock in these keys (except for the first and the last argument) and can return their values:\r\n- 0: timeout value\r\n- 1: numkeys value\r\n- min: min/max token\r\n- count: count token\r\n\r\nFixes #10762",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2022-05-23T07:04:38Z",
        "closed_at": "2022-05-26T14:23:05Z",
        "merged_at": "2022-05-26T14:23:05Z",
        "body": "instead of printing a log when a folder or a manifest is missing (level reduced), we print:\r\n* total time it took to load all the aof files\r\n* when creating a new base or incr file\r\n* starting to write to an existing incr file on startup\r\n\r\nissue link: #10757 ",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2022-05-23T03:15:15Z",
        "closed_at": "2022-06-02T05:36:55Z",
        "merged_at": "2022-06-02T05:36:55Z",
        "body": "A regression from #10285 (redis 7.0).\r\nCONFIG REWRITE would put lines with: `include`, `rename-command`, `user`,  `loadmodule`, and any module specific config in a comment.\r\n\r\nFor ACL `user`, `loadmodule` and module specific configs would be re-inserted at the end (instead of updating existing lines), so the only implication is a messy config file full of comments.\r\n\r\nBut for `rename-command` and `include`, the implication would be that they're now missing, so a server restart would lose them.",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 83,
        "deletions": 37,
        "changed_files": 3,
        "created_at": "2022-05-20T14:03:06Z",
        "closed_at": "2022-05-22T10:44:29Z",
        "merged_at": "2022-05-22T10:44:29Z",
        "body": "The code is based on upstream https://github.com/HdrHistogram/HdrHistogram_c master branch latest commit (e4448cf6d1cd08fff519812d3b1e58bd5a94ac42). The reason to pull this in now is that their final version of our optimization is even faster. See: https://github.com/HdrHistogram/HdrHistogram_c/pull/107.\r\n\r\nOriginal:\r\n```console\r\nyoav@yoav-thinkpad:~/HdrHistogram_c/build$ ./test/hdr_histogram_benchmark --benchmark_filter=BM_hdr_value_at_percentile/3/86400000 --benchmark_min_time=10\r\n2022-03-21 12:41:46\r\nRunning ./test/hdr_histogram_benchmark\r\nRun on (12 X 4600 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32K (x6)\r\n  L1 Instruction 32K (x6)\r\n  L2 Unified 256K (x6)\r\n  L3 Unified 12288K (x1)\r\nLoad Average: 0.68, 0.81, 0.89\r\n***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\r\n***WARNING*** Library was built as DEBUG. Timings may be affected.\r\n-----------------------------------------------------------------------------------\r\nBenchmark                                         Time             CPU   Iterations\r\n-----------------------------------------------------------------------------------\r\nBM_hdr_value_at_percentile/3/86400000         34937 ns        34955 ns       404859\r\nBM_hdr_value_at_percentile/3/86400000000      36561 ns        36581 ns       398987\r\n```\r\n\r\nAfter our original optimization:\r\n```console\r\nyoav@yoav-thinkpad:~/HdrHistogram_c/build$ ./test/hdr_histogram_benchmark --benchmark_filter=BM_hdr_value_at_percentile/3/86400000 --benchmark_min_time=10\r\n2022-03-21 13:36:17\r\nRunning ./test/hdr_histogram_benchmark\r\nRun on (12 X 4600 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32K (x6)\r\n  L1 Instruction 32K (x6)\r\n  L2 Unified 256K (x6)\r\n  L3 Unified 12288K (x1)\r\nLoad Average: 0.86, 0.96, 1.26\r\n***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\r\n***WARNING*** Library was built as DEBUG. Timings may be affected.\r\n-----------------------------------------------------------------------------------\r\nBenchmark                                         Time             CPU   Iterations\r\n-----------------------------------------------------------------------------------\r\nBM_hdr_value_at_percentile/3/86400000          5070 ns         5083 ns      2795009\r\nBM_hdr_value_at_percentile/3/86400000000       5124 ns         5135 ns      2753732\r\n```\r\n\r\nLatest (this PR):\r\n\r\n```console\r\nyoav@yoav-thinkpad:~/HdrHistogram_c/build(master)$ ./test/hdr_histogram_benchmark --benchmark_filter=BM_hdr_value_at_percentile/3/86400000 --benchmark_min_time=10\r\n2022-05-20T17:01:52+03:00\r\nRunning ./test/hdr_histogram_benchmark\r\nRun on (12 X 4600 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32 KiB (x6)\r\n  L1 Instruction 32 KiB (x6)\r\n  L2 Unified 256 KiB (x6)\r\n  L3 Unified 12288 KiB (x1)\r\nLoad Average: 0.51, 1.22, 1.53\r\n***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\r\n***WARNING*** Library was built as DEBUG. Timings may be affected.\r\n-----------------------------------------------------------------------------------\r\nBenchmark                                         Time             CPU   Iterations\r\n-----------------------------------------------------------------------------------\r\nBM_hdr_value_at_percentile/3/86400000          2583 ns         2597 ns      5424698\r\nBM_hdr_value_at_percentile/3/86400000000       2565 ns         2578 ns      4944717\r\n```\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-19T12:27:20Z",
        "closed_at": "2022-05-22T09:15:27Z",
        "merged_at": "2022-05-22T09:15:26Z",
        "body": "si is initialized by streamIteratorStart(), we should call streamIteratorStop() on it when done. #10752 ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-05-19T10:02:32Z",
        "closed_at": "2022-05-22T09:27:54Z",
        "merged_at": "2022-05-22T09:27:54Z",
        "body": "Remove some dead code in object.c, ziplist is no longer used in 7.0\r\n\r\nSome backgrounds:\r\nzipmap - hash: replaced by ziplist in #285\r\nziplist - hash: replaced by listpack in #8887\r\nziplist - zset: replaced by listpack in #9366\r\nziplist - list: replaced by quicklist (listpack) in #2143 / #9740",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 60,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2022-05-18T14:18:40Z",
        "closed_at": "2022-11-26T02:01:01Z",
        "merged_at": "2022-11-26T02:01:01Z",
        "body": "\u2026tls-port should take effect at runtime\r\n\r\nSigned-off-by: DevineLiu <hrliu@alauda.io>\r\n\r\n```\r\nRelease notes:\r\nFix a bug where updating cluster-announce-tls-port, cluster-announce-port or cluster-announce-bus-port at runtime would only show up on other nodes in the cluster.\r\n```",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 361,
        "deletions": 121,
        "changed_files": 12,
        "created_at": "2022-05-18T13:24:38Z",
        "closed_at": "2022-06-01T11:09:40Z",
        "merged_at": "2022-06-01T11:09:40Z",
        "body": "In order to better handle scripts with shebang flags that can indicate\r\nthe script is a read-only, allow-oom, allow-stale etc, or not.\r\n\r\nThe important part is that read-only scripts (not just EVAL_RO\r\nand FCALL_RO, but also ones with `no-writes` executed by normal EVAL or\r\nFCALL), will now be permitted to run during CLIENT PAUSE WRITE (unlike\r\nbefore where only the _RO commands would be processed).\r\n\r\nOther than that, some errors like OOM, READONLY, MASTERDOWN are now\r\nhandled by processCommand, rather than the command itself affects the\r\nerror string (and even error code in some cases), and command stats.\r\n\r\nBesides that, now the `may-replicate` commands, PFCOUNT and PUBLISH, will\r\nbe considered `write` commands in scripts and will be blocked in all\r\nread-only scripts just like other write commands.\r\nThey'll also be blocked in EVAL_RO (i.e. even for scripts without the\r\n`no-writes` shebang flag.\r\n\r\nThis commit also hides the `may_replicate` flag from the COMMAND command\r\noutput. this is a **breaking change**.\r\n\r\nbackground about may_replicate:\r\nWe don't want to expose a no-may-replicate flag or alike to scripts, since we\r\nconsider the may-replicate thing an internal concern of redis, that we may\r\nsome day get rid of.\r\nIn fact, the may-replicate flag was initially introduced to flag EVAL: since\r\nwe didn't know what it's gonna do ahead of execution, before function-flags\r\nexisted). PUBLISH and PFCOUNT, both of which because they have side effects\r\nwhich may some day be fixed differently.\r\n\r\ncode changes:\r\nThe changes in eval.c are mostly code re-ordering:\r\n- evalCalcFunctionName is extracted out of evalGenericCommand\r\n- evalExtractShebangFlags is extracted luaCreateFunction\r\n- evalGetCommandFlags is new code\r\n\r\nThis was discussed in https://github.com/redis/redis/pull/10364#issuecomment-1125119110\r\n\r\nDoc PR: https://github.com/redis/redis-doc/pull/1954",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 14,
        "changed_files": 3,
        "created_at": "2022-05-17T18:07:41Z",
        "closed_at": "2022-05-25T15:25:38Z",
        "merged_at": "2022-05-25T15:25:38Z",
        "body": "The regex support was added in:\r\n * https://github.com/redis/redis/pull/9352\r\n * https://github.com/redis/redis/pull/9555\r\n * https://github.com/redis/redis/pull/10212\r\n\r\nThese commits break backword compatiblity with older versions.\r\n\r\nThis fix keeps the test suite infra compatible with old versions by\r\ndefault. However, if you want regex, the string must start with `/`",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2022-05-17T16:03:14Z",
        "closed_at": "2022-05-18T05:29:39Z",
        "merged_at": "2022-05-18T05:29:39Z",
        "body": "minor cleanup in redismodule.h and module.c",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 20,
        "changed_files": 7,
        "created_at": "2022-05-17T07:54:01Z",
        "closed_at": "2022-05-31T05:55:25Z",
        "merged_at": "2022-05-31T05:55:25Z",
        "body": "since the sharded pubsub is different with pubsub, it's better to give users a hint to make it more clear.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 82,
        "deletions": 4,
        "changed_files": 6,
        "created_at": "2022-05-17T06:03:10Z",
        "closed_at": "2022-06-20T16:17:23Z",
        "merged_at": "2022-06-20T16:17:23Z",
        "body": "The current process to persist files is `write` the data, `fsync` and `rename` the file, but a underlying problem is that the rename may be lost when a sudden crash like power outage and the directory hasn't been persisted.\r\n\r\nThe article [Ensuring data reaches disk](https://lwn.net/Articles/457667/) mentions a safe way to update file should be:\r\n1. create a new temp file (on the same file system!)\r\n2. write data to the temp file\r\n3. fsync() the temp file\r\n4. rename the temp file to the appropriate name\r\n5. fsync() the containing directory\r\n\r\nReferring to [Leveldb](https://github.com/google/leveldb/blob/d019e3605f222ebc5a3a2484a2cb29db537551dd/util/env_posix.cc#L334 ) , \r\n```\r\n  Status Sync() override {\r\n    // Ensure new files referred to by the manifest are in the filesystem.\r\n    //\r\n    // This needs to happen before the manifest file is flushed to disk, to\r\n    // avoid crashing in a state where the manifest refers to files that are not\r\n    // yet on disk.\r\n    Status status = SyncDirIfManifest();\r\n    if (!status.ok()) {\r\n      return status;\r\n    }\r\n\r\n    status = FlushBuffer();\r\n    if (!status.ok()) {\r\n      return status;\r\n    }\r\n\r\n    return SyncFd(fd_, filename_);\r\n  }\r\n```\r\nSo we should also fsync the directory to ensure the safety.\r\n\r\nThis PR handles CONFIG REWRITE, AOF manifest, and RDB file (both for persistence, and the one the replica gets from the master).\r\nIt doesn't handle (yet), ACL SAVE and Cluster configs, since these don't yet follow this pattern.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2022-05-16T23:08:37Z",
        "closed_at": "2022-05-22T07:55:26Z",
        "merged_at": "2022-05-22T07:55:26Z",
        "body": "Finish current loop and display the scanned keys summery on SIGINT (Ctrl-C) signal.\r\nIt will also prepend the current scanned percentage to the scanned keys summery 1st line.\r\n\r\nIn this commit I've renamed and relocated `intrinsicLatencyModeStop` function as I'm using the exact same logic.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-05-16T08:43:30Z",
        "closed_at": "2022-07-19T09:08:55Z",
        "merged_at": null,
        "body": "return the total amount of memory redis has allocated so modules can do their own enforcment of memory limits",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 9,
        "changed_files": 7,
        "created_at": "2022-05-15T03:45:21Z",
        "closed_at": "2022-05-30T06:42:57Z",
        "merged_at": "2022-05-30T06:42:56Z",
        "body": "This PR does two things:\r\n1. It adds the read-only flag to EVAL_RO, EVALSHA_RO and FCALL_RO.\r\n2. Require users to explicitly declare `@scripting` to get access to lua scripting. EVAL is not added with `@write`, so I don't think `@read` should give access to EVAL_RO.\r\n\r\nA note about client side tracking: Client side tracking detects all commands marked with `READ-ONLY` and then tracks all of the keys that command declares. The exception to this rule is the EVAL command, which when it's scripting client executes a \"READONLY\" command it starts tracking all the keys declared by the EVAL script. There is some logic added so that the *_RO scripting command emulate this behavior. \r\n\r\n\r\n### Release notes information\r\nThe motivation is that many clients also better support routing the read-only script commands to replicas for applications that want to use replicas for read scaling.\r\nA minor secondary motivation is that administrators may want to restrict developers from writing allow-write scripts.\r\nThe recommended approach is to use the standard scripting commands with the `no-writes` flag unless you need one of the previously mentioned features.\r\n\r\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2022-05-14T17:36:18Z",
        "closed_at": "2022-05-15T12:54:06Z",
        "merged_at": null,
        "body": "Can return faster without more operations, when memory is resized.\r\n\r\nSigned-off-by: DevineLiu <hrliu@alauda.io>\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-05-14T15:45:03Z",
        "closed_at": "2022-05-16T09:08:19Z",
        "merged_at": "2022-05-16T09:08:19Z",
        "body": "We observed from our replication testing that when the master becomes unresponsive, or the replication connection is broken during PSYNC so the replica doesn't get a response from the master, it was not able to recognize that condition as a failure and instead moved into the full-sync code path. This fix makes the replica fail and retry the PSYNC with the master in such scenarios. ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2022-05-14T14:55:29Z",
        "closed_at": "2022-05-15T11:07:45Z",
        "merged_at": "2022-05-15T11:07:45Z",
        "body": "I noticed that when checking for whether script is running or not, sometimes it is using `scriptIsRunning()` and other times it is using `server.in_script`. We should use the `scriptIsRunning()` method consistently throughout the code base.\r\nRemove server.in_script sine it's no longer used.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-05-13T13:32:38Z",
        "closed_at": "2022-05-13T16:47:59Z",
        "merged_at": "2022-05-13T16:47:59Z",
        "body": "Alias was mistakenly forgotten when the sub commands introduced as json files.\r\n\r\nThis resolved issue https://github.com/redis/redis/issues/10722 for me. When I rebuilt redis with this change and deployed it like the following I no longer got the error of command not found.\r\n\r\n\r\n```bash\r\n$ redis-server sentinel.conf --sentinel\r\n```\r\n```bash\r\n127.0.0.1:26379> sentinel slaves mymaster\r\n(empty array)\r\n```",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-13T04:31:29Z",
        "closed_at": "2022-05-15T02:38:27Z",
        "merged_at": "2022-05-15T02:38:27Z",
        "body": "In `freeClientArgv`  `c->argv ` has been freed, so it's unnecessary to free it again(a trivial code clean).",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 797,
        "deletions": 732,
        "changed_files": 4,
        "created_at": "2022-05-12T15:25:40Z",
        "closed_at": "2023-05-18T13:32:56Z",
        "merged_at": null,
        "body": "This PR do 2 things:\r\n\r\n1. In the cluster mode, add cluster detail for info command as below, so client could get same information as run command: cluster info\r\n\r\ncluster_enabled:1\r\ncluster_state:fail\r\ncluster_slots_assigned:0\r\ncluster_slots_ok:0\r\ncluster_slots_pfail:0\r\ncluster_slots_fail:0\r\ncluster_known_nodes:1\r\ncluster_size:0\r\ncluster_current_epoch:0\r\ncluster_my_epoch:0\r\ncluster_stats_messages_sent:0\r\ncluster_stats_messages_received:0\r\ntotal_cluster_links_buffer_limit_exceeded:0\r\n\r\n2. Refactor genRedisInfoString function in server.c: split it into multiply small functions",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 16,
        "changed_files": 2,
        "created_at": "2022-05-12T13:33:59Z",
        "closed_at": "2022-05-12T17:10:38Z",
        "merged_at": "2022-05-12T17:10:38Z",
        "body": "The purpose of the test is to kill the child while it is running.\r\nFrom the last two lines we can see the child exits before being killed.\r\n```\r\n- Module fork started pid: 56998\r\n* <fork> fork child started\r\n- Killing running module fork child: 56998\r\n* <fork> fork child exiting\r\nsignal-handler (1652267501) Received SIGUSR1 in child, exiting now.\r\n```\r\n\r\nIn this commit, we pass an argument to `fork.create` indicating how\r\nlong it should sleep. For the fork kill test, we use a longer time to\r\navoid the child exiting before being killed.\r\n\r\nOther changes:\r\nuse wait_for_condition instead of hardcoded `after 250`.\r\nUnify the test for failing fork with the one for killing it (save time) ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 67,
        "deletions": 66,
        "changed_files": 11,
        "created_at": "2022-05-12T06:16:15Z",
        "closed_at": "2023-02-19T14:33:20Z",
        "merged_at": "2023-02-19T14:33:20Z",
        "body": "We have cases where we print information (might be important but by no means an error indicator) with the LL_WARNING level. Maybe we should demote these to LL_NOTICE:\r\n- oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n- User requested shutdown...\r\n\r\nThis is also true for cases that we encounter a rare but normal situation. Maybe here we should too demote to LL_NOTICE. Examples:\r\n- AOF was enabled but there is already another background operation. An AOF background was scheduled to start when possible.\r\n- Connection with master lost.\r\n\r\n\r\nbase on yoav-steinberg's https://github.com/redis/redis/pull/10650#issuecomment-1112280554 and yossigo's https://github.com/redis/redis/pull/10650#pullrequestreview-967677676\r\n\r\n",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2022-05-11T13:05:24Z",
        "closed_at": "2022-05-12T14:06:27Z",
        "merged_at": "2022-05-12T14:06:26Z",
        "body": "This fixes a possible regression in Redis 7.0.0, in which doing CONFIG SET on a TLS config would not reload the configuration in case the new config is the same file as before.\r\n\r\nA volatile configuration is a configuration value which is a reference to the configuration data and not the configuration data itself. In such a case Redis doesn't know if the config data changed under the hood and can't assume a change happens only when the config value changes. Therefore it needs to be applied even when setting a config value to the same value as it was before.\r\n\r\nSee discussion here: https://github.com/redis/redis/pull/9748#issuecomment-1120481244",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-10T18:42:31Z",
        "closed_at": "2022-05-11T09:06:33Z",
        "merged_at": "2022-05-11T09:06:33Z",
        "body": "Before this commit, all source files including those that are not going\r\nto be compiled were used. Some of these files are platform specific and\r\nwon't even pre-process on another platform. With GCC/Clang, that's not\r\nan issue and they'll simply ignore them, but ICC aborts in this case.\r\n\r\nThis commit only attempts to generate Makefile.dep from the actual set\r\nof C source files that will be compiled.\r\n\r\nResolves #10679 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 27,
        "changed_files": 1,
        "created_at": "2022-05-09T15:57:09Z",
        "closed_at": "2022-12-06T10:22:36Z",
        "merged_at": "2022-12-06T10:22:36Z",
        "body": "On v6.2.7 a new mechanism was added to Lua scripts that allows filtering the globals of the Lua interpreter. This mechanism was added on the following commit: https://github.com/redis/redis/commit/11b602fbf8f9cdf8fc741c24625ab6287ab998a9\r\n\r\nOne of the globals that was filtered out was `__redis__compare_helper`. This global was missed and was not added to the allow list or to the deny list. This is why we get the following warning when Redis starts: \r\n```\r\nA key '__redis__compare_helper' was added to Lua globals which is not on the globals allow list nor listed on the deny list.\r\n```\r\n\r\nAfter investigating the git blame log, the conclusion is that `__redis__compare_helper` is no longer needed, the PR deletes this function, and fixes the warning.\r\n\r\nDetailed Explanation:\r\n\r\n`__redis__compare_helper` was added on this commit: https://github.com/redis/redis/commit/2c861050c1\r\nIts purpose is to sort the replies of `SORT` command when script replication is enable and keep the replies deterministic and avoid primary and replica synchronization issues. On `SORT` command, there was a need for special compare function that are able to compare boolean values.\r\n\r\nThe need to sort the `SORT` command reply was removed on this commit: https://github.com/redis/redis/commit/36741b2c818a95e8ef167818271614ee6b1bc414\r\nThe sorting was moved to be part of the `SORT` command and there was not longer a need to sort it on the Lua interpreter. The commit made `__redis__compare_helper` a dead code but did not deleted it.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 11,
        "changed_files": 4,
        "created_at": "2022-05-09T13:35:34Z",
        "closed_at": "2022-05-22T13:02:59Z",
        "merged_at": "2022-05-22T13:02:59Z",
        "body": "Scripts that have the `no-writes` flag, cannot execute write commands, and since all `deny-oom` commands are write commands, we now act as if the `allow-oom` flag is implicitly set for scripts that set the `no-writes` flag.\r\nthis also implicitly means that the EVAL*_RO and FCALL_RO commands can never fails with OOM error.\r\n\r\nNote about a bug that's no longer relevant:\r\nThere was an issue with EVAL*_RO using shebang not being blocked correctly in OOM state:\r\nWhen an EVAL script declares a shebang, it was by default not allowed to run in OOM state.\r\nbut this depends on a flag that is updated before the command is executed, which was not updated in case of the `_RO` variants.\r\nthe result is that if the previous cached state was outdated (either true or false), the script will either unjustly fail with OOM, or unjustly allowed to run despite the OOM state.\r\nIt doesn't affect scripts without a shebang since these depend on the actual commands they run, and since these are only read commands, they don't care for that cached oom state flag.\r\nit did affect scripts with shebang and no allow-oom flag, bug after the change in this PR, scripts that are run with eval_ro would implicitly have that flag so again the cached state doesn't matter.\r\n\r\n### todo: \r\n- [x] update the documentation, and also add a note in the original [PR](https://github.com/redis/redis/pull/10066) which may be referred to by the release notes. Doc PR: Doc PR: https://github.com/redis/redis-doc/pull/1954\r\n\r\np.s. this isn't a breaking change since all it does is allow scripts to run when they should / could rather than blocking them.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-09T12:23:59Z",
        "closed_at": "2022-05-09T15:45:52Z",
        "merged_at": "2022-05-09T15:45:51Z",
        "body": "Changed `cursor`'s type from `int` to `unsigned long`. \r\n\r\n`cursor` is the return value of `dictScan()`:\r\n\r\nhttps://github.com/redis/redis/blob/6b44e4ea92edd0f06971b5b7354769255fde66a8/src/module.c#L9914\r\n\r\n`dictScan()` returns `unsigned long` but cursor was `int`. Slicing/UB can occur if `dictScan()` returns a value larger than 2^31.  ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-09T12:03:10Z",
        "closed_at": "2022-06-01T07:50:02Z",
        "merged_at": "2022-06-01T07:50:01Z",
        "body": "Fixes #10648 .\r\nFollowing @oranagra 's recommendation to move the client flags to a more cache friendly position within the client struct we indeed regain the lost 2% of CPU cycles since v6.2 ( from 630532.57 to 647449.80 ops/sec ).  \r\n\r\n## to reproduce:\r\n```\r\n# spin a standalone redis \r\ntaskset -c 0 `pwd`/src/redis-server --logfile redis-opt.log --save \"\" --daemonize yes\r\n\r\n# populate with data\r\nredis-cli \"RPUSH\" \"list:10\" \"lysbgqqfqw\" \"mtccjerdon\" \"jekkafodvk\" \"nmgxcctxpn\" \"vyqqkuszzh\" \"pytrnqdhvs\" \"oguwnmniig\" \"gekntrykfh\" \"nhfnbxqgol\" \"cgoeihlnei\"\r\n\r\n# benchmark \r\ntaskset -c 1-5 memtier_benchmark --command=\"LRANGE list:10 0 -1\"  --hide-histogram --test-time 60 --pipeline 10 -x 3\r\n```\r\n\r\nunstable 2bcd890d8aa645cab0d8fd0ed765c52a997de4f5 (merge-base )\r\n```\r\nBEST RUN RESULTS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nLranges    630593.68         3.16045         3.40700         4.31900         4.60700    134863.30 \r\nTotals     630593.68         3.16045         3.40700         4.31900         4.60700    134863.30 \r\n\r\n\r\nWORST RUN RESULTS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nLranges    630444.65         3.16119         3.40700         4.31900         4.60700    134831.42 \r\nTotals     630444.65         3.16119         3.40700         4.31900         4.60700    134831.42 \r\n\r\n\r\nAGGREGATED AVERAGE RESULTS (3 runs)\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nLranges    630532.57         3.16075         3.40700         4.31900         4.60700    134850.23 \r\nTotals     630532.57         3.16075         3.40700         4.31900         4.60700    134850.23 \r\n\r\n```\r\n\r\nthis PR\r\n```\r\nBEST RUN RESULTS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nLranges    647852.82         3.07598         3.31100         4.25500         4.51100    138554.46 \r\nTotals     647852.82         3.07598         3.31100         4.25500         4.51100    138554.46 \r\n\r\n\r\nWORST RUN RESULTS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nLranges    646919.60         3.08041         3.31100         4.25500         4.54300    138354.88 \r\nTotals     646919.60         3.08041         3.31100         4.25500         4.54300    138354.88 \r\n\r\n\r\nAGGREGATED AVERAGE RESULTS (3 runs)\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nLranges    647449.80         3.07789         3.31100         4.25500         4.54300    138468.27 \r\nTotals     647449.80         3.07789         3.31100         4.25500         4.54300    138468.27 \r\n\r\n```\r\n\r\n",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2022-05-09T11:43:29Z",
        "closed_at": "2022-07-20T23:59:28Z",
        "merged_at": "2022-07-20T23:59:28Z",
        "body": "As discussed in #9351, @bjosv mentioned that the cluster node IP may be updated to `?` when `getpeername()` fails due to some reasons like the fd is invalid somehow, and the log looks like:\r\n```\r\n15:S 27 Apr 2022 00:38:49.999 # Address updated for node f55db6b9314afec8816cbb258cf8a8358ad1718c, now ?:6380\r\n15:S 27 Apr 2022 00:38:49.999 # Address updated for node c16a99900ec94d8bfd4dc4e1b0c76340f7e853a6, now ?:6380\r\n15:S 27 Apr 2022 00:38:49.999 # Address updated for node 530b194579251e73473b6c9f94b2ce79ab1ad7eb, now ?:6380\r\n```\r\nTo solve this issue, the patch primarily do 2 things:\r\n1. forward the error code outside when the try to get peer name fails, and skip the current IP update in `nodeUpdateAddressIfNeeded`. It won't affect the normal IP update process as it will be retried the next round of PING.\r\n2. log the failure info of `getpeername()`,  for further debug and so on\r\n\r\n```\r\nrelease notes:\r\nFix a bug where a cluster enabled replica node may permanently set its master's hostname to '?'.\r\n```",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 114,
        "deletions": 14,
        "changed_files": 5,
        "created_at": "2022-05-08T16:55:14Z",
        "closed_at": "2022-05-11T08:21:17Z",
        "merged_at": "2022-05-11T08:21:17Z",
        "body": "## FLUSHALL\r\nWe used to restore the dirty counter after `rdbSave` zeroed it if we enable save.\r\nOtherwise FLUSHALL will not be replicated nor put into the AOF.\r\n\r\nAnd then we do increment it again below.\r\nWithout that extra dirty++, when db was already empty, FLUSHALL\r\nwill not be replicated nor put into the AOF.\r\n\r\nWe now gonna replace all that dirty counter magic with a call\r\nto forceCommandPropagation (REPL and AOF), instead of all the\r\nmessing around with the dirty counter.\r\nAdded tests to cover three part (dirty counter, REPL, AOF).\r\n\r\nOne benefit other than cleaner code is that the `rdb_changes_since_last_save` is correct in this case.\r\n\r\n## FLUSHDB\r\nFLUSHDB was not replicated nor put into the AOF when db was already empty.\r\nUnlike DEL on a non-existing key, FLUSHDB always does something, and that's to call the module hook. \r\nSo basically FLUSHDB is never a NOP, and thus it should always be propagated.\r\nNot doing that, could mean that if a module does something in that hook, and wants to\r\navoid issues of that hook being missing on the replica if the db is empty, it'll need to do complicated things.\r\n\r\nSo now FLUSHDB add call forceCommandPropagation, we will always propagate FLUSHDB.\r\nAlways propagating FLUSHDB seems like a safe approach that shouldn't have any drawbacks (other than looking odd)\r\n\r\nThis was mentioned in #8972\r\n\r\n## Test section:\r\nWe actually found it while solving a race condition in the BGSAVE test (other.tcl).\r\nIt was found in extra_ci Daily Arm64 (test-libc-malloc).\r\n```\r\n[exception]: Executing test client: ERR Background save already in progress.\r\nERR Background save already in progress\r\n```\r\n\r\nIt look like `r flushdb` trigger (schedule) a bgsave right after `waitForBgsave r` and before `r save`.\r\nChanging flushdb to flushall, FLUSHALL will do a foreground save and then set the dirty counter to 0.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2022-05-08T13:25:05Z",
        "closed_at": "2022-05-09T10:37:49Z",
        "merged_at": "2022-05-09T10:37:49Z",
        "body": "Unintentional change in #9644 (since RC1) meant that an empty `--save \"\"` config\r\nfrom command line, wouldn't have clear any setting from the config file\r\n\r\nAdded tests to cover that, and improved test infra to take additional\r\ncommand line args for redis-server",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-08T10:06:31Z",
        "closed_at": "2022-05-08T12:05:25Z",
        "merged_at": "2022-05-08T12:05:24Z",
        "body": "this seems to have been an oversight.  syncing the flags so that NOTIFY_NEW is available to modules.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-05-07T15:18:52Z",
        "closed_at": "2022-05-08T12:12:17Z",
        "merged_at": "2022-05-08T12:12:17Z",
        "body": "fixing already defined constants build warning while at it.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 41,
        "changed_files": 3,
        "created_at": "2022-05-06T02:51:10Z",
        "closed_at": "2022-05-08T06:54:28Z",
        "merged_at": null,
        "body": "Rename connection.c to socket.c, and move several functions.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-05T16:33:38Z",
        "closed_at": "2022-05-10T11:22:01Z",
        "merged_at": "2022-05-10T11:22:01Z",
        "body": "It used to returns slots as strings, like:\r\n```\r\nredis> cluster shards\r\n1) 1) \"slots\"\r\n   2) 1) \"10923\"\r\n      2) \"16383\"\r\n```\r\n\r\nCLUSTER SHARDS docs and the top comment of #10293 says that it returns integers.\r\nNote other commands like CLUSTER SLOTS, it returns slots as integers.\r\nUse addReplyLongLong instead of addReplyBulkLongLong, now it returns slots as integers:\r\n```\r\nredis> cluster shards\r\n1) 1) \"slots\"\r\n   2) 1) (integer) 10923\r\n      2) (integer) 16383\r\n```\r\n\r\nThis is a small breaking change, introduced in 7.0.0 (7.0 RC3, #10293)\r\n\r\nFixes #10680",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 24,
        "changed_files": 4,
        "created_at": "2022-05-05T15:15:53Z",
        "closed_at": "2022-05-10T11:56:12Z",
        "merged_at": "2022-05-10T11:56:12Z",
        "body": "Fix #10552\r\n\r\nWe no longer piggyback getkeys_proc to hold the RedisModuleCommand struct, when exists\r\n\r\nOthers:\r\nUse `doesCommandHaveKeys` in `RM_GetCommandKeysWithFlags` and `getKeysSubcommandImpl`. It causes a very minor behavioral change in commands that don't have actual keys, but have a spec with `CMD_KEY_NOT_KEY`.\r\nFor example, before this command `COMMAND GETKEYS SPUBLISH` would return `Invalid arguments specified for command` but not it returns `The command has no key arguments`",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-05-04T17:42:58Z",
        "closed_at": "2022-05-11T09:06:33Z",
        "merged_at": null,
        "body": " I faced with build issue during building Redis with ICC:\r\n```\r\n$ icc --version\r\nicc (ICC) 2021.5.0 20211109\r\nCopyright (C) 1985-2021 Intel Corporation.  All rights reserved.\r\n$ CC=icc CXX=icpc make\r\ncd src && make all\r\nmake[1]: Entering directory '/home/mmarkova/Work/redis_icc/src'\r\n    CC Makefile.dep\r\nMakefile.dep:11: *** target pattern contains no '%'.  Stop.\r\nmake[1]: Leaving directory '/home/mmarkova/Work/redis_icc/src'\r\nmake: *** [Makefile:6: all] Error 2\r\n```\r\nFail reason which I've found - files \"port.h\" and \"sys/event.h\" are absent on my system. I am using Ubuntu 20.4. \r\nSo, to fix this particular issue is enough to add check for used compiler before these files including. \r\n\r\nHowever, issue and possible fix can be wider. I've found that w/o these 2 \"includes\" GCC and Clang also build Redis successfully. Basically looks like these includes are ignored by GCC and Clang too on Ubuntu, but build process is not fail due to difference in processing. \r\nAs alternative way I see:\r\n1) Excluding these files on Ubuntu (if they are required on other platforms)\r\n2) Excluding there files at all (if they are not really needed)\r\n\r\nCould you please revise necessity of these 2 includes in for redis build?\r\n#include <port.h>\r\n#include <sys/event.h>\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-05-02T16:56:57Z",
        "closed_at": "2022-05-10T11:55:09Z",
        "merged_at": "2022-05-10T11:55:09Z",
        "body": "I suggest to use \"[fpclassify](https://en.cppreference.com/w/cpp/numeric/math/fpclassify)\" for float comparison with zero, because of expression \"value == 0\" with value very close to zero can be considered as true with some performance compiler optimizations.\r\n\r\nNote: this code was introduced by 9d520a7f to accept zset scores that get ERANGE in conversion due to precision loss near 0.\r\nBut with Intel compilers, ICC and ICX, where optimizations for 0 check are more aggressive, \"==0\" is true for mentioned functions, however should not be. Behavior is seen starting from O2.\r\nThis leads to a failure in the ZSCAN test in scan.tcl",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-05-02T16:03:39Z",
        "closed_at": "2022-05-03T07:34:18Z",
        "merged_at": "2022-05-03T07:34:18Z",
        "body": null,
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-05-02T03:41:53Z",
        "closed_at": "2022-05-02T09:43:28Z",
        "merged_at": null,
        "body": "It would be nice to use ZIPLIST_END_SIZE to represent the 1.\r\nThis is just a small cleanup.\r\n```\r\n    #define ZIPLIST_END_SIZE        (sizeof(uint8_t))\r\n    #define ZIPLIST_ENTRY_END(zl)   ((zl)+intrev32ifbe(ZIPLIST_BYTES(zl))-1)\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-05-02T02:44:02Z",
        "closed_at": "2022-05-09T12:04:39Z",
        "merged_at": "2022-05-09T12:04:39Z",
        "body": "fix some typo in \"t_zset.c\".\r\n1. `zzlisinlexrange` the function name mentioned in the comment is misspelled.\r\n2. fix typo in function name`zarndmemberReplyWithListpack` -> `zrandmemberReplyWithListpack`",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-05-01T15:12:41Z",
        "closed_at": "2022-05-02T06:37:15Z",
        "merged_at": "2022-05-02T06:37:15Z",
        "body": "Till now, on MacOS we only used to enable SO_KEEPALIVE,\r\nbut we didn't set the interval which is configurable via the `tcp-keepalive` config.\r\nThis adds support for that on MacOS, to match what we already do on Linux.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 17,
        "changed_files": 16,
        "created_at": "2022-04-30T03:26:40Z",
        "closed_at": "2022-05-28T03:24:04Z",
        "merged_at": null,
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-04-29T20:03:30Z",
        "closed_at": "2022-05-02T13:18:11Z",
        "merged_at": "2022-05-02T13:18:11Z",
        "body": "When user uses the same input key for SDIFF as the first one, the result must be empty, so we don't need to process the elements to test.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-04-29T18:25:02Z",
        "closed_at": "2022-05-01T09:09:44Z",
        "merged_at": "2022-05-01T09:09:44Z",
        "body": "`the redis object pointer was populated.` -> `the sds pointer was populated.`\r\nWe don't populate the redis object pointer in this function.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 16,
        "changed_files": 4,
        "created_at": "2022-04-28T08:49:43Z",
        "closed_at": "2022-05-09T10:36:53Z",
        "merged_at": "2022-05-09T10:36:53Z",
        "body": "If we want to support bits that can be overlapping, we need to make sure\r\nthat:\r\n1. we don't use the same bit for two return values.\r\n2. values should be sorted so that prefer ones (matching more\r\n   bits) come first.\r\n\r\nFixes for an API added in #10643",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 88,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2022-04-28T07:16:55Z",
        "closed_at": "2022-05-11T08:33:35Z",
        "merged_at": "2022-05-11T08:33:35Z",
        "body": "## Take one bulk string with spaces for MULTI_ARG configs parsing\r\nCurrently redis-server looks for arguments that start with `--`,\r\nand anything in between them is considered arguments for the config.\r\nlike: `src/redis-server --shutdown-on-sigint nosave force now --port 6380`\r\n\r\nMULTI_ARG configs behave differently for CONFIG command, vs the command\r\nline argument for redis-server.\r\ni.e. CONFIG command takes one bulk string with spaces in it, while the\r\ncommand line takes an argv array with multiple values.\r\n\r\nIn this PR, in config.c, if `argc > 1` we can take them as is,\r\nand if the config is a `MULTI_ARG` and `argc == 1`, we will split it by spaces.\r\n\r\nSo both of these will be the same:\r\n```\r\nredis-server --shutdown-on-sigint nosave force now --shutdown-on-sigterm nosave force\r\nredis-server --shutdown-on-sigint nosave \"force now\" --shutdown-on-sigterm nosave force\r\nredis-server --shutdown-on-sigint nosave \"force now\" --shutdown-on-sigterm \"nosave force\"\r\n```\r\n\r\n## Allow options value to use the `--` prefix\r\nCurrently it decides to switch to the next config, as soon as it sees `--`, \r\neven if there was not a single value provided yet to the last config,\r\nthis makes it impossible to define a config value that has `--` prefix in it.\r\n\r\nFor instance, if we want to set the logfile to `--my--log--file`,\r\nlike `redis-server --logfile --my--log--file --loglevel verbose`,\r\ncurrent code will handle that incorrectly.\r\n\r\nIn this PR, now we allow a config value that has `--` prefix in it.\r\n**But note that** something like `redis-server --some-config --config-value1 --config-value2 --loglevel debug`\r\nwould not work, because if you want to pass a value to a config starting with `--`, it can only be a single value.\r\nlike: `redis-server --some-config \"--config-value1 --config-value2\" --loglevel debug`\r\n\r\nAn example (using `--` prefix config value):\r\n```\r\nredis-server --logfile --my--log--file --loglevel verbose\r\nredis-cli config get logfile loglevel\r\n1) \"loglevel\"\r\n2) \"verbose\"\r\n3) \"logfile\"\r\n4) \"--my--log--file\"\r\n```\r\n\r\nThis is mentioned in https://github.com/redis/redis/pull/10594#issuecomment-1109638304\r\n\r\n### Potentially breaking change\r\n`redis-server --save --loglevel verbose` used to work the same as `redis-server --save \"\" --loglevel verbose`\r\nnow, it'll error!\r\n\r\nNote that this breaking change was undo in #10866 (7.0.3)",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2022-04-27T21:12:55Z",
        "closed_at": "2022-04-28T05:13:05Z",
        "merged_at": "2022-04-28T05:13:05Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-04-27T17:20:58Z",
        "closed_at": "2022-04-28T05:16:21Z",
        "merged_at": "2022-04-28T05:16:21Z",
        "body": "needed for https://github.com/redis/redis-doc/blob/master/docs/reference/modules/modules-api-ref.md",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-04-27T16:21:29Z",
        "closed_at": "2022-04-28T01:36:45Z",
        "merged_at": null,
        "body": "`$PROJECT/.cache/clangd/index` is the location for project index data from [clangd](https://clangd.llvm.org/).\r\n\r\nIt will generate a bunch of stuff like this.\r\n<img width=\"387\" alt=\"image\" src=\"https://user-images.githubusercontent.com/55286565/165572472-12e4ee8c-3732-4033-b9f4-fa37599985a6.png\">\r\n\r\n\r\nSigned-off-by: cndoit18 <cndoit18@outlook.com>",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-04-27T15:45:10Z",
        "closed_at": "2022-04-28T05:36:40Z",
        "merged_at": "2022-04-28T05:36:40Z",
        "body": "add a comment to `container` in `quicklist.h`.\r\nBecause `PLAIN` and `PACKED` are not as easy to understand as `NONE` and `LISTPACK` and we don't have a detailed comment on it.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1633,
        "deletions": 298,
        "changed_files": 64,
        "created_at": "2022-04-27T10:24:16Z",
        "closed_at": "2022-04-27T13:31:54Z",
        "merged_at": "2022-04-27T13:31:54Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-24736) An attacker attempting to load a specially crafted Lua script\r\n  can cause NULL pointer dereference which will result with a crash of the\r\n  redis-server process. This issue affects all versions of Redis.\r\n  [reported by Aviv Yahav].\r\n* (CVE-2022-24735) By exploiting weaknesses in the Lua script execution\r\n  environment, an attacker with access to Redis can inject Lua code that will\r\n  execute with the (potentially higher) privileges of another Redis user.\r\n  [reported by Aviv Yahav].\r\n\r\n\r\nPotentially Breaking Fixes\r\n==========================\r\n\r\n* LPOP/RPOP with count against non-existing list return null array (#10095)\r\n* LPOP/RPOP used to produce wrong replies when count is 0 (#9692)\r\n\r\n\r\nPerformance and resource utilization improvements\r\n=================================================\r\n\r\n* Speed optimization in command execution pipeline (#10502)\r\n* Fix regression in Z[REV]RANGE commands (by-rank) introduced in Redis 6.2 (#10337)\r\n\r\n\r\nPlatform / toolchain support related improvements\r\n=================================================\r\n\r\n* Fix RSS metrics on NetBSD and OpenBSD (#10116, #10149)\r\n* Fix OpenSSL 3.0.x related issues (#10291)\r\n\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Lua: Add checks for min-slave-* configs when evaluating Lua scripts (#10160)\r\n* Lua: fix crash on a script call with many arguments, a regression in v6.2.6 (#9809)\r\n* Tracking: Make invalidation messages always after command's reply (#9422)\r\n* Fix excessive stream trimming due to an overflow (#10068)\r\n* Add missed error counting for INFO errorstats (#9646)\r\n* Fix geo search bounding box check causing missing results (#10018)\r\n* Improve EXPIRE TTL overflow detection (#9839)\r\n* Modules: Fix thread safety violation when a module thread adds an error reply, broken in 6.2 (#10278)\r\n* Modules: Fix missing and duplicate error stats (#10278)\r\n* Module APIs: release clients blocked on module commands in cluster resharding\r\n  and down state (#9483)\r\n* Sentinel: Fix memory leak with TLS (#9753)\r\n* Sentinel: Fix issues with hostname support (#10146)\r\n* Sentinel: Fix election failures on certain container environments (#10197)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3062,
        "deletions": 1376,
        "changed_files": 124,
        "created_at": "2022-04-27T09:57:30Z",
        "closed_at": "2022-04-27T13:32:18Z",
        "merged_at": "2022-04-27T13:32:17Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2022-24736) An attacker attempting to load a specially crafted Lua script\r\n  can cause NULL pointer dereference which will result with a crash of the\r\n  redis-server process. This issue affects all versions of Redis.\r\n  [reported by Aviv Yahav].\r\n* (CVE-2022-24735) By exploiting weaknesses in the Lua script execution\r\n  environment, an attacker with access to Redis can inject Lua code that will\r\n  execute with the (potentially higher) privileges of another Redis user.\r\n  [reported by Aviv Yahav].\r\n\r\n\r\nNew Features\r\n============\r\n\r\n* Keyspace event for new keys (#10512)\r\n\r\n\r\nCommand replies that have been extended\r\n---------------------------------------\r\n\r\n* COMMAND DOCS shows deprecated_since field in command args (#10545)\r\n* COMMAND DOCS shows module name where applicable (#10544)\r\n\r\n\r\nPotentially Breaking Changes\r\n============================\r\n\r\n* Replicas panic when they fail writing persistence (#10504)\r\n* Prevent cross slot operations in functions and scripts with shebang (#10615)\r\n* Rephrased some error responses about invalid commands or args (#10612)\r\n* Lua scripts do not have access to the print() function (#10651)\r\n\r\n\r\nPerformance and resource utilization improvements\r\n=================================================\r\n\r\n* Speed optimization in streams (#10574)\r\n* Speed optimization in command execution pipeline (#10502)\r\n* Speed optimization in listpack encoded sorted (#10486)\r\n* Speed optimization in latency tracking at INFO (relevant for 7.0 RCs) (#10606)\r\n* Speed optimization when there are many replicas (relevant for 7.0 RCs) (#10588)\r\n\r\n\r\nNew configuration options\r\n=========================\r\n\r\n* Allow ignoring disk persistence errors on replicas (#10504)\r\n* Allow abort with panic when replica fails to execute a command sent by the master (#10504)\r\n* Allow configuring shutdown flags of SIGTERM and SIGINT (#10594)\r\n* Allow attaching an operating system-specific identifier to Redis sockets (#10349)\r\n\r\n\r\nModule API changes\r\n==================\r\n\r\n* Add argument specifying ACL reason for module log entry (#10559)\r\n  Breaking API compatibility with 7.0 RCs\r\n* Add the deprecated_since field in command args of COMMAND DOCS (#10545)\r\n  Breaking API/ABI compatibility with 7.0 RCs\r\n* Add module API flag for using enum configs as bit flags (#10643)\r\n* Add RM_PublishMessageShard (#10543)\r\n* Add RM_MallocSizeString, RM_MallocSizeDict (#10542)\r\n* Add RM_TryAlloc (#10541)\r\n\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Replica report disk persistence errors in PING (#10603)\r\n* Fixes around rejecting commands on replicas and AOF when they must be respected (#10603)\r\n* Durability fixes for appendfsync=always policy (#9678)\r\n\r\n\r\nFixes for issues in previous release candidates of Redis 7.0\r\n------------------------------------------------------------\r\n\r\n* Fix possible crash on CONFIG REWRITE (#10598)\r\n* Fix regression not aborting transaction on errors (#10612)\r\n* Fix auto-aof-rewrite-percentage based AOFRW trigger after restart (#10550)\r\n* Fix bugs when AOF enabled after startup, in case of failure before the first rewrite completes (#10616)\r\n* Fix RM_Yield module API bug processing future commands of the current client (#10573)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2022-04-27T02:22:48Z",
        "closed_at": "2022-04-27T05:07:52Z",
        "merged_at": "2022-04-27T05:07:52Z",
        "body": "Followup fix for #10616",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 7,
        "changed_files": 5,
        "created_at": "2022-04-26T15:22:38Z",
        "closed_at": "2022-05-13T14:55:50Z",
        "merged_at": "2022-05-13T14:55:50Z",
        "body": "Updated the comments for:\r\ninfo command\r\nlmpopCommand and blmpopCommand\r\nsinterGenericCommand \r\n\r\nFix the missing \"key\" words in the srandmemberCommand function\r\nFor LPOS command, when rank is 0, prompt user that rank could be positive number or negative number, and add a test for it\r\nThis closes PRs, https://github.com/redis/redis/pull/10617, https://github.com/redis/redis/pull/10637, https://github.com/redis/redis/pull/10638",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 7,
        "changed_files": 5,
        "created_at": "2022-04-26T11:50:26Z",
        "closed_at": "2022-04-26T17:29:20Z",
        "merged_at": "2022-04-26T17:29:20Z",
        "body": "Enables registration of an enum config that'll let the user pass multiple keywords that will be combined with `|` as flags into the integer config value.\r\n\r\n```\r\n    const char *enum_vals[] = {\"none\", \"one\", \"two\", \"three\"};\r\n    const int int_vals[] = {0, 1, 2, 4};\r\n\r\n    if (RedisModule_RegisterEnumConfig(ctx, \"flags\", 3, REDISMODULE_CONFIG_DEFAULT | REDISMODULE_CONFIG_BITFLAGS, enum_vals, int_vals, 4, getFlagsConfigCommand, setFlagsConfigCommand, NULL, NULL) == REDISMODULE_ERR) {\r\n        return REDISMODULE_ERR;\r\n    }\r\n```\r\ndoing:\r\n`config set moduleconfigs.flags \"two three\"` will result in 6 being passed to`setFlagsConfigCommand`.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-04-25T20:27:57Z",
        "closed_at": "2022-04-25T21:34:01Z",
        "merged_at": "2022-04-25T21:34:01Z",
        "body": "A change in #10612 introduced a regression.\r\nwhen replying with garbage bytes to the caller, we must make sure it\r\ndoesn't include any newlines.\r\n\r\nin the past it called rejectCommandFormat which did that trick.\r\nbut now it calls rejectCommandSds, which doesn't, so we need to make sure\r\nto sanitize the sds.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-04-25T18:33:38Z",
        "closed_at": "2022-04-26T15:46:16Z",
        "merged_at": null,
        "body": null,
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-04-25T18:28:46Z",
        "closed_at": "2022-04-26T15:47:25Z",
        "merged_at": null,
        "body": "This PR fix the following minor issues before Redis 7 release:\r\n\r\nSINTER and SMEMBERS call the same function sinterCommand and sinterGenericCommand, so add one more SMEMBERS in the \r\nsinterGenericCommand comment\r\n\r\nFix the missing \"key\" words in the srandmemberCommand function",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 495,
        "deletions": 208,
        "changed_files": 8,
        "created_at": "2022-04-25T15:24:40Z",
        "closed_at": "2022-05-22T14:10:31Z",
        "merged_at": "2022-05-22T14:10:31Z",
        "body": "This PR does 2 main things:\r\n1) Add warning for suspected slow system clocksource setting. This is Linux specific.\r\n2) Add a `--check-system` argument to redis which runs all system checks and prints a report.\r\n\r\n## System checks\r\nAdd a command line option `--check-system` which runs all known system checks and provides a report to stdout of which systems checks have failed with details on how to reconfigure the system for optimized redis performance.\r\nThe `--check-system` mode exists with an appropriate error code after running all the checks.\r\n\r\n## Slow clocksource details\r\nWe check the system's clocksource performance by running `clock_gettime()` in a loop and then checking how much time was spent in a system call (via `getrusage()`). If we spend more than 10% of the time in the kernel then we print a warning. I verified that using the slow clock sources: `acpi_pm` (~90% in the kernel on my laptop) and `xen` (~30% in the kernel on an ec2 `m4.large`) we get this warning.\r\n\r\nThe check runs 5 system ticks so we can detect time spent in kernel at 20% jumps (0%,20%,40%...). Anything more accurate will require the test to run longer. Typically 5 ticks are 50ms. This means running the test on startup will delay startup by 50ms. To avoid this we make sure the test is only executed in the `--check-system` mode.\r\n\r\nFor a quick startup check, we specifically warn if the we see the system is using the `xen` clocksource which we know has bad performance and isn't recommended (at least on ec2). In such a case the user should manually rung redis with `--check-system` to force the slower clocksource test described above.\r\n\r\n## Other changes in the PR\r\n\r\n* All the system checks are now implemented as functions in _syscheck.c_. They are implemented using a standard interface (see details in _syscheck.c_). To do this I moved the checking functions `linuxOvercommitMemoryValue()`, `THPIsEnabled()`, `linuxMadvFreeForkBugCheck()` out of _server.c_ and _latency.c_ and into the new _syscheck.c_. When moving these functions I made sure they don't depend on other functionality provided in _server.c_ and made them use a standard \"check functions\" interface. Specifically:\r\n  * I removed all logging out of `linuxMadvFreeForkBugCheck()`. In case there's some unexpected error during the check aborts as before, but without any logging. It returns an error code 0 meaning the check didn't not complete.\r\n  * All these functions now return 1 on success, -1 on failure, 0 in case the check itself cannot be completed.\r\n  * The `linuxMadvFreeForkBugCheck()` function now internally calls `exit()` and not `exitFromChild()` because the latter is only available in _server.c_ and I wanted to remove that dependency. This isn't an because we don't need to worry about the child process created by the test doing anything related to the rdb/aof files which is why `exitFromChild()` was created.\r\n\r\n* This also fixes parsing of other /proc/\\<pid\\>/stat fields to correctly handle spaces in the process name and be more robust in general. Not that before this fix the rss info in `INFO memory` was corrupt in case of spaces in the process name. To recreate just rename `redis-server` to `redis server`, start it, and run `INFO memory`.\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-04-25T15:16:27Z",
        "closed_at": "2022-05-04T08:40:08Z",
        "merged_at": "2022-05-04T08:40:08Z",
        "body": "Bumps [github/codeql-action](https://github.com/github/codeql-action) from 1 to 2.\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/github/codeql-action/blob/main/CHANGELOG.md\">github/codeql-action's changelog</a>.</em></p>\n<blockquote>\n<h2>2.1.8 - 08 Apr 2022</h2>\n<ul>\n<li>Update default CodeQL bundle version to 2.8.5. <a href=\"https://github-redirect.dependabot.com/github/codeql-action/pull/1014\">#1014</a></li>\n<li>Fix error where the init action would fail due to a GitHub API request that was taking too long to complete <a href=\"https://github-redirect.dependabot.com/github/codeql-action/pull/1025\">#1025</a></li>\n</ul>\n<h2>2.1.7 - 05 Apr 2022</h2>\n<ul>\n<li>A bug where additional queries specified in the workflow file would sometimes not be respected has been fixed. <a href=\"https://github-redirect.dependabot.com/github/codeql-action/pull/1018\">#1018</a></li>\n</ul>\n<h2>2.1.6 - 30 Mar 2022</h2>\n<ul>\n<li>[v2+ only] The CodeQL Action now runs on Node.js v16. <a href=\"https://github-redirect.dependabot.com/github/codeql-action/pull/1000\">#1000</a></li>\n<li>Update default CodeQL bundle version to 2.8.4. <a href=\"https://github-redirect.dependabot.com/github/codeql-action/pull/990\">#990</a></li>\n<li>Fix a bug where an invalid <code>commit_oid</code> was being sent to code scanning when a custom checkout path was being used. <a href=\"https://github-redirect.dependabot.com/github/codeql-action/pull/956\">#956</a></li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/github/codeql-action/commit/2c03704a6c1a830d08e4d9bec16d5e11341fdfbd\"><code>2c03704</code></a> Allow the version of the ML-powered pack to depend on the CLI version</li>\n<li><a href=\"https://github.com/github/codeql-action/commit/dd6b592e3e5e9cb8d577f77fcbac3e0a277834f4\"><code>dd6b592</code></a> Simplify ML-powered query status report definition</li>\n<li><a href=\"https://github.com/github/codeql-action/commit/a90d8bf7113ff4d559a93e924657f47182b7ff14\"><code>a90d8bf</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/github/codeql-action/issues/1011\">#1011</a> from github/henrymercer/ml-powered-queries-pr-check</li>\n<li><a href=\"https://github.com/github/codeql-action/commit/dc0338e4932696fa7e12853666bd55126f578ec7\"><code>dc0338e</code></a> Use latest major version of actions/upload-artifact</li>\n<li><a href=\"https://github.com/github/codeql-action/commit/57096fe795dd4d80156b5aca370361a411c788ac\"><code>57096fe</code></a> Add a PR check to validate that ML-powered queries are run correctly</li>\n<li><a href=\"https://github.com/github/codeql-action/commit/b0ddf36abe59aeef1e1161800244ed201a198092\"><code>b0ddf36</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/github/codeql-action/issues/1012\">#1012</a> from github/henrymercer/update-actions-major-versions</li>\n<li><a href=\"https://github.com/github/codeql-action/commit/1ea2f2d7f1d93eaf4eac2be602aac0c587fd74ec\"><code>1ea2f2d</code></a> Merge branch 'main' into henrymercer/update-actions-major-versions</li>\n<li><a href=\"https://github.com/github/codeql-action/commit/9dcc141f122e30f8d48b9927b17b081acd406b1d\"><code>9dcc141</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/github/codeql-action/issues/1010\">#1010</a> from github/henrymercer/stop-running-ml-powered-quer...</li>\n<li><a href=\"https://github.com/github/codeql-action/commit/ea751a9fae12fc5267ceb93f51622421afc5e87b\"><code>ea751a9</code></a> Update other Actions from v2 to v3</li>\n<li><a href=\"https://github.com/github/codeql-action/commit/a2949f47b3d667fc2d35d39f10089aa60cbd7071\"><code>a2949f4</code></a> Update actions/checkout from v2 to v3</li>\n<li>Additional commits viewable in <a href=\"https://github.com/github/codeql-action/compare/v1...v2\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=github/codeql-action&package-manager=github_actions&previous-version=1&new-version=2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2022-04-24T12:52:50Z",
        "closed_at": "2022-04-24T14:29:35Z",
        "merged_at": "2022-04-24T14:29:35Z",
        "body": "This avoids random memory spikes and enables --large-memory tests to run\r\non moderately sized systems.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2022-04-24T08:32:04Z",
        "closed_at": "2022-06-21T04:02:22Z",
        "merged_at": "2022-06-21T04:02:22Z",
        "body": "use a numerical comparison instead of the memcpy call, which makes little difference in small cluster scenarios, but has a performance improvement for large clusters.\r\n\r\nbackground\r\nMy production environment has a set of clusters with thousands of nodes, and I found through the perf tool that clusterNodeIsInGossipSection consumes 7% of progress\u2018s cpu resource in idle state\r\n\r\n\r\n![1653396440(1)](https://user-images.githubusercontent.com/15336980/170038589-27a45cd4-775d-44d8-862a-24c864bc84be.jpg)\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-04-23T07:59:02Z",
        "closed_at": "2022-04-24T07:21:04Z",
        "merged_at": "2022-04-24T07:21:04Z",
        "body": "Fix incorrect function name `harndfieldReplyWithListpack` -> `hrandfieldReplyWithListpack`",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-04-22T18:12:45Z",
        "closed_at": "2022-04-24T07:19:47Z",
        "merged_at": "2022-04-24T07:19:47Z",
        "body": "fix typo. `LCS[j+(blen+1)*j]` -> `LCS[j+(blen+1)*i]`",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2022-04-21T18:23:03Z",
        "closed_at": "2022-04-26T15:48:17Z",
        "merged_at": null,
        "body": "This PR fix the following minor errors before Redis 7 release:\r\n\r\nThe comments of function **lmpopCommand** and **blmpopCommand** are not consistent with document description, fix them\r\n\r\nFor LPOS command, when rank is 0, prompt user that rank could be positive number or negative number, and add a test for it",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 270,
        "deletions": 32,
        "changed_files": 4,
        "created_at": "2022-04-21T01:48:56Z",
        "closed_at": "2022-04-26T13:31:19Z",
        "merged_at": "2022-04-26T13:31:19Z",
        "body": "Changes:\r\n\r\n- When AOF is enabled **after** startup, the data accumulated during `AOF_WAIT_REWRITE` will only be stored in a temp INCR AOF file. Only after the first AOFRW is successful, we will add it to manifest file.\r\n  Before this fix, the manifest referred to the temp file which could cause a restart during that time to load it without it's base.\r\n- Add `aof_rewrites_consecutive_failures` info field for  aofrw limiting implementation.\r\n\r\nNow we can guarantee that these behaviors of MP-AOF are the same as before (past redis releases):\r\n- When AOF is enabled after startup, the data accumulated during `AOF_WAIT_REWRITE` will only be stored in a visible place. Only after the first AOFRW is successful, we will add it to manifest file.\r\n- When disable AOF, we did not delete the AOF file in the past so there's no need to change that behavior now (yet).\r\n- When toggling AOF off and then on (could be as part of a full-sync), a crash or restart before the first rewrite is completed, would result with the previous version being loaded (might not be right thing, but that's what we always had).\r\n\r\n<s>\r\nOne issues:\r\n- `aofRewriteLimited` will not work for AOFRW triggered by enabling AOF after startup (because we won't have many INCR AOFs in this scenario). I think we should no longer judge the AOFRW failures based on the current number of INCR AOFs, but directly based on `aof_lastbgrewrite_status`. But I feel this is best fixed in a separate PR. \r\n</s>\r\nMore discussion: https://github.com/redis/redis/pull/10582#discussion_r853772495",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 111,
        "deletions": 23,
        "changed_files": 10,
        "created_at": "2022-04-21T00:33:22Z",
        "closed_at": "2022-04-26T09:09:21Z",
        "merged_at": "2022-04-26T09:09:21Z",
        "body": "Adds the `allow-cross-slot-keys` flag to Eval scripts and Functions to allow scripts to access keys from multiple slots.\r\nThe default behavior is now that they are not allowed to do that (unlike before).\r\nThis is a breaking change for 7.0 release candidates (to be part of 7.0.0), but not for previous redis releases since EVAL without shebang isn't doing this check.\r\n\r\nNote that the check is done on both the keys declared by the EVAL / FCALL command arguments, and also the ones used by the script when making a `redis.call`.\r\n\r\nA note about the implementation, there seems to have been some confusion about allowing access to non local keys. I thought I missed something in our wider conversation, but Redis scripts do block access to non-local keys. So the issue was just about cross slots being accessed.\r\n\r\nto do:\r\n- [X] make a redis-doc PR: https://github.com/redis/redis-doc/pull/1893/files\r\n\r\nResolves, https://github.com/redis/redis/issues/10503.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2022-04-20T16:00:17Z",
        "closed_at": "2022-04-24T09:16:30Z",
        "merged_at": "2022-04-24T09:16:30Z",
        "body": "This test failed once in my daily CI (test-sanitizer-address (clang))\r\n```\r\n*** [err]: SLOWLOG - Some commands can redact sensitive fields in tests/unit/slowlog.tcl\r\nExpected 'migrate 127.0.0.1 25649 key 9 5000 AUTH2 (redacted) (redacted)' to match '* key 9 5000 AUTH (redacted)' (context: type eval line 12 cmd {assert_match {* key 9 5000 AUTH (redacted)} [lindex [lindex [r slowlog get] 1] 3]} proc ::test)\r\n```\r\n\r\nThe reason is that with slowlog-log-slower-than 10000,\r\nslowlog get will have a chance to exceed 10ms.\r\n\r\nChange slowlog-log-slower-than from 10000 to -1, distable it.\r\nAvoid repeated calls to `SLOWLOG GET`, assert to use the previous execution result.\r\n\r\nAlso handles a same potentially problematic test above.\r\nThis is actually the same timing issue as #10432.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2022-04-20T15:55:20Z",
        "closed_at": "2022-04-25T10:05:06Z",
        "merged_at": "2022-04-25T10:05:06Z",
        "body": "This case is interesting because it originates from cron,\r\nrather than from another command.\r\n\r\nThe idea came from looking at https://github.com/redis/redis/pull/9890 and https://github.com/redis/redis/pull/10573, and I was wondering if RM_Call would work properly when `server.current_client == NULL`",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 88,
        "deletions": 56,
        "changed_files": 11,
        "created_at": "2022-04-20T15:16:28Z",
        "closed_at": "2022-04-25T10:08:13Z",
        "merged_at": "2022-04-25T10:08:13Z",
        "body": "1. Disk error and slave count checks didn't flag the transactions or counted correctly in command stats (regression from #10372  , 7.0 RC3)\r\n2. RM_Call will reply the same way Redis does, in case of non-exisitng command or arity error\r\n3. RM_WrongArtiy will consider the full command name\r\n4. Use lowercase 'u' in \"unknonw subcommand\" (to align with \"unknown command\")\r\n\r\nFollowup work of #10127",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-04-20T13:49:50Z",
        "closed_at": "2022-04-24T13:12:01Z",
        "merged_at": "2022-04-24T13:12:00Z",
        "body": "If was first added in #9890 to solve the problem of\r\nCONFIG SET maxmemory causing eviction inside MULTI/EXEC,\r\nbut that problem is already fixed (CONFIG SET doesn't evict\r\ndirectly, it just schedules a later eviction)\r\n\r\nKeep that condition may hide bugs (i.e. performEvictions\r\nshould always expect to have an empty server.also_propagate)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 204,
        "deletions": 148,
        "changed_files": 10,
        "created_at": "2022-04-19T13:04:00Z",
        "closed_at": "2022-04-20T06:38:21Z",
        "merged_at": "2022-04-20T06:38:21Z",
        "body": "`hdr_value_at_percentile()` is part of the Hdr_Histogram library used when generating `latencystats` report. \r\n\r\nThere's a [pending optimization](https://github.com/HdrHistogram/HdrHistogram_c/pull/107) for this function which greatly affects the performance of `info latencystats`.\r\n\r\nThis PR:\r\n1. Upgrades the sources in _deps/hdr_histogram_ to the latest Hdr_Histogram version 0.11.5\r\n2. Applies the referenced optimization.\r\n3. Adds minor documentation about the hdr_histogram dependency which was missing under _deps/README.md_.\r\n\r\nbenchmark on my machine:\r\nrunning: `redis-benchmark -n 100000 info latencystats` on a clean build with no data.\r\n\r\n| benchmark | RPS |\r\n| ---- | ---- |\r\n| before upgrade to v0.11.05  | 7,681 |\r\n| before optimization | 12,474 |\r\n| after optimization | 52,606 |\r\n\r\n\r\n\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 23,
        "changed_files": 8,
        "created_at": "2022-04-19T09:55:26Z",
        "closed_at": "2022-04-20T08:11:21Z",
        "merged_at": "2022-04-20T08:11:21Z",
        "body": "This PR unifies all the places that test if the current client is the\r\nmaster client or AOF client, and uses a method to test that on all of\r\nthese.\r\n\r\nOther than some refactoring, these are the actual implications:\r\n- Replicas **don't** ignore disk error when processing commands not\r\n  coming from their master.\r\n  **This is important for PING to be used for health check of replicas**\r\n- SETRANGE, APPEND, SETBIT, BITFIELD don't do proto_max_bulk_len check for AOF\r\n- RM_Call in SCRIPT_MODE ignores disk error when coming from master /\r\n  AOF\r\n- RM_Call in cluster mode ignores slot check when processing AOF\r\n- Scripts ignore disk error when processing AOF\r\n- Scripts **don't** ignore disk error on a replica, if the command comes\r\n  from clients other than the master\r\n- SCRIPT KILL won't kill script coming from AOF\r\n- Scripts **don't** skip OOM check on replica if the command comes from\r\n  clients other than the master\r\n\r\nNote that Script, AOF, and module clients don't reach processCommand,\r\nwhich is why some of the changes don't actually have any implications.\r\n\r\nNote, reverting the change done to processCommand in 2f4240b9d9\r\nshould be dead code due to the above mentioned fact.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2022-04-19T08:40:19Z",
        "closed_at": "2022-04-20T06:54:55Z",
        "merged_at": "2022-04-20T06:54:55Z",
        "body": "We should stop RDB child in advance before flushing to reduce COW in diskless replication too.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-04-19T06:08:04Z",
        "closed_at": "2022-04-19T08:31:16Z",
        "merged_at": "2022-04-19T08:31:16Z",
        "body": "When `oom-score-adj-values` is out of range, `setConfigOOMScoreAdjValuesOption` should return 0, not -1, otherwise it will be considered as success.\r\n\r\nBefore this PR:\r\n```sh\r\n127.0.0.1:6379> config set oom-score-adj-values \"2001 2001 2001\"\r\nOK\r\n\r\n127.0.0.1:6379> config set oom-score-adj-values \"-2001 -2001 -2001\"\r\nOK\r\n```\r\n\r\nthis PR:\r\n```sh\r\n127.0.0.1:6379> config set oom-score-adj-values \"2001 2001 2001\"\r\n(error) ERR CONFIG SET failed (possibly related to argument 'oom-score-adj-values') - Invalid oom-score-adj-values, elements must be between -2000 and 2000.\r\n\r\n127.0.0.1:6379> config set oom-score-adj-values \"-2001 -2001 -2001\"\r\n(error) ERR CONFIG SET failed (possibly related to argument 'oom-score-adj-values') - Invalid oom-score-adj-values, elements must be between -2000 and 2000.\r\n```\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-04-18T20:18:42Z",
        "closed_at": "2022-04-19T03:04:11Z",
        "merged_at": "2022-04-19T03:04:11Z",
        "body": "Fixing sdsConfigRewrite() which wrongly frees config->data.sds.config.\r\nAffects config rewrite of masterauth and requirepass\r\n\r\nFollowing bda9d74da, at function `sdsConfigRewrite()`, if reached  as non-module\r\nconfiguration, then `*config->data.sds.config` is referenced (with `val`) and get\r\ndeleted at the end of the function as if it is temporary variable. Later on, accessing\r\nthis parameter can cause various issues, such as file configuration corruption, crash,\r\nand so on.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 17,
        "changed_files": 4,
        "created_at": "2022-04-18T19:00:20Z",
        "closed_at": "2022-04-19T11:57:29Z",
        "merged_at": "2022-04-19T11:57:29Z",
        "body": "some skip tags where missing on some tests....",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-04-18T02:49:04Z",
        "closed_at": "2022-04-18T05:34:22Z",
        "merged_at": "2022-04-18T05:34:22Z",
        "body": "There is a implicit conversion warning in clang:\r\n```\r\nutil.c:574:23: error: implicit conversion from 'long long' to 'double'\r\nchanges value from -4611686018427387903 to -4611686018427387904\r\n[-Werror,-Wimplicit-const-int-float-conversion]\r\n    if (d < -LLONG_MAX/2 || d > LLONG_MAX/2)\r\n```\r\n\r\nintroduced in #10486",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 111,
        "deletions": 34,
        "changed_files": 7,
        "created_at": "2022-04-17T22:18:44Z",
        "closed_at": "2022-04-26T11:34:04Z",
        "merged_at": "2022-04-26T11:34:04Z",
        "body": "The SHUTDOWN command has various flags to change it's default behavior,\r\nbut in some cases establishing a connection to redis is complicated and it's easier\r\nfor the management software to use signals. however, so far the signals could only\r\ntrigger the default shutdown behavior.\r\nHere we introduce the option to control shutdown arguments for SIGTERM and SIGINT.\r\n\r\nNew config options:\r\n`shutdown-on-sigint [nosave | save] [now] [force]` \r\n`shutdown-on-sigterm [nosave | save] [now] [force]`\r\n\r\nImplementation:\r\nSupport MULTI_ARG_CONFIG on createEnumConfig to support multiple enums to be applied as bit flags.\r\n\r\n### background\r\n\r\nAs discussed on #10525\r\n\r\nOn replicas using RDB, it can be acceptable to not save the latest data on shutdown, especially if we know it will never become a master.\r\nSometimes the periodic SAVE in background is enough and the blocking SAVE on shutdown undesired.\r\nAlso, if guaranteed durability is desired AOF could be used instead.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-04-15T09:39:04Z",
        "closed_at": "2022-04-17T06:41:47Z",
        "merged_at": "2022-04-17T06:41:46Z",
        "body": "From #9166, we call several times of prepareReplicasToWrite when propagating\r\none write command to replication stream (once per argument, same as we do for\r\nnormal clients), that is not necessary. Now we only call it one time per command\r\nat the begin of feeding replication stream.\r\n\r\nThis results in reducing CPU consumption and slightly better performance,\r\nspecifically when there are many replicas.\r\n\r\nResolves https://github.com/redis/redis/issues/10428",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 662,
        "deletions": 23,
        "changed_files": 18,
        "created_at": "2022-04-14T16:28:54Z",
        "closed_at": "2022-10-15T09:17:41Z",
        "merged_at": "2022-10-15T09:17:41Z",
        "body": "Fixes #8826 8826\r\n\r\nAs pointed out on #8826 all commands / use cases that heavily rely on double to a string representation conversion, (e.g. meaning take a double-precision floating-point number like 1.5 and return a string like \"1.5\" ), could benefit from a performance boost by swapping snprintf(buf,len,\"%.17g\",value) by the equivalent [fpconv_dtoa](https://github.com/night-shift/fpconv) or any other algorithm that ensures 100% coverage of conversion.\r\n\r\nThis is a well-studied topic and Projects like MongoDB. RedPanda, PyTorch leverage libraries ( fmtlib ) that use the optimized double to string conversion underneath.\r\n\r\n\r\nThe positive impact can be substantial. This PR uses the grisu2 approach ( grisu explained on https://www.cs.tufts.edu/~nr/cs257/archive/florian-loitsch/printf.pdf section 5 ). \r\n\r\n### test suite changes\r\n\r\nDespite being compatible, in some cases it produces a different result from printf, and some tests had to be adjusted.\r\none case is that `%.17g` (which means %e or %f which ever is shorter), chose to use `5000000000` instead of 5e+9, which sounds like a bug?\r\nIn other cases, we changed TCL to compare numbers instead of strings to ignore minor rounding issues (`expr 0.8 == 0.79999999999999999`) \r\n\r\n\r\n## Impact on Sorted Sets\r\n\r\n\r\n### READS \r\n\r\nIf we look at ZRANGE WITHSCORES command impact we saw 23% improvement on the achievable ops/sec on replies with 10 elements, 50% on replies with 100 elements and 68% on replies with 1000 elements. For more details check the section below.\r\n\r\n-----------------------------------\r\n\r\n## Testing positive impact on ZSET commands  (with double scores) :\r\n\r\n\r\nThis can be easily tested using https://github.com/redis/redis-benchmarks-specification ( install details on repo )\r\n\r\n```bash\r\nredis-benchmarks-spec-client-runner --db_server_host <IP> --test memtier_benchmark-1key-zset-100-elements-zrange-all-elements.yml,memtier_benchmark-1key-zset-10-elements-zrange-all-elements.yml,memtier_benchmark-1key-zset-1K-elements-zrange-all-elements.yml --flushall_on_every_test_start --flushall_on_every_test_end\r\n```\r\n\r\nIf you want to manually replicate the results, then follow the next steps:\r\n\r\n\r\nTo populate the data\r\n```\r\n# 10 elements zset\r\n\r\nredis-cli \"ZADD\" \"zset:10\" \"0.306275\" \"lysbgqqfqw\" \"0.486004\" \"mtccjerdon\" \"0.941626\" \"jekkafodvk\" \"0.602656\" \"nmgxcctxpn\" \"0.630771\" \"vyqqkuszzh\" \"0.406379\" \"pytrnqdhvs\" \"0.521814\" \"oguwnmniig\" \"0.182854\" \"gekntrykfh\" \"0.657658\" \"nhfnbxqgol\" \"0.218066\" \"cgoeihlnei\"\r\n\r\n# 100 elements zset\r\n\r\nredis-cli \"ZADD\" \"zset:100\" \"0.306275\" \"lysbgqqfqw\" \"0.486004\" \"mtccjerdon\" \"0.941626\" \"jekkafodvk\" \"0.602656\" \"nmgxcctxpn\" \"0.630771\" \"vyqqkuszzh\" \"0.406379\" \"pytrnqdhvs\" \"0.521814\" \"oguwnmniig\" \"0.182854\" \"gekntrykfh\" \"0.657658\" \"nhfnbxqgol\" \"0.218066\" \"cgoeihlnei\" \"0.366328\" \"kgqrovsxce\" \"0.520723\" \"imyizmhzjk\" \"0.426630\" \"urvgxwbiiz\" \"0.878001\" \"onzjrteqmu\" \"0.126097\" \"alqlzsvuuz\" \"0.562698\" \"lgrkndhekf\" \"0.357487\" \"tcjvjofxtd\" \"0.395563\" \"ouuybhtvyb\" \"0.745796\" \"owbbdezgxn\" \"0.165356\" \"ksqpdywgdd\" \"0.757214\" \"qrosceblyo\" \"0.346153\" \"agsdggdghc\" \"0.297342\" \"gfdqdrondm\" \"0.668749\" \"oapbvnisyq\" \"0.281552\" \"htvbwmfyic\" \"0.013874\" \"vyoomgwuzv\" \"0.052023\" \"pcxdhdjwnf\" \"0.648518\" \"ibhohmfxzt\" \"0.967245\" \"erxulvebrj\" \"0.971791\" \"qwxrsgafzy\" \"0.154577\" \"whmotknaff\" \"0.404082\" \"ueyrvldzwl\" \"0.324634\" \"nsxaigrnje\" \"0.342459\" \"xsepfhdizi\" \"0.988925\" \"exvhmqxvvp\" \"0.345154\" \"owtkxlzaci\" \"0.728413\" \"ypfeltycpy\" \"0.406783\" \"qbiwbqiubb\" \"0.714527\" \"wqiwdbncmt\" \"0.760975\" \"dapacykoah\" \"0.212614\" \"hhjclfbbka\" \"0.445330\" \"ujrxcmpvsq\" \"0.576866\" \"lbtpbknelm\" \"0.764225\" \"wvcnqbvlnf\" \"0.281427\" \"mibvtmqxcy\" \"0.596978\" \"aovfsvbwjg\" \"0.650632\" \"qbyhorvill\" \"0.948063\" \"ypekeuutef\" \"0.028779\" \"xamjodnbpf\" \"0.678338\" \"hhniikmtsx\" \"0.757806\" \"sahqeskveq\" \"0.671949\" \"bqntlsaqjb\" \"0.190314\" \"luemuetmia\" \"0.219219\" \"zwnitejtpg\" \"0.212301\" \"ncjfckgkcl\" \"0.730613\" \"qjyeqcfhjj\" \"0.657992\" \"wkbasfyzqz\" \"0.556851\" \"ccagtnjilc\" \"0.543822\" \"pqyurbvifk\" \"0.101854\" \"djetcyfxuc\" \"0.878471\" \"vpmpffdoqz\" \"0.696931\" \"hlpdstpvzw\" \"0.662898\" \"imdqxmkzdj\" \"0.887214\" \"eouliovvra\" \"0.193298\" \"gxephxbdru\" \"0.978115\" \"dlsjwmqzhx\" \"0.743150\" \"uapsgmizgh\" \"0.181609\" \"gorgpnnqwr\" \"0.638726\" \"rojeolnopp\" \"0.561627\" \"mbxohpancs\" \"0.223001\" \"upodnpqenn\" \"0.957952\" \"dlbqcynhrn\" \"0.586659\" \"ichqzmiyot\" \"0.813623\" \"ulbrotlhze\" \"0.111318\" \"licotqplim\" \"0.525834\" \"mxwgdcutnb\" \"0.405858\" \"vpbkvwgxsf\" \"0.129768\" \"ijsmoyesvd\" \"0.591274\" \"mbgehjiauu\" \"0.718370\" \"vymjzlzqcn\" \"0.189343\" \"rjkknoigmu\" \"0.566307\" \"jxoxtnzujs\" \"0.563082\" \"eqlgkwosie\" \"0.040883\" \"ljcgdooafo\" \"0.326765\" \"cwaveajmcz\" \"0.423381\" \"ssjqrsluod\" \"0.865798\" \"fhuvzpxjbc\" \"0.784445\" \"perfwnpvkl\" \"0.659790\" \"mjjuylgssm\" \"0.918901\" \"vxcbagyymm\" \"0.282908\" \"nxcdcaqgit\" \"0.030110\" \"ewomnmugfa\" \"0.164237\" \"rkaznetutk\" \"0.281729\" \"rqvryfvlie\" \"0.877790\" \"holjcdpijr\" \"0.878293\" \"pquewclxuy\" \"0.674968\" \"ocrcszcznp\" \"0.520069\" \"xczfmrxrja\" \"0.314841\" \"nxzsnkmxvi\" \"0.719988\" \"hhjchwjlmc\"\r\n\r\n# zadd 1000 elements\r\n\r\n\"ZADD\" \"zset:1000\" \"0.645108\" \"iehxaaneev\" \"0.793870\" \"icatrrbcjs\" \"0.558445\" \"aieekmivcb\" \"0.712596\" \"xwtwtwmbgu\" \"0.138467\" \"ctnaggdbru\" \"0.319593\" \"nouncgkoik\" \"0.193744\" \"naggqdxcjm\" \"0.238679\" \"qjrhtqgwjc\" \"0.308197\" \"ihealvwnxb\" \"0.903443\" \"hggqmlgwha\" \"0.210797\" \"pjdundsxrd\" \"0.706360\" \"zcffhzusrl\" \"0.392186\" \"hmcpcrjumm\" \"0.392543\" \"iiissmznfe\" \"0.024854\" \"caaqjozcqh\" \"0.169284\" \"ooeswhfdnj\" \"0.545674\" \"txgjxzovte\" \"0.943467\" \"xpcpytommm\" \"0.130763\" \"icqjxcermo\" \"0.167315\" \"vqtixjkcbb\" \"0.629596\" \"wwfbpjatrp\" \"0.413113\" \"vtaiyncmyg\" \"0.457128\" \"aslibwggrp\" \"0.105554\" \"bpvbnmpekh\" \"0.771857\" \"gaotvjctjh\" \"0.306570\" \"diwmfuckel\" \"0.939014\" \"dgcesswkvc\" \"0.905870\" \"cflarkquuv\" \"0.063393\" \"lqqivzlppd\" \"0.582370\" \"iwqanwtvcd\" \"0.031539\" \"nvonwipkta\" \"0.720765\" \"rcwbzvwbva\" \"0.315533\" \"sbrjnwxdip\" \"0.688803\" \"kuiumwomxi\" \"0.890704\" \"qgobfhgspy\" \"0.334490\" \"wmvhvwnowp\" \"0.673452\" \"uewpgskfpu\" \"0.819089\" \"cvnlzjdfgf\" \"0.846270\" \"mhbfsuaovv\" \"0.652710\" \"anmcogawkg\" \"0.778304\" \"xsueeljljp\" \"0.771838\" \"padscbypdo\" \"0.177771\" \"tjnjhejnju\" \"0.793709\" \"dxdngrmypg\" \"0.799628\" \"itmekixthv\" \"0.017343\" \"giubyhvaav\" \"0.004991\" \"bbfvuialin\" \"0.580003\" \"tctgzmjads\" \"0.405932\" \"mvfnirshbd\" \"0.851340\" \"asbtvfzien\" \"0.226860\" \"vqashxkrik\" \"0.391362\" \"gazojexans\" \"0.287772\" \"wybmlpqblt\" \"0.191989\" \"mobwpcyxuk\" \"0.224179\" \"sjbaedopjb\" \"0.202665\" \"axhoxkubdv\" \"0.968856\" \"rqiyxwpuyv\" \"0.483072\" \"jpphbjtbrh\" \"0.991346\" \"mvmsikqfxu\" \"0.350243\" \"ltkauvxggz\" \"0.588627\" \"rwllkdzxrj\" \"0.039496\" \"hhwvslfxmf\" \"0.700638\" \"cgpvvnbvlk\" \"0.313291\" \"fgcehqgsso\" \"0.343576\" \"oownjpxrov\" \"0.192722\" \"umwunfzsvo\" \"0.273418\" \"jzceexkqam\" \"0.968219\" \"arqqmfmmbz\" \"0.394788\" \"plqbwsiuzw\" \"0.378863\" \"rhnkzlsjtk\" \"0.605956\" \"eioxaswdee\" \"0.438622\" \"rspghuhpbp\" \"0.356931\" \"zexblqeora\" \"0.009694\" \"stttkrbfqs\" \"0.620955\" \"rmqxtqcxua\" \"0.713440\" \"emdwpvauyc\" \"0.999344\" \"olezxlliej\" \"0.361739\" \"imnjbkmsls\" \"0.308247\" \"thqttakyzy\" \"0.424859\" \"jjlefkekuw\" \"0.036802\" \"qgasrnjrld\" \"0.160174\" \"whdftyqojv\" \"0.725586\" \"snepgcispg\" \"0.004061\" \"ipubipttcb\" \"0.702881\" \"ocesqguvym\" \"0.206341\" \"qwxzqlqter\" \"0.831086\" \"xmknbbmdbf\" \"0.353833\" \"spoqshzjoi\" \"0.029532\" \"slskrnekbc\" \"0.389068\" \"gybxnvwchp\" \"0.520218\" \"uvlswctlhx\" \"0.872011\" \"ujybghjfnf\" \"0.299529\" \"lgqazzajpx\" \"0.153536\" \"yhlalzusch\" \"0.611873\" \"jeazfmhrcb\" \"0.795973\" \"vealtjlqyc\" \"0.284121\" \"ejzwnidqwr\" \"0.429848\" \"nifzrybfuh\" \"0.133500\" \"qlorevyltp\" \"0.870113\" \"wvdmobfisx\" \"0.240635\" \"aqyfvxyjqr\" \"0.903983\" \"zybosgbtxt\" \"0.305875\" \"bvvoibkfrt\" \"0.132423\" \"iziwqojsoq\" \"0.041522\" \"qpffifpdiz\" \"0.085682\" \"ocoflktdhp\" \"0.159059\" \"ncabqesziv\" \"0.650525\" \"zuccleayil\" \"0.354068\" \"pfvfxrrfhl\" \"0.546089\" \"rdipvyeqoi\" \"0.452595\" \"yrsizkbbwz\" \"0.723931\" \"iwuuxdactm\" \"0.595940\" \"mbewprqunw\" \"0.933906\" \"ajdkqflpen\" \"0.696150\" \"wmyphdckda\" \"0.841744\" \"lxoaezrdxs\" \"0.461889\" \"jqqogagqni\" \"0.141414\" \"yhdgqenmwv\" \"0.347382\" \"wrrgxxkxkc\" \"0.398727\" \"omwbgglqsp\" \"0.443000\" \"ymqxhmnidz\" \"0.765094\" \"nqwjivcosg\" \"0.986056\" \"tctrsxgnrc\" \"0.576107\" \"ndwiompimr\" \"0.349571\" \"cmhkstsjok\" \"0.734824\" \"jfbgmhtjke\" \"0.929214\" \"gdlztbhpeq\" \"0.102103\" \"kmifjielrw\" \"0.796464\" \"khkkfmzkow\" \"0.044303\" \"akzsgaeqon\" \"0.349244\" \"ozpyyaznsh\" \"0.282090\" \"dvgqwzignk\" \"0.124180\" \"lawrpikwsk\" \"0.145508\" \"qnqzibcmip\" \"0.014177\" \"jlghqxtvok\" \"0.271586\" \"orqqevpmca\" \"0.941327\" \"yrvmdhnnfc\" \"0.964838\" \"ajkgxmtamu\" \"0.681722\" \"mcbuftndrr\" \"0.524113\" \"caxgjftjyj\" \"0.432128\" \"bzwxqcpftf\" \"0.167489\" \"krfosgrmjb\" \"0.309948\" \"ilkuddrdvh\" \"0.910226\" \"byrkeibrfb\" \"0.382510\" \"bpnfopinub\" \"0.152318\" \"gbeizdzdyb\" \"0.570025\" \"vgfgbsbnwy\" \"0.290087\" \"zbfeyptemz\" \"0.376504\" \"wcouaiatsu\" \"0.071776\" \"omeatkwwtc\" \"0.029506\" \"cbjovvgqqy\" \"0.484980\" \"ofykgotycd\" \"0.043870\" \"bnxtlmiwup\" \"0.751689\" \"uqvzpcvugl\" \"0.419781\" \"rwfykeeqgx\" \"0.087991\" \"nubsytpfao\" \"0.808363\" \"xeyxlxiqch\" \"0.187833\" \"rrrfhnclbv\" \"0.864568\" \"mnsaklzgob\" \"0.725490\" \"zujobawsct\" \"0.204763\" \"pcuaesomdc\" \"0.223025\" \"lftmqoxhfc\" \"0.506344\" \"fgygwdazbm\" \"0.261202\" \"maxylirjgh\" \"0.410370\" \"jgaieawkcu\" \"0.260167\" \"ffrviosqzo\" \"0.250480\" \"fiafoggekm\" \"0.563103\" \"wksgvbkbyw\" \"0.974324\" \"nmguhcygct\" \"0.259207\" \"bekchebgys\" \"0.783447\" \"hmnaoeeasz\" \"0.764739\" \"zknlteeaxq\" \"0.828842\" \"qzxxwlfeki\" \"0.464344\" \"knycrcrsbm\" \"0.984059\" \"izizuzzjuv\" \"0.384700\" \"blujwnyluy\" \"0.930637\" \"zrwgpknaop\" \"0.656346\" \"fffreqppjj\" \"0.051069\" \"alfftdxaxc\" \"0.320961\" \"ywmqoaztmy\" \"0.279848\" \"lbjarnpxhh\" \"0.294326\" \"ecsburyjhh\" \"0.806331\" \"jetamrlglx\" \"0.208517\" \"ruxsmttpak\" \"0.257535\" \"hxbmykkugi\" \"0.821400\" \"jzstgleeby\" \"0.546144\" \"bovkdabcdo\" \"0.226768\" \"jhkdwtqvwl\" \"0.398483\" \"iqbyikqjmx\" \"0.125020\" \"snomfrutha\" \"0.264133\" \"eiziligjfr\" \"0.043614\" \"bqoabrqwwj\" \"0.411120\" \"utymwhxigo\" \"0.850358\" \"bstezdkmig\" \"0.485876\" \"csjfbpjyzq\" \"0.528742\" \"yzrdbalexf\" \"0.110554\" \"uidvmrqyjd\" \"0.751337\" \"yjhypaonqq\" \"0.074346\" \"gktcjcfxbb\" \"0.020820\" \"lqxpnplleo\" \"0.234692\" \"vfcpevtekf\" \"0.157122\" \"xglrugpjkt\" \"0.174485\" \"fidsuuizcf\" \"0.939223\" \"gdqgmwxkmt\" \"0.436623\" \"ghepftfjgk\" \"0.071851\" \"mjwrbndexu\" \"0.847464\" \"shlwywnxpk\" \"0.646937\" \"wlogkzxkpo\" \"0.532516\" \"pwgjoppmgc\" \"0.592209\" \"ossjrvqmaa\" \"0.598701\" \"rnvhfxbuoi\" \"0.443002\" \"lydebbpmfb\" \"0.279549\" \"sdnkjddyut\" \"0.877963\" \"zewylkylsy\" \"0.080681\" \"rnfodijavx\" \"0.696470\" \"ukdnaklmcp\" \"0.102279\" \"tcpwkimype\" \"0.881048\" \"bgcoyoduda\" \"0.415925\" \"gpodilvrnm\" \"0.848481\" \"srkvjhetmj\" \"0.040637\" \"ltgidziwzm\" \"0.244640\" \"tnratexlre\" \"0.075067\" \"tfonhwnuxj\" \"0.034629\" \"rpyroriegs\" \"0.887395\" \"rjrtvpntke\" \"0.685654\" \"vvmkjgajwa\" \"0.400525\" \"nywteueaig\" \"0.435228\" \"vklwhyzqhk\" \"0.060039\" \"afzlyodwiz\" \"0.510563\" \"ropuqidkxv\" \"0.048751\" \"roczxpuisd\" \"0.269116\" \"mhxojehvxx\" \"0.988874\" \"dqqfcdugde\" \"0.240165\" \"qtpcwuafar\" \"0.750459\" \"uvqtefqdhk\" \"0.828087\" \"tstbluhyhj\" \"0.861661\" \"kxggjpatkd\" \"0.351980\" \"wgtjxahmef\" \"0.440586\" \"tdceweesxh\" \"0.897607\" \"qzlfnntjar\" \"0.564519\" \"ljklggibcy\" \"0.372248\" \"nwnyjkugcf\" \"0.098880\" \"qmhyoyctod\" \"0.545321\" \"bwsdzrxzse\" \"0.996184\" \"kmcwshsbye\" \"0.819622\" \"ifhkjgmxrd\" \"0.034752\" \"lnuuootxmp\" \"0.243929\" \"yfdsmhtria\" \"0.317712\" \"pupmdjgyed\" \"0.706819\" \"aryiyaltqw\" \"0.755626\" \"eyhgspybnr\" \"0.066966\" \"babfjxxabw\" \"0.681568\" \"qhhhipzncq\" \"0.289070\" \"rwhzzytdsq\" \"0.040067\" \"rbdabbmnrf\" \"0.325428\" \"sfuvzzxbxq\" \"0.377881\" \"fgjnsosfrp\" \"0.737437\" \"llimzyabsp\" \"0.035594\" \"ufdxqlonpg\" \"0.343033\" \"mjlpvuoghe\" \"0.189450\" \"dyzedskzkq\" \"0.671180\" \"dbcxnsiacw\" \"0.679152\" \"fsuovvpgng\" \"0.919742\" \"tvlckdoyfe\" \"0.191694\" \"pkehotsmka\" \"0.219460\" \"nryvfijxhj\" \"0.188159\" \"yqvkykgjbe\" \"0.564495\" \"jlgepeyhpc\" \"0.238642\" \"nwywtydqew\" \"0.751883\" \"cakvxrdpmj\" \"0.894821\" \"eaetplspga\" \"0.630431\" \"lrhrkuyzry\" \"0.358602\" \"ksppwhhqzj\" \"0.489667\" \"skkawcmqqt\" \"0.778531\" \"mkrrypcfzy\" \"0.101774\" \"tkgihmsrha\" \"0.268255\" \"oqdlpaubsc\" \"0.577108\" \"mdcwoblmkl\" \"0.260332\" \"rjrptuhkav\" \"0.516929\" \"wvsnfinuil\" \"0.941773\" \"aexxjlgwuo\" \"0.196086\" \"kohpozxkhl\" \"0.137598\" \"qxxinxaqxn\" \"0.378124\" \"pltsnzqvpi\" \"0.579231\" \"mcojdazpfq\" \"0.240701\" \"xkvgnzjgrm\" \"0.898586\" \"fzzcioobeb\" \"0.264136\" \"tqblforkpa\" \"0.448451\" \"skbzfbeziu\" \"0.051773\" \"vyiqkfoics\" \"0.536133\" \"lxzdcbveuy\" \"0.502064\" \"hskkmrluuf\" \"0.698222\" \"ltomuspfzc\" \"0.590610\" \"fmpdixcckx\" \"0.057498\" \"ukprrucywq\" \"0.660543\" \"vldwfdnicm\" \"0.826104\" \"fzatoyblsr\" \"0.754438\" \"wbouqpojzl\" \"0.117783\" \"pmaagvqldo\" \"0.533611\" \"wxjdgbugeu\" \"0.025197\" \"nlqtadqinl\" \"0.760125\" \"jgcqryhsvk\" \"0.551085\" \"uboipezuni\" \"0.203084\" \"tmkoxwdgpx\" \"0.415950\" \"xgfzndhodu\" \"0.271285\" \"vuqlqdpfdn\" \"0.385063\" \"zorzyqtjtr\" \"0.537420\" \"uedehyieof\" \"0.972732\" \"jphvxuqipp\" \"0.733064\" \"faxedqgskm\" \"0.342786\" \"cdghgcsoth\" \"0.903818\" \"qwowxqzrkz\" \"0.653952\" \"roqzbzpbbt\" \"0.768345\" \"sxcihybfci\" \"0.892237\" \"pbxrbaxnor\" \"0.764531\" \"geizujxrkg\" \"0.620438\" \"beqsnrixhl\" \"0.521196\" \"llamjvxyqo\" \"0.699100\" \"yzcspfvcot\" \"0.459957\" \"ubghghklvj\" \"0.297416\" \"giswndixdf\" \"0.086339\" \"kgopxvsdah\" \"0.000782\" \"tocawprsxz\" \"0.426201\" \"ctusqixohm\" \"0.339036\" \"hklchdenoe\" \"0.276883\" \"btgeubdzbb\" \"0.860669\" \"cjcrpmggtu\" \"0.147353\" \"ngcxqjjpdm\" \"0.875443\" \"vorgqhmaoq\" \"0.923510\" \"bgcnzgcmza\" \"0.087182\" \"pyjpxqnavq\" \"0.634058\" \"tdmjyuitvv\" \"0.963838\" \"igwekdegcw\" \"0.235680\" \"spogjykkfs\" \"0.675610\" \"vlcnbfqvox\" \"0.661566\" \"tzhjrlfvfp\" \"0.317132\" \"ebhhhgabjd\" \"0.118838\" \"evuxmkrrfl\" \"0.246357\" \"pxzkuasjek\" \"0.450004\" \"hnnfmyurhx\" \"0.623882\" \"yzutuazhmh\" \"0.331435\" \"eovsizpcjp\" \"0.218206\" \"dfemamyevk\" \"0.935043\" \"evjrybtwww\" \"0.820919\" \"audguegpmo\" \"0.966038\" \"nwybjbhgep\" \"0.748183\" \"nrencopzqn\" \"0.287079\" \"hyikggurti\" \"0.375950\" \"xwxwosqkhm\" \"0.388941\" \"kcbotffyca\" \"0.676492\" \"xhspgwheck\" \"0.782514\" \"rkwlgzhvvy\" \"0.628528\" \"tumnalubch\" \"0.672107\" \"dhfundvlpn\" \"0.661732\" \"qsxfnsicrx\" \"0.975140\" \"gamcdtywne\" \"0.608014\" \"asidljmwgb\" \"0.897256\" \"gygftrsdbm\" \"0.349942\" \"ybckvbeoib\" \"0.932954\" \"anznywecwk\" \"0.726428\" \"cgmivhyskk\" \"0.843359\" \"xkiuuciwrn\" \"0.023708\" \"lyhqvxolfw\" \"0.822799\" \"eafrzhdhhq\" \"0.530864\" \"dbtbtvkqss\" \"0.848648\" \"hvxefqtmqu\" \"0.866428\" \"eraxdyjftw\" \"0.446144\" \"eyrbqexkff\" \"0.048181\" \"dxtzxeguoi\" \"0.683328\" \"owjfgjqqjc\" \"0.489625\" \"pisgqibyae\" \"0.970240\" \"nsdvirehqh\" \"0.583183\" \"wgtrwefdsw\" \"0.174619\" \"mamtueyrqn\" \"0.222798\" \"wiqhmhkiel\" \"0.704784\" \"cnngbbpowp\" \"0.837632\" \"curhymvwsx\" \"0.256654\" \"uttazeawix\" \"0.701426\" \"farwqgfyjf\" \"0.463106\" \"mivctgaajt\" \"0.534385\" \"qchpfcigwa\" \"0.538479\" \"lspvrnxnjo\" \"0.571538\" \"bzjzucrypq\" \"0.599488\" \"pyrpwxalpc\" \"0.924799\" \"rncdgqxqfq\" \"0.315947\" \"nenhiiibwx\" \"0.909998\" \"arqfxfqkzh\" \"0.405248\" \"fpdflprzvn\" \"0.674952\" \"cuuytorpnp\" \"0.955493\" \"lfojnetxdc\" \"0.692204\" \"uqbpcvkipa\" \"0.991647\" \"pribqncfuf\" \"0.597850\" \"xbvbujurqw\" \"0.498266\" \"expefhkisx\" \"0.231363\" \"wwowdvybjj\" \"0.357729\" \"qsoiwsugdv\" \"0.401551\" \"ntmgbzaivy\" \"0.230953\" \"kjblkrvknt\" \"0.707000\" \"xkaailrpns\" \"0.246097\" \"muocqqypmt\" \"0.720597\" \"dspznsgszk\" \"0.147380\" \"gkneclxnnt\" \"0.753306\" \"higdkhodzy\" \"0.003853\" \"csqcfxyzsy\" \"0.782493\" \"iyeiercbxr\" \"0.732835\" \"hjekcxfyds\" \"0.644764\" \"ahjawbsqcw\" \"0.038385\" \"kapxglqccs\" \"0.229066\" \"ygbfgfefac\" \"0.791507\" \"qpzkuxsipr\" \"0.991541\" \"kmqlwfbsex\" \"0.698087\" \"vwayaqmtid\" \"0.972772\" \"ghtoyhrfxh\" \"0.890508\" \"gcsswbksnc\" \"0.949597\" \"snpuvnebpy\" \"0.107435\" \"mugdxqnjxj\" \"0.618836\" \"vkqalcokst\" \"0.433195\" \"ltghdkluqq\" \"0.448407\" \"mumhqarhgg\" \"0.135863\" \"gbjzsedhag\" \"0.564412\" \"hvfprkjlbc\" \"0.228627\" \"jqgtbgbybq\" \"0.826784\" \"ydqppngxvh\" \"0.990663\" \"iamjlqlznh\" \"0.361827\" \"zzenkvuesw\" \"0.922425\" \"rmdayyptch\" \"0.150657\" \"ripecixnpr\" \"0.174850\" \"pkgpjwyfbh\" \"0.949969\" \"ortxlvmdoc\" \"0.857772\" \"rcaatkjyur\" \"0.649739\" \"qqfnugftmr\" \"0.332970\" \"bsavjyaksg\" \"0.801014\" \"pudgkcbwdx\" \"0.499720\" \"ynnhyctikq\" \"0.782441\" \"ekjgqnjxyl\" \"0.495010\" \"kbnjiilaqd\" \"0.186830\" \"cvmicoarvv\" \"0.439088\" \"iibgagtkpg\" \"0.715350\" \"jznackjcrd\" \"0.289687\" \"symbgeyple\" \"0.333963\" \"xlmvatfsly\" \"0.958199\" \"njmufqrykx\" \"0.541363\" \"nbizrabfuo\" \"0.933496\" \"amuwlfaxwv\" \"0.714080\" \"ahfktrqmgh\" \"0.100087\" \"kdodndexvr\" \"0.149844\" \"isqymcuffe\" \"0.706632\" \"yzmodbpsnb\" \"0.847058\" \"jlpobgvouj\" \"0.951551\" \"eqiukbyscu\" \"0.068236\" \"mqqrgbacfa\" \"0.490453\" \"mmqblvrscy\" \"0.235491\" \"fiugzrozky\" \"0.377347\" \"nvkfnfzoki\" \"0.127271\" \"vjihaakiof\" \"0.795512\" \"apyzwvajot\" \"0.348210\" \"lxsinouutc\" \"0.567777\" \"ystnkbogee\" \"0.477872\" \"oeawjlqnyg\" \"0.623260\" \"hyanpicfan\" \"0.664609\" \"gtqrsktbaa\" \"0.320234\" \"fffylsswky\" \"0.096201\" \"zdmgjjyukl\" \"0.421767\" \"ffcqkkzllx\" \"0.421724\" \"ogyvxbgcwi\" \"0.195373\" \"dtkfydidli\" \"0.127873\" \"jccorylnjg\" \"0.049319\" \"myzjxvtvjh\" \"0.808586\" \"dtuhvpszzt\" \"0.320150\" \"kamgfgbxel\" \"0.370480\" \"nraylduhut\" \"0.265837\" \"abinkgshoi\" \"0.975716\" \"ghqwxaqlef\" \"0.502953\" \"ftnbjymlll\" \"0.310925\" \"pzrchtwaaw\" \"0.567703\" \"wdqygrxkya\" \"0.728237\" \"snunzlgfkd\" \"0.896651\" \"nkyuframnm\" \"0.867424\" \"rxvhmzvbcv\" \"0.063563\" \"rrzcqyzdzf\" \"0.629553\" \"weqhfkosif\" \"0.462773\" \"ctgwmawlgl\" \"0.718046\" \"kpzmuwqbnt\" \"0.906977\" \"klaeknlbrm\" \"0.264978\" \"cejkfhuykf\" \"0.619446\" \"jhibapuhga\" \"0.061965\" \"wemmvswznk\" \"0.061592\" \"vdaebbupfe\" \"0.689222\" \"cglxptkcsz\" \"0.174769\" \"quemrlmwod\" \"0.898675\" \"ydigxptqbl\" \"0.948273\" \"gjutzwoxlf\" \"0.075856\" \"vefgwelnfo\" \"0.746864\" \"dcepfcdddn\" \"0.965491\" \"qkyfpamste\" \"0.633385\" \"gbkqhfumyu\" \"0.737572\" \"iwapedwyle\" \"0.754054\" \"ormdblyhhn\" \"0.932719\" \"dtjljhzqcm\" \"0.767225\" \"pthacnunjj\" \"0.649379\" \"xzswnnldvs\" \"0.216633\" \"muhewfzihs\" \"0.639269\" \"fuftndsnim\" \"0.909138\" \"xyxmlrdbui\" \"0.508704\" \"jwvqixjhho\" \"0.582770\" \"nfucelqjfe\" \"0.089356\" \"glynpmsjcf\" \"0.838816\" \"avchkjnlwm\" \"0.908984\" \"ylxiwiesps\" \"0.043408\" \"sadqcfniau\" \"0.886747\" \"qgdgujdvtg\" \"0.661810\" \"gfhrrjczsp\" \"0.907605\" \"dpauqcpgyi\" \"0.728562\" \"ppdxnadmje\" \"0.330399\" \"kqzjnkdlxd\" \"0.082110\" \"yhejmjwwni\" \"0.711500\" \"xsgcuvxzor\" \"0.866779\" \"fswhywqxhy\" \"0.421784\" \"rtnhivnxtb\" \"0.497701\" \"veegnotgmj\" \"0.518567\" \"tsmzfswaxo\" \"0.005142\" \"ifpfyncdfe\" \"0.249159\" \"vpqlxtfkjz\" \"0.741728\" \"twmbtaxdro\" \"0.139049\" \"cvnnitrrow\" \"0.070475\" \"erahoeivfw\" \"0.488547\" \"buzhjxsbkm\" \"0.741781\" \"nmtmjmhmdl\" \"0.514985\" \"fqtktfghcv\" \"0.866908\" \"iqzxblqkeo\" \"0.505662\" \"qydrgilxxt\" \"0.773945\" \"beicnwdryg\" \"0.668057\" \"htawohddyn\" \"0.675705\" \"jbqibabrmv\" \"0.784213\" \"aquymkrswt\" \"0.845563\" \"irrovfyshb\" \"0.239145\" \"outdlyeqvq\" \"0.083427\" \"ofcurtthcs\" \"0.865472\" \"xfxlervrgn\" \"0.410198\" \"myrrmvflyw\" \"0.757061\" \"lywsezpzgf\" \"0.634949\" \"jvtalmlkng\" \"0.435516\" \"ghwcrdlbjj\" \"0.228243\" \"rqkcyxiwhz\" \"0.337555\" \"gzekysdunp\" \"0.252513\" \"qtewhixedb\" \"0.767732\" \"bzfzxzecrs\" \"0.092367\" \"whsxmqffqg\" \"0.920685\" \"dmxcbvzrxg\" \"0.238809\" \"dhzgpwewsx\" \"0.569625\" \"flvftlpbjq\" \"0.681014\" \"wxswusqpeo\" \"0.558549\" \"aeafusfzdn\" \"0.076310\" \"gayysuldha\" \"0.343809\" \"nvqfyljbef\" \"0.131309\" \"ocgjeuljxf\" \"0.709863\" \"cnbqnvxmjp\" \"0.339533\" \"sotbjzlsvz\" \"0.818601\" \"cvbbbdzmie\" \"0.244589\" \"dpyjoihqrs\" \"0.924136\" \"ivinvxopgz\" \"0.004308\" \"zdulfflfqx\" \"0.113676\" \"iosqxoobrk\" \"0.009689\" \"kjeevccyof\" \"0.975034\" \"jigyicdeft\" \"0.351974\" \"sqwhsgboef\" \"0.575486\" \"bvurseeqmh\" \"0.701469\" \"lbxvlwzony\" \"0.108482\" \"tqqmmvwact\" \"0.639351\" \"keklddczkd\" \"0.361829\" \"kbfqdppnfa\" \"0.342636\" \"qjwrnhooax\" \"0.365558\" \"koujdppfua\" \"0.866551\" \"xrvonyieqa\" \"0.907022\" \"ccnbldglgl\" \"0.327711\" \"egmgddriry\" \"0.657241\" \"gfdzgxfdcg\" \"0.080151\" \"eirhwkdgfq\" \"0.599621\" \"adlryhdbpr\" \"0.645347\" \"ezbiwqnabg\" \"0.216803\" \"dplonqlliz\" \"0.436938\" \"aiqqyusnuv\" \"0.630922\" \"fmyleefltp\" \"0.387614\" \"mjgvtydjtm\" \"0.239791\" \"trwzipsers\" \"0.686253\" \"wvlvshnhmx\" \"0.710512\" \"nugjvhftma\" \"0.937218\" \"yafipxfsip\" \"0.887614\" \"shgetgsird\" \"0.730346\" \"cblsafugqk\" \"0.940470\" \"spdyueanru\" \"0.203652\" \"wjhaavxfge\" \"0.358953\" \"otadcihtmd\" \"0.428536\" \"irlduoinie\" \"0.203054\" \"asretszbav\" \"0.686840\" \"ekponflaeq\" \"0.116664\" \"btxehrokkw\" \"0.841981\" \"ctkwlhmgfz\" \"0.461922\" \"emfqsjraia\" \"0.729528\" \"ncremxgfdb\" \"0.747886\" \"dnvwyhyhsn\" \"0.314724\" \"gjiwldcfqh\" \"0.915762\" \"kluswgtjsf\" \"0.630900\" \"uvbtcgtopw\" \"0.734056\" \"jjczogqdwz\" \"0.954115\" \"iesbitdnjd\" \"0.922486\" \"glwrmjpotx\" \"0.330378\" \"nmfihtnkel\" \"0.752510\" \"tvzacklhdz\" \"0.826313\" \"xtwlklqdna\" \"0.190858\" \"rfhlttsuqy\" \"0.286279\" \"nlxwjmzwln\" \"0.334573\" \"vjjozwrovk\" \"0.266818\" \"gqybtjuhvq\" \"0.477130\" \"phfuspevwk\" \"0.424753\" \"vzcclamtun\" \"0.334857\" \"gbazuqnmit\" \"0.926985\" \"zmmwzkjrjl\" \"0.273126\" \"xigznrdgqy\" \"0.872996\" \"yufagalzhk\" \"0.973287\" \"kngwkkzwts\" \"0.642007\" \"fuipidfbjt\" \"0.640561\" \"rwzijctxzs\" \"0.547026\" \"fhobhpwwkp\" \"0.976843\" \"nqxdrqigvf\" \"0.889949\" \"zsikdzycyt\" \"0.955177\" \"inboyxgoqa\" \"0.570888\" \"rsivptwulz\" \"0.069483\" \"eqaxrccwjq\" \"0.476052\" \"cctlfgicpv\" \"0.950332\" \"gdozstnglr\" \"0.523253\" \"wovoupawzt\" \"0.396718\" \"nunchscyqc\" \"0.124266\" \"socoxaegfa\" \"0.847380\" \"qbpmtomqpu\" \"0.157463\" \"ngwifjdpha\" \"0.444848\" \"ztjuqomjck\" \"0.834028\" \"hrhiqcarju\" \"0.242287\" \"paitaeqrpb\" \"0.601139\" \"umoicweaab\" \"0.937897\" \"xekxarmwcq\" \"0.318636\" \"aejnvyfdst\" \"0.723443\" \"mlznoaajqq\" \"0.216821\" \"wjibkklezg\" \"0.418684\" \"jxiewthqls\" \"0.261108\" \"sldzewoxas\" \"0.123533\" \"fvemodlpgz\" \"0.541682\" \"fgzwwaedjy\" \"0.327706\" \"twpiiaedpc\" \"0.286415\" \"brrlblrxwa\" \"0.354345\" \"fojjpqmbck\" \"0.162167\" \"rhzqdtxucc\" \"0.553529\" \"fzsoiryhfn\" \"0.995917\" \"zavrjnezrf\" \"0.015279\" \"uxvibjduto\" \"0.179399\" \"puvgjfjyaf\" \"0.592098\" \"jybzltmwrs\" \"0.786701\" \"xftfzsoiwc\" \"0.632713\" \"kkrxiaiife\" \"0.023264\" \"nnfxoqebys\" \"0.639560\" \"wrqnytptzm\" \"0.931933\" \"powzkcrtvv\" \"0.102203\" \"gefoharnza\" \"0.893614\" \"viwarrumob\" \"0.548257\" \"pkcqdokojd\" \"0.804829\" \"kzuywkxlku\" \"0.252774\" \"iijjcabgak\" \"0.105055\" \"nxllkzroin\" \"0.261153\" \"uesalivsis\" \"0.153373\" \"aqlapmghln\" \"0.538430\" \"pfaytznuaa\" \"0.706254\" \"ucxeoqcssr\" \"0.506384\" \"tivnqailcl\" \"0.550096\" \"dtgjnddwch\" \"0.228131\" \"suahxaebee\" \"0.693047\" \"ubzgvzputq\" \"0.255977\" \"tqjpijliii\" \"0.320788\" \"mxapzqqqsw\" \"0.392916\" \"qvpuudyuks\" \"0.807373\" \"kprzbyngsw\" \"0.579009\" \"dflxukffgl\" \"0.639254\" \"xqvjnlpssl\" \"0.509891\" \"wvrlxfoxff\" \"0.079358\" \"tqpqihwjtl\" \"0.174471\" \"vxrtzngznb\" \"0.354177\" \"nahweurftw\" \"0.735534\" \"nehqnkqnld\" \"0.760483\" \"yfvwesgulw\" \"0.392384\" \"zejhycldyy\" \"0.545858\" \"cbtpbbfrdd\" \"0.509818\" \"oglqutqfcx\" \"0.171142\" \"jhctncrzlw\" \"0.366168\" \"wqsqzzbqhm\" \"0.547857\" \"mkbkflixkr\" \"0.503947\" \"nbkaxrojqq\" \"0.081083\" \"erqgyscser\" \"0.370359\" \"owovlhorvw\" \"0.309822\" \"rofnkytnhm\" \"0.231350\" \"yajpmxmuwz\" \"0.394314\" \"gviypfayfm\" \"0.784763\" \"ulnnuwyptq\" \"0.089208\" \"ddpgrvwowd\" \"0.572023\" \"bweysooxiv\" \"0.646567\" \"pnnzqcutoq\" \"0.839930\" \"tyephutkmb\" \"0.264179\" \"nszbrpweoz\" \"0.128647\" \"gehuriygwq\" \"0.659204\" \"vpehhmoxva\" \"0.491950\" \"dpkiubfzbx\" \"0.384848\" \"wgtmckqknh\" \"0.188043\" \"xeurpmfdmo\" \"0.418849\" \"nbwksmwxpx\" \"0.605004\" \"plbxaamppj\" \"0.276890\" \"nojnedgabk\" \"0.833175\" \"ygrpkpstxq\" \"0.297792\" \"etsngvbrff\" \"0.952694\" \"qzcrpbvatq\" \"0.474358\" \"qbhdjhoohc\" \"0.067502\" \"btcvhacldb\" \"0.014993\" \"osncqcuest\" \"0.409761\" \"uzktwqcdeb\" \"0.335957\" \"jttqzbazgz\" \"0.615791\" \"qmxxfyuodo\" \"0.802366\" \"zuibhuihtz\" \"0.431484\" \"ctqxoyxbwc\" \"0.405363\" \"azkdbpnshy\" \"0.590182\" \"qwozutlufu\" \"0.916406\" \"yqmzmmzwpd\" \"0.493531\" \"yivxcecwlp\" \"0.436352\" \"lzzptujbjp\" \"0.743434\" \"ewoqykjbkc\" \"0.132159\" \"zxlbhyeckf\" \"0.302988\" \"nswjopvtqv\" \"0.543728\" \"gkmwutvars\" \"0.228101\" \"mupcilqfjg\" \"0.585761\" \"skstqdgbos\" \"0.306041\" \"kjqeujfkoh\" \"0.441690\" \"pvwvdaorrl\" \"0.920910\" \"pmytvtksfi\" \"0.666617\" \"dniplpxfof\" \"0.512864\" \"twuvkpjzzw\" \"0.600784\" \"aufhfrhccf\" \"0.106240\" \"ljinllovsw\" \"0.889183\" \"ywgeotcect\" \"0.010523\" \"ltvfnuuwil\" \"0.081719\" \"nnnxouavyp\" \"0.369352\" \"tglieutcis\" \"0.790975\" \"wabroeeoop\" \"0.431765\" \"vsvhjrymqc\" \"0.033449\" \"jhdcicttmm\" \"0.334186\" \"dlhjfafskj\" \"0.311725\" \"ffksbrtbfq\" \"0.735770\" \"lcdchjadll\" \"0.402876\" \"ijdgnlzprg\" \"0.013454\" \"znartcywze\" \"0.320563\" \"agswwrfabr\" \"0.859299\" \"euleuicawb\" \"0.237979\" \"aoqlctikzg\" \"0.084421\" \"idmjjbjvni\" \"0.540346\" \"fkdmuxraqf\" \"0.827762\" \"vyewicgjio\" \"0.264901\" \"rzanpefsfy\" \"0.249106\" \"pubqtzzzko\" \"0.641931\" \"btakuczlec\" \"0.028675\" \"hfjxrrsszf\" \"0.112206\" \"dthtfrqkce\" \"0.881211\" \"vnsufnurol\" \"0.158854\" \"hmdzsuuyrn\" \"0.657811\" \"shckmujxzo\" \"0.823770\" \"fmmammvdyj\" \"0.794376\" \"fhuptkhkzm\" \"0.920863\" \"qquwyuyvvw\" \"0.237467\" \"tdggmsxysk\" \"0.382295\" \"ysnndkycix\" \"0.164685\" \"ftyxhyfokj\" \"0.924193\" \"dmbarohbfj\" \"0.985108\" \"mallnshtok\" \"0.932159\" \"cszvzbrmoy\" \"0.948943\" \"stnfirydgi\" \"0.243979\" \"bxwvqvndcc\" \"0.729360\" \"wtzqqecgfy\" \"0.827464\" \"mkngszsxeu\" \"0.066282\" \"ncckxlmsvg\" \"0.832378\" \"pdjmftsmob\" \"0.546000\" \"vqgztpmzhz\" \"0.880249\" \"vvmaucizkv\" \"0.529144\" \"fnpdsuozxt\" \"0.599864\" \"gritvkzfgw\" \"0.575660\" \"wtcpliaxmk\" \"0.095307\" \"cqfnhujrbj\" \"0.428143\" \"osaekeukqx\" \"0.988758\" \"nepxmyiuhr\" \"0.438792\" \"lfkqrtxocm\" \"0.337112\" \"pgdhjrxhga\" \"0.029529\" \"wcpbfslakk\" \"0.817147\" \"cynhehkcxs\" \"0.796564\" \"trzqdcaqdw\" \"0.292661\" \"mxydilgynv\" \"0.296909\" \"lscjhgztom\" \"0.658885\" \"rqurawzebz\" \"0.291664\" \"pghbwbtfmk\" \"0.176822\" \"ckibsdtfff\" \"0.884684\" \"svvdufedug\" \"0.806829\" \"fjdjumschq\" \"0.960759\" \"ybcdthmgws\" \"0.806253\" \"ogtqmpnzie\" \"0.749828\" \"yjyffpgoop\" \"0.313174\" \"uwiqrvcqvu\" \"0.978051\" \"xepfvvcovk\" \"0.935539\" \"oxsdmrdbit\" \"0.949876\" \"rfxibyjmpg\" \"0.527448\" \"gwzqcetcji\" \"0.136049\" \"mkxysrkpug\" \"0.207709\" \"jjumoixniz\" \"0.302963\" \"pypepewjvq\" \"0.097174\" \"gcfcbjybkx\" \"0.982914\" \"ezgxjiwwig\" \"0.643767\" \"kjiqagynco\" \"0.789879\" \"urkkyscfti\" \"0.345265\" \"tsnawydcru\" \"0.657757\" \"sshbuxfljd\" \"0.849243\" \"fazsvkljef\" \"0.122817\" \"jfqxkxgqhj\" \"0.874860\" \"qxbqbfcgjp\" \"0.019772\" \"joijmgposs\" \"0.396742\" \"qocjpufxio\" \"0.317664\" \"xpkwqbfban\" \"0.417027\" \"saqilljaid\" \"0.436455\" \"qlvgfplbod\" \"0.689103\" \"aoydkdfrpe\" \"0.151562\" \"dxpepbctea\" \"0.004086\" \"jqurtadjro\" \"0.275095\" \"szupcnvvij\" \"0.975388\" \"nunpqugdit\" \"0.619831\" \"cmqraybrlw\" \"0.021593\" \"bnatichltp\" \"0.615263\" \"zuoqjiciij\" \"0.516554\" \"suhwnartid\" \"0.500129\" \"bhfmhanvxe\" \"0.970410\" \"qckueiqiwh\" \"0.310292\" \"hmwfncgzxg\" \"0.136794\" \"bhrvnadcdk\" \"0.537331\" \"bwjyghaztz\" \"0.845703\" \"hwuofuftlr\" \"0.062857\" \"xzbqjpzqlm\" \"0.148334\" \"rhkpfsuhoq\" \"0.903658\" \"ywlqbjqeug\" \"0.171792\" \"haxesjafmh\" \"0.607711\" \"ouroipthpq\" \"0.213063\" \"kdklhpxntt\" \"0.566853\" \"mhrvuemywb\" \"0.066576\" \"cpjveufsvk\" \"0.575035\" \"mszjkgsrio\" \"0.883155\" \"rtskokvklv\" \"0.326063\" \"kdcvbkrbsj\" \"0.497748\" \"pbfijwccjp\" \"0.096181\" \"gsvkmnluiz\" \"0.651896\" \"brwlqbfoat\" \"0.698481\" \"wzxdkpehwf\" \"0.241647\" \"hhbceuegvh\" \"0.335103\" \"ubwlcefgqb\" \"0.139593\" \"vlhtdpqavh\" \"0.043865\" \"wghyakzbit\" \"0.232605\" \"adfhfatarh\" \"0.280517\" \"wevfinjbqk\" \"0.240397\" \"scgjdkyetq\" \"0.661665\" \"ymwwctfodg\" \"0.406742\" \"sotsxznskx\" \"0.651442\" \"ckqebhazel\" \"0.954087\" \"dwzqowbrsd\" \"0.118853\" \"dclualrzqb\" \"0.859469\" \"ifiizdeong\" \"0.715490\" \"etcsjxoqab\" \"0.365077\" \"igehetokzq\" \"0.303267\" \"tuajnnqtcq\" \"0.187568\" \"mxpzuzrzuo\" \"0.447848\" \"oqmuhlruqy\" \"0.385352\" \"hvtlkrungk\" \"0.055211\" \"dygkzcpakt\" \"0.403664\" \"rnlaakgsrf\" \"0.314530\" \"yoblelzlkd\" \"0.082484\" \"mwmcwqzbld\" \"0.916300\" \"mgldvzleyy\" \"0.169185\" \"ahcaaodvgi\" \"0.972997\" \"erglflfnql\" \"0.188418\" \"behdxlfdho\" \"0.605785\" \"ikpikupjoi\" \"0.348162\" \"ylulwsnjay\" \"0.512359\" \"qcsxjrjcfc\" \"0.376004\" \"ollacusjzj\" \"0.312060\" \"ethxaycsil\" \"0.912136\" \"laepwenqmc\" \"0.629227\" \"eksvvoxziw\" \"0.473402\" \"ulepgommyy\" \"0.112999\" \"efhynoxlul\" \"0.141312\" \"vhjaphfdpj\" \"0.501631\" \"otclvmbilg\" \"0.622360\" \"ndltyojjxj\" \"0.560323\" \"ehnrizfmfo\" \"0.856890\" \"tqkprkoixe\" \"0.295874\" \"cvohdragtx\" \"0.144378\" \"emfjcnujqn\" \"0.013908\" \"bzursuzuei\" \"0.765880\" \"qmnxipsiga\" \"0.655198\" \"dxnprfawun\" \"0.921417\" \"umttshfkpk\" \"0.269042\" \"nrbfkysxaf\" \"0.426194\" \"xjksnqifds\" \"0.009747\" \"qatkvfuttq\" \"0.222498\" \"bqqohkuylc\" \"0.487539\" \"thmmmlqluk\" \"0.447940\" \"gnrmnwaxls\" \"0.757365\" \"usykkwszvh\" \"0.127757\" \"nnhrgirrtw\" \"0.114722\" \"sujbwndgwl\" \"0.320579\" \"pkvcbelpos\" \"0.028885\" \"fussukqrph\" \"0.898756\" \"bgtxhxkhvv\" \"0.440242\" \"ywiurvfbpg\" \"0.195203\" \"rakustfykw\" \"0.455870\" \"txhllnvudv\" \"0.038326\" \"smwbxeqbed\" \"0.636364\" \"rdsfcdvkqz\" \"0.602638\" \"nknlysgviv\" \"0.703795\" \"yzviqaobku\" \"0.517737\" \"rngtndwjyg\" \"0.896203\" \"jqmscuprwq\" \"0.758854\" \"bcwncpnibg\" \"0.497263\" \"rwrxxrnwtq\" \"0.810537\" \"fpmbbgiaao\" \"0.816854\" \"mshexjmkmn\" \"0.132051\" \"rhzpguhsws\" \"0.319074\" \"krxneqolle\" \"0.336648\" \"dozecfsvue\" \"0.607888\" \"jbzyfznpdn\" \"0.971581\" \"tjnbsybxws\" \"0.261454\" \"vpzsmbjkvy\" \"0.581137\" \"dewdgfrhos\" \"0.680898\" \"gcjruttnno\" \"0.999251\" \"uzaejrbwue\" \"0.158681\" \"jvekvvldai\" \"0.606900\" \"imexfccbxk\" \"0.986671\" \"exhjfssojj\" \"0.999539\" \"hjjxyybxiv\" \"0.548141\" \"mjifqzmtsd\" \"0.838391\" \"tbqidtevrl\" \"0.812230\" \"hjlhurakwh\" \"0.308053\" \"ughnpilqqm\" \"0.047394\" \"kfselnpkim\" \"0.852908\" \"vewfxcxkpf\" \"0.201866\" \"usjmfkopln\" \"0.126260\" \"yxsnreuepl\" \"0.246804\" \"flrsaczxzc\" \"0.835412\" \"aadzbodres\" \"0.293504\" \"bhwytqsafu\" \"0.708503\" \"lpahctqgna\" \"0.045136\" \"zwlhpcahwu\" \"0.601683\" \"kgirldeylz\" \"0.556444\" \"krtsiucvvu\" \"0.595704\" \"adlxahxsbq\" \"0.191964\" \"alokvrpbih\" \"0.572409\" \"mmcunsiwad\" \"0.551146\" \"dfdodbelzn\" \"0.441988\" \"ejlunxlwxn\" \"0.419999\" \"tlnkrncpwi\" \"0.037276\" \"jhocasnttw\" \"0.132050\" \"qslrwqmixc\" \"0.767421\" \"afamsavgsi\" \"0.697485\" \"ramoirrdyd\" \"0.245522\" \"hplvvuoscb\" \"0.506283\" \"dxufcyurjx\" \"0.614086\" \"dablvesuho\" \"0.085394\" \"ovqohpxjft\" \"0.343138\" \"qclkaeciey\" \"0.148648\" \"dgodkfjzos\" \"0.740439\" \"iobkwbwceu\" \"0.829028\" \"ocmtsfpsgh\" \"0.472991\" \"ubtiscdgrn\" \"0.349979\" \"fsoardckcw\" \"0.094781\" \"sstqpivwip\" \"0.846751\" \"wzuhzzdezi\" \"0.746618\" \"tmyuncyoyd\" \"0.338101\" \"ygoiannoht\" \"0.538581\" \"zkbqvttlzy\" \"0.306575\" \"bwizktcwmb\" \"0.560909\" \"dcjlwhfstw\" \"0.075409\" \"pheajlhymx\" \"0.142967\" \"ysntbzffxq\" \"0.385727\" \"rgtondctpo\" \"0.825053\" \"uncqdpbhwb\" \"0.231924\" \"bdtbaxnuko\" \"0.138864\" \"fsthobmdxk\" \"0.309461\" \"auwfujaoya\" \"0.288084\" \"hertbwuzyw\" \"0.452947\" \"azpwrzovza\" \"0.987974\" \"yilvzcevlj\" \"0.374557\" \"kpfqxroqbs\" \"0.224445\" \"dlomhvkoxg\" \"0.205225\" \"vjhpmffzxc\" \"0.863114\" \"klwqsggtob\"\r\n```\r\n\r\nTo benchmark (swap keyname for 10 and 100 variants) :\r\n```\r\nmemtier_benchmark --test-time 60 -c 10 -t 8 --pipeline 1 --hide-histogram --command=\"zrange zset:100 0 1 byscore withscores\"\r\n```\r\n\r\n\r\nresults comparing unstable ( 9ab873d9 ) vs this PR:\r\n\r\n### achiavable ops/sec on a standalone scenario \r\n\r\n\r\ntest | unstable ( 9ab873d9 ) | this PR (filipecosta90/ac54ab41) | % change\r\n-- | -- | -- | --\r\nZRANGE 10 elements | 96405 | 118367.83 | 22.8%\r\nZRANGE 100 elements | 19947 | 29793.25 | 49.4%\r\nZRANGE 1000 elements | 3159 | 5324.53 | 68.6%\r\n\r\n### overall client side p50(ms)\r\n\r\n\r\n\r\ntest | unstable ( 9ab873d9 ) | this PR (filipecosta90/ac54ab41) | % change\r\n-- | -- | -- | --\r\nZRANGE 10 elements | 1.815 | 1.479 | 22.7%\r\nZRANGE 100 elements | 10.623 | 7.007 | 51.6%\r\nZRANGE 1000 elements | 63.487 | 37.631 | 68.7%\r\n\r\n\r\n\r\n\r\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-04-14T08:04:45Z",
        "closed_at": "2022-04-17T12:28:50Z",
        "merged_at": "2022-04-17T12:28:50Z",
        "body": "The tests verify that loading a binary payload to the Lua interpreter raises an error.\r\nThe Lua code modification was done here:  fdf9d455098f54f7666c702ae464e6ea21e25411\r\nhttps://github.com/redis/redis/blob/effa707e9db3e9d00fff06d45e2a5a81d3d55fa9/deps/lua/src/ldo.c#L498\r\nwhich force the Lua interpreter to always use the text parser.\r\n\r\n- [x] fix test according to the changes here https://github.com/redis/redis/pull/10575 after it will be merged.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 27,
        "changed_files": 2,
        "created_at": "2022-04-14T05:46:23Z",
        "closed_at": "2022-04-19T09:06:39Z",
        "merged_at": "2022-04-19T09:06:39Z",
        "body": "Changes:\r\n1. Check the failed rewrite time threshold only when we actually consider triggering a rewrite.\r\n  i.e. this should be the last condition tested, since the test has side effects (increasing time threshold)\r\n  Could have happened in some rare scenarios \r\n2. no limit in startup state (e.g. after restarting redis that previously failed and had many incr files)\r\n3. the \u201ctriggered the limit\u201d log would be recorded only when the limit status is returned\r\n4. remove failure count in log (could be misleading in some cases)\r\n\r\nBackground:\r\n\r\nMP-aof is an exciting update, recently I was testing it and found some issues with the current aofwrite limit, the log is as follows\r\n\r\n```bash\r\n#cat appendonly.aof.manifest\r\nfile appendonly.aof.38.base.rdb seq 38 type b\r\nfile appendonly.aof.45.incr.aof seq 45 type i\r\nfile appendonly.aof.46.incr.aof seq 46 type i\r\nfile appendonly.aof.47.incr.aof seq 47 type i\r\n```\r\n```\r\nredislog\r\n9136:M 13 Apr 2022 23:50:47.885 # Background AOF rewrite has repeatedly failed 3 times and triggered the limit, will retry in 1 minutes\r\n9136:M 13 Apr 2022 23:51:47.045 # Background AOF rewrite has repeatedly failed 3 times and triggered the limit, will retry in 2 minutes\r\n9136:M 13 Apr 2022 23:53:47.065 # Background AOF rewrite has repeatedly failed 3 times and triggered the limit, will retry in 4 minutes\r\n9136:M 13 Apr 2022 23:57:47.070 # Background AOF rewrite has repeatedly failed 3 times and triggered the limit, will retry in 8 minutes\r\n9136:M 14 Apr 2022 00:05:47.041 # Background AOF rewrite has repeatedly failed 3 times and triggered the limit, will retry in 16 minutes\r\n```\r\nI explain the problems of the log above:\r\n1. Failed twice, but the log was printed \"3 times\"\r\n2. When the grow percent is not reached, the limit time will continue to increase until it reaches 60 minutes\r\n3. When my grow percent already needs to be rewritten, I have to wait for this time limit to arrive, which may take 60 minutes\r\n4. There are some unclear places in the code\uff0c when the log records \"will retry in xx minutes\", the actual return value may be 'no limit'\r\n\r\nI optimized this function and where it is called to fix this problems",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2022-04-13T20:13:22Z",
        "closed_at": "2022-08-05T14:44:53Z",
        "merged_at": null,
        "body": "This change set addHashIteratorCursorToReply to use hashTypeCurrentObject\r\nfor accessing the element at an iterator position instead of doing it\r\ndirectly, making the code cleaner.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-04-13T14:58:29Z",
        "closed_at": "2022-04-14T05:29:36Z",
        "merged_at": "2022-04-14T05:29:36Z",
        "body": "we had a panic in streamLastValidID when the stream metadata said it's not empty, but the rax is empty.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 46,
        "changed_files": 13,
        "created_at": "2022-04-12T21:27:08Z",
        "closed_at": "2022-04-14T08:18:33Z",
        "merged_at": "2022-04-14T08:18:32Z",
        "body": "By the convention of errors, there is supposed to be a space between the code and the name. While looking at some lua stuff I noticed that interpreter errors were not adding the space, so some clients will try to map the detailed error message into the error.\r\n\r\nWe have tests that hit this condition, but they were just checking that the string \"starts\" with ERR. I updated some other tests with similar incorrect string checking. This isn't complete though, as there are other ways we check for ERR I didn't fix.\r\n\r\n\r\nProduces some fun output like:\r\n```\r\n# Errorstats\r\nerrorstat_ERR:count=1\r\nerrorstat_ERRuser_script_1_:count=1\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2022-04-12T17:54:39Z",
        "closed_at": "2022-04-13T09:29:24Z",
        "merged_at": "2022-04-13T09:29:24Z",
        "body": "we can observe that when adding to a stream without ID there is a duplicate work on sds creation/freeing/sdslen that costs ~11% of the CPU cycles. \r\n\r\n![image](https://user-images.githubusercontent.com/5832149/163021023-f6d8586e-f36d-4370-99c7-47b604d1f981.png)\r\n\r\nThis PR avoids it by not freeing the sds after the first reply. \r\nThe expected reduction in CPU cycles is around 9-10% as explained on the bellow image:\r\n![image](https://user-images.githubusercontent.com/5832149/163022053-0f5aca20-d4b5-481e-9e36-de3e648e6267.png)\r\n\r\nAdditionally, we now pre-allocate the sds to the right size, to avoid realloc. this brought another ~10% improvement\r\n\r\n\r\nTo reproduce:\r\n```\r\nmemtier_benchmark --test-time 60 -c 10 -t 8 --pipeline 15 --hide-histogram --command=\"XADD key * field value\"\r\n```\r\n\r\nbaseline on unstable branch ( 6b403f56a523480a08b1143c676ce174d0d0251c ) :\r\n\r\n```\r\nALL STATS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nXadds      526817.35         2.27484         2.20700         4.41500         4.51100     38992.30 \r\nTotals     526817.35         2.27484         2.20700         4.41500         4.51100     38992.30 \r\n```\r\n\r\nFirst commit of this PR (avoid dup work): \r\n```\r\nALL STATS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nXadds      590160.88         2.03049         1.96700         3.93500         4.03100     43693.61 \r\nTotals     590160.88         2.03049         1.96700         3.93500         4.03100     43693.61 \r\n```\r\n\r\nSecond commit (avoid reallocs):\r\n```\r\nALL STATS\r\n==================================================================================================\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nXadds      624186.13         1.88814         1.82300         3.64700         3.74300     46220.68 \r\nTotals     624186.13         1.88814         1.82300         3.64700         3.74300     46220.68\r\n```\r\n\r\n### Platform details\r\nThe performance results provided in this PR were collected using:\r\n- *Hardware platform*: A physical HPE ProLiant DL380 Gen10 Server, with one Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz, Maximum memory capacity of 3 TB, disabling CPU Frequency Scaling and with all configurable BIOS and CPU system settings set to performance.\r\n- *OS*: Ubuntu 20.04 Linux release 5.4.0-107.\r\n- *Installed Memory*: 64GB DDR4-SDRAM @ 2933 MHz\r\n- *Compiler*: gcc-11 (Ubuntu 11.1.0-1ubuntu1~20.04) 11.1.0\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2022-04-12T15:55:17Z",
        "closed_at": "2022-04-18T11:56:00Z",
        "merged_at": "2022-04-18T11:56:00Z",
        "body": "RM_Yield was missing a call to protectClient to prevent redis from\r\nprocessing future commands of the yielding client.\r\n\r\nAdding tests that fail without this fix.\r\n\r\nThis would be complicated to solve since nested calls to RM_Call used to\r\nreplace the current_client variable with the module temp client.\r\n\r\nIt looks like it's no longer necessary to do that, since it was added\r\nback in #9890 to solve two issues, both already gone:\r\n1. call to CONFIG SET maxmemory could trigger a module hook calling\r\n   RM_Call. although this specific issue is gone, arguably other hooks\r\n   like keyspace notification, can do the same.\r\n2. an assertion in lookupKey that checks the current command of the\r\n   current client, introduced in #9572 and removed in #10248",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 46,
        "changed_files": 1,
        "created_at": "2022-04-11T16:36:57Z",
        "closed_at": "2022-04-11T23:05:45Z",
        "merged_at": "2022-04-11T23:05:45Z",
        "body": "Functions hashTypeExists, hashTypeGetValueLength and addHashFieldToReply\r\nhave a similar pattern on calling hashTypeGetFromHashTable or\r\nhashTypeGetFromZipList depending on the underlying data structure. What\r\ndoes functions are during is exactly what hashTypeGetValue does. Those\r\nfunctions were changed to use existing function hashTypeGetValue making\r\nthe code more consistent.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-04-11T15:43:15Z",
        "closed_at": "2022-04-12T10:23:41Z",
        "merged_at": "2022-04-12T10:23:41Z",
        "body": "Bumps [actions/upload-artifact](https://github.com/actions/upload-artifact) from 2 to 3.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/upload-artifact/releases\">actions/upload-artifact's releases</a>.</em></p>\n<blockquote>\n<h2>v3.0.0</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Update default runtime to node16 (<a href=\"https://github-redirect.dependabot.com/actions/upload-artifact/issues/293\">#293</a>)</li>\n<li>Update package-lock.json file version to 2 (<a href=\"https://github-redirect.dependabot.com/actions/upload-artifact/issues/302\">#302</a>)</li>\n</ul>\n<h3>Breaking Changes</h3>\n<p>With the update to Node 16, all scripts will now be run with Node 16 rather than Node 12.</p>\n<h2>v2.3.1</h2>\n<p>Fix for empty fails on Windows failing on upload <a href=\"https://github-redirect.dependabot.com/actions/upload-artifact/issues/281\">#281</a></p>\n<h2>v2.3.0 Upload Artifact</h2>\n<ul>\n<li>Optimizations for faster uploads of larger files that are already compressed</li>\n<li>Significantly improved logging when there are chunked uploads</li>\n<li>Clarifications in logs around the upload size and prohibited characters that aren't allowed in the artifact name or any uploaded files</li>\n<li>Various other small bugfixes &amp; optimizations</li>\n</ul>\n<h2>v2.2.4</h2>\n<ul>\n<li>Retry on HTTP 500 responses from the service</li>\n</ul>\n<h2>v2.2.3</h2>\n<ul>\n<li>Fixes for proxy related issues</li>\n</ul>\n<h2>v2.2.2</h2>\n<ul>\n<li>Improved retryability and error handling</li>\n</ul>\n<h2>v2.2.1</h2>\n<ul>\n<li>Update used actions/core package to the latest version</li>\n</ul>\n<h2>v2.2.0</h2>\n<ul>\n<li>Support for artifact retention</li>\n</ul>\n<h2>v2.1.4</h2>\n<ul>\n<li>Add Third Party License Information</li>\n</ul>\n<h2>v2.1.3</h2>\n<ul>\n<li>Use updated version of the <code>@action/artifact</code> NPM package</li>\n</ul>\n<h2>v2.1.2</h2>\n<ul>\n<li>Increase upload chunk size from 4MB to 8MB</li>\n<li>Detect case insensitive file uploads</li>\n</ul>\n<h2>v2.1.1</h2>\n<ul>\n<li>Fix for certain symlinks not correctly being identified as directories before starting uploads</li>\n</ul>\n<h2>v2.1.0</h2>\n<ul>\n<li>Support for uploading artifacts with multiple paths</li>\n<li>Support for using exclude paths</li>\n<li>Updates to dependencies</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/6673cd052c4cd6fcf4b4e6e60ea986c889389535\"><code>6673cd0</code></a> Update <code>lockfileVersion</code> in <code>package-lock.json</code> (<a href=\"https://github-redirect.dependabot.com/actions/upload-artifact/issues/302\">#302</a>)</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/2244c8200304ec9588bf9399eac622d9fadc28c4\"><code>2244c82</code></a> Update to node16 (<a href=\"https://github-redirect.dependabot.com/actions/upload-artifact/issues/293\">#293</a>)</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/87348cee5fefa95e42e9b4804b4d68e130304158\"><code>87348ce</code></a> Add 503 warning when uploading to the same artifact</li>\n<li>See full diff in <a href=\"https://github.com/actions/upload-artifact/compare/v2...v3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/upload-artifact&package-manager=github_actions&previous-version=2&new-version=3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-04-09T14:09:40Z",
        "closed_at": "2022-04-10T06:29:51Z",
        "merged_at": "2022-04-10T06:29:51Z",
        "body": "Fix incorrect resp type character in the comment.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2022-04-08T21:27:40Z",
        "closed_at": "2022-04-12T05:16:17Z",
        "merged_at": "2022-04-12T05:16:17Z",
        "body": "Allow specifying an ACL log reason, which is shown in the log. Right now it always shows \"unknown\", which is a little bit cryptic. This is a breaking change, but this API was added as part of 7 so it seems ok to stabilize it still.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 39,
        "deletions": 39,
        "changed_files": 5,
        "created_at": "2022-04-07T09:39:46Z",
        "closed_at": "2023-03-15T09:05:43Z",
        "merged_at": "2023-03-15T09:05:43Z",
        "body": "Replace NBSP character (0xC2 0xA0) with space (0x20).\r\n\r\nWithout this fix, these code looks like this in CLion's editor:\r\n\r\n![image](https://user-images.githubusercontent.com/3883644/162170024-b921aea9-e1ea-4702-8d84-7f685318a49d.png)\r\n ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-04-07T06:15:23Z",
        "closed_at": "2022-04-07T11:47:07Z",
        "merged_at": "2022-04-07T11:47:07Z",
        "body": "The `auto-aof-rewrite-percentage` config defines at what growth percentage an automatic AOF rewrite is triggered.\r\nThis normally works OK since the size of the AOF file at the end of a rewrite is stored in `server.aof_rewrite_base_size`.\r\nHowever, on startup, redis used to store the entire size of the AOF file into that variable, resulting in a wrong automatic AOF rewrite trigger (could have been triggered much later than desired).\r\nThis issue would only affect the first AOFRW after startup, after that future AOFRW would have been triggered correctly.\r\nThis bug existed in all previous versions of Redis.\r\n\r\nThis PR unifies the meaning of `server.aof_rewrite_base_size`, which only represents the size of BASE AOF.\r\nNote that after an AOFRW this size includes the size of the incremental file (all the commands that executed during rewrite), so that auto-aof-rewrite-percentage is the ratio from the size of the AOF after rewrite.\r\nHowever, on startup, it is complicated to know that size, and we compromised on taking just the size of the base file, this means that the first rewrite after startup can happen a little bit too soon.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-04-06T16:17:40Z",
        "closed_at": "2022-04-07T08:52:28Z",
        "merged_at": "2022-04-07T08:52:28Z",
        "body": "The bug was when using REDISMODULE_YIELD_FLAG_CLIENTS.\r\nin that case we would have only set the CLIENTS type flag in\r\nserver.busy_module_yield_flags and then clear that flag when exiting\r\nRM_Yield, so we would never call unblockPostponedClients when the\r\ncontext is destroyed.\r\n\r\nThis didn't really have any actual implication, which is why the tests\r\ncouldn't (and still can't) find that since the bug only happens when\r\nusing CLIENT, but in this case we won't have any clients to un-postpone\r\ni.e. clients will get rejected with BUSY error, rather than being\r\npostponed.\r\n\r\nUnrelated:\r\n* Adding tests for nested contexts, just in case.\r\n* Avoid nested RM_Yield calls",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2022-04-06T11:52:12Z",
        "closed_at": "2022-04-13T08:33:36Z",
        "merged_at": "2022-04-13T08:33:36Z",
        "body": "Fix #10414\r\n\r\nApparently, some modules can afford deprecating command arguments (something that was never done in Redis, AFAIK), so in order to represent this piece of information, we added the `deprecated_since` field to redisCommandArg (in symmetry to the already existing `since` field).\r\n\r\nThis commit adds `const char *deprecated_since` to `RedisModuleCommandArg`, which is technically a breaking change, but since 7.0 was not released yet, we decided to let it slide\r\n\r\nTODO\r\n- [x] open a doc PR",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2022-04-06T11:32:51Z",
        "closed_at": "2022-04-10T08:41:31Z",
        "merged_at": "2022-04-10T08:41:31Z",
        "body": "Add field to COMMAND DOCS response to denote the name of the module that added that command.\r\nCOMMAND LIST can filter by module, but if you get the full commands list, you may still wanna know which command belongs to which module.\r\nThe alternative would be to do MODULE LIST, and then multiple calls to COMMAND LIST\r\n\r\nFix #10416",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 113,
        "deletions": 120,
        "changed_files": 14,
        "created_at": "2022-04-06T11:12:58Z",
        "closed_at": "2022-04-17T12:43:22Z",
        "merged_at": "2022-04-17T12:43:22Z",
        "body": "since PUBLISH and SPUBLISH use different dictionaries for channels and clients, and we already have an API for PUBLISH, it only makes sense to have one for SPUBLISH\r\n\r\nAdd test coverage and unifying some test infrastructure.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 287,
        "deletions": 2,
        "changed_files": 7,
        "created_at": "2022-04-06T11:12:50Z",
        "closed_at": "2022-04-17T05:31:58Z",
        "merged_at": "2022-04-17T05:31:58Z",
        "body": "Add APIs to allow modules to compute the memory consumption of opaque objects owned by redis.\r\nWithout these, the `mem_usage` callbacks of module data types are useless in many cases.\r\n\r\nOther changes:\r\nFix streamRadixTreeMemoryUsage to include the size of the rax structure itself\r\n\r\nto do:\r\n- [x] add test coverage",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-04-06T10:22:18Z",
        "closed_at": "2022-04-10T08:43:59Z",
        "merged_at": "2022-04-10T08:43:59Z",
        "body": "Similarly to LCS, some modules would want to try to allocate memory, and fail gracefully if the allocation fails.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 567,
        "deletions": 162,
        "changed_files": 7,
        "created_at": "2022-04-05T19:00:17Z",
        "closed_at": "2022-11-17T03:24:18Z",
        "merged_at": "2022-11-17T03:24:18Z",
        "body": "1. Added a new \"shard_id\" field to \"cluster nodes\" output and nodes.conf after \"hostname\". (removed in RC3)\r\n2. Added a new PING extension to propagate \"shard_id\"\r\n3. Handled upgrade from pre-7.2 releases automatically\r\n4. Refactored PING extension assembling/parsing logic\r\n5. Expose the shard-id in a new command `CLUSTER MYSHARDID`\r\n\r\nBehavior of Shard IDs:\r\n1. Replicas will always follow the shards of their reported primaries. If a primary updates its shard ID, the replica will follow. (This need not follow for cluster v2) This is not an expected use case.\r\n\r\n```\r\nRelease notes R1:\r\nIntroduce Shard IDs to logically group nodes in cluster mode based off replication. Shard IDs are automatically assigned and visible via `CLUSTER MYSHARDID`.\r\n\r\nRelease notes R3:\r\nCluster SHARD IDs are no longer visible in the cluster nodes output.\r\n```",
        "comments": 50
    },
    {
        "merged": true,
        "additions": 441,
        "deletions": 336,
        "changed_files": 11,
        "created_at": "2022-04-05T15:57:06Z",
        "closed_at": "2022-04-06T06:33:34Z",
        "merged_at": "2022-04-06T06:33:34Z",
        "body": "Fixes in command argument in json files\r\n* Fixes BITFIELD's syntax (\"sub-commands\" can be repeated, and OVERFLOW is only valid for SET and INCR)\r\n* Improves readability of SET (reordered)\r\n* Fixes GEOSEARCH and GEOSEARCH_RO syntices (use `oneof` for mutually exclusive group instead of `optional`)\r\n* Fixes MIGRATE syntax (use `oneof` for mutually exclusive group instead of `optional`)\r\n* Fixes MODULE LOADEX syntax (the `CONFIG` token should be repeated too when using multiple configs)\r\n\r\nother:\r\n* make generate-command-help.rb accept a path to commands.json, or read it from stdin (e.g. `generate-commands-json.py | generate-command-help.rb -`)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2022-04-05T15:18:58Z",
        "closed_at": "2022-04-10T02:52:37Z",
        "merged_at": "2022-04-10T02:52:36Z",
        "body": "**What this PR does :**\r\n\r\n1. Remove confusing fields :**slot_info_pairs_alloc** \r\n\r\n> **slot_info_pairs_alloc** is used to preallocate **slot_info_pairs** memory size.But **slot_info_pairs_count** is fixed for one  unique clusterNode and It less than **2*n->numslots**. So I remove field **slot_info_pairs_alloc**,use **2*n->numslots** instead.\r\n\r\n2. Performance improvement\r\n\r\n> To one unique clusterNode,function zrealloc is called only once  after modification.So this PR reduce the call times of brk system call and reduces memory fragmentation.  \r\n\r\n\r\n**Related PR**:\r\nimprove malloc efficiency for cluster slots_info_pairs https://github.com/redis/redis/pull/10488\r\n\r\nAdd cluster shards support : https://github.com/redis/redis/pull/10293\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2022-04-05T09:08:35Z",
        "closed_at": "2022-04-05T14:08:28Z",
        "merged_at": "2022-04-05T14:08:28Z",
        "body": "when my module have multi source files, such as A.c and B.c; and my Makefile copy from redis module demo\r\n``` makefile\r\nmymodule.so: A.xo B.xo\r\n    $(LD) -o $@ $< $(SHOBJ_LDFLAGS) $(LIBS) -lc\r\n```\r\nmy target file  was only include the symbols of A.xo file, no symbols from B.xo file\r\n\r\nfrom the mananl of GUN Make[makefile document](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html#Automatic-Variables)\r\n* `$<` : The name of the first prerequisite.\r\n* `$^`:  The names of all the prerequisites, with spaces between them. \r\n\r\nI think using the `$^` is better than `$<` when our target file is a shared object ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 7,
        "changed_files": 7,
        "created_at": "2022-04-05T03:29:42Z",
        "closed_at": "2022-04-05T05:21:42Z",
        "merged_at": "2022-04-05T05:21:42Z",
        "body": "`REDISMODULE_EXPERIMENTAL_API` have be outdated",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2022-04-04T20:02:35Z",
        "closed_at": "2022-04-27T06:54:36Z",
        "merged_at": null,
        "body": "The blocking behavior of SAVE during shutdown can be undesired if we want a stable and responsive connection without compromising. Switching to a viable server should be fast.\r\nOn replicas using RDB, it can be acceptable to not save the latest data, especially if we know it will never become a master.\r\nSometimes the periodic SAVE in background is enough.\r\nAlso, if guaranteed durability is desired AOF could be used instead.\r\nHere I suggest introducing the boolean option `replica_shutdown_nosave` to disable auto SAVE on shutdown, and explicitly restricting this to replicas, where it most matters to have responsive connections.\r\n\r\nAn alternative (maybe even better) would be to close all normal client connections as soon as SAVE starts on shutdown and not accept any new normal connection.\r\n\r\nTests to be include if proposal is accepted.",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2022-04-04T07:33:10Z",
        "closed_at": "2022-04-05T14:07:59Z",
        "merged_at": "2022-04-05T14:07:59Z",
        "body": "Sentinel once in a while experience Sentinel TILT period or leader election \r\nfailure cycle. The problem is that those default timeout are too big and once \r\nit happens, it breaks our tests.  Suggesting:\r\n- Reducing failover-timeout from 20 to 10sec (actually it is multiplied by 2 \r\n  and reach 40sec of timeout) \r\n- Modify tilt-period from default of 30sec to 5sec. When TILT period happens \r\n  it might lead to failover in our tests, and might cause also to failover cycle\r\n  cycle failure.\r\n\r\nSentinel tests should `wait_for_condition` up to 50seconds, where needed, \r\nto be stable in case having single TILT period or failover failure cycle.\r\n\r\nIn addition relax timing configuration for \"manual failover\" Sentinel test \r\n(was modified several months ago as part of an effort to reduce tests runtime)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12015,
        "deletions": 9888,
        "changed_files": 15,
        "created_at": "2022-04-03T20:51:21Z",
        "closed_at": "2023-03-30T16:03:56Z",
        "merged_at": "2023-03-30T16:03:56Z",
        "body": "Now that the command argument specs are available at runtime (#9656), this PR addresses #8084 by implementing a complete solution for command-line hinting in `redis-cli`.\r\n\r\nIt correctly handles nearly every case in Redis's complex command argument definitions, including `BLOCK` and `ONEOF` arguments, reordering of optional arguments, and repeated arguments (even when followed by mandatory arguments). It also validates numerically-typed arguments. It may not correctly handle all possible combinations of those, but overall it is quite robust.\r\n\r\nArguments are only matched after the space bar is typed, so partial word matching is not supported - that proved to be more confusing than helpful. When the user's current input cannot be matched against the argument specs, hinting is disabled.\r\n\r\nPartial support has been implemented for legacy (pre-7.0) servers that do not support `COMMAND DOCS`, by falling back to a statically-compiled command argument table. On startup, if the server does not support `COMMAND DOCS`, `redis-cli` will now issue an `INFO SERVER` command to retrieve the server version (unless `HELLO` has already been sent, in which case the server version will be extracted from the reply to `HELLO`). The server version will be used to filter the commands and arguments in the command table, removing those not supported by that version of the server. However, the static table only includes core Redis commands, so with a legacy server hinting will not be supported for module commands.\r\nThe auto generated help.h and the scripts that generates it are gone.\r\n\r\nCommand and argument tables for the server and CLI use different structs, due primarily to the need to support different runtime data. In order to generate code for both, macros have been added to `commands.def` (previously `commands.c`) to make it possible to configure the code generation differently for different use cases (one linked with redis-server, and one with redis-cli).\r\n\r\nAlso adding a basic testing framework for the command hints based on new (undocumented) command line options to `redis-cli`: `--test_hint 'INPUT'` prints out the command-line hint for a given input string, and `--test_hint_file <filename>` runs a suite of test cases for the hinting mechanism. The test suite is in `tests/assets/test_cli_hint_suite.txt`, and it is run from `tests/integration/redis-cli.tcl`.",
        "comments": 35
    },
    {
        "merged": false,
        "additions": 224,
        "deletions": 4,
        "changed_files": 7,
        "created_at": "2022-04-03T19:41:36Z",
        "closed_at": "2023-08-25T03:21:02Z",
        "merged_at": null,
        "body": "In order to set extra properties to specific clients, different than the name, it could be handled by a new prop called meta, all extra data could be saved there. I have the case where for example in bullmq (queue management package) the name of each client instance makes reference to the same queue, but I would like also to differenciate each instance per type (workers, schedulers, events), the type could be saved in this meta prop or if I would like to consider more attributes per instance, I could save it there too as in my test cases were I'm saving a stringified object.\r\n\r\n```\r\nRelease notes:\r\nAdd ability to store and retrieve custom client metadata.\r\n```",
        "comments": 23
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2022-04-03T18:30:34Z",
        "closed_at": "2022-04-04T05:38:41Z",
        "merged_at": "2022-04-04T05:38:41Z",
        "body": "function `aofRewriteLimited` in aof.c, `deley`->`delay` and `NAX->MAX`",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 1,
        "changed_files": 6,
        "created_at": "2022-04-03T17:42:51Z",
        "closed_at": "2022-04-13T08:36:39Z",
        "merged_at": "2022-04-13T08:36:39Z",
        "body": "Add an optional keyspace event when new keys are added to the db.  \r\n\r\nThis is useful for applications where clients need to be aware of the redis keyspace. Such an application can SCAN once at startup and then listen for \"new\" events (plus others associated with DEL, RENAME, etc).\r\n\r\nI was unsure if this should be included in the 'A' event class, so I left that out and documented.\r\nIt would be great if this could make it into the next 6.2.x release as well as 7.\r\n\r\nto do:\r\n- [x] redis-doc PR",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-04-03T06:36:18Z",
        "closed_at": "2022-04-03T07:56:15Z",
        "merged_at": "2022-04-03T07:56:15Z",
        "body": "Fix by replacing in test blind sleep with wait_for_condition().",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 136,
        "deletions": 7,
        "changed_files": 8,
        "created_at": "2022-04-01T03:45:18Z",
        "closed_at": "2022-04-26T10:25:33Z",
        "merged_at": "2022-04-26T10:25:33Z",
        "body": "* Till now, replicas that were unable to persist, would still execute the commands they got from the master, now they'll panic by default, and we add a new `replica-ignore-disk-errors` config to change that.\r\n* Till now, when a command failed on a replica or AOF-loading, it only logged a warning and a stat, we add a new `propagation-error-behavior` config to allow panicking in that state (may become the default one day)\r\n\r\nNote that commands that fail on the replica can either indicate a bug that could cause data inconsistency between the replica and the master, or they could be in some cases (specifically in previous versions), a result of a command (e.g. EVAL) that failed on the master, but still had to be propagated to fail on the replica as well.\r\n\r\n### Background\r\n\r\nBased off our conversations about data corruption (https://github.com/redis/redis/pull/10419), it seemed like maybe we should add some defense here. If a replica tries to apply an invalid command, this flag will crash the replica. This has been enabled in tests.\r\n\r\nto do:\r\n- [X] consider removing the check added here d3b466234 and panicking instead (either depending on the new config, or always). (Originally we thought this was an issue, but it doesn't seem like this makes sense since primaries/replicas can disagree on slot ownership)\r\n- [X] consider panicking on disk errors in replica (possibly using another new config: `replica-ignore-disk-errors`)",
        "comments": 23
    },
    {
        "merged": true,
        "additions": 57,
        "deletions": 20,
        "changed_files": 3,
        "created_at": "2022-03-31T17:36:06Z",
        "closed_at": "2022-04-20T11:00:30Z",
        "merged_at": "2022-04-20T11:00:30Z",
        "body": "In #7491 (part of redis 6.2), we started using the monotonic timer instead of mstime to measure command execution time for stats, apparently this meant sampling the clock 3 times per command rather than two (wince we also need the wall-clock time).\r\nIn some cases this causes a significant overhead.\r\n\r\nThis PR fixes that by avoiding the use of monotonic timer, except for the cases were we know it should be extremely fast.\r\nThis PR also adds a new INFO field called `monotonic_clock` that shows which clock redis is using.\r\n\r\nbackground\r\n====\r\n\r\nWe saw in #10460 that updateCachedTime took ~1.5% of CPU cycles to get the time once again, when we already keep track of time within call() for the command latency tracking. \r\nGiven we need the UNIX timestamp for updateCachedTime() call we might as well measure time once before the command logic by using ustime() instead of having to use ustime() + elapsedStart(). \r\n\r\nto reproduce:\r\n```\r\n# populate data\r\nmemtier_benchmark --ratio 1:0  -d 1000 --key-pattern P:P --key-maximum 1000000  --key-minimum 1 --pipeline 15 --hide-histogram\r\n\r\n# benchmark\r\nmemtier_benchmark --ratio 0:1 --test-time 60  -d 1000  --key-maximum 1000000  --key-minimum 1 --pipeline 15 --hide-histogram\r\n```\r\n\r\n## before profile and results:\r\n```\r\nALL STATS\r\n============================================================================================================================\r\nType         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n----------------------------------------------------------------------------------------------------------------------------\r\nSets            0.00          ---          ---             ---             ---             ---             ---         0.00 \r\nGets       676114.65    676114.65         0.00         4.43638         3.74300         6.94300         7.48700    688585.47 \r\nWaits           0.00          ---          ---             ---             ---             ---             ---          --- \r\nTotals     676114.65    676114.65         0.00         4.43638         3.74300         6.94300         7.48700    688585.47 \r\n\r\n```\r\n![image](https://user-images.githubusercontent.com/5832149/161114458-86f07acb-666c-4e64-97c1-9d15cf6c13a5.png)\r\n\r\n## after results:\r\n\r\nustime:\r\n```\r\n============================================================================================================================\r\nType         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n----------------------------------------------------------------------------------------------------------------------------\r\nSets            0.00          ---          ---             ---             ---             ---             ---         0.00 \r\nGets       689547.70    689547.70         0.00         4.34994         3.74300         7.29500         7.67900    702266.28 \r\nWaits           0.00          ---          ---             ---             ---             ---             ---          --- \r\nTotals     689547.70    689547.70         0.00         4.34994         3.74300         7.29500         7.67900    702266.28 \r\n```\r\n\r\nhw clock (monotonic clock: X86 TSC @ 2100 ticks/us) :\r\n```\r\n============================================================================================================================\r\nType         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n----------------------------------------------------------------------------------------------------------------------------\r\nSets            0.00          ---          ---             ---             ---             ---             ---         0.00 \r\nGets       691869.31    691869.31         0.00         4.33539         3.71100         7.23100         7.58300    704630.70 \r\nWaits           0.00          ---          ---             ---             ---             ---             ---          --- \r\nTotals     691869.31    691869.31         0.00         4.33539         3.71100         7.23100         7.58300    704630.70\r\n```\r\n\r\nthis means we get ~2% improvement by default @oranagra. the HW clock reduces the overhead of time tracking by 0.3%. \r\n\r\n",
        "comments": 34
    },
    {
        "merged": true,
        "additions": 425,
        "deletions": 296,
        "changed_files": 17,
        "created_at": "2022-03-31T07:56:09Z",
        "closed_at": "2022-04-05T07:27:24Z",
        "merged_at": "2022-04-05T07:27:24Z",
        "body": "## Move library meta data to be part of the library payload.\r\n\r\nFollowing the discussion on https://github.com/redis/redis/issues/10429 and the intention to add (in the future) library versioning support, we believe that the entire library metadata (like name and engine) should be part of the library payload and not provided by the `FUNCTION LOAD` command. The reasoning behind this is that the programmer who developed the library should be the one who set those values (name, engine, and in the future also version). **It is not the responsibility of the admin who load the library into the database.**\r\n\r\nThe PR moves all the library metadata (engine and function name) to be part of the library payload. The metadata needs to be provided on the first line of the payload using the shebang format (`#!<engine> name=<name>`), example:\r\n\r\n```lua\r\n#!lua name=test\r\nredis.register_function('foo', function() return 1 end)\r\n```\r\n\r\nThe above script will run on the Lua engine and will create a library called `test`.\r\n\r\n## API Changes (compare to 7.0 rc2)\r\n\r\n* `FUNCTION LOAD` command was change and now it simply gets the library payload and extract the engine and name from the payload. In addition, the command will now return the function name which can later be used on `FUNCTION DELETE` and `FUNCTION LIST`.\r\n* The description field was completely removed from`FUNCTION LOAD`, and `FUNCTION LIST`\r\n\r\n\r\n## Breaking Changes (compare to 7.0 rc2)\r\n\r\n* Library description was removed (we can re-add it in the future either as part of the shebang line or an additional line).\r\n* Loading an AOF file that was generated by either 7.0 rc1 or 7.0 rc2 will fail because the old command syntax is invalid.\r\n\r\n## Notes\r\n\r\n* Loading an RDB file that was generated by rc1 / rc2 **is** supported, Redis will automatically add the shebang to the libraries payloads (we can probably delete that code after 7.0.3 or so since there's no need to keep supporting upgrades from an RC build).",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-03-31T02:32:41Z",
        "closed_at": "2022-04-04T15:38:18Z",
        "merged_at": "2022-04-04T15:38:18Z",
        "body": "The warning:\r\n```\r\npqsort.c:106:7: warning: performing pointer subtraction with a null pointer has undefined behavior [-Wnull-pointer-subtraction]\r\nloop:   SWAPINIT(a, es);\r\n        ^~~~~~~~~~~~~~~\r\npqsort.c:65:47: note: expanded from macro 'SWAPINIT'\r\n#define SWAPINIT(a, es) swaptype = ((char *)a - (char *)NULL) % sizeof(long) || \\\r\n```\r\nClang version:\r\n```\r\nApple clang version 13.1.6 (clang-1316.0.21.2)\r\nTarget: x86_64-apple-darwin21.3.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-03-30T14:11:37Z",
        "closed_at": "2022-03-31T03:14:22Z",
        "merged_at": "2022-03-31T03:14:21Z",
        "body": "There are three timing issues:\r\n1. [ERR] Not all 16384 slots are covered by nodes.\r\nWe can see new_owner doesn't have the old_owner slots information.\r\nI guess the cluster is unstable after SETSLOT, using wait_for_condition\r\n--cluster check make sure the check in reshard won't fail.\r\n```\r\n[exception]: Executing test client: >>> Performing Cluster Check (using node 127.0.0.1:21586)\r\nM: def6b00d3ab27bdfde1e0c92a159022b7ed0006b 127.0.0.1:21586\r\n   slots:[12182] (1 slots) master\r\nM: 045c64c41bb18fc7f69e7db0ebd4737bc88dd865 127.0.0.1:21587\r\n   slots:[10923-12181],[12183-16383] (5460 slots) master\r\nM: b36176d5d999d43c1d385adab10c097418dc7c23 127.0.0.1:21589\r\n   slots:[0-5460] (5461 slots) master\r\n```\r\n\r\n2. clusterManagerMoveSlot failed: CLUSTERDOWN The cluster is down\r\nDuring the reshard, clusterManagerMoveSlot may fail in MIGRATE\r\ndue to the cluster is down.\r\n\r\n3. Expected 'MOVED 12182 xxx' to be equal to 'CLUSTERDOWN The cluster is down'\r\nAfter the reshard, the cluster is unstable. So we also need to wait for it.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-03-30T09:01:33Z",
        "closed_at": "2022-04-11T06:23:53Z",
        "merged_at": null,
        "body": "`If the node, after the score update, would be still exactly at the same position, we can just update the score without actually removing and re-inserting the element in the skiplist.` according to #5179",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 49,
        "changed_files": 1,
        "created_at": "2022-03-29T13:54:35Z",
        "closed_at": "2022-03-29T15:35:17Z",
        "merged_at": "2022-03-29T15:35:17Z",
        "body": "* missing parenthesis meant that the ubuntu and centos jobs were not\r\n  skipped\r\n* the recently divided freebsd, macos, and valgrind jobs, which are now\r\n  split into distict jobs for redis, modules, sentinel, cluster. were\r\n  all executed, producing a build, but not running anything.\r\n  now they're filtered at the job level\r\n* iothreads was missing from the skip list defaults, so was not skipped",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2022-03-28T20:01:17Z",
        "closed_at": "2022-04-02T21:58:07Z",
        "merged_at": "2022-04-02T21:58:07Z",
        "body": "When a cluster node loses its last slot and the SETSLOT command from the client arrives before the cluster bus PONG from the new owner, the logic turning the node into a replica of the new slot owner didn't kick in. The race condition where the order of these two messages mattered is solved with this change, so the end result is the same regardless of the order of these messages.",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 41,
        "changed_files": 2,
        "created_at": "2022-03-28T14:15:48Z",
        "closed_at": "2022-03-29T07:05:06Z",
        "merged_at": "2022-03-29T07:05:06Z",
        "body": "Recently the cluster tests are consistently failing when executed with ASAN in the CI.\r\nThe failure is usually in `04-resharding.tcl`:\r\n```\r\n00:42:54> Verify 50000 keys for consistency with logical content: FAILED: caught an error in the test CLUSTERDOWN The cluster is down\r\n```\r\n\r\nwhen it happens with `test-sanitizer-address (gcc)`\r\nwe can later see this:\r\n```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n38902:M 27 Mar 2022 02:33:24.780 # Redis 255.255.255 crashed by signal: 11, si_code: 0\r\n38902:M 27 Mar 2022 02:33:24.780 # Accessing address: 0x3e90000b308\r\n38902:M 27 Mar 2022 02:33:24.781 # Killed by PID: 45832, UID: 1001\r\n38902:M 27 Mar 2022 02:33:24.821 # Crashed running the instruction at: 0x7f04a759789b\r\n\r\n------ STACK TRACE ------\r\nEIP:\r\n/lib/x86_64-linux-gnu/libc.so.6(__sched_yield+0xb)[0x7f04a759789b]\r\n\r\nBacktrace:\r\n../../../src/redis-server *:30000 [cluster](sigsegvHandler+0x1d2)[0x56170ae5e7b2]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x153c0)[0x7f04a76993c0]\r\n/lib/x86_64-linux-gnu/libc.so.6(__sched_yield+0xb)[0x7f04a759789b]\r\n/lib/x86_64-linux-gnu/libasan.so.5(+0x1323f5)[0x7f04a792e3f5]\r\n/lib/x86_64-linux-gnu/libasan.so.5(+0x13b874)[0x7f04a7[937](https://github.com/redis/redis/runs/5706456172?check_suite_focus=true#step:9:937)874]\r\n/lib/x86_64-linux-gnu/libc.so.6(dl_iterate_phdr+0x185)[0x7f04a75f4375]\r\n/lib/x86_64-linux-gnu/libasan.so.5(+0x13bbd0)[0x7f04a7937bd0]\r\n/lib/x86_64-linux-gnu/libasan.so.5(+0x13b031)[0x7f04a7937031]\r\n/lib/x86_64-linux-gnu/libasan.so.5(+0x13b3b9)[0x7f04a79373b9]\r\n/lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xce)[0x7f04a74dc15e]\r\n/lib/x86_64-linux-gnu/libasan.so.5(+0x22be7)[0x7f04a781ebe7]\r\n```\r\n\r\n\r\nwhen it happens with `test-sanitizer-address (clang)` we see:\r\n```\r\n46928:M 28 Mar 2022 01:11:25.753 # Redis 255.255.255 crashed by signal: 11, si_code: 0\r\n46928:M 28 Mar 2022 01:11:25.753 # Accessing address: 0x3e90000c3dc\r\n46928:M 28 Mar 2022 01:11:25.754 # Killed by PID: 50140, UID: 1001\r\n46928:M 28 Mar 2022 01:11:25.754 # Crashed running the instruction at: 0x7f2a964f289b\r\n\r\n------ STACK TRACE ------\r\nEIP:\r\n/lib/x86_64-linux-gnu/libc.so.6(__sched_yield+0xb)[0x7f2a964f289b]\r\n\r\nBacktrace:\r\n../../../src/redis-server *:30000 [cluster](sigsegvHandler+0x193)[0x688c33]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x153c0)[0x7f2a9660f3c0]\r\n/lib/x86_64-linux-gnu/libc.so.6(__sched_yield+0xb)[0x7f2a964f289b]\r\n../../../src/redis-server *:30000 [cluster][0x4e8765]\r\n../../../src/redis-server *:30000 [cluster][0x4f32ba]\r\n/lib/x86_64-linux-gnu/libc.so.6(dl_iterate_phdr+0x185)[0x7f2a9654f375]\r\n../../../src/redis-server *:30000 [cluster][0x4f328f]\r\n../../../src/redis-server *:30000 [cluster][0x4f0a58]\r\n../../../src/redis-server *:30000 [cluster][0x4f09c2]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x49a27)[0x7f2a96436a27]\r\n/lib/x86_64-linux-gnu/libc.so.6(on_exit+0x0)[0x7f2a96436be0]\r\n../../../src/redis-server *:30000 [cluster](serverCron+0xfa8)[0x5267b8]\r\n../../../src/redis-server *:30000 [cluster](aeProcessEvents+0xb0a)[0x515eba]\r\n../../../src/redis-server *:30000 [cluster](aeMain+0x3d)[0x5165fd]\r\n../../../src/redis-server *:30000 [cluster](main+0xb81)[0x540251]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3)[0x7f2a964140b3]\r\n../../../src/redis-server *:30000 [cluster](_start+0x2e)[0x44f9ee]\r\n```\r\nthese traces indicate that the test suite infra is attempting to terminate the redis instance, and when it refuses to terminate, the test suite sends a SIGSEGV in order to see where it is hung.\r\nwe see it is hung inside `exit`\r\n\r\nI tried to track down the commit that started it, and it appears to be #10293.\r\nLooking at the commit, i realize it didn't affect this test / flow, other than the replacement of the slots_info_pairs from sds to list.\r\ni concluded that what could be happening is that the slot range is very fragmented, and that results in many allocations.\r\nwith sds, it results in one allocation and also, we have a greedy growth mechanism, but with adlist, we just have many many small allocations.\r\nthis probably causes stress on ASAN, and causes it to be slow at termination.\r\n\r\nThis commit improve malloc efficiency of this mechanism by changing adlist into an array being realloced with greedy growth mechanism\r\n.\r\ntests: https://github.com/redis/redis/actions/runs/2052717847",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 13,
        "changed_files": 5,
        "created_at": "2022-03-28T12:23:40Z",
        "closed_at": "2022-03-28T15:35:57Z",
        "merged_at": "2022-03-28T15:35:57Z",
        "body": "There are a few places that use a hard coded const of 128 to allocate a buffer for d2string.\r\nReplace these with a clear macro.\r\nNote that In theory, converting double into string could take as much as nearly 400 chars, but since d2string uses `%g` and not `%f`, it won't pass some 40 chars.\r\n\r\nunrelated:\r\nrestore some changes to auto generated commands.c that got accidentally reverted in #10293",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 35,
        "changed_files": 6,
        "created_at": "2022-03-28T12:17:53Z",
        "closed_at": "2022-04-17T14:16:47Z",
        "merged_at": "2022-04-17T14:16:47Z",
        "body": "When the score doesn't have fractional part, and can be stored as an integer,\r\nwe use the integer capabilities of listpack to store it, rather than convert it to string.\r\nThis already existed before this PR (lpInsert dose that conversion implicitly).\r\n\r\nBut to do that, we would have first converted the score from double to string (calling `d2string`),\r\nthen pass the string to `lpAppend` which identified it as being an integer and convert it back to an int.\r\nNow, instead of converting it to a string, we store it using lpAppendInteger`.\r\n\r\nUnrelated:\r\n---\r\n* Fix the double2ll range check (negative and positive ranges, and also the comparison operands\r\n  were slightly off. but also, the range could be made much larger, see comment).\r\n* Unify the double to string conversion code in rdb.c with the one in util.c\r\n* Small optimization in lpStringToInt64, don't attempt to convert strings that are obviously too long.\r\n\r\nBenchmark;\r\n---\r\nUp to 20% improvement in certain tight loops doing zzlInsert with large integers.\r\n(if listpack is pre-allocated to avoid realloc, and insertion is sorted from largest to smaller)",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-03-28T07:19:46Z",
        "closed_at": "2022-03-28T09:40:52Z",
        "merged_at": "2022-03-28T09:40:52Z",
        "body": "A timing issue of debug sleep master isn't long enough to ensure\r\nthat master is down and let the test identify it. Replaced the code\r\nwith suspend PID until verified master-is-down.\r\n\r\nfailed here: https://github.com/redis/redis/runs/5699779792?check_suite_focus=true",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-03-28T06:27:25Z",
        "closed_at": "2022-03-28T08:54:35Z",
        "merged_at": "2022-03-28T08:54:35Z",
        "body": "a missing of resp3 judgement which may lead to the crash using `debug protocol push`\r\nintroduced in #9235\r\nSimilar improvement in RM_ReplySetAttributeLength in case the module ignored the error that returned from RM_ReplyWithAttribute.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2022-03-27T14:03:05Z",
        "closed_at": "2022-03-27T15:39:20Z",
        "merged_at": "2022-03-27T15:39:20Z",
        "body": "#10381 fixed an issue in `redis-cli --cluster reshard` that used to fail it (redis-cli) because of a race condition.\r\nthe race condition is / was that when moving the last slot from a node, sometimes the PONG messages delivering the configuration change arrive to that node before the SETSLOT arrives to it, and it becomes a replica.\r\nother times the the SETSLOT arrive first, and then PONG **doesn't** demote it.\r\n\r\n**however**, the PR also added a new test that suffers from exactly the same race condition, and the tests started failing a lot.\r\n\r\nThe fact is (if i understand it correctly), that this test (the one being deleted here), isn't related to the fix that PR fixed (which was to fix redis-cli).\r\nThe race condition in the cluster code still happens, and as long as we don't solve it, there's no reason to test it.\r\n\r\np.s. IIUC, the state in which we have a master with no slots, is an invalid state, so race condition or not, we must fix this, right?\r\n@enjoy-binbin investigated it and suggested a fix here: https://github.com/redis/redis/pull/10381#issuecomment-1074859036\r\n\r\nFor now, even if my understandings are wrong, i'm gonna delete that failing test, since as far as i understand, #10381 didn't introduce any new risks for that matter (which are gonna be compromised by removing this check), this race existed since forever, and still exists, and the fact that redis-cli is now immune to it is still being tested.\r\n\r\nAdditional work should be carried to fix it, and i live it for other PRs to handle",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2022-03-27T12:21:24Z",
        "closed_at": "2022-03-27T14:56:22Z",
        "merged_at": "2022-03-27T14:56:22Z",
        "body": "Replace condition with wait_for_condition On \"Verify sentinel that restarted \r\nfailed to reconnect master after ACL change\"\r\n\r\nThe reason we reach it, is because the test is fast enough to modify ACL and test sentinel connection status with the server - before its scheduled operation got the chance to update connection status with the server:\r\n```\r\n/* Perform scheduled operations for the specified Redis instance. */\r\nvoid sentinelHandleRedisInstance(sentinelRedisInstance *ri) {\r\n    /* ========== MONITORING HALF ============ */\r\n    /* Every kind of instance */\r\n    sentinelReconnectInstance(ri);\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-03-25T14:19:24Z",
        "closed_at": "2022-04-05T18:45:45Z",
        "merged_at": "2022-04-05T18:45:45Z",
        "body": "Fixed a bug that used the `hincrbyfloat` or `hincrby` commands to make the field or value exceed the `hash_max_listpack_value` but did not change the object encoding of the hash structure.\r\n\r\nAdd a check for field and value in hashTypeSet\r\n\r\nIf the length of field or value is too long, it will reduce the efficiency of listpack, and the object encoding will become hashtable after AOF restart, so this is also to keep the same before and after AOF restart.\r\n\r\nExample:\r\n![](https://s2.loli.net/2022/03/28/kFtIVzqwuJjG52Z.jpg)\r\n\r\nAfter:\r\n![redis_bug1.jpg](https://s2.loli.net/2022/03/28/Xhi5lwVzx4Rjbg1.jpg)",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 301,
        "deletions": 218,
        "changed_files": 32,
        "created_at": "2022-03-23T16:56:53Z",
        "closed_at": "2022-03-30T06:39:26Z",
        "merged_at": "2022-03-30T06:39:26Z",
        "body": "This PR do some command json files cleanups:\r\n\r\n1. Add COMMAND TIPS to some commands\r\n- command-docs: add `NONDETERMINISTIC_OUTPUT_ORDER`\r\n- command-info: add `NONDETERMINISTIC_OUTPUT_ORDER`\r\n- command-list: add `NONDETERMINISTIC_OUTPUT_ORDER`\r\n- command: change `NONDETERMINISTIC_OUTPUT` to `NONDETERMINISTIC_OUTPUT_ORDER`\r\n- function-list: add `NONDETERMINISTIC_OUTPUT_ORDER`\r\n- latency-doctor: add `NONDETERMINISTIC_OUTPUT`, `REQUEST_POLICY:ALL_NODES` and `RESPONSE_POLICY:SPECIAL`\r\n- latency-graph: add `NONDETERMINISTIC_OUTPUT`, `REQUEST_POLICY:ALL_NODES` and `RESPONSE_POLICY:SPECIAL`\r\n- memory-doctor: add `REQUEST_POLICY:ALL_SHARDS` and `RESPONSE_POLICY:SPECIAL`\r\n- memory-malloc-stats: add `REQUEST_POLICY:ALL_SHARDS` and `RESPONSE_POLICY:SPECIAL`\r\n- memory-purge: add `REQUEST_POLICY:ALL_SHARDS` and `RESPONSE_POLICY:ALL_SUCCEEDED`\r\n- module-list: add `NONDETERMINISTIC_OUTPUT_ORDER`\r\n- msetnx: add `REQUEST_POLICY:MULTI_SHARD` and `RESPONSE_POLICY:AGG_MIN`\r\n- object-refcount: add `NONDETERMINISTIC_OUTPUT`\r\n3. Only (mostly) indentation and formatting changes:\r\n- cluster-shards\r\n- latency-history\r\n- pubsub-shardchannels\r\n- pubsub-shardnumsub\r\n- spublish\r\n- ssubscribe\r\n- sunsubscribe\r\n4. add doc_flags (DEPRECATED) to cluster-slots,  replaced_by `CLUSTER SHARDS` in 7.0\r\n5. command-getkeysandflags: a better summary (the old one is copy from command-getkeys)\r\n6. adjustment of command parameter types\r\n- `port` is integer, not string (`MIGRATE`, `REPLICAOF`, `SLAVEOF`)\r\n- `replicationid` is string, not integer (`PSYNC`)\r\n- `pattern` is pattern, not string (`PUBSUB CHANNELS`, `SENTINEL RESET`, `SORT`, `SORT_RO`)\r\n\r\nOther issues:\r\n1. DUMP is NONDETERMINISTIC_OUTPUT, should FUNCTION DUMP be the same? (i guess not)\r\n2. i notice SENTINEL commands are not list in redis.io/commands (i think we should list it)",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-03-23T13:48:10Z",
        "closed_at": "2022-03-26T06:53:11Z",
        "merged_at": "2022-03-26T06:53:11Z",
        "body": ".gitignore add  appendonlydir-* ",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 227,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2022-03-22T08:29:58Z",
        "closed_at": "2022-03-22T15:38:02Z",
        "merged_at": "2022-03-22T15:38:02Z",
        "body": "this should aid find the CI issues with freebsd and macos runs, and also\r\nget faster results from valgrind and tls",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2022-03-22T06:28:32Z",
        "closed_at": "2022-03-30T18:16:04Z",
        "merged_at": "2022-03-30T18:16:03Z",
        "body": "Use exit code 1 on error in redis-cli.\r\n\r\nOn error, redis-cli was returning `REDIS_ERR` on some cases by mistake. `REDIS_ERR` is `-1` which becomes `255` as exit code. This PR changes it and returns `1` on errors to be consistent. \r\n\r\nRelated discussion: https://github.com/redis/redis/issues/10437",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 80,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2022-03-22T05:33:49Z",
        "closed_at": "2022-03-22T11:31:37Z",
        "merged_at": null,
        "body": "Related issues: #9561 #10341 #9187 #10162 #10442\r\nFix pages not being zeroed correctly cause crashes on qemu.\r\nThese codes copy from https://github.com/jemalloc/jemalloc/pull/2005.\r\n\r\n\r\n@oranagra I need to take back my words in https://github.com/redis/redis/issues/10341#issuecomment-1055126227\r\nWhen detected `MADV_DONTNEED` zeroes pages, `madvise_dont_need_zeros_is_faulty` will be set to `true` and `pages_purge_forced()` will alway return `true`, at this time, jemalloc will use `memset` to force the page to be zeroed.\r\n\r\nReproduce steps:\r\n1) Start redis docker\r\n```sh\r\ndocker run -p 6379:6379 --name redis -e ALLOW_EMPTY_PASSWORD=yes bitnami/redis:6.0.16\r\n```\r\n\r\n2) Run benchmark\r\n```sh\r\nredis-benchmark -p 6380 -t zadd -r 100000\r\n```\r\n\r\nRedis will crash with follow logs\r\n```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n87720:M 22 Mar 2022 04:04:20.900 # Redis 255.255.255 crashed by signal: 11, si_code: 1\r\n87720:M 22 Mar 2022 04:04:20.901 # Accessing address: 0x73696465522022\r\n87720:M 22 Mar 2022 04:04:20.901 # Crashed running the instruction at: 0x400006e6e3\r\n\r\n------ STACK TRACE ------\r\nEIP:\r\n./src/redis-server 127.0.0.1:6380(dictSdsKeyCompare+0x33)[0x400006e6e3]\r\n...\r\nqemu: uncaught target signal 11 (Segmentation fault) - core dumped\r\nSegmentation fault\r\n```\r\n\r\nAfter this pr, when we start redis in qemu, we will see the following log in the startup log:\r\n```\r\n<jemalloc>: MADV_DONTNEED does not work (memset will be used instead)\r\n<jemalloc>: (This is the expected behaviour if you are running under QEMU)\r\n87730:C 22 Mar 2022 05:39:56.866 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n87730:C 22 Mar 2022 05:39:56.867 # Redis version=255.255.255, bits=64, commit=a6941ebd, modified=1, pid=87730, just started\r\n```\r\n\r\nFully CI: https://github.com/sundb/redis/actions/runs/2019868398\r\nBTW, this PR has been tested pass in m1 docker with qemu.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-03-22T02:34:55Z",
        "closed_at": "2022-03-22T08:46:16Z",
        "merged_at": "2022-03-22T08:46:16Z",
        "body": "Resolve [issue 10462](https://github.com/redis/redis/issues/10462).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-03-21T15:15:12Z",
        "closed_at": "2022-03-30T13:16:21Z",
        "merged_at": "2022-03-30T13:16:21Z",
        "body": "Bumps [actions/cache](https://github.com/actions/cache) from 2 to 3.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/cache/releases\">actions/cache's releases</a>.</em></p>\n<blockquote>\n<h2>v3.0.0</h2>\n<ul>\n<li>\n<p>This change adds a minimum runner version(node12 -&gt; node16), which can break users using an out-of-date/fork of the runner. This would be most commonly affecting users on GHES 3.3 or before, as those runners do not support node16 actions and they can use actions from github.com via <a href=\"https://docs.github.com/en/enterprise-server@3.0/admin/github-actions/managing-access-to-actions-from-githubcom/enabling-automatic-access-to-githubcom-actions-using-github-connect\">github connect</a> or manually copying the repo to their GHES instance.</p>\n</li>\n<li>\n<p>Few dependencies and cache action usage examples have also been updated.</p>\n</li>\n</ul>\n<h2>v2.1.7</h2>\n<p>Support 10GB cache upload using the latest version <code>1.0.8</code> of <a href=\"https://www.npmjs.com/package/@actions/cache\"><code>@actions/cache</code> </a></p>\n<h2>v2.1.6</h2>\n<ul>\n<li>Catch unhandled &quot;bad file descriptor&quot; errors that sometimes occurs when the cache server returns non-successful response (<a href=\"https://github-redirect.dependabot.com/actions/cache/pull/596\">actions/cache#596</a>)</li>\n</ul>\n<h2>v2.1.5</h2>\n<ul>\n<li>Fix permissions error seen when extracting caches with GNU tar that were previously created using BSD tar (<a href=\"https://github-redirect.dependabot.com/actions/cache/issues/527\">actions/cache#527</a>)</li>\n</ul>\n<h2>v2.1.4</h2>\n<ul>\n<li>Make caching more verbose <a href=\"https://github-redirect.dependabot.com/actions/toolkit/pull/650\">#650</a></li>\n<li>Use GNU tar on macOS if available <a href=\"https://github-redirect.dependabot.com/actions/toolkit/pull/701\">#701</a></li>\n</ul>\n<h2>v2.1.3</h2>\n<ul>\n<li>Upgrades <code>@actions/core</code> to v1.2.6 for <a href=\"https://github.com/advisories/GHSA-mfwh-5m23-j46w\">CVE-2020-15228</a>. This action was not using the affected methods.</li>\n<li>Fix error handling in <code>uploadChunk</code> where 400-level errors were not being detected and handled correctly</li>\n</ul>\n<h2>v2.1.2</h2>\n<ul>\n<li>Adds input to limit the chunk upload size, useful for self-hosted runners with slower upload speeds</li>\n<li>No-op when executing on GHES</li>\n</ul>\n<h2>v2.1.1</h2>\n<ul>\n<li>Update <code>@actions/cache</code> package to <code>v1.0.2</code> which allows cache action to use posix format when taring files.</li>\n</ul>\n<h2>v2.1.0</h2>\n<ul>\n<li>Replaces the <code>http-client</code> with the Azure Storage SDK for NodeJS when downloading cache content from Azure.  This should help improve download performance and reliability as the SDK downloads files in 4 MB chunks, which can be parallelized and retried independently</li>\n<li>Display download progress and speed</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/actions/cache/commit/4b0cf6cc4619e737324ddfcec08fff2413359514\"><code>4b0cf6c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/actions/cache/issues/769\">#769</a> from actions/users/ashwinsangem/bump_major_version</li>\n<li><a href=\"https://github.com/actions/cache/commit/60c606a2b4c5358e11c2ca7b4694e59049d008d1\"><code>60c606a</code></a> Update licensed files</li>\n<li><a href=\"https://github.com/actions/cache/commit/b6e9a919a7da3606e9b2db756823ee1c39c7b48d\"><code>b6e9a91</code></a> Revert &quot;Updated to the latest version.&quot;</li>\n<li><a href=\"https://github.com/actions/cache/commit/c8425035834f98c304ecf92f5d50f41d433885c1\"><code>c842503</code></a> Updated to the latest version.</li>\n<li><a href=\"https://github.com/actions/cache/commit/2b7da2a62c3af9fa2692cd8d2d117da76faf31ac\"><code>2b7da2a</code></a> Bumped up to a major version.</li>\n<li><a href=\"https://github.com/actions/cache/commit/deae296ab340574da1ec86242984dfc91f0a7b81\"><code>deae296</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/actions/cache/issues/651\">#651</a> from magnetikonline/fix-golang-windows-example</li>\n<li><a href=\"https://github.com/actions/cache/commit/c7c46bcb6db3c571021a3a2dc2d2557b512ecace\"><code>c7c46bc</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/actions/cache/issues/707\">#707</a> from duxtland/main</li>\n<li><a href=\"https://github.com/actions/cache/commit/6535c5fb5fe2870754afba7bd4e514867ac9cb98\"><code>6535c5f</code></a> Regenerated <code>examples.md</code> TOC</li>\n<li><a href=\"https://github.com/actions/cache/commit/3fdafa472e0db16435add384585aa138ffdd16d3\"><code>3fdafa4</code></a> Update GitHub Actions status badge markdown in <code>README.md</code></li>\n<li><a href=\"https://github.com/actions/cache/commit/341e6d75d9826beb2fa659263d862f6aec63a064\"><code>341e6d7</code></a> Merge branch 'actions:main' into fix-golang-windows-example</li>\n<li>Additional commits viewable in <a href=\"https://github.com/actions/cache/compare/v2...v3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/cache&package-manager=github_actions&previous-version=2&new-version=3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-03-20T16:41:53Z",
        "closed_at": "2022-03-20T19:08:16Z",
        "merged_at": null,
        "body": "The default behaviour has changed.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-03-20T12:48:04Z",
        "closed_at": "2022-03-22T15:37:34Z",
        "merged_at": "2022-03-22T15:37:34Z",
        "body": "Avoid printing \"Killed by PID\" when si_code != SI_USER.\r\nApparently SI_USER isn't always set to 0. e.g. on Mac it's 0x10001 and the check that did `<=` was wrong.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2022-03-20T08:34:36Z",
        "closed_at": "2022-03-27T09:03:38Z",
        "merged_at": "2022-03-27T09:03:38Z",
        "body": "Did some cleanups:\r\n1. local local typo\r\n2. replace the only slave word in the file\r\n3. add FUNCTION FLUSH to `lazyfree-lazy-user-flush` description\r\n4. thought it would be better to use these, there are actually \"four\" options\r\n5. the the typo\r\n6. remove a extra space\r\n7. change comment next to `activedefrag no` to match the default value",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2022-03-19T00:49:50Z",
        "closed_at": "2022-03-21T05:08:40Z",
        "merged_at": "2022-03-21T05:08:40Z",
        "body": "Fixes two bugs:\r\n1. We weren't correctly using tls port, like at all, now we are correctly choosing the port. (Test was correct though)\r\n2. Some of the replication related timing tests weren't succeeding, so extending the wait_for made them much more reliable.\r\n\r\nRelated test run: https://github.com/redis/redis/actions/runs/2007096938",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 50,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-03-18T20:08:29Z",
        "closed_at": "2022-07-04T15:31:13Z",
        "merged_at": "2022-07-04T15:31:13Z",
        "body": "Added tests for slot migration mentioned in https://github.com/redis/redis/issues/9950\r\nThe PR includes test for the ASK and MOVED responses.\r\nSending a request containing multiple keys from the same slot, sent to a slot which is not stable does not trigger the UNSTABLE response. That part is commented out in the PR. \r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 160,
        "deletions": 10,
        "changed_files": 5,
        "created_at": "2022-03-18T14:35:24Z",
        "closed_at": "2022-04-08T20:41:11Z",
        "merged_at": null,
        "body": "This is a new command which extends existing \"persist\" command.\r\nThis command allows client to remove the timeout for multiple keys in a single call,\r\nand return the number of keys which are removed the timeout successfully.\r\n\r\n\r\n ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-03-18T06:46:15Z",
        "closed_at": "2022-03-18T12:10:25Z",
        "merged_at": "2022-03-18T12:10:25Z",
        "body": "When `::singledb` is 0, we will use db 9 for the test db.\r\nSince `::singledb` is set to 1 in the cluster-related tests, but not restored, some subsequent tests associated with db 9 will fail.\r\n\r\nFor example:\r\n```tcl\r\n# unit/moduleapi/keyspace_events.tcl\r\ntest \"Keyspace notifications: module events test\" {\r\n    ...\r\n   assert_equal {pmessage * __keyspace@9__:x notify} [$rd1 read]\r\n    ...\r\n}\r\n```\r\n\r\n```tcl\r\n# unit/moduleapi/hooks\r\ntest {Test flushdb hooks} {\r\n    r flushdb\r\n    assert_equal [r hooks.event_last flush-start] 9\r\n    assert_equal [r hooks.event_last flush-end] 9\r\n    r flushall\r\n    assert_equal [r hooks.event_last flush-start] -1\r\n    assert_equal [r hooks.event_last flush-end] -1\r\n}\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-03-18T05:56:49Z",
        "closed_at": "2022-03-20T13:18:54Z",
        "merged_at": "2022-03-20T13:18:53Z",
        "body": "fix #10439. see https://github.com/redis/redis/pull/9872\r\nWhen executing SHUTDOWN we pause the client so we can un-pause it if the shutdown fails.\r\nthis could happen during the timeout, if the shutdown is aborted, but could also happen from withing the initial `call()` to shutdown, if the rdb save fails.\r\nin that case when we return to `call()`, we'll crash if `c->cmd` has been set to NULL.\r\n\r\nThe call stack is:\r\n```\r\nunblockClient(c)\r\nreplyToClientsBlockedOnShutdown()\r\ncancelShutdown()\r\nfinishShutdown()\r\nprepareForShutdown()\r\nshutdownCommand()\r\n```\r\n\r\nwhat's special about SHUTDOWN in that respect is that it can be paused, and then un-paused before the original `call()` returns.\r\ntests where added for both failed shutdown, and a followup successful one.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-03-17T13:21:01Z",
        "closed_at": "2022-03-21T11:40:02Z",
        "merged_at": "2022-03-21T11:40:02Z",
        "body": "Use exit code 1 if redis-cli fails to connect.\r\n\r\nBefore https://github.com/redis/redis/pull/10382/, on a connection failure, exit code would be 1.  After this PR, whether connection is established or not, `noninteractive()` return value is used as the exit code. On a failure, this function returns `REDIS_ERR` which is `-1`. It becomes `255` as exit codes are between `0-255`.\r\n\r\nThere is nothing wrong by returning 1 or 255 on failure as far as I know but it'll break things that expect to see 1 as exit code on a connection failure. This is also how we realized the issue. With this PR, changing behavior back to using 1 as exit code to preserve backward compatibility. \r\n\r\nOpened https://github.com/redis/redis/issues/10437, to discuss further improvement for exit codes. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-03-17T08:54:03Z",
        "closed_at": "2022-03-29T12:45:15Z",
        "merged_at": "2022-03-29T12:45:14Z",
        "body": "Affects `--cluster create` and `--cluster add-node`.\r\n\r\nFixes #10433.\r\n\r\nThere are no tests added. (Is it possible without setting up DNS records? Can we create a hosts file in github actions?)\r\n\r\n@liuchong can you test this with your nodes? (`redis-cli --cluster create redis-node-1:7001 redis-node-2:7002 redis-node-3:7003 redis-node-4:7004 redis-node-5:7005 redis-node-6:7006 --cluster-replicas 1`)",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2022-03-16T18:39:50Z",
        "closed_at": "2022-03-16T21:07:24Z",
        "merged_at": "2022-03-16T21:07:24Z",
        "body": "Resolves the issue brought up here, https://github.com/redis/redis/pull/10293#discussion_r827669805. What I found is that the TCL function \"restart_instance\" only restarts the instance if the node was actually previously killed, but also mangles the global structure if the node wasn't killed so a subsequent \"restart_instance\" will fail. The usage in the cluster shards test was correct, but the usage in endpoints.tcl was wrong, it was not actually restarting the instance.\r\n\r\nFixing the usage in endpoints.tcl, it was revealed the underlying code was incorrect. It was always loading in NULL from the nodes.conf file, so that was fixed so now the tests passes. One last change is that in endpoints.tcl, a `config rewrite` is called in order to persist the \"cluster-announce-endpoint\" setting, since that will overwrite the value of \"myself->node_name\" in the cluster struct on startup.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 56,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2022-03-14T02:18:36Z",
        "closed_at": "2022-03-16T01:21:14Z",
        "merged_at": "2022-03-16T01:21:14Z",
        "body": "* Add a new module API for redacting client arguments, which mirrors our internal implementation for AUTH. Intended use is for implementing custom authentication commands.\r\n* Hardened the redact API to make it handle more cases.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-03-13T14:58:22Z",
        "closed_at": "2022-03-14T09:13:15Z",
        "merged_at": "2022-03-14T09:13:14Z",
        "body": "We need to wait for `sentinelTimer` to kick in, and then\r\ntrigger the reconnect.\r\n\r\nAs for another change, i think we should better call\r\n`server_set_password` before calling SENTINEL SET auth-pass.\r\n\r\nIntroduced in #10400",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-03-10T15:25:51Z",
        "closed_at": "2022-03-13T06:27:41Z",
        "merged_at": "2022-03-13T06:27:41Z",
        "body": "It maybe a copy-paste error, the getname part occurs twice, this pr remove the redundancy one",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-03-10T14:53:52Z",
        "closed_at": "2022-03-13T14:19:42Z",
        "merged_at": "2022-03-13T14:19:42Z",
        "body": "I think it's more rational to check the `monitors` list instead of `server.monitors` in the function, although they are basically the same in the context.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 58,
        "deletions": 61,
        "changed_files": 6,
        "created_at": "2022-03-10T10:50:49Z",
        "closed_at": "2022-03-25T02:45:41Z",
        "merged_at": "2022-03-25T02:45:41Z",
        "body": "It's a long story about network optimization in redis, you can skip the background.\r\n\r\n# Background\r\n\r\nIn issue #5229 @oranagra  found a problem that **slave hung in processUnblockedClients**. One reason is that in replication stream master pipeline to replica, and when processing pipeline redis spend too many time on `memmove`, because redis `sdsrange` querybuf after every command processed.\r\n\r\nTo fix it in PR #5244 I moved `sdsrange` out of `processMultibulkBuffer`, in other words we only call `sdsrange` after all commands processed in `c->querybuf`.\r\n\r\nBut when implement `meaningful replication offset` feature(which is already discarded) in 6.0, we met some problems, and @oranagra tried to fix it in #7143, but commit 4447ddc8bb36879db9fe49498165b360bf35ba1b introduced a new problem about `pending_querybuf`... after this commit `pending_querybuf` does `sdsrange` after every command processed, that means too many `memmove` appears just like issue #5229, and seems @madolson met the problem in https://github.com/redis/redis/pull/5284#issuecomment-1063132766.\r\n\r\n# Target\r\n\r\nThe `pending_querybuf` consumes unnecessary memory(it's a copy of `querybuf`) and `sdsrange()` calls, I tried remove it in PR #5284 but didn't finish.\r\n\r\nNow I think we can optimize and must fix both memory usage problem and `sdsrange` problem on `pending_querybuf`.\r\n\r\nThe main idea is reuse `querybuf`, we know the `pending_querybuf` is a copy of `querybuf` just used to send data to replica, see the details in `commandProcessed` below:\r\n\r\n```c\r\nvoid commandProcessed(client *c) {\r\n    if (c->flags & CLIENT_BLOCKED) return;\r\n...\r\n    long long prev_offset = c->reploff;\r\n    if (c->flags & CLIENT_MASTER && !(c->flags & CLIENT_MULTI)) {\r\n        /* Update the applied replication offset of our master. */\r\n        c->reploff = c->read_reploff - sdslen(c->querybuf) + c->qb_pos;\r\n    }\r\n...\r\n    if (c->flags & CLIENT_MASTER) {\r\n        long long applied = c->reploff - prev_offset;\r\n        if (applied) {\r\n            replicationFeedStreamFromMasterStream(c->pending_querybuf,applied);\r\n            sdsrange(c->pending_querybuf,applied,-1);\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n# Implementation\r\n\r\nTo remove `pending_querybuf`, the key point is reusing `querybuf`, it means master client's `querybuf` is not only used to parse command, but also proxy to sub-replicas.\r\n\r\n1. add a new variable `repl_applied` for master client to record how many data applied (propagated via `replicationFeedStreamFromMasterStream()`) but not trimmed in `querybuf`.\r\n\r\n2. don't sdsrange `querybuf` in `commandProcessed()`, we trim it to `repl_applied` after the whole replication pipeline processed to avoid fragmented `sdsrange`. And here are some scenarios we cannot trim to `qb_pos`:\r\n    * we don't receive complete command from master\r\n    * master client blocked because of client pause\r\n    * IO threads operate read, master client flagged with CLIENT_PENDING_COMMAND\r\n\r\n    In these scenarios, `qb_pos` points to the part of the current command or the beginning of next command, and the current command is not applied yet, so the `repl_applied` is not equal to `qb_pos`.\r\n\r\nSome other notes:\r\n* Do not do big arg optimization on master client, since we can only sdsrange `querybuf` after data sent to replicas.\r\n* Set `qb_pos` and `repl_applied` to 0 when `freeClient` in `replicationCacheMaster`.\r\n* Rewrite `processPendingCommandsAndResetClient` to `processPendingCommandAndInputBuffer`, let `processInputBuffer` to be called successively after `processCommandAndResetClient`.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-03-10T06:46:12Z",
        "closed_at": "2022-03-10T07:52:49Z",
        "merged_at": "2022-03-10T07:52:49Z",
        "body": "### Changes\r\nchange \"disable-thp\" config from modifiable to immutable\r\n### Reason\r\nIt's confusing for this config to be modifiable since it only takes effect on startup.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2022-03-10T03:34:29Z",
        "closed_at": "2022-03-14T06:22:57Z",
        "merged_at": "2022-03-14T06:22:57Z",
        "body": "For an integer string like \"123456789012345678901\" which could cause overflow-failure in string2ll() conversion, \r\nwe could compare its length at the beginning to avoid extra work. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-03-09T14:18:14Z",
        "closed_at": "2022-03-10T08:08:41Z",
        "merged_at": "2022-03-10T08:08:41Z",
        "body": "As a result we segfault when parsing and matching the command keys:\r\n\r\nexample:\r\n\r\n>acl setuser FOO +@all ~v*\r\nOK\r\n>acl dryrun FOO eval <-- cause exception\r\n\r\n```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n88064:M 09 Mar 2022 14:11:56.706 # Redis 255.255.255 crashed by signal: 11, si_code: 1\r\n88064:M 09 Mar 2022 14:11:56.706 # Accessing address: 0x29\r\n88064:M 09 Mar 2022 14:11:56.706 # Crashed running the instruction at: 0x46399a\r\n\r\n------ STACK TRACE ------\r\nEIP:\r\n./src/redis-server *:6379(getKeysUsingKeySpecs+0xea)[0x46399a]\r\n\r\nBacktrace:\r\n/lib64/libpthread.so.0(+0x118e0)[0x7f879a8758e0]\r\n./src/redis-server *:6379(getKeysUsingKeySpecs+0xea)[0x46399a]\r\n./src/redis-server *:6379(getKeysFromCommandWithSpecs+0xf4)[0x463d44]\r\n./src/redis-server *:6379[0x4dea8f]\r\n./src/redis-server *:6379(aclCommand+0xa95)[0x4e3485]\r\n./src/redis-server *:6379(call+0x96)[0x43fb66]\r\n./src/redis-server *:6379(processCommand+0x9f6)[0x442656]\r\n./src/redis-server *:6379(processCommandAndResetClient+0x1c)[0x455dfc]\r\n./src/redis-server *:6379(processInputBuffer+0xc0)[0x458420]\r\n./src/redis-server *:6379(readQueryFromClient+0x241)[0x45b271]\r\n./src/redis-server *:6379[0x4e5565]\r\n./src/redis-server *:6379(aeProcessEvents+0x19d)[0x43888d]\r\n./src/redis-server *:6379(aeMain+0x18)[0x438c08]\r\n./src/redis-server *:6379(main+0x301)[0x435141]\r\n/lib64/libc.so.6(__libc_start_main+0xea)[0x7f879a4da13a]\r\n./src/redis-server *:6379(_start+0x2a)[0x4355ba]\r\n\r\n```\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-03-09T08:35:10Z",
        "closed_at": "2022-03-09T11:32:03Z",
        "merged_at": "2022-03-09T11:32:03Z",
        "body": "Fail CI: https://github.com/redis/redis/runs/5473064515?check_suite_focus=true\r\n`c->buf` is not `sds`, so we should use `dismissMemory` instead of `dismissSds` to dismiss it.\r\nFullly CI: https://github.com/sundb/redis/actions/runs/1955336170",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 9,
        "changed_files": 7,
        "created_at": "2022-03-09T08:19:53Z",
        "closed_at": "2022-03-09T11:58:23Z",
        "merged_at": "2022-03-09T11:58:23Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 41,
        "changed_files": 3,
        "created_at": "2022-03-09T07:27:54Z",
        "closed_at": "2022-03-15T12:18:23Z",
        "merged_at": "2022-03-15T12:18:23Z",
        "body": "In a benchmark done by @filipecosta90 he noticed we spend a relatively long time updating the client memory usage leading to performance degradation.\r\nBefore #8687 this was performed in the client's cron and didn't affect performance. But since introducing client eviction we need to perform this after filling the input buffers and after processing commands. This also lead me to write this code to be thread safe and perform it in the i/o threads.\r\n\r\nIt turns out that the main performance issue here is related to atomic operations being performed while updating the total clients memory usage stats used for client eviction (`server.stat_clients_type_memory[]`). This update needed to be atomic because `updateClientMemUsage()` was called from the IO threads.\r\n\r\nIn this PR I make sure to call `updateClientMemUsage()` only from the main thread. In case of threaded IO I call it for each client during the \"fan-in\" phase of the read/write operation. This also means I could chuck the `updateClientMemUsageBucket()` function which was called during this phase and embed it into `updateClientMemUsage()`.\r\n\r\nProfiling shows this makes `updateClientMemUsage()` (on my x86_64 linux) roughly x4 faster.\r\n\r\nThis is still a WIP since I need to clean up the code and it requires some attention during review to make sure all threading issues are resolved and client eviction wasn't broken in any way including when running with read/write io threads.\r\n\r\nAttached are my profiling flame graphs.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 187,
        "deletions": 8,
        "changed_files": 3,
        "created_at": "2022-03-09T06:20:14Z",
        "closed_at": "2022-03-13T08:13:47Z",
        "merged_at": "2022-03-13T08:13:47Z",
        "body": "When updating SENTINEL with master\u2019s new password (command:\r\n`SENTINEL SET mymaster auth-pass some-new-password`), \r\nsentinel might still keep the old connection and avoid reconnecting \r\nwith the new password. This is because of wrong logic that traces \r\nthe last ping (pong) time to servers. In fact it worked fine until 8631e64 \r\nchanged the condition to send ping. To resolve it with minimal risk, \r\nlet\u2019s disconnect master and replicas once changing password/user. \r\n\r\nBased on earlier work of: [yz1509](https://github.com/yz1509)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 11,
        "changed_files": 7,
        "created_at": "2022-03-09T04:33:47Z",
        "closed_at": "2022-03-09T11:55:17Z",
        "merged_at": "2022-03-09T11:55:17Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 55,
        "changed_files": 10,
        "created_at": "2022-03-08T21:51:56Z",
        "closed_at": "2022-04-05T07:36:06Z",
        "merged_at": "2022-04-05T07:36:06Z",
        "body": "The command json documents should just include information about the \"arguments\" and the \"outputs\". I removed all of the 'functional wording' so it's clear. Maybe we should document the history field somewhere too?\r\n\r\nto do:\r\n- [x] add all of this information manually to the docs once we have consensus here",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-03-08T18:29:34Z",
        "closed_at": "2022-03-08T20:52:54Z",
        "merged_at": "2022-03-08T20:52:54Z",
        "body": "Typo in conf file comment.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2022-03-08T12:07:18Z",
        "closed_at": "2022-03-10T07:51:55Z",
        "merged_at": "2022-03-10T07:51:55Z",
        "body": "The described capacity\r\n `to schedule a new BGSAVE if there are slaves that attached while a BGSAVE was in progress`\r\nwas moved to `checkChildrenDone()`  named by `replicationStartPendingFork` according to https://github.com/redis/redis/pull/6271\r\n\r\nBut the comment was not changed, may misleading others.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-03-08T08:26:53Z",
        "closed_at": "2022-03-08T11:10:24Z",
        "merged_at": "2022-03-08T11:10:24Z",
        "body": "Currently, CLUSTER NODES is parsed and was not done correctly for IPv6\r\naddresses.\r\n\r\nFixes #10251 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-03-08T05:50:38Z",
        "closed_at": "2022-03-08T11:35:37Z",
        "merged_at": "2022-03-08T11:35:37Z",
        "body": "introduced in #10147 since we blocked the first-arg mechanism on subcommands",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 57,
        "deletions": 31,
        "changed_files": 3,
        "created_at": "2022-03-08T03:44:25Z",
        "closed_at": "2022-04-05T05:51:52Z",
        "merged_at": "2022-04-05T05:51:52Z",
        "body": "# Problem\r\n\r\nFor 'cluster forget':\r\n```c\r\nclusterNode *n = clusterLookupNode(c->argv[2]->ptr);\r\n```\r\nCall link:\r\n+ clusterLookupNode(c->argv[2]->ptr)\r\n  + sdsnewlen(name, CLUSTER_NAMELEN)\r\n    + _sdsnewlen(init, initlen, 0);\r\n        + memcpy(s, init, initlen);\r\n\r\nIf the length of 'c->argv[2]->ptr' is not CLUSTER_NAMELEN, eg. less than CLUSTER_NAMELEN, memcpy will copy the data which overflow the size of init, I don't think this is a standard way to use it. Therefore, I think redis should judge the length of node id when using.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 36,
        "changed_files": 5,
        "created_at": "2022-03-07T15:21:27Z",
        "closed_at": "2022-03-30T13:18:03Z",
        "merged_at": "2022-03-30T13:18:03Z",
        "body": "[//]: # (dependabot-start)\n\u26a0\ufe0f  **Dependabot is rebasing this PR** \u26a0\ufe0f \n\nRebasing might not happen immediately, so don't worry if this takes some time.\n\nNote: if you make any changes to this PR yourself, they will take precedence over the rebase.\n\n---\n\n[//]: # (dependabot-end)\n\nBumps [actions/checkout](https://github.com/actions/checkout) from 2 to 3.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/checkout/releases\">actions/checkout's releases</a>.</em></p>\n<blockquote>\n<h2>v3.0.0</h2>\n<ul>\n<li>Update default runtime to node16</li>\n</ul>\n<h2>v2.4.0</h2>\n<ul>\n<li>Convert SSH URLs like <code>org-&lt;ORG_ID&gt;@github.com:</code> to <code>https://github.com/</code> - <a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/621\">pr</a></li>\n</ul>\n<h2>v2.3.5</h2>\n<p>Update dependencies</p>\n<h2>v2.3.4</h2>\n<ul>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/379\">Add missing <code>await</code>s</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/360\">Swap to Environment Files</a></li>\n</ul>\n<h2>v2.3.3</h2>\n<ul>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/345\">Remove Unneeded commit information from build logs</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/326\">Add Licensed to verify third party dependencies</a></li>\n</ul>\n<h2>v2.3.2</h2>\n<p><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/320\">Add Third Party License Information to Dist Files</a></p>\n<h2>v2.3.1</h2>\n<p><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/284\">Fix default branch resolution for .wiki and when using SSH</a></p>\n<h2>v2.3.0</h2>\n<p><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/278\">Fallback to the default branch</a></p>\n<h2>v2.2.0</h2>\n<p><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/258\">Fetch all history for all tags and branches when fetch-depth=0</a></p>\n<h2>v2.1.1</h2>\n<p>Changes to support GHES (<a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/236\">here</a> and <a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/248\">here</a>)</p>\n<h2>v2.1.0</h2>\n<ul>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/191\">Group output</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/199\">Changes to support GHES alpha release</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/184\">Persist core.sshCommand for submodules</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/163\">Add support ssh</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/179\">Convert submodule SSH URL to HTTPS, when not using SSH</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/157\">Add submodule support</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/144\">Follow proxy settings</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/141\">Fix ref for pr closed event when a pr is merged</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/128\">Fix issue checking detached when git less than 2.22</a></li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/checkout/blob/main/CHANGELOG.md\">actions/checkout's changelog</a>.</em></p>\n<blockquote>\n<h1>Changelog</h1>\n<h2>v2.3.1</h2>\n<ul>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/284\">Fix default branch resolution for .wiki and when using SSH</a></li>\n</ul>\n<h2>v2.3.0</h2>\n<ul>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/278\">Fallback to the default branch</a></li>\n</ul>\n<h2>v2.2.0</h2>\n<ul>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/258\">Fetch all history for all tags and branches when fetch-depth=0</a></li>\n</ul>\n<h2>v2.1.1</h2>\n<ul>\n<li>Changes to support GHES (<a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/236\">here</a> and <a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/248\">here</a>)</li>\n</ul>\n<h2>v2.1.0</h2>\n<ul>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/191\">Group output</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/199\">Changes to support GHES alpha release</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/184\">Persist core.sshCommand for submodules</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/163\">Add support ssh</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/179\">Convert submodule SSH URL to HTTPS, when not using SSH</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/157\">Add submodule support</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/144\">Follow proxy settings</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/141\">Fix ref for pr closed event when a pr is merged</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/128\">Fix issue checking detached when git less than 2.22</a></li>\n</ul>\n<h2>v2.0.0</h2>\n<ul>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/108\">Do not pass cred on command line</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/107\">Add input persist-credentials</a></li>\n<li><a href=\"https://github-redirect.dependabot.com/actions/checkout/pull/104\">Fallback to REST API to download repo</a></li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/actions/checkout/commit/a12a3943b4bdde767164f792f33f40b04645d846\"><code>a12a394</code></a> update readme for v3 (<a href=\"https://github-redirect.dependabot.com/actions/checkout/issues/708\">#708</a>)</li>\n<li><a href=\"https://github.com/actions/checkout/commit/8f9e05e482293f862823fcca12d9eddfb3723131\"><code>8f9e05e</code></a> Update to node 16 (<a href=\"https://github-redirect.dependabot.com/actions/checkout/issues/689\">#689</a>)</li>\n<li><a href=\"https://github.com/actions/checkout/commit/230611dbd0eb52da1e1f4f7bc8bb0c3a339fc8b7\"><code>230611d</code></a> Change secret name for PAT to not start with GITHUB_ (<a href=\"https://github-redirect.dependabot.com/actions/checkout/issues/623\">#623</a>)</li>\n<li>See full diff in <a href=\"https://github.com/actions/checkout/compare/v2...v3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/checkout&package-manager=github_actions&previous-version=2&new-version=3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-03-07T02:26:22Z",
        "closed_at": "2022-03-07T11:44:07Z",
        "merged_at": "2022-03-07T11:44:07Z",
        "body": "`Expected '*table size: 4096*' to match '*table size: 8192*'`\r\n\r\nThis test failed once on daily macOS, the reason is because\r\nthe bgsave has not stopped after the kill and `after 200`.\r\nSo there is a child process and no rehash triggered.\r\n\r\nThis commit use `waitForBgsave` to wait for it to finish.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-03-06T15:35:08Z",
        "closed_at": "2022-03-07T15:59:51Z",
        "merged_at": "2022-03-07T15:59:51Z",
        "body": "Add `DEPRECATED` doc_flag.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-03-06T08:57:10Z",
        "closed_at": "2022-03-06T11:02:36Z",
        "merged_at": "2022-03-06T11:02:36Z",
        "body": "Apparently using `\\x` produces different results between tclsh 8.5 and\r\n8.6, whereas `\\u` is more consistent.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-03-06T03:47:51Z",
        "closed_at": "2022-06-22T09:09:43Z",
        "merged_at": null,
        "body": "In `clusterSaveConfigOrDie`, we will call `clusterSaveConfig`,\r\nand if it fails, we will exit without any error logs.\r\nThe log format is from `sentinelFlushConfig`.\r\n\r\nclusterSaveConfigOrDie: \r\nhttps://github.com/redis/redis/blob/e3ef73dc2a557232c60c732705e8e6ff2050eba9/src/cluster.c#L432-L443\r\n\r\nsentinel log format:\r\nhttps://github.com/redis/redis/blob/e3ef73dc2a557232c60c732705e8e6ff2050eba9/src/sentinel.c#L2274-L2278",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-03-04T11:48:43Z",
        "closed_at": "2022-03-09T11:48:52Z",
        "merged_at": null,
        "body": "In #9798, QUIT became a command. In redis-cli intercative\r\nmode, we will exit directly if we match quit. Instead of\r\nsending the QUIT command to redis server.\r\n\r\nNow in this PR, we will send the real QUIT command to the\r\nredis sever. Execute it as a real command.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2022-03-04T02:48:34Z",
        "closed_at": "2022-03-10T16:20:02Z",
        "merged_at": "2022-03-10T16:20:02Z",
        "body": "The following usage will output an empty newline:\r\n```\r\n> redis-cli help set\r\nempty line\r\n```\r\n\r\nThe reason is that in interactive mode, we have called\r\n`cliInitHelp`, which initializes help.\r\n\r\nWhen using `redis-cli help xxx` or `redis-cli help ? xxx`,\r\nwe can't match the command due to empty `helpEntries`,\r\nso we output an empty newline.\r\n\r\nIn this commit, we will call `cliInitHelp` to init the help.\r\nNote that in this case, we need to call `cliInitHelp` (COMMAND DOCS)\r\nevery time, which i think is acceptable.\r\n\r\nSo now the output will look like:\r\n```\r\n[redis]# src/redis-cli help get\r\n\r\n  GET key\r\n  summary: Get the value of a key\r\n  since: 1.0.0\r\n  group: string\r\n\r\n[redis]#\r\n```\r\n\r\nFixes #10378\r\n\r\nThis PR also fix a redis-cli crash when using `--ldb --eval`:\r\n```\r\n[root]# src/redis-cli --ldb --eval test.lua test 1\r\nLua debugging session started, please use:\r\nquit    -- End the session.\r\nrestart -- Restart the script in debug mode again.\r\nhelp    -- Show Lua script debugging commands.\r\n\r\n* Stopped at 1, stop reason = step over\r\n-> 1   local num = redis.call('GET', KEYS[1]);\r\nredis-cli: redis-cli.c:718: cliCountCommands: Assertion\r\n`commandTable->element[i]->type == 1' failed.\r\nAborted\r\n```\r\nBecause in ldb mode, `COMMAND DOCS` or `COMMAND` will\r\nreturn an array, only with one element, and the type\r\nis `REDIS_REPLY_STATUS`, the result is `<error> Unknown\r\nRedis Lua debugger command or wrong number of arguments`.\r\n\r\nSo if we are in the ldb mode, and init the Redis HELP, we\r\nwill get the wrong response and crash the redis-cli.\r\nIn ldb mode we don't initialize HELP, help is only initialized\r\nafter the lua debugging session ends.\r\n\r\nIt was broken in #10043",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 124,
        "deletions": 43,
        "changed_files": 3,
        "created_at": "2022-03-03T23:24:24Z",
        "closed_at": "2022-03-16T17:11:39Z",
        "merged_at": "2022-03-16T17:11:38Z",
        "body": "After migrating a slot, send CLUSTER SETSLOT NODE to the destination\r\nnode first to make sure the slot isn't left without an owner in case\r\nthe destination node crashes before it is set as new owner.\r\n\r\nWhen informing the source node, it can happen that the destination\r\nnode has already informed it and if the source node has lost its\r\nlast slot, it has already turned itself into a replica. Redis-cli\r\nshould ignore this error in this case.\r\n\r\nFixes #7116. Fixes #9223.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-03-03T02:36:56Z",
        "closed_at": "2022-03-03T06:41:32Z",
        "merged_at": "2022-03-03T06:41:31Z",
        "body": "Cluster node name is not null terminated, so need to be constrained.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 8,
        "changed_files": 7,
        "created_at": "2022-03-02T16:45:11Z",
        "closed_at": "2022-03-09T14:40:27Z",
        "merged_at": "2022-03-09T14:40:27Z",
        "body": "* stats and latency commands have non-deterministic output.\r\n* the ones about latency should be sent to ALL_NODES (considering\r\n  reads from replicas)\r\n* the ones about running scripts and memory usage only to masters.\r\n* stats aggregation is SPECIAL (like in INFO)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 331,
        "deletions": 17,
        "changed_files": 17,
        "created_at": "2022-03-02T15:30:05Z",
        "closed_at": "2022-03-15T16:38:08Z",
        "merged_at": "2022-03-15T16:38:08Z",
        "body": "Currently, for all sentinel subcommand json file, we miss argument part. \r\nIn this PR, according to the command format, I just add argument part for the following sentinel subcommand:\r\n\r\nCKQUORUM\r\nDEBUG\r\nGET-MASTER-ADDR-BY-NAME\r\nFAILOVER\r\nINFO-CACHE\r\nIS-MASTER-DOWN-BY-ADDR\r\nMASTER\r\nMONITOR\r\nREMOVE\r\nREPLICAS\r\nRESET\r\nSENTINELS\r\nSET\r\nSIMULATE-FAILURE [CRASH-AFTER-ELECTION] [CRASH-AFTER-PROMOTION] [HELP]\r\n\r\nAnd I create 2 separated json file for CONFIG SET and  CONFIG GET commands\r\nThanks",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 256,
        "deletions": 57,
        "changed_files": 14,
        "created_at": "2022-03-02T15:00:51Z",
        "closed_at": "2022-03-22T12:13:28Z",
        "merged_at": "2022-03-22T12:13:28Z",
        "body": "The PR extends RM_Call with 3 new capabilities using new flags that are given to RM_Call as part of the `fmt` argument.\r\nIt aims to assist modules that are getting a list of commands to be executed from the user (not hard coded as part of the module logic), think of a module that implements a new scripting language...\r\n\r\n* `S` - Run the command in a script mode, this means that it will raise an error if a command which are not allowed inside a script (flaged with the `deny-script` flag) is invoked (like SHUTDOWN). In addition, on script mode, write commands are not allowed if there is not enough good replicas (as configured with `min-replicas-to-write`) and/or a disk error happened.\r\n\r\n* `W` - no writes mode, Redis will reject any command that is marked with `write` flag. Again can be useful to modules that implement a new scripting language and wants to prevent any write commands.\r\n\r\n* `E` - Return errors as RedisModuleCallReply. Today the errors that happened before the command was invoked (like unknown commands or acl error) return a NULL reply and set errno. This might be missing important information about the failure and it is also impossible to just pass the error to the user using RM_ReplyWithCallReply. This new flag allows you to get a RedisModuleCallReply object with the relevant error message and treat it as if it was an error that was raised by the command invocation.\r\n\r\nTests were added to verify the new code paths.\r\n\r\nIn addition small refactoring was done to share some code between modules, scripts, and `processCommand` function:\r\n1. `getAclErrorMessage` was added to `acl.c` to unified to log message extraction from the acl result\r\n2. `checkGoodReplicasStatus` was added to `replication.c` to check the status of good replicas. It is used on `scriptVerifyWriteCommandAllow`, `RM_Call`, and `processCommand`.\r\n3. `writeCommandsGetDiskErrorMessage` was added to `server.c` to get the error message on persistence failure. Again it is used on `scriptVerifyWriteCommandAllow`, `RM_Call`, and `processCommand`.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-03-02T11:21:07Z",
        "closed_at": "2022-03-08T13:17:15Z",
        "merged_at": "2022-03-08T13:17:15Z",
        "body": "since #9822, the static reply buffer is no longer part of the client structure, so we need to dismiss it.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 150,
        "deletions": 27,
        "changed_files": 8,
        "created_at": "2022-03-02T08:32:27Z",
        "closed_at": "2022-08-24T07:59:45Z",
        "merged_at": null,
        "body": "for modules that provide their own database snapshotting capabilities, the current api doesn't allow them to easily/cleanly include ACL state in their snapshots.\r\n\r\nThis is a small refactor of the ACLSaveToFile/ACLLoadFromFile to enable adding simple module APIs to use the same code path, but only save to a buffer.\r\n\r\nTODO:\r\n- [x] make it more \"redis module api like\"\r\n- [x] memory allocations should be able to associate it to module.\r\n- [ ] documentation\r\n- [x] test(s)",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-03-02T01:13:43Z",
        "closed_at": "2022-03-02T05:51:30Z",
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-03-01T09:13:08Z",
        "closed_at": "2022-03-08T14:53:12Z",
        "merged_at": "2022-03-08T14:53:12Z",
        "body": "In some special commands like `eval_ro`/`fcall_ro` we allow no-writes commands. But may-replicate commands are no-writes too, that leads crash when client pause write:\r\n```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n51328:M 01 Mar 2022 17:08:59.273 # === ASSERTION FAILED ===\r\n51328:M 01 Mar 2022 17:08:59.273 # ==> server.c:3036 '!(areClientsPaused() && !server.client_pause_in_transaction)' is not true\r\n\r\n------ STACK TRACE ------\r\n\r\nBacktrace:\r\n./redis-server *:6789[0x447127]\r\n./redis-server *:6789(propagatePendingCommands+0x72)[0x44ab82]\r\n./redis-server *:6789(afterCommand+0x33)[0x44ae53]\r\n./redis-server *:6789(call+0x31a)[0x44b17a]\r\n./redis-server *:6789(processCommand+0x990)[0x44cf90]\r\n./redis-server *:6789(processCommandAndResetClient+0x1c)[0x462a5c]\r\n./redis-server *:6789(processInputBuffer+0xd1)[0x465541]\r\n./redis-server *:6789(readQueryFromClient+0x238)[0x468558]\r\n./redis-server *:6789[0x4fd373]\r\n./redis-server *:6789(aeProcessEvents+0x235)[0x443735]\r\n./redis-server *:6789(aeMain+0x1d)[0x443a7d]\r\n./redis-server *:6789(main+0x406)[0x43fe16]\r\n/lib64/libc.so.6(__libc_start_main+0xf5)[0x7f509184f445]\r\n./redis-server *:6789[0x4401cd]\r\n\r\n...\r\n\r\n------ CURRENT CLIENT INFO ------\r\nid=3 addr=127.0.0.1:46466 laddr=127.0.0.1:6789 fd=7 name= age=3 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=67 qbuf-free=20407 argv-mem=44 multi-mem=0 rbs=1024 rbp=5 obl=4 oll=0 omem=0 tot-mem=22340 events=r cmd=eval_ro user=default redir=-1 resp=2\r\nargv[0]: '\"eval_ro\"'\r\nargv[1]: '\"return redis.call('publish','ch','msg')\"'\r\nargv[2]: '\"0\"'\r\n```",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 239,
        "deletions": 95,
        "changed_files": 3,
        "created_at": "2022-02-28T21:38:59Z",
        "closed_at": "2023-05-17T07:26:02Z",
        "merged_at": "2023-05-17T07:26:02Z",
        "body": "This PR extends `SENTINEL CONFIG SET` and  `SENTINEL CONFIG GET` to be compatible with variadic `CONFIG SET` and `CONFIG GET` and allow multiple parameters to be modified in a single call atomically.\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/234968495-9d76c5b8-fefd-4110-b842-4476a338c789.png)\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/235501102-da045ba7-3d1b-45d3-81a6-565931fe073d.png)\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/235520360-7fa42a70-48cf-4ce8-8494-ce176a39bf45.png)\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/235501918-b48699cd-03fa-478e-9680-502f3de866c6.png)\r\n\r\n**Duplicates are Ignored** \r\n\r\n![image](https://user-images.githubusercontent.com/51993843/235519779-f4ded108-8f29-4ea5-b784-bbc0950687ce.png)\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/236000297-6df83f89-dcfc-4e90-a679-fce82ca59849.png)\r\n\r\n",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 25,
        "changed_files": 5,
        "created_at": "2022-02-28T16:04:03Z",
        "closed_at": "2022-03-01T12:40:29Z",
        "merged_at": "2022-03-01T12:40:29Z",
        "body": "In order to resolve some flaky tests which hard rely on examine memory footprint.\r\nwe introduce the following fixes:\r\n\r\n# Fix in client-eviction test - by @yoav-steinberg \r\nSometime the libc allocator can use different size client struct allocations.\r\nthis may cause unexpected memory calculations to fail the test.\r\n\r\n# Introduce new DEBUG command for disabling reply buffer resizing\r\nIn order to eliminate reply buffer resizing during specific tests.\r\nwe introduced the ability to disable (and enable) the resizing cron job",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-02-28T11:58:11Z",
        "closed_at": "2022-03-01T04:26:58Z",
        "merged_at": "2022-03-01T04:26:58Z",
        "body": "Before the change\r\n\r\n```\r\n127.0.0.1:6379> acl dryrun hp set r 5\r\n(error) NOPERM this user has no permissions to run the 'acl|dryrun' command\r\n```\r\n\r\nAfter the change\r\n\r\n```\r\n127.0.0.1:6379> acl dryrun hp set r 5\r\n\"This user has no permissions to run the 'set' command\"\r\n```",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 8139,
        "deletions": 1923,
        "changed_files": 144,
        "created_at": "2022-02-28T09:40:36Z",
        "closed_at": "2022-02-28T13:35:46Z",
        "merged_at": "2022-02-28T13:35:46Z",
        "body": "New Features\r\n============\r\n\r\n* Add stream consumer group lag tracking and reporting (#9127)\r\n* Add API for functions and eval Lua scripts to check ACL explicitly (#10220)\r\n\r\nNew user commands or command arguments\r\n--------------------------------------\r\n\r\n* COMMAND GETKEYSANDFLAGS sub-command (#10237)\r\n* INFO command can take multiple section arguments (#6891)\r\n* XGROUP CREATE and SETID: new ENTRIESREAD optional argument (#9127)\r\n* XSETID new ENTRIESADDED and MAXDELETEDID optional arguments (#9127)\r\n\r\nCommand replies that have been extended\r\n---------------------------------------\r\n\r\n* XINFO reports consumer group lag and a few other fields (#9127)\r\n* XAUTOCLAIM returns a new element with a list of deletes IDs (#10227)\r\n\r\nPotentially Breaking Changes\r\n============================\r\n\r\n* X[AUTO]CLAIM skips deleted entries instead of replying with Nil, and deletes\r\n  them from the pending entry list (#10227)\r\n* Fix messed up error codes returned from EVAL scripts (#10218, #10329)\r\n* COMMAND INFO, Renames key-spec \"CHANNEL\" flag to be \"NOT_KEY\" (#10299)\r\n\r\nPerformance and resource utilization improvements\r\n=================================================\r\n\r\n* Reduce system calls and small packets for client replies (#9934)\r\n* Reduce memory usage of stale clients (#9822)\r\n* Fix regression in Z[REV]RANGE commands (by-rank) introduced in Redis 6.2 (#10337)\r\n\r\nChanges in CLI tools\r\n===================\r\n\r\n* Adapt redis-check-aof tool for Multi Part AOF (#10061)\r\n* Enable redis-benchmark to use RESP3 protocol mode (#10335)\r\n\r\nPlatform / toolchain support related improvements\r\n=================================================\r\n\r\n* Fix OpenSSL 3.0.x related issues (#10291)\r\n\r\nINFO fields and introspection changes\r\n=====================================\r\n\r\n* COMMAND INFO key-specs has new variable_flags flag (#10237, #10148)\r\n* INFO stats: add aof_rewrites and rdb_snapshots counters (#10178)\r\n* INFO stats: add reply_buffer_shrinks and reply_buffer_expends (#9822)\r\n* INFO modules: add no-implicit-signal-modified module option (#10284)\r\n\r\nModule API changes\r\n==================\r\n\r\n* Add RM_SetCommandInfo API to set command metadata for the new COMMAND\r\n  introspection features and ACL key permissions (#10108)\r\n* Add RM_KeyAtPosWithFlags and RM_GetCommandKeysWithFlags APIs (#10237)\r\n* Add getchannels-api command flag and RM_IsChannelsPositionRequest,\r\n  RM_ChannelAtPosWithFlags APIs (#10299)\r\n* Change RM_ACLCheckChannelPermissions and RM_ACLCheckKeyPermissions APIs\r\n  (released in RC1) to take different flags (#10299)\r\n* Fix RM_SetModuleOptions flag collision. Bug in 7.0 RC1 header file, modules\r\n  that used OPTIONS_HANDLE_REPL_ASYNC_LOAD will mess up key invalidations (#10284)\r\n\r\nBug Fixes\r\n=========\r\n\r\n* Modules: Fix thread safety violation when a module thread adds an error reply,\r\n  broken in 6.2 (#10278)\r\n* Lua: Fix Eval scripts active defrag, broken 7.0 in RC1 (#10271)\r\n* Fix geo search bounding box check causing missing results (#10018)\r\n* Lua: Add checks for min-slave-* configs when evaluating Lua scripts and\r\n  Functions (#10160)\r\n* Modules: Prevent crashes and memory leaks when MODULE UNLOAD is used on module\r\n  with a pending timer (#10187)\r\n* Fix error stats and failed command stats for blocked clients (#10309)\r\n* Lua/Modules: Fix missing and duplicate error stats for scripts and modules (#10329, #10278)\r\n* Check target node is a primary during cluster setslot (#10277)\r\n* Fix key deletion not to invalidate WATCH when used on a logically expired key (#10256)\r\n* Sentinel: return an error if configuration save fails (#10151)\r\n* Sentinel: fix a free-after-use issue re-registering Sentinels (#10333)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-02-28T08:58:53Z",
        "closed_at": "2022-02-28T15:06:40Z",
        "merged_at": "2022-02-28T15:06:40Z",
        "body": "Forget to call `streamIteratorStop` after `streamIteratorStart`",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2022-02-28T07:37:19Z",
        "closed_at": "2022-02-28T08:59:17Z",
        "merged_at": "2022-02-28T08:59:16Z",
        "body": "re-generate help.h from commands.json",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 616,
        "deletions": 503,
        "changed_files": 6,
        "created_at": "2022-02-27T17:02:40Z",
        "closed_at": "2022-03-21T06:36:13Z",
        "merged_at": null,
        "body": "Optimize dict resize implementation to use in-place realloc(). That is,            \r\navoid from allocating a new hash table and starting to move entries gradually   \r\nto new HT. Basically, it extends the hash table to the desired size by calling  \r\nrealloc() and start iterating over the table and rehash each entry to its          \r\nupdated bucket.                                                  \r\nIn case of big rehash, this approach relaxes the risk to keys eviction since it  \r\navoids from holding two hash tables until rehashing ends, which might take quite \r\nsome time. \r\nAnother outcome of this change, and in continuation to earlier effort, is reducing\r\nfurther dict struct size from 56 bytes to 40 bytes.                                                                     \r\n                                                                                    \r\nPreliminary performance tests available here:  https://tinyurl.com/yc882naf",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 50,
        "deletions": 0,
        "changed_files": 8,
        "created_at": "2022-02-27T10:57:26Z",
        "closed_at": "2022-04-20T06:29:38Z",
        "merged_at": "2022-04-20T06:29:37Z",
        "body": "Add a configuration option to attach an operating system-specific identifier to Redis sockets, supporting advanced network configurations using iptables (Linux) or ipfw (FreeBSD).",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2022-02-27T09:16:27Z",
        "closed_at": "2022-02-28T07:46:57Z",
        "merged_at": "2022-02-28T07:46:57Z",
        "body": "The type of node-id should be string, not integer.\r\nAlso improve the CLUSTER SETSLOT help message.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-02-26T01:09:59Z",
        "closed_at": "2022-02-27T03:52:09Z",
        "merged_at": "2022-02-27T03:52:09Z",
        "body": "I guess our spell checker can't introspect into variable names.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 153,
        "deletions": 56,
        "changed_files": 3,
        "created_at": "2022-02-25T13:24:38Z",
        "closed_at": "2022-07-11T08:23:31Z",
        "merged_at": "2022-07-11T08:23:31Z",
        "body": "In #9389, we add a new `cluster-port` config and make cluster bus port configurable,\r\nand currently redis-cli --cluster create/add-node doesn't support with a configurable `cluster-port` instance.\r\nBecause redis-cli uses the old way (port + 10000) to send the `CLUSTER MEET` command.\r\n\r\nNow we add this support on redis-cli `--cluster`, note we don't need to explicitly pass in the\r\n`cluster-port` parameter, we can get the real `cluster-port` of the node in `clusterManagerNodeLoadInfo`,\r\nso the `--cluster create` and `--cluster add-node` interfaces have not changed.\r\n\r\nWe will use the `cluster-port` when we are doing `CLUSTER MEET`, also note that `CLUSTER MEET` bus-port\r\nparameter was added in 4.0, so if the bus_port (the one in redis-cli) is 0, or equal (port + 10000),\r\nwe just call `CLUSTER MEET` with 2 arguments, using the old form.\r\n\r\nFixes #10342. Fixes #10954",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 113,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2022-02-24T11:38:44Z",
        "closed_at": "2022-03-15T15:14:53Z",
        "merged_at": "2022-03-15T15:14:53Z",
        "body": "**TL;DR**\r\nCurrently the sort and sort_ro can access external keys via `GET` and `BY`\r\nin order to make sure the user cannot violate the authorization ACL\r\nrules, the decision is to reject external keys access patterns unless ACL allows SORT full access to all keys.\r\nI.e. for backwards compatibility, SORT with GET/BY keeps working, but if ACL has restrictions to certain keys, these features get permission denied.\r\nsee issue: https://github.com/redis/redis/issues/10106\r\n\r\n### ***Implemented solution***\r\nWe have discussed several potential solutions and decided to only allow the GET and BY arguments when the user has all key permissions with the SORT command. The reasons being that SORT with GET or BY is problematic anyway, for instance it is not supported in cluster mode since it doesn't declare keys, and we're not sure the combination of that feature with ACL key restriction is really required.\r\n**HOWEVER** If in the fullness of time we will identify a real need for fine grain access support for SORT, we would implement the complete solution which is **Option [3]**\r\n\r\n### ***Proposed solutions***\r\n\r\n### **1. (this PR) Only allow use of \"GET\" and \"BY\" patterns in sort in case the user has full key access**\r\nIn the scope of this solution we will only allow sort and sort_ro to use \"get\" and \"by\" key patterns ('get #' is NOT considered key access pattern, as it does not access any key)\r\n```\r\nUSER FOO (+sort ~* ~mylist) \r\n#FOO> sort mylist by w* get v*  - is O.K since ~* provides full key access\r\n```\r\n\r\n```\r\nUSER FOO (+sort %R~* ~mylist) \r\n#FOO> sort mylist by w* get v*  - is O.K since %R~* provides full key READ access**\r\n```\r\n```\r\nUSER FOO (+sort %W~* ~mylist)\r\n#FOO> sort mylist by w* get v*  - will fail since $W~* only provides full key WRITE access\r\n```\r\n\r\n```\r\nUSER FOO (+sort ~v* ~mylist)\r\n#FOO> sort mylist by w* get v*  - will fail since ~v* only provides partial key access\r\n```\r\n\r\n### **2. treat restricted keys as \"not-exist\"**\r\nIn the scope of this solution we will have to \"keep\" the selector which was matched to validate the sort command, and use it to verify every key access during the actual sort execution. in case of an unauthorized access to a key, the command will NOT fail, but treat that key as \"not exist\" and null be be used for the value.\r\nexample:\r\n```\r\nUSER FOO (+sort ~v1[0-9]* ~w[0-9]* ~mylist)\r\nmset w1 1 w2 2 w3 3 w4 4 w5 5 w6 6 w7 7 w8 8 w9 9 w10 10 w11 11\r\nmset v1 1 v2 2 v3 3 v4 4 v5 5 v6 6 v7 7 v8 8 v9 9 v10 10 v11 11\r\nlpush mylist 1 2 3 4 5 6 7 8 9 10 11\r\n```\r\ncalling `sort mylist by w* get v*` will return:\r\n```\r\n 1) \"10\"\r\n 2) \"11\"\r\n 3) (nil)\r\n 4) (nil)\r\n 5) (nil)\r\n 6) (nil)\r\n 7) (nil)\r\n 8) (nil)\r\n 9) (nil)\r\n10) (nil)\r\n11) (nil)\r\n```\r\nthe downside of this solution is that the first selector matching the sort command might not be the optimal selector.\r\nfor example in case in the above scenario:\r\n```\r\nUSER FOO (+sort ~mylist) (+sort ~v1[0-9]* ~w[0-9]* ~mylist)\r\n```\r\ncalling `sort mylist by w* get v*` will return:\r\n```\r\n 1) (nil)\r\n 2) (nil)\r\n 3) (nil)\r\n 4) (nil)\r\n 5) (nil)\r\n 6) (nil)\r\n 7) (nil)\r\n 8) (nil)\r\n 9) (nil)\r\n10) (nil)\r\n11) (nil)\r\n```\r\nwhich is sub-optimal.\r\n\r\n### **3. Check sort ACL rules after executing it and before commiting output (either via store or to COB)**\r\nThis is the most complicated of the 3 solutions. it would require making several changes to the sort command itself. and would potentially cause performance degradation since we will have to collect all the get keys instead of just applying them to a temp array and then scan the access keys against the ACL selectors.\r\nThis solution can include an optimization to avoid the overheads of collecting the key names, in case the ACL rules grant SORT full key-access, or if the ACL key pattern literal matches the one used in GET/BY (see option 1).\r\nIt would also mean that authorization would be O(nlogn) since we will have to complete most of the command execution before we can perform verification\r\nSuggested implementation (**still needs some refactoring**): https://github.com/ranshid/redis/tree/oss-sort-acl-option3\r\n\r\ntodo:\r\n- [x] Documentation describing the change. (Maybe this should be in the history?)",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-02-24T02:08:14Z",
        "closed_at": "2022-02-24T06:31:40Z",
        "merged_at": "2022-02-24T06:31:40Z",
        "body": "Add a comma, this would have resulted in missing newline in the message.\r\nForgot to add in #9127",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2022-02-23T23:09:29Z",
        "closed_at": "2022-02-24T12:20:01Z",
        "merged_at": "2022-02-24T12:20:01Z",
        "body": "Addresses #10310.\r\n\r\nIn progress:\r\n- [x] re-measure now with the latest changes of unstable and assess if unstable vs 5.0 has the same performance.\r\n- [X] reprofile pipeline 16 use-case\r\n    - All details were added to issue comment https://github.com/redis/redis/issues/10310#issuecomment-1049730172.\r\n    - pipeline 1 regression vs v5.0.7: this PR change will get us to approximately the same performance as v5 ( -3.30% ).\r\n    - pipeline 16 regression vs 5.0.7: on unstable we're  -33.15% against v5.0. With this PR we're -15% which means there is still a gap on the performance vs v5. In a nutshell we've added some logic/features that cost ~=8% CPU cycles. So we can explain the majority of the difference. @oranagra please advise if there is something we can improve/reduce the overhead on those 8% CPU cycles extra. \r\n\r\n```\r\nrecipe:\r\n\r\nrm dump.rdb ; src/redis-server --save \"\" &\r\nredis-cli zadd zz 0 a 1 b 2 c 3 d 4 e 5 f\r\nmemtier_benchmark --pipeline 16 --command \"zrange zz 0 -1\" --hide-histogram\r\n```\r\n\r\npipeline | 5.0.7 ops/sec | unstable ( c81c7f51c38de6dff5ffc55b5184061b84c7ea5f ) | % change vs v5.0.7 | this PR ( d95ae93fa4f2a8428d9cd8c43daba48135f7b079 ) | % change vs v5.0.7 | this PR after reply to replyProto ( 9478d532d1d55b6b9b6e074dd3277395dd36d24a ) | % change vs v5.0.7\r\n-- | -- | -- | -- | -- | -- | -- | --\r\n1 | 179077 | 143278 | -19.99% | 173164 | -3.30% | 172104 | -3.89%\r\n16 | 1022965 | 683870 | -33.15% | 851282 | -16.78% | 866938 | -15.25%\r\n\r\n\r\n\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-02-23T15:24:09Z",
        "closed_at": "2022-02-27T08:30:39Z",
        "merged_at": "2022-02-27T08:30:39Z",
        "body": "Adds `-3` option to cause redis-benchmark to send a `HELLO 3` to it can benchmark the effects of RESP3 on the server.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2022-02-23T12:42:35Z",
        "closed_at": "2022-02-23T16:15:12Z",
        "merged_at": "2022-02-23T16:15:12Z",
        "body": "This was raised on https://github.com/redis/redis/issues/10310#issuecomment-1048608645 in a discussion with @oranagra . \r\n![image](https://user-images.githubusercontent.com/5832149/155320701-9c73488f-eedf-4120-8d9d-ca254c6992d5.png)\r\n\r\nGiven that `sprintf` is consuming 1.6% of CPU cycles of the process on pipeline 1 tests,  trying to avoid it will benefit any command that uses deferred replies. Pipelining will make the difference even more evident.\r\nZREVRANGE results:\r\n- pipeline 1: \r\n   - unstable: 488aecb3abca3088735f1bffa60a74e22832d44b : 138479 ops/sec. p50(ms)=1.43900\r\n   - this PR: 143115 ops/sec. p50(ms)=1.39100. **%change = 3.4%**\r\n\r\n- pipeline 16 ( reduces the relative percentage of `__GI___writev` and makes more evident the command performance ): \r\n   - unstable: 488aecb3abca3088735f1bffa60a74e22832d44b : 621587 ops/sec. p50(ms)=4.25500\r\n   - this PR: 680124 ops/sec. p50(ms)=3.83900. **%change = 9.4%**\r\n\r\nTo reproduce:\r\n```\r\nrm dump.rdb ; src/redis-server --save \"\" &\r\nredis-cli zadd zz 0 a 1 b 2 c 3 d 4 e 5 f\r\nmemtier_benchmark --pipeline 16 --command \"zrange zz 0 -1\" --hide-histogram\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-02-23T09:49:37Z",
        "closed_at": "2022-02-23T15:59:22Z",
        "merged_at": "2022-02-23T15:59:22Z",
        "body": "In case HELLO message received from another sentinel, with same address like another instance registered in the past but with different runid. Then there was cumbersome logic to modify the instance the port to 0 to in order to mark as invalid and later on to delete it. But the deletion is happening during update of instances in such a way that we might end up accessing an instance that was deleted just before.\r\n\r\nDidn't find a good reason why to postpone the deletion action of an obsolete instance (deletion is taking place instantly, for other cases ) -> Lets delete at once\r\nThere is a mixture of logic of Sentinel address update with the logic of deletion of Sentinels that match a given Address -> Split to two!",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2022-02-23T02:44:30Z",
        "closed_at": "2022-02-23T06:47:16Z",
        "merged_at": "2022-02-23T06:47:16Z",
        "body": "The test will fail on slow machines (valgrind or FreeBsd).\r\nBecause in #10256 when WATCH is called on a key that's already\r\nlogically expired, we will add an `expired` flag, and we will\r\nskip it in `isWatchedKeyExpired` check.\r\n\r\nApparently we need to increase the expiration time so that\r\nthe key can not expire logically then the WATCH is called.\r\nAlso added retries to make sure it doesn't fail. I suppose\r\n100ms is enough in valgrind, tested locally, no need to retry.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 472,
        "deletions": 150,
        "changed_files": 12,
        "created_at": "2022-02-22T11:18:40Z",
        "closed_at": "2022-02-27T11:40:58Z",
        "merged_at": "2022-02-27T11:40:58Z",
        "body": "Related PR's: https://github.com/redis/redis/pull/10279, https://github.com/redis/redis/pull/10218, https://github.com/redis/redis/pull/10278, https://github.com/redis/redis/pull/10309\r\n\r\nThis PR fix 2 issues on Lua scripting:\r\n* Server error reply statistics (some errors were counted twice).\r\n* Error code and error strings returning from scripts (error code was missing / misplaced).\r\n\r\n## Statistics\r\na Lua script user is considered part of the user application, a sophisticated transaction, so we want to count an error even if handled silently by the script, but when it is propagated outwards from the script we don't wanna count it twice. on the other hand, if the script decides to throw an error on its own (using `redis.error_reply`), we wanna count that too.\r\nBesides, we do count the `calls` in command statistics for the commands the script calls, we we should certainly also count `failed_calls`.\r\nSo when a simple `eval \"return redis.call('set','x','y')\" 0` fails, it should count the failed call to both SET and EVAL, but the `errorstats` and `total_error_replies` should be counted only once.\r\n\r\nThe PR changes the error object that is raised on errors. Instead of raising a simple Lua string, Redis will raise\r\na Lua table in the following format:\r\n\r\n```\r\n{\r\n    err='<error message (including error code)>',\r\n    source='<User source file name>',\r\n    line='<line where the error happned>',\r\n    ignore_error_stats_update=true/false,\r\n}\r\n```\r\n\r\nThe `luaPushError` function was modified to construct the new error table as describe above. The `luaRaiseError` was renamed to `luaError` and is now simply called `lua_error` to raise the table on the top of the Lua stack as the error object.\r\nThe reason is that since its functionality is changed, in case some Redis branch / fork uses it, it's better to have a compilation error than a bug.\r\n\r\nThe `source` and `line` fields are enriched by the error handler (if possible) and the `ignore_error_stats_update`\r\nis optional and if its not present then the default value is `false`. If `ignore_error_stats_update` is true, the\r\nerror will not be counted on the error stats.\r\n\r\nWhen parsing Redis call reply, each error is translated to a Lua table on the format describe above and the\r\n`ignore_error_stats_update` field is set to `true` so we will not count errors twice (we counted this error\r\nwhen we invoke the command).\r\n\r\nThe changes in this PR might have been considered as a breaking change for users that used Lua `pcall` function. Before, the error was a string and now its a table. To keep backward comparability the PR override the `pcall` implementation and extract the error message from the error table and return it.\r\n\r\nExample of the error stats update:\r\n\r\n```\r\n127.0.0.1:6379> lpush l 1\r\n(integer) 2\r\n127.0.0.1:6379> eval \"return redis.call('get', 'l')\" 0\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value. script: e471b73f1ef44774987ab00bdf51f21fd9f7974a, on @user_script:1.\r\n\r\n127.0.0.1:6379> info Errorstats\r\n# Errorstats\r\nerrorstat_WRONGTYPE:count=1\r\n\r\n127.0.0.1:6379> info commandstats\r\n# Commandstats\r\ncmdstat_eval:calls=1,usec=341,usec_per_call=341.00,rejected_calls=0,failed_calls=1\r\ncmdstat_info:calls=1,usec=35,usec_per_call=35.00,rejected_calls=0,failed_calls=0\r\ncmdstat_lpush:calls=1,usec=14,usec_per_call=14.00,rejected_calls=0,failed_calls=0\r\ncmdstat_get:calls=1,usec=10,usec_per_call=10.00,rejected_calls=0,failed_calls=1\r\n```\r\n\r\n## error message\r\nWe can now construct the error message (sent as a reply to the user) from the error table, so this solves\r\nissues where the error message was malformed and the error code appeared in the middle of the error message:\r\n\r\n```diff\r\n127.0.0.1:6379> eval \"return redis.call('set','x','y')\" 0\r\n-(error) ERR Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479): @user_script:1: OOM command not allowed when used memory > 'maxmemory'.\r\n+(error) OOM command not allowed when used memory > 'maxmemory' @user_script:1. Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479)\r\n```\r\n\r\n```diff\r\n127.0.0.1:6379> eval \"redis.call('get', 'l')\" 0\r\n-(error) ERR Error running script (call to f_8a705cfb9fb09515bfe57ca2bd84a5caee2cbbd1): @user_script:1: WRONGTYPE Operation against a key holding the wrong kind of value\r\n+(error) WRONGTYPE Operation against a key holding the wrong kind of value script: 8a705cfb9fb09515bfe57ca2bd84a5caee2cbbd1, on @user_script:1.\r\n```\r\n\r\nNotica that `redis.pcall` was not change:\r\n```\r\n127.0.0.1:6379> eval \"return redis.pcall('get', 'l')\" 0\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n```\r\n\r\n\r\n## other notes\r\nNotice that Some commands (like GEOADD) changes the cmd variable on the client stats so we can not count on it to update the command stats. In order to be able to update those stats correctly we needed to promote `realcmd` variable to be located on the client struct.\r\n\r\nTests was added and modified to verify the changes.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-02-22T09:07:09Z",
        "closed_at": "2022-02-23T10:48:36Z",
        "merged_at": null,
        "body": "Running sentinel tests on macOS with address sanitizer results with the\r\nerror below.\r\n\r\n```\r\n=================================================================\r\n==26381==ERROR: AddressSanitizer: heap-use-after-free on address 0x61300000cd10 at pc 0x00010f90c934 bp 0x7ffee04eb070 sp 0x7ffee04eb068\r\nREAD of size 8 at 0x61300000cd10 thread T0\r\n    #0 0x10f90c933 in sentinelUpdateSentinelAddressInAllMasters sentinel.c:1182\r\n    #1 0x10f91a50b in sentinelProcessHelloMessage sentinel.c:2883\r\n    #2 0x10f925038 in sentinelPublishCommand sentinel.c:4384\r\n    #3 0x10f7460c3 in call server.c:3156\r\n    #4 0x10f749a80 in processCommand server.c:3756\r\n    #5 0x10f7af057 in processInputBuffer networking.c:2380\r\n    #6 0x10f7944ce in readQueryFromClient networking.c:2492\r\n    #7 0x10f9e5e35 in connSocketEventHandler connection.c:295\r\n    #8 0x10f723868 in aeProcessEvents ae.c:436\r\n    #9 0x10f7245bc in aeMain ae.c:496\r\n    #10 0x10f75e2ce in main server.c:6906\r\n    #11 0x7fff20954f3c in start+0x0 (libdyld.dylib:x86_64+0x15f3c)\r\n\r\n0x61300000cd10 is located 16 bytes inside of 344-byte region [0x61300000cd00,0x61300000ce58)\r\nfreed by thread T0 here:\r\n    #0 0x10ffcc4e9 in wrap_free+0xa9 (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x444e9)\r\n    #1 0x10f72a9b4 in dictGenericDelete dict.c:421\r\n    #2 0x10f72a4aa in dictDelete dict.c:437\r\n    #3 0x10f90c6af in sentinelUpdateSentinelAddressInAllMasters sentinel.c:1195\r\n    #4 0x10f91a50b in sentinelProcessHelloMessage sentinel.c:2883\r\n    #5 0x10f925038 in sentinelPublishCommand sentinel.c:4384\r\n    #6 0x10f7460c3 in call server.c:3156\r\n    #7 0x10f749a80 in processCommand server.c:3756\r\n    #8 0x10f7af057 in processInputBuffer networking.c:2380\r\n    #9 0x10f7944ce in readQueryFromClient networking.c:2492\r\n    #10 0x10f9e5e35 in connSocketEventHandler connection.c:295\r\n    #11 0x10f723868 in aeProcessEvents ae.c:436\r\n    #12 0x10f7245bc in aeMain ae.c:496\r\n    #13 0x10f75e2ce in main server.c:6906\r\n    #14 0x7fff20954f3c in start+0x0 (libdyld.dylib:x86_64+0x15f3c)\r\n\r\npreviously allocated by thread T0 here:\r\n    #0 0x10ffcc3a0 in wrap_malloc+0xa0 (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x443a0)\r\n    #1 0x10f770f6a in zmalloc zmalloc.c:126\r\n    #2 0x10f90cf0b in createSentinelRedisInstance sentinel.c:1301\r\n    #3 0x10f91a56e in sentinelProcessHelloMessage sentinel.c:2888\r\n    #4 0x10fa088c0 in redisProcessCallbacks async.c:572\r\n    #5 0x10f723868 in aeProcessEvents ae.c:436\r\n    #6 0x10f7245bc in aeMain ae.c:496\r\n    #7 0x10f75e2ce in main server.c:6906\r\n    #8 0x7fff20954f3c in start+0x0 (libdyld.dylib:x86_64+0x15f3c)\r\n```",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 185,
        "deletions": 107,
        "changed_files": 3,
        "created_at": "2022-02-22T05:29:34Z",
        "closed_at": "2022-03-01T07:02:48Z",
        "merged_at": "2022-03-01T07:02:48Z",
        "body": "Related to https://github.com/redis/redis/pull/10285.\r\n\r\nNow that all the configs are mostly standardized, we can replace the loop with a HashMap. The structure is that all configs are added to a dictionary. Configs with Aliases are duplicated, with a flag indicating which one is preferred. I initially wanted to observe the performance, and found it was mostly the same outside of about a 10% performance degradation for get, so not sure how compelling it is to move.\r\n\r\nSome other side effects:\r\n* Configs are returned in a non-deterministic order. It's possible that a client was relying on order (hopefully not).\r\n* Fixed an esoteric bug where if you did a set with an alias with an error, it would throw an error indicating a bug with the preferred name for that config.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-02-22T03:35:32Z",
        "closed_at": "2022-02-22T05:12:35Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2022-02-20T04:21:10Z",
        "closed_at": "2022-02-22T08:55:55Z",
        "merged_at": "2022-02-22T08:55:55Z",
        "body": "In #9963 we added the `ALLOW_BUSY` flag to REPLICAOF command.\r\nThis flag should have been applied to REPLCONF not REPLICAOF.\r\n\r\nAdd `DEPRECATED` doc flag to SLAVEOF, mark it as deprecated.\r\nWhile the s-word isn't going away soon, the actual command has\r\nan alternative (REPLICAOF) and we should direct people to use it.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2022-02-19T15:02:47Z",
        "closed_at": "2022-02-20T05:11:21Z",
        "merged_at": "2022-02-20T05:11:21Z",
        "body": "publishshard was added in #8621 (7.0 RC1), but the publishshard_sent\r\nstat is not shown in CLUSTER INFO command.\r\n\r\nOther changes:\r\n1. Remove useless `needhelp` statements, it was removed in 3dad819.\r\n2. Fix typos that saw by the way.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-02-18T13:16:12Z",
        "closed_at": "2022-02-20T06:09:19Z",
        "merged_at": "2022-02-20T06:09:19Z",
        "body": "There is no variable named by REPL_STATE_RECEIVE_PSYNC_REPLY, it should be REPL_STATE_RECEIVE_PSYNC_REPLY according to the contexts.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 66,
        "deletions": 3,
        "changed_files": 5,
        "created_at": "2022-02-17T12:22:26Z",
        "closed_at": "2022-03-07T15:37:58Z",
        "merged_at": "2022-03-07T15:37:58Z",
        "body": "Adds a simple event that is triggered anytime a config change occurs.\r\n\r\nProvides a module unit test to ensure its being triggered",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 77,
        "deletions": 22,
        "changed_files": 8,
        "created_at": "2022-02-17T07:55:22Z",
        "closed_at": "2022-02-21T09:20:42Z",
        "merged_at": "2022-02-21T09:20:42Z",
        "body": "This is a followup work for #10278, and a discussion about #10279\r\n\r\nThe changes:\r\n- fix failed_calls in command stats for blocked clients that got error.\r\n  including CLIENT UNBLOCK, and module replying an error from a thread.\r\n- fix latency stats for XREADGROUP that filed with -NOGROUP\r\n\r\nTheory behind which errors should be counted:\r\n- error stats represents errors returned to the user, so an error handled by a\r\n  module should not be counted.\r\n- total error counter should be the same.\r\n- command stats represents execution of commands (even with RM_Call, and if\r\n  they fail or get rejected it counts these calls in commandstats, so it should\r\n  also count failed_calls)\r\n\r\nSome thoughts about Scripts:\r\nfor scripts it could be different since they're part of user code, not the infra (not an extension to redis)\r\nwe certainly want commandstats to contain all calls and errors\r\na simple script is like mult-exec transaction so an error inside it should be counted in error stats\r\na script that replies with an error to the user (using redis.error_reply) should also be counted in error stats\r\nbut then the problem is that a plain `return redis.call(\"SET\")` should not be counted twice (once for the SET and once for EVAL)\r\nso that's something left to be resolved in #10279",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 295,
        "deletions": 52,
        "changed_files": 7,
        "created_at": "2022-02-16T13:59:47Z",
        "closed_at": "2022-03-08T15:10:37Z",
        "merged_at": "2022-03-08T15:10:36Z",
        "body": "Fix #6887\r\nDeleting a stream while a client is blocked XREADGROUP should unblock the client.\r\n\r\nThe idea is that if a client is blocked via XREADGROUP is different from\r\nany other blocking type in the sense that it depends on the existence of both\r\nthe key and the group. Even if the key is deleted and then revived with XADD\r\nit won't help any clients blocked on XREADGROUP because the group no longer\r\nexist, so they would fail with -NOGROUP anyway.\r\nThe conclusion is that it's better to unblock these clients (with error) upon\r\nthe deletion of the key, rather than waiting for the first XADD. \r\n\r\nOther changes:\r\n1. Slightly optimize all `serveClientsBlockedOn*` functions by checking `server.blocked_clients_by_type`\r\n2. All `serveClientsBlockedOn*` functions now use a list iterator rather than looking at `listFirst`, relying on `unblockClient` to delete the head of the list. Before this commit, only `serveClientsBlockedOnStreams` used to work like that.\r\n3. bugfix: CLIENT UNBLOCK ERROR should work even if the command doesn't have a timeout_callback (only relevant to module commands)",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-02-16T11:29:26Z",
        "closed_at": "2022-02-16T14:42:05Z",
        "merged_at": "2022-02-16T14:42:04Z",
        "body": "Hi\r\n\r\nI have removed a wrong double semicolon at the end of 2 lines\r\n\r\nciao\r\nmatteo",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-02-16T06:18:17Z",
        "closed_at": "2022-06-23T04:56:26Z",
        "merged_at": "2022-06-23T04:56:26Z",
        "body": "I see that the `bus-port` parameter is listed in cluster help, but not documented in redis-doc\r\nhttps://github.com/redis/redis/blob/3881f7850f9f81720315bd4f33f2f9dedcc242bb/src/cluster.c#L5022-L5023\r\n\r\nIn `CLUSTER MEET`, bus-port argument was added in 11436b1.\r\nFor cluster announce ip / port implementation, part of the\r\n4.0-RC1.\r\n\r\nAnd in #9389, we add a new cluster-port config and make\r\ncluster bus port configurable, part of the 7.0-RC1.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-02-16T02:36:51Z",
        "closed_at": "2022-03-29T08:54:45Z",
        "merged_at": "2022-03-29T08:54:45Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 173,
        "deletions": 72,
        "changed_files": 13,
        "created_at": "2022-02-15T08:03:49Z",
        "closed_at": "2022-10-09T05:18:35Z",
        "merged_at": "2022-10-09T05:18:35Z",
        "body": "Freeze time during execution of scripts and all other commands.\r\nThis means that a key is either expired or not, and doesn't change\r\nstate during a script execution. resolves #10182\r\n\r\nThis PR try to add a new `commandTimeSnapshot` function.\r\nThe function logic is extracted from `keyIsExpired`, but the related\r\ncalls to `fixed_time_expire` and `mstime()` are removed, see below.\r\n\r\nIn commands, we will avoid calling `mstime()` multiple times\r\nand just use the one that sampled in call. The background is,\r\ne.g. using `PEXPIRE 1` with valgrind sometimes result in the key\r\nbeing deleted rather than expired. The reason is that both `PEXPIRE`\r\ncommand and `checkAlreadyExpired` call `mstime()` separately.\r\n\r\nThere are other more important changes in this PR:\r\n1. Eliminate `fixed_time_expire`, it is no longer needed. \r\n   When we want to sample time we should always use a time snapshot. \r\n   We will use `in_nested_call` instead to update the cached time in `call`.\r\n2. Move the call for `updateCachedTime` from `serverCron` to `afterSleep`.\r\n    Now `commandTimeSnapshot` will always return the sample time, the\r\n    `lookupKeyReadWithFlags` call in `getNodeByQuery` will get a outdated\r\n    cached time (because `processCommand` is out of the `call` context).\r\n    We put the call to `updateCachedTime` in `aftersleep`.\r\n3. Cache the time each time the module lock Redis.\r\n    Call `updateCachedTime` in `moduleGILAfterLock`, affecting `RM_ThreadSafeContextLock`\r\n    and `RM_ThreadSafeContextTryLock`\r\n\r\nCurrently the commandTimeSnapshot change affects the following TTL commands:\r\n- SET EX / SET PX\r\n- EXPIRE / PEXPIRE\r\n- SETEX / PSETEX\r\n- GETEX EX / GETEX PX\r\n- TTL / PTTL\r\n- EXPIRETIME / PEXPIRETIME\r\n- RESTORE key TTL\r\n\r\nAnd other commands just use the cached mstime (including TIME).\r\n\r\nThis is considered to be a breaking change since it can break a script that uses a loop to wait for a key to expire.",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 407,
        "deletions": 107,
        "changed_files": 15,
        "created_at": "2022-02-14T21:18:53Z",
        "closed_at": "2022-02-22T09:00:05Z",
        "merged_at": "2022-02-22T09:00:04Z",
        "body": "Resolves https://github.com/redis/redis/issues/10156.\r\n\r\nThis implements the following main pieces of functionality:\r\n* Renames key spec \"CHANNEL\" to be \"NOT_KEY\", and update the documentation to indicate it's for cluster routing and not for any other key related purpose.\r\n* Add the getchannels-api, so that modules can now define commands that are subject to ACL channel permission checks. \r\n* Add 4 new flags that describe how a module interacts with a command (SUBSCRIBE, PUBLISH, UNSUBSCRIBE, and PATTERN). They are all technically composable, however not sure how a command could both subscribe and unsubscribe from a command at once, but didn't see a reason to add explicit validation there.\r\n* Add two new module apis RM_ChannelAtPosWithFlags and RM_IsChannelsPositionRequest to duplicate the functionality provided by the keys position APIs.\r\n* The RM_ACLCheckChannelPermissions (only released in 7.0 RC1) was changed to take flags rather than a boolean literal.\r\n* The RM_ACLCheckKeyPermissions (only released in 7.0 RC1) was changed to take flags corresponding to keyspecs instead of custom permission flags. These keyspec flags mimic the flags for ACLCheckChannelPermissions.\r\n\r\nNOTE: I considered renaming all of the keyReference/keyResult -> argReference/argResult, but decided against and just added a small comment. \r\n\r\nTo Do:\r\n- [x] doc PR about `channel` -> `not_key` flag (https://github.com/redis/redis-doc/pull/1799)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 76,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-02-14T16:33:45Z",
        "closed_at": "2022-04-12T13:48:27Z",
        "merged_at": null,
        "body": "In our test case, now we missed test coverage for RENAME and RENAMENX command in cluster mode\r\nThis pr goal is to add some test coverage cases in this scenario.\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1559,
        "deletions": 323,
        "changed_files": 28,
        "created_at": "2022-02-14T12:11:13Z",
        "closed_at": "2022-02-24T08:51:04Z",
        "merged_at": "2022-02-24T08:51:04Z",
        "body": "This is basically just a subtree pull of the latest (unreleased) hiredis, with two caveats:\r\n\r\n* Unfortunately, the `sds -> hisds` patch was pulled as a subtree update from a remote branch rather than a local redis change. Because of that, it goes away on every subtree update. It is now applied as a local commit so it should survive in the future.\r\n* This PR needs to be merged committed due to the use of subtree.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 18,
        "changed_files": 2,
        "created_at": "2022-02-13T21:49:07Z",
        "closed_at": "2022-02-22T06:59:23Z",
        "merged_at": "2022-02-22T06:59:23Z",
        "body": "Make sure the status return from loading multiple AOF files reflects the overall\r\nresult, not just the one of the last file.\r\n\r\nWhen one of the AOF files succeeded to load, but the last AOF file\r\nwas empty, the loadAppendOnlyFiles will return AOF_EMPTY.\r\nThis commit changes this behavior, and return AOF_OK in that case.\r\n\r\nThis can happen for example, when loading old AOF file, and no more commands processed,\r\nthe manifest file will include base AOF file with data, and empty incr AOF file.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-02-13T21:31:10Z",
        "closed_at": "2022-02-14T06:46:59Z",
        "merged_at": "2022-02-14T06:46:59Z",
        "body": "In order to make sure no more commands processed, we wait that\r\nthe 'load handlers' will disconncet.\r\n\r\nThe test by mistake waited on the (last) slave instead of the master.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 420,
        "deletions": 58,
        "changed_files": 8,
        "created_at": "2022-02-13T16:39:45Z",
        "closed_at": "2022-03-16T01:24:41Z",
        "merged_at": "2022-03-16T01:24:41Z",
        "body": "Ref #10168 \r\n\r\nImplement a new `CLUSTER SHARDS` command which provides more information to clients in an extensible format. This intended to be an alternative to `CLUSTER SLOTS` that provides more information and is more efficient for clusters with disjoint slot assignment. It is also to help migrate some clients currently using `CLUSTER NODES`.\r\n\r\nThe cluster slot command is a map of \"shards\", where a shard is a collection of nodes serving the same set of slots. If a node is serving no slots, it is said to exist in its own shard. Each shard contains a dictionary with two properties, \"slots\" and \"nodes\". Slots are a list of two value integer which indicate start and stop slot ranges. Nodes is a dictionary that contains the following attributres:\r\n* ID: The unique ID for this node.\r\n* port: The TCP port for this node, if available.\r\n* tls-port: The TLS port for this node, if available.\r\n* hostname: The propagated hostname for this node, if available. \r\n* endpoint: The preferred endpoint for the node. See https://redis.io/commands/cluster-slots for what the value of this represents.\r\n* role: Either master or replica. Please consider the alternative where we say the \"first\" node in the list is the master. \r\n* replication-offset: Reported for masters and replicas. This field can be used to read from the most \"up-to date\" replica, or checking how far behind replicas are from the master.\r\n* health: The reported health of this node to clients, one of three values:\r\n* * online: The node is online and healthy.\r\n* * fail: The node is confirmed dead.\r\n* * loading: The node is currently doing a full sync, so don't try to connect to it.\r\n\r\n(Note, I think endpoint is missing, also replication-offset is missing from masters)\r\n\r\nSample request/response:\r\n```\r\n127.0.0.1:6379> CLUSTER SHARDS\r\n1) 1) \"slots\"\r\n   2) 1) (integer) 10923\r\n      2) (integer) 11110\r\n      3) (integer) 11113\r\n      4) (integer) 16111\r\n      5) (integer) 16113\r\n      6) (integer) 16383\r\n   3) \"nodes\"\r\n   4) 1)  1) \"id\"\r\n          2) \"71f058078c142a73b94767a4e78e9033d195dfb4\"\r\n          3) \"port\"\r\n          4) (integer) 6381\r\n          5) \"ip\"\r\n          6) \"127.0.0.1\"\r\n          7) \"role\"\r\n          8) \"primary\"\r\n          9) \"health\"\r\n         10) \"online\"\r\n      2)  1) \"id\"\r\n          2) \"1461967c62eab0e821ed54f2c98e594fccfd8736\"\r\n          3) \"port\"\r\n          4) (integer) 7381\r\n          5) \"ip\"\r\n          6) \"127.0.0.1\"\r\n          7) \"role\"\r\n          8) \"replica\"\r\n          9) \"replication-offset\"\r\n         10) (integer) 0\r\n         11) \"health\"\r\n         12) \"fail\"\r\n2) 1) \"slots\"\r\n   2) 1) (integer) 5461\r\n      2) (integer) 10922\r\n   3) \"nodes\"\r\n   4) 1)  1) \"id\"\r\n          2) \"9215e30cd4a71070088778080565de6ef75fd459\"\r\n          3) \"port\"\r\n          4) (integer) 6380\r\n          5) \"ip\"\r\n          6) \"127.0.0.1\"\r\n          7) \"role\"\r\n          8) \"primary\"\r\n          9) \"health\"\r\n         10) \"online\"\r\n      2)  1) \"id\"\r\n          2) \"877fa59da72cb902d0563d3d8def3437fc3a0196\"\r\n          3) \"port\"\r\n          4) (integer) 7380\r\n          5) \"ip\"\r\n          6) \"127.0.0.1\"\r\n          7) \"role\"\r\n          8) \"replica\"\r\n          9) \"replication-offset\"\r\n         10) (integer) 0\r\n         11) \"health\"\r\n         12) \"loading\"\r\n3) 1) \"slots\"\r\n   2) 1) (integer) 0\r\n      2) (integer) 5460\r\n      3) (integer) 11111\r\n      4) (integer) 11112\r\n      3) (integer) 16112\r\n      4) (integer) 16112\r\n   3) \"nodes\"\r\n   4) 1)  1) \"id\"\r\n          2) \"b7e9acc0def782aabe6b596f67f06c73c2ffff93\"\r\n          3) \"port\"\r\n          4) (integer) 7379\r\n          5) \"ip\"\r\n          6) \"127.0.0.1\"\r\n          7) \"role\"\r\n          8) \"primary\"\r\n          9) \"health\"\r\n         10) \"online\"\r\n      2)  1) \"id\"\r\n          2) \"e2acf1a97c055fd09dcc2c0dcc62b19a6905dbc8\"\r\n          3) \"port\"\r\n          4) (integer) 6379\r\n          5) \"ip\"\r\n          6) \"127.0.0.1\"\r\n          7) \"hostname\"\r\n          8) \"host-1.com\"\r\n          9) \"role\"\r\n         10) \"replica\"\r\n         11) \"replication-offset\"\r\n         12) (integer) 0\r\n         13) \"health\"\r\n         14) \"loading\"\r\n```\r\n\r\n### Open considerations\r\n* Should we expose cluster epoch in some capacity.\r\n* * Pros: Clients will be able to compare multiple different slot responses, and compare epochs. (Omitted for now, we can add it later)\r\n* * Cons: Epochs are not coherent except for mastership membership, so not a great indicator of anything.\r\n* Should we also expose PFAIL (or timedout) as a health indicator. (Omitted, since it's not a reliable indicator for node health and we don't want clients to use it)\r\n* * Pros: Some clients want to be able to know a node is timedout and might be in fail soon. This may also be useful in split network situations, but I'm not clear of that use case.\r\n* * Cons: The time delta between pfail and fail for a hard down node is pretty small, since it will start gossiping the failure immediately. Clients might implement extra logic to handle this case.\r\n\r\n- [x] Code changes\r\n- [x] Test cases for slot/slotrange/nodes/roles/ip/port/node_name\r\n- [x] Test cases for health as LOADING\r\n- [x] Test cases for endpoint ",
        "comments": 38
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2022-02-13T14:04:28Z",
        "closed_at": "2022-02-15T06:37:06Z",
        "merged_at": "2022-02-15T06:37:06Z",
        "body": "* Drop obsolete initialization calls.\r\n* Use decoder API for DH parameters.\r\n* Enable auto DH parameters if not explicitly used, which should be the\r\n  preferred configuration going forward.\r\n  \r\n  Fixes #10280 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 32,
        "changed_files": 3,
        "created_at": "2022-02-13T09:23:18Z",
        "closed_at": "2022-02-16T21:35:50Z",
        "merged_at": "2022-02-16T21:35:49Z",
        "body": "* Fix #10213 \r\n* Provide a utility for providing global static assertion functionality",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-02-13T07:33:43Z",
        "closed_at": "2022-02-13T15:43:20Z",
        "merged_at": "2022-02-13T15:43:20Z",
        "body": "sometimes you just wanna run one test on one system (e.g. memefficiency\r\non macos), so you want all other tests to be skipped",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 100,
        "deletions": 14,
        "changed_files": 3,
        "created_at": "2022-02-12T16:08:29Z",
        "closed_at": "2022-02-13T07:52:38Z",
        "merged_at": "2022-02-13T07:52:38Z",
        "body": "Added regression tests for #10020 / #10081 / #10243.\r\nThe above PRs fixed some crashes due to an asserting,\r\nsee function `clientHasPendingReplies` (introduced in #9166).\r\n\r\nThis commit added some tests to cover the above scenario.\r\nThese tests will all fail in #9166, althought fixed not,\r\nthere is value in adding these tests to cover and verify\r\nthe changes. And it also can cover #8868 (verify the logs).\r\n\r\nOther changes: \r\n1. Reduces the wait time in `waitForBgsave` and `waitForBgrewriteaof`\r\nfrom 1s to 50ms, which should reduce the time for some tests.\r\n2. Improve the test infra to print context when `assert_match` fails.\r\n3. Improve the test infra to print `$error` when `assert_error` fails.\r\n```\r\nExpected an error matching 'ERR*' but got 'OK' (context: type eval line 4 cmd {assert_error \"ERR*\" {r set a b}} proc ::test)\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 105,
        "deletions": 17,
        "changed_files": 4,
        "created_at": "2022-02-12T07:24:08Z",
        "closed_at": "2022-03-05T19:25:53Z",
        "merged_at": "2022-03-05T19:25:53Z",
        "body": "Normally, `redis-cli` escapes non-printable data received from Redis, using a custom scheme (which is also used to handle quoted input). When using `--json` this is not desired as it is not compatible with RFC 7159, which specifies JSON strings are assumed to be Unicode and how they should be escaped.\r\n\r\nThis commit changes `--json` to follow RFC 7159, which means that properly encoded Unicode strings in Redis will result with a valid Unicode JSON.\r\n\r\nHowever, this introduces a new problem with `--json` and data that is not valid Unicode (e.g., random binary data, text that follows other encoding, etc.). To address this, we add `--quoted-json` which produces JSON strings that follow the original redis-cli quoting scheme.\r\n\r\nFor example, a value that consists of only null (0x00) bytes will show up as:\r\n* `\"\\u0000\\u0000\\u0000\"` when using `--json`\r\n* `\"\\\\x00\\\\x00\\\\x00\"` when using `--quoted-json`",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 1530,
        "deletions": 233,
        "changed_files": 13,
        "created_at": "2022-02-11T21:19:45Z",
        "closed_at": "2022-03-30T12:47:07Z",
        "merged_at": "2022-03-30T12:47:07Z",
        "body": "Implementation based off of [#9437](https://github.com/redis/redis/issues/9437)\r\n\r\n**Implementation details**:\r\n\r\nThis feature adds the ability to add four different types (Bool, Numeric, String, Enum) of configurations to a module to be accessed via the redis config file, and the CONFIG command.\r\n\r\n**Configuration Names**:\r\n\r\nWe impose a restriction that a module configuration always starts with the module name and contains a '.' followed by the config name. If a module passes \"config1\" as the name to a register function, it will be registered as MODULENAME.config1.\r\n\r\n**Configuration Persistence**:\r\n\r\nModule Configurations exist only as long as a module is loaded. If a module is unloaded, the configurations are removed.\r\nThere is now also a minimal core API for removal of standardConfig objects from configs by name.\r\n\r\n**Get and Set Callbacks**:\r\n\r\nStorage of config values is owned by the module that registers them, and provides callbacks for Redis to access and manipulate the values. This is exposed through a GET and SET callback.\r\n\r\nThe get callback returns a typed value of the config to redis. The callback takes the name of the configuration, and also a privdata pointer. Note that these only take the CONFIGNAME portion of the config, not the entire MODULENAME.CONFIGNAME.\r\n\r\n```\r\n typedef RedisModuleString * (*RedisModuleConfigGetStringFunc)(const char *name, void *privdata);\r\n typedef long long (*RedisModuleConfigGetNumericFunc)(const char *name, void *privdata);\r\n typedef int (*RedisModuleConfigGetBoolFunc)(const char *name, void *privdata);\r\n typedef int (*RedisModuleConfigGetEnumFunc)(const char *name, void *privdata);\r\n```\r\n\r\nConfigs must also must specify a set callback, i.e. what to do on a CONFIG SET XYZ 123 or when loading configurations from cli/.conf file matching these typedefs. *name* is again just the CONFIGNAME portion, *val* is the parsed value from the core, *privdata* is the registration time privdata pointer, and *err* is for providing errors to a client.\r\n\r\n```\r\ntypedef int (*RedisModuleConfigSetStringFunc)(const char *name, RedisModuleString *val, void *privdata, RedisModuleString **err);\r\ntypedef int (*RedisModuleConfigSetNumericFunc)(const char *name, long long val, void *privdata, RedisModuleString **err);\r\ntypedef int (*RedisModuleConfigSetBoolFunc)(const char *name, int val, void *privdata, RedisModuleString **err);\r\ntypedef int (*RedisModuleConfigSetEnumFunc)(const char *name, int val, void *privdata, RedisModuleString **err);\r\n```\r\n\r\nModules can also specify an optional apply callback that will be called after value(s) have been set via CONFIG SET:\r\n\r\n```\r\ntypedef int (*RedisModuleConfigApplyFunc)(RedisModuleCtx *ctx, void *privdata, RedisModuleString **err);\r\n```\r\n\r\n**Flags:**\r\nWe expose 7 new flags to the module, which are used as part of the config registration.\r\n\r\n```\r\n#define REDISMODULE_CONFIG_MODIFIABLE 0 /* This is the default for a module config. */\r\n#define REDISMODULE_CONFIG_IMMUTABLE (1ULL<<0) /* Can this value only be set at startup? */\r\n#define REDISMODULE_CONFIG_SENSITIVE (1ULL<<1) /* Does this value contain sensitive information */\r\n#define REDISMODULE_CONFIG_HIDDEN (1ULL<<4) /* This config is hidden in `config get <pattern>` (used for tests/debugging) */\r\n#define REDISMODULE_CONFIG_PROTECTED (1ULL<<5) /* Becomes immutable if enable-protected-configs is enabled. */\r\n#define REDISMODULE_CONFIG_DENY_LOADING (1ULL<<6) /* This config is forbidden during loading. */\r\n/* Numeric Specific Configs */\r\n#define REDISMODULE_CONFIG_MEMORY (1ULL<<7) /* Indicates if this value can be set as a memory value */\r\n```\r\n\r\n**Module Registration APIs**:\r\n\r\n```\r\nint (*RedisModule_RegisterBoolConfig)(RedisModuleCtx *ctx, char *name, int default_val, unsigned int flags, RedisModuleConfigGetBoolFunc getfn, RedisModuleConfigSetBoolFunc setfn, RedisModuleConfigApplyFunc applyfn, void *privdata);\r\nint (*RedisModule_RegisterNumericConfig)(RedisModuleCtx *ctx, const char *name, long long default_val, unsigned int flags, long long min, long long max, RedisModuleConfigGetNumericFunc getfn, RedisModuleConfigSetNumericFunc setfn, RedisModuleConfigApplyFunc applyfn, void *privdata);\r\nint (*RedisModule_RegisterStringConfig)(RedisModuleCtx *ctx, const char *name, const char *default_val, unsigned int flags, RedisModuleConfigGetStringFunc getfn, RedisModuleConfigSetStringFunc setfn, RedisModuleConfigApplyFunc applyfn, void *privdata);\r\nint (*RedisModule_RegisterEnumConfig)(RedisModuleCtx *ctx, const char *name, int default_val, unsigned int flags, const char **enum_values, const int *int_values, int num_enum_vals, RedisModuleConfigGetEnumFunc getfn, RedisModuleConfigSetEnumFunc setfn, RedisModuleConfigApplyFunc applyfn, void *privdata);\r\nint (*RedisModule_LoadConfigs)(RedisModuleCtx *ctx);\r\n```\r\n\r\nThe module name will be auto appended along with a \".\" to the front of the name of the config.\r\n\r\n**What RM_Register[...]Config does**:\r\n\r\nA RedisModule struct now keeps a list of ModuleConfig objects which look like:\r\n```\r\ntypedef struct ModuleConfig {\r\n    sds name; /* Name of config without the module name appended to the front */\r\n    void *privdata; /* Optional data passed into the module config callbacks */\r\n    union get_fn { /* The get callback specificed by the module */\r\n        RedisModuleConfigGetStringFunc get_string;\r\n        RedisModuleConfigGetNumericFunc get_numeric;\r\n        RedisModuleConfigGetBoolFunc get_bool;\r\n        RedisModuleConfigGetEnumFunc get_enum;\r\n    } get_fn;\r\n    union set_fn { /* The set callback specified by the module */\r\n        RedisModuleConfigSetStringFunc set_string;\r\n        RedisModuleConfigSetNumericFunc set_numeric;\r\n        RedisModuleConfigSetBoolFunc set_bool;\r\n        RedisModuleConfigSetEnumFunc set_enum;\r\n    } set_fn;\r\n    RedisModuleConfigApplyFunc apply_fn;\r\n    RedisModule *module;\r\n} ModuleConfig;\r\n```\r\nIt also registers a standardConfig in the configs array, with a pointer to the ModuleConfig object associated with it.\r\n\r\n**What happens on a CONFIG GET/SET MODULENAME.MODULECONFIG:**\r\n\r\nFor CONFIG SET, we do the same parsing as is done in config.c and pass that as the argument to the module set callback. For CONFIG GET, we call the module get callback and return that value to config.c to return to a client.\r\n\r\n**CONFIG REWRITE**:\r\n\r\nStarting up a server with module configurations in a .conf file but no module load directive will fail. The flip side is also true, specifying a module load and a bunch of module configurations will load those configurations in using the module defined set callbacks on a RM_LoadConfigs call. Configs being rewritten works the same way as it does for standard configs, as the module has the ability to specify a default value. If a module is unloaded with configurations specified in the .conf file those configurations will be commented out from the .conf file on the next config rewrite.\r\n\r\n**RM_LoadConfigs:**\r\n\r\n`RedisModule_LoadConfigs(RedisModuleCtx *ctx);`\r\n\r\nThis last API is used to make configs available within the onLoad() after they have been registered. The expected usage is that a module will register all of its configs, then call LoadConfigs to trigger all of the set callbacks, and then can error out if any of them were malformed. LoadConfigs will attempt to set all configs registered to either a .conf file argument/loadex argument or their default value if an argument is not specified. **LoadConfigs is a required function if configs are registered.** Also note that LoadConfigs **does not** call the apply callbacks, but a module can do that directly after the LoadConfigs call.\r\n\r\n**New Command: MODULE LOADEX [CONFIG NAME VALUE] [ARGS ...]:**\r\n\r\nThis command provides the ability to provide startup context information to a module. LOADEX stands for \"load extended\" similar to GETEX. Note that provided config names need the full MODULENAME.MODULECONFIG name. Any additional arguments a module might want are intended to be specified after ARGS. Everything after ARGS is passed to onLoad as RedisModuleString **argv.\r\n\r\n**Notes:**\r\nThere was an ask for double, my suggestion is that you should store a double as just a string and do conversion within the module. You get into weird cases like precision with floating points that vary between systems.",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2022-02-11T11:34:29Z",
        "closed_at": "2022-02-11T18:15:52Z",
        "merged_at": "2022-02-11T18:15:52Z",
        "body": "The bug is introduced by #9323. (released in 7.0 RC1)\r\nThe define of `REDISMODULE_OPTIONS_HANDLE_IO_ERRORS` and `REDISMODULE_OPTION_NO_IMPLICIT_SIGNAL_MODIFIED` have the same value.\r\n\r\nThis will result in skipping `signalModifiedKey()` after `RM_CloseKey()` if the module has set `REDISMODULE_OPTIONS_HANDLE_REPL_ASYNC_LOAD` option.\r\nThe implication is missing WATCH and client side tracking invalidations.\r\n\r\nOther changes:\r\n- add `no-implicit-signal-modified` to the options in INFO modules",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2022-02-11T09:30:48Z",
        "closed_at": "2022-02-11T16:47:04Z",
        "merged_at": "2022-02-11T16:47:04Z",
        "body": "In multi-part aof,  We no longer have the concept of `RDB-preamble`, so the related logs should be removed. However, in order to print compatible logs when loading old-style AOFs, we also have to keep the relevant code.\r\nAdditionally, when saving an RDB, change the RDB aux field from \"aof-preamble\" to \"aof-base\".",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 576,
        "deletions": 37,
        "changed_files": 22,
        "created_at": "2022-02-11T07:41:03Z",
        "closed_at": "2022-02-11T08:45:20Z",
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": false,
        "additions": 50,
        "deletions": 18,
        "changed_files": 5,
        "created_at": "2022-02-10T20:55:45Z",
        "closed_at": "2022-02-22T11:22:43Z",
        "merged_at": null,
        "body": "This PR messes with two things\r\n1. Server error reply statistics in script calls.\r\n2. Error code and error strings returning from scripts.\r\n\r\nFor error statistics, this PR does the following:\r\n* When a script gets an error from redis.pcall and handles it, the error should not be counted (used to be counted).\r\n* If the error from redis.pcall is returned by the script to the client, then it is counted (was already counted, so with the above it means it was counted twice).\r\n* Error that is thrown by redis.call or by any other mechanism is obviously counted (not changed by this PR)\r\n\r\nFor error strings and error codes, this PR attempts to make sure the error replies that are generated by the redis command calls and wrapped with additional Lua context details retain the original error code.\r\nThis is an additional improvement on top of what was recently done in #10218\r\n\r\n```diff\r\n 127.0.0.1:6379> eval \"return redis.call('set','x','y')\" 0\r\n-(error) ERR Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479): @user_script:1: OOM command not allowed when used memory > 'maxmemory'.\r\n+(error) OOM command not allowed when used memory > 'maxmemory'.. @user_script:1. Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479)\r\n```\r\n\r\nOther changes:\r\n1. remove excessive newlines when calling addReplyErrorFormat in luaCreateFunction and luaCallFunction\r\n\r\nUnresolved issues:\r\n1. Many parts of the code simply push an error string to Lua without an error code, e.g `lua_pushstring(lua, \"Invalid command passed to redis.acl_check_cmd()\");`, now that this is the initial part of the error string (not appended after a generic Lua context), it lacks an error code (return will return `-Invalid`)\r\n2. errors that are returned from the global protection code are starting like this: `user_script:1: Script attempted to create global` and i'm not sure how to properly prepend an `ERR` error code to them.\r\n\r\nNote somewhat related change done for modules: #10278",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 85,
        "deletions": 4,
        "changed_files": 6,
        "created_at": "2022-02-10T20:18:13Z",
        "closed_at": "2022-02-13T16:37:33Z",
        "merged_at": "2022-02-13T16:37:33Z",
        "body": "This PR handles several aspects\r\n1. Calls to RM_ReplyWithError from thread safe contexts don't violate thread safety.\r\n2. Errors returning from RM_Call to the module aren't counted in the statistics (they might be handled silently by the module)\r\n3. When a module propagates a reply it got from RM_Call to it's client, then the error statistics are counted.\r\n\r\nThis is done by:\r\n1. When appending an error reply to the output buffer, we avoid updating the global error statistics, instead we cache that error in a deferred list in the client struct.\r\n2. When creating a RedisModuleCallReply object, the deferred error list is moved from the client into that object.\r\n3. when a module calls RM_ReplyWithCallReply we copy the deferred replies to the dest client (if that's a real client, then that's when the error statistics are updated to the server)\r\n\r\nNote about RM_ReplyWithCallReply: if the original reply had an array with errors, and the module\r\nreplied with just a portion of the original reply, and not the entire reply, the errors are currently not\r\npropagated and the errors stats will not get propagated.\r\n\r\nFix #10180",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-02-10T18:38:46Z",
        "closed_at": "2022-02-11T07:14:27Z",
        "merged_at": "2022-02-11T07:14:27Z",
        "body": "A replica can't own a slot, only primaries can own the slot. Hence, we need to disable the slot transfer from a primary to a replica. \r\n\r\nVerify if the target node is a primary during `cluster setslot <slot> NODE <id>`\r\n\r\nBefore the change\r\n```\r\n127.0.0.1:7379> CLUSTER SETSLOT 1 NODE e2acf1a97c055fd09dcc2c0dcc62b19a6905dbc8\r\nOK\r\n127.0.0.1:7379> cluster nodes\r\nb7e9acc0def782aabe6b596f67f06c73c2ffff93 127.0.0.1:7379@17379 myself,master - 0 1644506261000 18 connected 0 5-5460 11111-11112 16112\r\n1461967c62eab0e821ed54f2c98e594fccfd8736 127.0.0.1:7381@17381 slave,fail? 71f058078c142a73b94767a4e78e9033d195dfb4 1644506176155 1644506173615 3 disconnected\r\n9215e30cd4a71070088778080565de6ef75fd459 127.0.0.1:6380@16380 master,fail? - 1644506176155 1644506173615 16 disconnected 5461-10922\r\ne2acf1a97c055fd09dcc2c0dcc62b19a6905dbc8 127.0.0.1:6379@16379 slave b7e9acc0def782aabe6b596f67f06c73c2ffff93 0 1644506261963 18 connected 1-4\r\n71f058078c142a73b94767a4e78e9033d195dfb4 127.0.0.1:6381@16381 master,fail? - 1644506175545 1644506173615 3 disconnected 10923-11110 11113-16111 16113-16383\r\n877fa59da72cb902d0563d3d8def3437fc3a0196 127.0.0.1:7380@17380 slave,fail? 9215e30cd4a71070088778080565de6ef75fd459 1644506174532 1644506173615 16 disconnected\r\n1\r\n```\r\n\r\nAfter the change\r\n```\r\n127.0.0.1:6379> CLUSTER SETSLOT 1 NODE e2acf1a97c055fd09dcc2c0dcc62b19a6905dbc8\r\n(error) ERR Please use SETSLOT only with masters.\r\n127.0.0.1:6379> CLUSTER NODES\r\nb7e9acc0def782aabe6b596f67f06c73c2ffff93 127.0.0.1:7379@17379 master - 0 1644518177561 18 connected 0-5460 11111-11112 16112\r\ne2acf1a97c055fd09dcc2c0dcc62b19a6905dbc8 127.0.0.1:6379@16379 myself,slave b7e9acc0def782aabe6b596f67f06c73c2ffff93 0 1644518177000 18 connected\r\n1461967c62eab0e821ed54f2c98e594fccfd8736 127.0.0.1:7381@17381 slave,fail? 71f058078c142a73b94767a4e78e9033d195dfb4 1644505039475 1644505037542 3 disconnected\r\n9215e30cd4a71070088778080565de6ef75fd459 127.0.0.1:6380@16380 master,fail? - 1644505040086 1644505037542 16 disconnected 5461-10922\r\n71f058078c142a73b94767a4e78e9033d195dfb4 127.0.0.1:6381@16381 master,fail? - 1644505040086 1644505037542 3 disconnected 10923-11110 11113-16111 16113-16383\r\n877fa59da72cb902d0563d3d8def3437fc3a0196 127.0.0.1:7380@17380 slave,fail? 9215e30cd4a71070088778080565de6ef75fd459 1644505038461 1644505037542 16 disconnected\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-02-10T16:23:00Z",
        "closed_at": "2023-08-20T16:17:52Z",
        "merged_at": "2023-08-20T16:17:52Z",
        "body": "In our test case, now we missed some test coverage for client sub-commands.\r\nThis pr goal is to add some test coverage cases of the following commands:\r\n\r\nClient caching\r\nClient kill\r\nClient no-evict\r\nClient pause\r\nClient reply\r\nClient tracking\r\nClient setname\r\n\r\nAt the very least, this is useful to make sure there are no leaks and crashes in these code paths.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-02-10T08:59:07Z",
        "closed_at": "2022-02-10T11:22:57Z",
        "merged_at": "2022-02-10T11:22:57Z",
        "body": "Allow binary value of a key, such as embedded nulls, to be seen in the log.\r\nAdditionally limit their length to 128 chars\r\n\r\nTest case:\r\nUse rejson.so from [2.0.6 module](http://redismodules.s3.amazonaws.com/rejson/rejson.Linux-ubuntu18.04-x86_64.2.0.6.zip)\r\n```\r\nMODULE LOAD /path_to/rejson.so\r\njson.set test $ '{}'\r\njson.get test \"\\x00abcdefghijklmnopqrstuvwxyz\\x00ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\r\n\r\n```\r\nThis leads to a crash (fixed on RedisJson master)\r\nBefore this fix, the value in the crash report is seen as empty `''`\r\nAfter this fix, up to 128 chars of the value are seen in the log (nulls will be seen as `\\x00`):\r\n...\r\nargv[1]: '\"test\"'\r\nargv[2]: `'\"\\x00abcdefghijklmnopqrstuvwxyz\\x00ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuv\"'`\r\n...\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-02-10T07:19:18Z",
        "closed_at": "2022-03-17T03:26:31Z",
        "merged_at": "2022-03-17T03:26:31Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6763,
        "deletions": 315,
        "changed_files": 403,
        "created_at": "2022-02-09T17:23:34Z",
        "closed_at": "2023-03-11T08:14:16Z",
        "merged_at": "2023-03-11T08:14:16Z",
        "body": "Work in progress towards implementing a reply schema as part of COMMAND DOCS, see #9845\r\nSince ironing the details of the reply schema of each and every command can take a long time, we would like to merge this PR when the infrastructure is ready, and let this mature in the unstable branch.\r\nMeanwhile the changes of this PR are internal, they are part of the repo, but do not affect the produced build.\r\n\r\n### Background\r\nIn https://github.com/redis/redis/pull/9656 we add a lot of information about Redis commands, but we are missing information about the replies\r\n\r\n### Motivation\r\n1. Documentation. This is the primary goal.\r\n2. It should be possible, based on the output of COMMAND, to be able to generate client code in typed languages. In order to do that, we need Redis to tell us, in detail, what each reply looks like.\r\n3. We would like to build a fuzzer that verifies the reply structure (for now we use the existing testsuite, see the \"Testing\" section)\r\n\r\n### Schema\r\nThe idea is to supply some sort of schema for the various replies of each command.\r\nThe schema will describe the conceptual structure of the reply (for generated clients), as defined in RESP3.\r\nNote that the reply structure itself may change, depending on the arguments (e.g. `XINFO STREAM`, with and without the `FULL` modifier)\r\nWe decided to use the standard json-schema (see https://json-schema.org/) as the reply-schema.\r\n\r\nExample for `BZPOPMIN`:\r\n```\r\n\"reply_schema\": {\r\n    \"oneOf\": [\r\n        {\r\n            \"description\": \"Timeout reached and no elements were popped.\",\r\n            \"type\": \"null\"\r\n        },\r\n        {\r\n            \"description\": \"The keyname, popped member, and its score.\",\r\n            \"type\": \"array\",\r\n            \"minItems\": 3,\r\n            \"maxItems\": 3,\r\n            \"items\": [\r\n                {\r\n                    \"description\": \"Keyname\",\r\n                    \"type\": \"string\"\r\n                },\r\n                {\r\n                    \"description\": \"Member\",\r\n                    \"type\": \"string\"\r\n                },\r\n                {\r\n                    \"description\": \"Score\",\r\n                    \"type\": \"number\"\r\n                }\r\n            ]\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\n#### Notes\r\n1.  It is ok that some commands' reply structure depends on the arguments and it's the caller's responsibility to know which is the relevant one. this comes after looking at other request-reply systems like OpenAPI, where the reply schema can also be oneOf and the caller is responsible to know which schema is the relevant one.\r\n2. The reply schemas will describe RESP3 replies only. even though RESP3 is structured, we want to use reply schema for documentation (and possibly to create a fuzzer that validates the replies)\r\n3. For documentation, the description field will include an explanation of the scenario in which the reply is sent, including any relation to arguments. for example, for `ZRANGE`'s two schemas we will need to state that one is with `WITHSCORES` and the other is without.\r\n4. For documentation, there will be another optional field \"notes\" in which we will add a short description of the representation in RESP2, in case it's not trivial (RESP3's `ZRANGE`'s nested array vs. RESP2's flat array, for example)\r\n\r\nGiven the above:\r\n1. We can generate the \"return\" section of all commands in [redis-doc](https://redis.io/commands/) (given that \"description\" and \"notes\" are comprehensive enough)\r\n2. We can generate a client in a strongly typed language (but the return type could be a conceptual `union` and the caller needs to know which schema is relevant). see the section below for RESP2 support.\r\n3. We can create a fuzzer for RESP3.\r\n\r\n### Limitations (because we are using the standard json-schema)\r\nThe problem is that Redis' replies are more diverse than what the json format allows. This means that, when we convert the reply to a json (in order to validate the schema against it), we lose information (see the \"Testing\" section below).\r\nThe other option would have been to extend the standard json-schema (and json format) to include stuff like sets, bulk-strings, error-string, etc. but that would mean also extending the schema-validator - and that seemed like too much work, so we decided to compromise.\r\n\r\nExamples:\r\n1. We cannot tell the difference between an \"array\" and a \"set\"\r\n2. We cannot tell the difference between simple-string and bulk-string\r\n3. we cannot verify true uniqueness of items in commands like ZRANGE: json-schema doesn't cover the case of two identical members with different scores (e.g. `[[\"m1\",6],[\"m1\",7]]`) because `uniqueItems` compares (member,score) tuples and not just the member name. \r\n\r\n### Testing\r\nThis commit includes some changes inside Redis in order to verify the schemas (existing and future ones) are indeed correct (i.e. describe the actual response of Redis).\r\nTo do that, we added a debugging feature to Redis that causes it to produce a log of all the commands it executed and their replies.\r\nFor that, Redis needs to be compiled with `-DLOG_REQ_RES` and run with `--reg-res-logfile <file> --client-default-resp 3` (the testsuite already does that if you run it with `--log-req-res --force-resp3`)\r\nYou should run the testsuite with the above args (and `--dont-clean`) in order to make Redis generate `.reqres` files (same dir as the `stdout` files) which contain request-response pairs.\r\nThese files are later on processed by `./utils/req-res-log-validator.py` which does:\r\n1. Goes over req-res files, generated by redis-servers, spawned by the testsuite (see logreqres.c)\r\n2. For each request-response pair, it validates the response against the request's reply_schema (obtained from the extended COMMAND DOCS)\r\n5. In order to get good coverage of the Redis commands, and all their different replies, we chose to use the existing redis test suite, rather than attempt to write a fuzzer.\r\n\r\n#### Notes about RESP2\r\n1. We will not be able to use the testing tool to verify RESP2 replies (we are ok with that, it's time to accept RESP3 as the future RESP)\r\n2. Since the majority of the test suite is using RESP2, and we want the server to reply with RESP3 so that we can validate it, we will need to know how to convert the actual reply to the one expected.\r\n   - number and boolean are always strings in RESP2 so the conversion is easy\r\n   - objects (maps) are always a flat array in RESP2\r\n   - others (nested array in RESP3's `ZRANGE` and others) will need some special per-command handling (so the client will not be totally auto-generated)\r\n\r\nExample for ZRANGE:\r\n```\r\n\"reply_schema\": {\r\n    \"anyOf\": [\r\n        {\r\n            \"description\": \"A list of member elements\",\r\n            \"type\": \"array\",\r\n            \"uniqueItems\": true,\r\n            \"items\": {\r\n                \"type\": \"string\"\r\n            }\r\n        },\r\n        {\r\n            \"description\": \"Members and their scores. Returned in case `WITHSCORES` was used.\",\r\n            \"notes\": \"In RESP2 this is returned as a flat array\",\r\n            \"type\": \"array\",\r\n            \"uniqueItems\": true,\r\n            \"items\": {\r\n                \"type\": \"array\",\r\n                \"minItems\": 2,\r\n                \"maxItems\": 2,\r\n                \"items\": [\r\n                    {\r\n                        \"description\": \"Member\",\r\n                        \"type\": \"string\"\r\n                    },\r\n                    {\r\n                        \"description\": \"Score\",\r\n                        \"type\": \"number\"\r\n                    }\r\n                ]\r\n            }\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\n### Other changes\r\n1. Some tests that behave differently depending on the RESP are now being tested for both RESP, regardless of the special log-req-res mode (\"Pub/Sub PING\" for example)\r\n2. Update the history field of CLIENT LIST\r\n3. Added basic tests for commands that were not covered at all by the testsuite\r\n\r\n### TODO\r\n\r\n- [x] (maybe a different PR) add a \"condition\" field to anyOf/oneOf schemas that refers to args. e.g. when `SET` return NULL, the condition is `arguments.get||arguments.condition`, for `OK` the condition is `!arguments.get`, and for `string` the condition is `arguments.get` - https://github.com/redis/redis/issues/11896\r\n- [x] (maybe a different PR) also run `runtest-cluster` in the req-res logging mode\r\n- [x] add the new tests to GH actions (i.e. compile with `-DLOG_REQ_RES`, run the tests, and run the validator)\r\n- [x] (maybe a different PR) figure out a way to warn about (sub)schemas that are uncovered by the output of the tests - https://github.com/redis/redis/issues/11897\r\n- [x] (probably a separate PR) add all missing schemas\r\n- [x] check why \"SDOWN is triggered by misconfigured instance replying with errors\" fails with --log-req-res\r\n- [x] move the response transformers to their own file (run both regular, cluster, and sentinel tests - need to fight with the tcl including mechanism a bit)\r\n- [x] issue: module API - https://github.com/redis/redis/issues/11898\r\n- [x] (probably a separate PR): improve schemas: add `required` to `object`s - https://github.com/redis/redis/issues/11899",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 18,
        "changed_files": 2,
        "created_at": "2022-02-09T12:32:10Z",
        "closed_at": "2022-02-09T13:44:09Z",
        "merged_at": "2022-02-09T13:44:09Z",
        "body": "append for PR #9812 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 115,
        "deletions": 7,
        "changed_files": 4,
        "created_at": "2022-02-09T08:41:52Z",
        "closed_at": "2022-02-11T19:58:05Z",
        "merged_at": "2022-02-11T19:58:05Z",
        "body": "Fix scripts defragger since it was broken since #10126 (released in 7.0 RC1).\r\nwould crash the server if defragger starts in a server that contains eval scripts.\r\n\r\nIn #10126 the global `lua_script` dict became a dict to a custom `luaScript` struct with an internal `robj` in it instead of a generic `sds` -> `robj` dict.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-02-09T03:00:33Z",
        "closed_at": "2022-02-09T05:32:41Z",
        "merged_at": "2022-02-09T05:32:41Z",
        "body": "Fixed some syntax errors in the comments",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-02-09T02:49:48Z",
        "closed_at": "2022-02-09T05:33:24Z",
        "merged_at": "2022-02-09T05:33:24Z",
        "body": "Introduced in #6891",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-02-08T17:21:52Z",
        "closed_at": "2022-02-09T20:09:20Z",
        "merged_at": "2022-02-09T20:09:20Z",
        "body": "Add a missed test case of more than 3 arguments as input ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-02-08T16:09:50Z",
        "closed_at": "2022-02-08T17:10:14Z",
        "merged_at": "2022-02-08T17:10:13Z",
        "body": "The theory is that a replica gets disconnected from within REPLCONF ACK,\r\nso when we go up the stack, we'll crash when attempting to access\r\nc->cmd->flags\r\n\r\nfix #10221\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 21,
        "changed_files": 1,
        "created_at": "2022-02-08T05:21:25Z",
        "closed_at": "2022-02-08T16:45:48Z",
        "merged_at": "2022-02-08T16:45:48Z",
        "body": "There are two issues in SENTINEL DEBUG:\r\n1. The error message should mention SENTINEL DEBUG\r\n2. Add missing reuturn in args parse.\r\n\r\n```\r\nredis> sentinel debug INFO-PERIOD aaa\r\n(error) ERR Invalid argument 'aaa' for SENTINEL SET 'INFO-PERIOD'\r\n\r\nredis> sentinel debug a b c d\r\n(error) ERR Unknown option or number of arguments for SENTINEL SET 'a'\r\nredis> ping\r\n(error) ERR Unknown option or number of arguments for SENTINEL SET 'b'\r\n```\r\n\r\nIntroduced in #9291. Also do some cleanups in the code.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 146,
        "deletions": 20,
        "changed_files": 3,
        "created_at": "2022-02-07T21:30:13Z",
        "closed_at": "2022-02-22T10:09:47Z",
        "merged_at": "2022-02-22T10:09:47Z",
        "body": "When WATCH is called on a key that's already logically expired, avoid discarding the transaction when the keys is actually deleted.\r\n\r\nThis is a revamp of #9234.\r\n\r\nWhen WATCH is called, a flag is stored if the key is already expired\r\nat the time of watch. The expired key is not deleted, only checked.\r\n\r\nWhen a key is \"touched\", if it is deleted and it was already expired\r\nwhen a client watched it, the client is not marked as dirty.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2022-02-07T16:33:21Z",
        "closed_at": "2022-02-07T17:57:51Z",
        "merged_at": "2022-02-07T17:57:51Z",
        "body": "If summary or since is empty, we used to return NULL in\r\nCOMMAND DOCS. Currently all redis commands will have these\r\ntwo fields.\r\n\r\nBut not for module command, summary and since are optional\r\nfor RM_SetCommandInfo. With the change in #10043, if a module\r\ncommand doesn't have the summary or since, redis-cli will\r\ncrash (see #10250).\r\n\r\nIn this commit, COMMAND DOCS avoid adding summary or since\r\nwhen they are missing.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2022-02-07T10:30:30Z",
        "closed_at": "2022-04-11T09:59:28Z",
        "merged_at": "2022-04-11T09:59:28Z",
        "body": "### **What problem does this PR solve?**\r\nThis PR makes changes in PR #10160 against the 6.2 branch.\r\n### **Check List**\r\nTests\r\n\r\n- Tests added in `integration-4.tcl`.\r\n\r\nCode changes\r\n\r\n- Add check enough good slaves for write command when evaluating scripts.",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2022-02-07T08:10:38Z",
        "closed_at": "2022-02-08T11:19:27Z",
        "merged_at": "2022-02-08T11:19:27Z",
        "body": "There's an assertion added recently to make sure that non-write commands don't use lookupKeyWrite,\r\nIt was initially meant to be used only on read-only replicas, but we thought it'll not have enough coverage,\r\nso used it on the masters too.\r\nWe now realize that in some cases this can cause issues for modules, so we remove the assert.\r\n\r\nOther than that, we also make sure not to force expireIfNeeded on read-only replicas.\r\neven if they somehow run a write command.\r\n\r\nSee https://github.com/redis/redis/pull/9572#discussion_r800179373",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-02-06T19:58:32Z",
        "closed_at": "2022-02-06T22:10:05Z",
        "merged_at": "2022-02-06T22:10:05Z",
        "body": "So far we only tested attributes using `readraw`, not the resp parser caches them, so that after getting the reply, you can query them if you want.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-02-06T14:58:55Z",
        "closed_at": "2022-02-07T04:13:34Z",
        "merged_at": "2022-02-07T04:13:34Z",
        "body": "add define guards to avoid multi-inclusion",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2022-02-06T14:54:11Z",
        "closed_at": "2022-02-07T05:53:15Z",
        "merged_at": null,
        "body": "header inclusions should be contained by the define guards.\r\nMove define guard ahead of them.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2022-02-06T08:48:41Z",
        "closed_at": "2022-02-06T11:13:56Z",
        "merged_at": "2022-02-06T11:13:56Z",
        "body": "`PSYNC replicationid str_offset` will crash the server.\r\n\r\nThe reason is in `masterTryPartialResynchronization`,\r\nwe will call `getLongLongFromObjectOrReply` check the\r\noffset. With a wrong offset, it will add a reply and\r\nthen trigger a full SYNC and the client become a replica.\r\n\r\nSo crash in `c->bufpos == 0 && listLength(c->reply) == 0`.\r\nIn this commit, we check the psync_offset before entering\r\nfunction `masterTryPartialResynchronization`, and return.\r\n\r\nFix #10242\r\n\r\nThe server logs:\r\n```\r\n20332:M 06 Feb 2022 16:49:27.237 * Replica 127.0.0.1:<unknown-replica-port> asks for synchronization\r\n20332:M 06 Feb 2022 16:49:27.237 # Replica 127.0.0.1:<unknown-replica-port> asks for synchronization but with a wrong offset\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-02-05T15:03:17Z",
        "closed_at": "2022-02-05T16:40:09Z",
        "merged_at": "2022-02-05T16:40:09Z",
        "body": "make sure the scripts are executable",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2022-02-05T04:38:05Z",
        "closed_at": "2022-02-05T08:35:08Z",
        "merged_at": "2022-02-05T08:35:08Z",
        "body": "1. Update fcall.json and fcall_ro.json\r\n2. Update command.c\r\n3. Update help.h",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 319,
        "deletions": 80,
        "changed_files": 12,
        "created_at": "2022-02-04T19:45:32Z",
        "closed_at": "2022-02-08T08:01:36Z",
        "merged_at": "2022-02-08T08:01:35Z",
        "body": "- add COMMAND GETKEYSANDFLAGS sub-command\r\n- add RM_KeyAtPosWithFlags and GetCommandKeysWithFlags\r\n- RM_KeyAtPos and RM_CreateCommand set flags requiring full access for keys\r\n- RM_CreateCommand set VARIABLE_FLAGS\r\n- expose `variable_flags` flag in COMMAND INFO key-specs\r\n- getKeysFromCommandWithSpecs prefers key-specs over getkeys-api\r\n- add tests for all of these\r\n\r\nfix #10189, fix #10144",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-02-04T16:19:32Z",
        "closed_at": "2022-02-07T20:57:12Z",
        "merged_at": "2022-02-07T20:57:11Z",
        "body": "The protocol error was caused by the buggy `writeHandler` in `redis-benchmark.c`,\r\nwhich didn't handle one of the cases, thereby repeating data, leading to protocol errors\r\nwhen the values being sent are very long.\r\n\r\nThis PR fixes #10233, issue introduced by #7959",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-02-04T01:51:00Z",
        "closed_at": "2022-02-04T09:32:30Z",
        "merged_at": "2022-02-04T09:32:30Z",
        "body": "Introduced in #10128",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-02-02T18:33:07Z",
        "closed_at": "2022-02-03T08:25:37Z",
        "merged_at": "2022-02-03T08:25:37Z",
        "body": "The script which generates the markdown docs from module.c is updated to include the version in which each module API function was introduced.\r\n\r\nThe script uses git tags to find this information. If git is not available or if we're not in a git repo, the 'since' is silently skipped.\r\n\r\nThe line `**Available since:** (version)` is added after the function prototype. Example markdown:\r\n\r\n```diff\r\n <span id=\"RedisModule_Alloc\"></span>\r\n \r\n ### `RedisModule_Alloc`\r\n \r\n     void *RedisModule_Alloc(size_t bytes);\r\n \r\n+**Available since:** 4.0.0\r\n+\r\n Use like `malloc()`. Memory allocated with this function is reported in\r\n Redis INFO memory, used for keys eviction according to maxmemory settings\r\n and in general is taken into account as memory allocated by Redis.\r\n You should avoid using `malloc()`.\r\n \r\n```\r\n\r\nRename to utils/generate-module-api-doc.rb",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 189,
        "deletions": 48,
        "changed_files": 4,
        "created_at": "2022-02-02T12:53:37Z",
        "closed_at": "2022-02-08T08:20:10Z",
        "merged_at": "2022-02-08T08:20:09Z",
        "body": "Fix #7021 #8924 #10198\r\n\r\n# Intro\r\nBefore this commit X[AUTO]CLAIM used to transfer deleted entries from one PEL to another, but reply with \"nil\" for every such entry (instead of the entry id).\r\nThe idea (for XCLAIM) was that the caller could see this \"nil\", realize the entry no longer exists, and XACK it in order to remove it from PEL.\r\nThe main problem with that approach is that it assumes there's a correlation between the index of the \"id\" arguments and the array indices, which there isn't (in case some of the input IDs to XCLAIM never existed/read):\r\n```\r\n127.0.0.1:6379> XADD x 1 f1 v1\r\n\"1-0\"\r\n127.0.0.1:6379> XADD x 2 f1 v1\r\n\"2-0\"\r\n127.0.0.1:6379> XADD x 3 f1 v1\r\n\"3-0\"\r\n127.0.0.1:6379> XGROUP CREATE x grp 0\r\nOK\r\n127.0.0.1:6379> XREADGROUP GROUP grp Alice COUNT 2 STREAMS x >\r\n1) 1) \"x\"\r\n   2) 1) 1) \"1-0\"\r\n         2) 1) \"f1\"\r\n            2) \"v1\"\r\n      2) 1) \"2-0\"\r\n         2) 1) \"f1\"\r\n            2) \"v1\"\r\n127.0.0.1:6379> XDEL x 1 2\r\n(integer) 2\r\n127.0.0.1:6379> XCLAIM x grp Bob 0 0-99 1-0 1-99 2-0\r\n1) (nil)\r\n2) (nil)\r\n```\r\n\r\n# Changes\r\nNow,  X[AUTO]CLAIM acts in the following way:\r\n1. If one tries to claim a deleted entry, we delete it from the PEL we found it in (and the group PEL too). So de facto, such entry is not claimed, just cleared from PEL (since anyway it doesn't exist in the stream)\r\n2. since we never claim deleted entries, X[AUTO]CLAIM will never return \"nil\" instead of an entry.\r\n3. add a new element to XAUTOCLAIM's response (see below)\r\n\r\n# Knowing which entries were cleared from the PEL\r\nThe caller may want to log any entries that were found in a PEL but deleted from the stream itself (it would suggest that there might be a bug in the application: trimming the stream while some entries were still no processed by the consumers)\r\n\r\n## XCLAIM\r\nthe set {XCLAIM input ids} - {XCLAIM returned ids} contains all the entry ids that were not claimed which means they were deleted (assuming the input contains only entries from some PEL). The user doesn't need to XACK them because XCLAIM had already deleted them from the source PEL.\r\n\r\n## XAUTOCLAIM\r\nXAUTOCLAIM has a new element added to its reply: it's an array of all the deleted stream IDs it stumbled upon.\r\n\r\nThis is somewhat of a breaking change since X[AUTO]CLAIM used to be able to reply with \"nil\" and now it can't... But since it was undocumented (and generally a bad idea to rely on it, as explained above) the breakage is not that bad.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-02-01T15:39:22Z",
        "closed_at": "2022-02-01T19:39:10Z",
        "merged_at": "2022-02-01T19:39:10Z",
        "body": "1) Following #9658.\r\nFix build failed due to OS X 11.x doesn't have /usr/lib/libSystem.dylib.\r\n2) Fix a compile warning introduced by #10064.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2022-02-01T13:04:29Z",
        "closed_at": "2022-02-07T05:58:15Z",
        "merged_at": "2022-02-07T05:58:15Z",
        "body": "In #9788, now we stores all persistent append-only files in\r\na dedicated directory. The name of the directory is determined\r\nby the appenddirname configuration parameter in redis.conf\r\n\r\nUpdate create-cluster clean to clean this default directory.\r\nEach node have a separate folder `appendonlydir-{PORT}`.\r\nThis PR also do some cleanups, logs and stricter wildcard matching.\r\nFixes #10222",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 125,
        "deletions": 95,
        "changed_files": 3,
        "created_at": "2022-01-31T15:43:01Z",
        "closed_at": "2022-02-07T06:04:01Z",
        "merged_at": "2022-02-07T06:04:01Z",
        "body": "See issue #10091.\r\nThis PR:\r\n1. Adds the `redis.acl_check_cmd()` api to lua scripts. It can be used to check if the current user has permissions to execute a given command. The new function receives the command to check as an argument exactly like `redis.call()` receives the command to execute as an argument.\r\n2. In the PR I unified the code used to convert lua arguments to redis argv arguments from both the new `redis.acl_check_cmd()` API and the `redis.[p]call()` API. This cleans up potential duplicate code.\r\n3. While doing the refactoring in 2 I noticed there's an optimization to reduce allocation calls when parsing lua arguments into an `argv` array in the `redis.[p]call()` implementation. These optimizations were introduced years ago in 48c49c485155ba9e4a7851fd1644c171674c6f0f and 4f686555ce962e6632235d824512ea8fdeda003c. It is unclear why this was added. The original commit message claims a 4% performance increase which I couldn't recreate and might not be worth it even if it did recreate. This PR removes that optimization. Following are details of the benchmark I did that couldn't reveal any performance improvements due to this optimization:\r\n\r\n```\r\nbenchmark 1: src/redis-benchmark -P 500 -n 10000000 eval 'return redis.call(\"ping\")' 0\r\nbenchmark 2: src/redis-benchmark -P 500 -r 1000 -n 1000000 eval 'return redis.call(\"mset\",\"k1__rand_int__\",\"v1__rand_int__\",\"k2__rand_int__\",\"v2__rand_int__\",\"k3__rand_int__\",\"v3__rand_int__\",\"k4__rand_int__\",\"v4__rand_int__\")' 0\r\nbenchmark 3: src/redis-benchmark -P 500 -r 1000 -n 100000 eval \"for i=1,100,1 do redis.call('set','kk'..i,'vv'..__rand_int__) end return redis.call('get','kk5')\" 0\r\nbenchmark 4: src/redis-benchmark -P 500 -r 1000 -n 1000000 eval 'return redis.call(\"mset\",\"k1__rand_int__\",\"v1__rand_int__\",\"k2__rand_int__\",\"v2__rand_int__\",\"k3__rand_int__\",\"v3__rand_int__\",\"k4__rand_int__\",\"v4__rand_int__xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")'\r\n```\r\nI ran the benchmark on this branch with and without commit 68b71680a4d3bb8f0509e06578a9f15d05b92a47\r\nResults in requests per second:\r\ncmd | without optimization | without optimization 2nd run | with original optimization | with original optimization 2nd run\r\n-- | -- | -- | -- | --\r\n1 | 461233.34 | 477395.31 | 471098.16 | 469946.91\r\n2 | 34774.14 | 35469.8 | 35149.38 | 34464.93\r\n3 | 6390.59 | 6281.41 | 6146.28 | 6464.12\r\n4 | 28005.71 | \u00a0 | 27965.77 | \u00a0\r\n\r\nAs you can see, different use cases showed identical or negligible performance differences. So finally I decided to chuck the original optimization and simplify the code.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-31T15:15:32Z",
        "closed_at": "2022-02-02T08:39:34Z",
        "merged_at": "2022-02-02T08:39:34Z",
        "body": "Bumps [vmactions/freebsd-vm](https://github.com/vmactions/freebsd-vm) from 0.1.5 to 0.1.6.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vmactions/freebsd-vm/releases\">vmactions/freebsd-vm's releases</a>.</em></p>\n<blockquote>\n<p>support copyback option</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/45665d548dfeadbbebb5bb7971150211525fee46\"><code>45665d5</code></a> add copyback option</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/f6f0dc39c777f1b53bf712e06116ea8db0ce6740\"><code>f6f0dc3</code></a> Update README.md (<a href=\"https://github-redirect.dependabot.com/vmactions/freebsd-vm/issues/35\">#35</a>)</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/4ec2d7f11c4bfcc971032941c5d201efdc93cdfe\"><code>4ec2d7f</code></a> Update README.md</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/e014576c7ed4613fb32e0c86c03de9f4178fad54\"><code>e014576</code></a> Update test.yml</li>\n<li><a href=\"https://github.com/vmactions/freebsd-vm/commit/b44495860f3a5bbd8ba139afa2e84d1495f8f4e6\"><code>b444958</code></a> Update of the readme to use the latest version (<a href=\"https://github-redirect.dependabot.com/vmactions/freebsd-vm/issues/31\">#31</a>)</li>\n<li>See full diff in <a href=\"https://github.com/vmactions/freebsd-vm/compare/v0.1.5...v0.1.6\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vmactions/freebsd-vm&package-manager=github_actions&previous-version=0.1.5&new-version=0.1.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 88,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2022-01-31T10:50:20Z",
        "closed_at": "2022-02-08T09:44:41Z",
        "merged_at": "2022-02-08T09:44:40Z",
        "body": "This PR handles inconsistencies in errors returned from lua scripts. Details of the problem can be found in #10165.\r\n\r\n### Changes\r\n\r\n- Remove double stack trace. It's enough that a stack trace is automatically added by the engine's error handler see https://github.com/redis/redis/blob/d0bc4fff18afdf9e5421cc88e23ffbb876ecaec3/src/function_lua.c#L472-L485 and https://github.com/redis/redis/blob/d0bc4fff18afdf9e5421cc88e23ffbb876ecaec3/src/eval.c#L243-L255\r\n- Make sure all errors a preceded with an error code. Passing a simple string to `luaPushError()` will prepend it with a generic `ERR` error code.\r\n- Make sure lua error table doesn't include a RESP `-` error status. Lua stores redis error's as a lua table with a single `err` field and a string. When the string is translated back to RESP we add a `-` to it. See https://github.com/redis/redis/blob/d0bc4fff18afdf9e5421cc88e23ffbb876ecaec3/src/script_lua.c#L510-L517 So there's no need to store it in the lua table.\r\n\r\n### Before & After\r\n```diff\r\n--- <unnamed>\r\n+++ <unnamed>\r\n@@ -1,14 +1,14 @@\r\n  1: config set maxmemory 1\r\n  2: +OK\r\n  3: eval \"return redis.call('set','x','y')\" 0\r\n- 4: -ERR Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479): @user_script:1: @user_script: 1: -OOM command not allowed when used memory > 'maxmemory'.\r\n+ 4: -ERR Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479): @user_script:1: OOM command not allowed when used memory > 'maxmemory'.\r\n  5: eval \"return redis.pcall('set','x','y')\" 0\r\n- 6: -@user_script: 1: -OOM command not allowed when used memory > 'maxmemory'.\r\n+ 6: -OOM command not allowed when used memory > 'maxmemory'.\r\n  7: eval \"return redis.call('select',99)\" 0\r\n  8: -ERR Error running script (call to 4ad5abfc50bbccb484223905f9a16f09cd043ba8): @user_script:1: ERR DB index is out of range\r\n  9: eval \"return redis.pcall('select',99)\" 0\r\n 10: -ERR DB index is out of range\r\n 11: eval_ro \"return redis.call('set','x','y')\" 0\r\n-12: -ERR Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479): @user_script:1: @user_script: 1: Write commands are not allowed from read-only scripts.\r\n+12: -ERR Error running script (call to 71e6319f97b0fe8bdfa1c5df3ce4489946dda479): @user_script:1: ERR Write commands are not allowed from read-only scripts.\r\n 13: eval_ro \"return redis.pcall('set','x','y')\" 0\r\n-14: -@user_script: 1: Write commands are not allowed from read-only scripts.\r\n+14: -ERR Write commands are not allowed from read-only scripts.\r\n```\r\n \r\n### to do\r\n- [x] tests",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 53,
        "deletions": 17,
        "changed_files": 5,
        "created_at": "2022-01-29T23:40:10Z",
        "closed_at": "2022-01-30T09:30:19Z",
        "merged_at": "2022-01-30T09:30:19Z",
        "body": "Try to fix the rebalance cluster test that's failing with ASAN daily:\r\nhttps://github.com/redis/redis/runs/4987852171?check_suite_focus=true\r\n\r\nLooks like `redis-cli --cluster rebalance` gets `ERR Please use SETSLOT only with masters` in `clusterManagerMoveSlot()`.\r\nit happens when `12-replica-migration-2.tcl` is run with ASAN in GH Actions.\r\nin `Resharding all the master #0 slots away from it`\r\n\r\nSo the fix (assuming i got it right) is to call `redis-cli --cluster check` before `--cluster rebalance`.\r\np.s. it looks like a few other checks in these tests needed that wait, added them too.\r\n\r\nOther changes:\r\n* in instances.tcl, make sure to catch tcl test crashes and let the rest of the code proceed, so that if there was a redis crash, we'll find it and print it too.\r\n* redis-cli, try to make sure it prints an error instead of silently exiting.\r\n\r\nspecifically about redis-cli:\r\n1. clusterManagerMoveSlot used to print an error, only if the caller also asked for it (should be the other way around).\r\n2. clusterManagerCommandReshard asked for an error, but didn't use it (probably tried to avoid the double print).\r\n3. clusterManagerCommandRebalance didn't ask for the error, now it does.\r\n4. making sure that other places in clusterManagerCommandRebalance print something before exiting with an error.\r\n  ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2022-01-29T22:01:22Z",
        "closed_at": "2022-01-30T17:43:37Z",
        "merged_at": "2022-01-30T17:43:37Z",
        "body": "Fixes #10205",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-01-29T13:45:09Z",
        "closed_at": "2022-01-29T17:02:35Z",
        "merged_at": "2022-01-29T17:02:35Z",
        "body": "keep the push triggers for all repos, but run the scheduled ones only on redis/redis",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-29T01:39:30Z",
        "closed_at": "2022-01-29T12:43:17Z",
        "merged_at": "2022-01-29T12:43:17Z",
        "body": "By a happy coincidence, sizeof(sds *) is equal to sizeof(sds) here,\r\nwhile it's logically consistent to use sizeof(sds) instead.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-01-28T16:28:54Z",
        "closed_at": "2022-01-28T20:13:46Z",
        "merged_at": "2022-01-28T20:13:46Z",
        "body": "In ACL v2 there are two changes to the GETUSER command, that weren't documented as part of history.\r\n1) There is a new 6th field, selectors.\r\n2) The format of the keys and channels responses was changed from a list representation to the DLS representation.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2022-01-28T13:38:54Z",
        "closed_at": "2022-01-29T16:34:59Z",
        "merged_at": "2022-01-29T16:34:59Z",
        "body": "We recently removed capabilities from the first-arg feature of ACL and added a warning. but we didn't document it. \r\nref: #10147 and https://github.com/redis/redis-doc/pull/1761",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-01-27T19:41:11Z",
        "closed_at": "2022-01-30T14:39:23Z",
        "merged_at": "2022-01-30T14:39:23Z",
        "body": "As Sentinel relies upon consensus algorithm, all sentinel instances,\r\nrandomize a time to initiate their next attempt to become the\r\nleader of the group. But time after time, all raffled the same value.\r\nThe problem is in the line `srand(time(NULL)^getpid())` such that\r\nall spinned up containers get same time (in seconds) and same pid\r\nwhich is always PID 1. I added material `tv_usec` and verified that even\r\nconsecutive calls brings different values and makes the difference.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 496,
        "deletions": 83,
        "changed_files": 2,
        "created_at": "2022-01-27T16:20:32Z",
        "closed_at": "2022-01-27T19:48:06Z",
        "merged_at": "2022-01-27T19:48:06Z",
        "body": "There are some inevitable changes between the old and new output, as a\r\nresult of the different `commands.json` semantics.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-01-27T08:21:02Z",
        "closed_at": "2022-04-15T16:47:45Z",
        "merged_at": null,
        "body": "On Tencent Cloud, for a Redis cluster, when readonly is enabled on the slave nodes, we usually use cluster slots to get all the slots information, and then use the slots information to select which slave node to run the command on. However, when the status of the slave node is `fail`, we use `cluster slots` to select which slave node to run the command on. Therefore, Tencent Cloud introduced the `pfail` status judgment of the slave node to solve the problem of the slave node in the `fail?` state obtained by the cluster slots command.\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 241,
        "deletions": 212,
        "changed_files": 16,
        "created_at": "2022-01-26T15:48:40Z",
        "closed_at": "2022-01-30T10:00:03Z",
        "merged_at": "2022-01-30T10:00:03Z",
        "body": "Add optional `notes` to keyspecs.\r\n\r\nOther changes:\r\n\r\n1. Remove the \"incomplete\" flag from SORT and SORT_RO: it is misleading since \"incomplete\" means \"this spec may not return all the keys it describes\" but SORT and SORT_RO's specs (except the input key) do not return any keys at all.\r\nSo basically:\r\nIf a spec's begin_search is \"unknown\" you should not use it at all, you must use COMMAND KEYS;\r\nif a spec itself is \"incomplete\", you can use it to get a partial list of keys, but if you want all of them you must use COMMAND GETKEYS;\r\notherwise, the spec will return all the keys\r\n\r\n2. `getKeysUsingKeySpecs` handles incomplete specs internally",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-01-26T12:41:57Z",
        "closed_at": "2022-01-26T17:45:31Z",
        "merged_at": "2022-01-26T17:45:31Z",
        "body": "Failed on a non-valgrind run. on this line:\r\n```\r\nassert_equal 0 [$slave exists k]\r\n```\r\nthe condition in `keyIsExpired` is `now > when`.\r\nso if the test is really fast, maybe it can get to EXISTS exactly 1000 milliseconds after the\r\nexpiration was set, and the key isn't yet gone)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-01-26T12:19:38Z",
        "closed_at": "2022-01-27T02:37:33Z",
        "merged_at": null,
        "body": "`luaRegisterLogFunction` and `luaRegisterVersion` are already called in `luaRegisterRedisAPI`, or I misunderstand something?",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2022-01-26T11:05:53Z",
        "closed_at": "2022-01-26T17:46:02Z",
        "merged_at": "2022-01-26T17:46:02Z",
        "body": "This PR attempts to solve two problems that happen sometime in valgrind:\r\n`ERR Background save already in progress`\r\nand\r\n`not bgsave not aborted`\r\n\r\nthe test used to populate the database with DEBUG, which didn't\r\nincrement the dirty counter, so couldn't trigger an automatic bgsave.\r\nthen it used a manual bgsave, and aborted it (when it got aborted it\r\npopulated the dirty counter), and then it tried to do another bgsave.\r\nthat other bgsave could have failed if the automatic one already\r\nstarted.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-26T09:29:57Z",
        "closed_at": "2022-01-26T10:49:24Z",
        "merged_at": "2022-01-26T10:49:24Z",
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 66,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2022-01-26T08:19:06Z",
        "closed_at": "2022-02-01T12:54:12Z",
        "merged_at": "2022-02-01T12:54:11Z",
        "body": "This is done to avoid a crash when the timer fires after the module was unloaded.\r\nOr memory leaks in case we wanted to just ignore the timer.\r\nIt'll cause the MODULE UNLOAD command to return with an error\r\nfix https://github.com/redis/redis/issues/10186",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-01-25T20:37:24Z",
        "closed_at": "2022-02-06T05:58:29Z",
        "merged_at": "2022-02-06T05:58:29Z",
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 63,
        "deletions": 36,
        "changed_files": 5,
        "created_at": "2022-01-25T14:30:43Z",
        "closed_at": "2022-01-30T10:02:55Z",
        "merged_at": "2022-01-30T10:02:55Z",
        "body": "For backwards compatibility in 6.x, channels default permission was set to `allchannels` however with 7.0, we should modify it and the default value should be `resetchannels` for better security posture. Also, with selectors in ACL, a client doesn't have to set channel rules everytime and by default the value will be `resetchannels`.\r\n\r\nThis is a breaking change, users that are badly affected by it, can easily revert the config back to the old default.\r\n\r\nBefore this change\r\n```\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* &* +@all\"\r\n127.0.0.1:6379>  acl setuser hp on nopass +@all ~*\r\nOK\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* &* +@all\"\r\n2) \"user hp on nopass ~* &* +@all\"\r\n127.0.0.1:6379>  acl setuser hp1 on nopass -@all (%R~sales*)\r\nOK\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* &* +@all\"\r\n2) \"user hp on nopass ~* &* +@all\"\r\n3) \"user hp1 on nopass &* -@all (%R~sales* &* -@all)\"\r\n```\r\n\r\nAfter this change\r\n```\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* &* +@all\"\r\n127.0.0.1:6379> acl setuser hp on nopass +@all ~*\r\nOK\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* &* +@all\"\r\n2) \"user hp on nopass ~* resetchannels +@all\"\r\n127.0.0.1:6379> acl setuser hp1 on nopass -@all (%R~sales*)\r\nOK\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* &* +@all\"\r\n2) \"user hp on nopass ~* resetchannels +@all\"\r\n3) \"user hp1 on nopass resetchannels -@all (%R~sales* resetchannels -@all)\"\r\n```",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 97,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-01-25T10:19:16Z",
        "closed_at": "2022-01-25T13:50:14Z",
        "merged_at": "2022-01-25T13:50:14Z",
        "body": "Added the following statistics (per engine) to FUNCTION STATS command:\r\n* number of functions\r\n* number of libraries\r\n\r\nOutput example:\r\n```\r\n> FUNCTION stats\r\n1) \"running_script\"\r\n2) (nil)\r\n3) \"engines\"\r\n4) 1) \"LUA\"\r\n   2) 1) \"libraries_count\"\r\n      2) (integer) 1\r\n      3) \"functions_count\"\r\n      4) (integer) 1\r\n```\r\n\r\nTo collect the stats, added a new dictionary to libraries_ctx that contains\r\nfor each engine, the engine statistics representing the current libraries_ctx.\r\nUpdate the stats on:\r\n1. Link library to libraries_ctx\r\n2. Unlink library from libraries_ctx\r\n3. Flushing libraries_ctx",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2022-01-25T09:56:12Z",
        "closed_at": "2022-02-17T12:32:49Z",
        "merged_at": "2022-02-17T12:32:49Z",
        "body": "Add aof_rewrites and rdb_snapshots counters to info. This is useful to figure out if a rewrite or snapshot happened since last check. This was part of the (ongoing) effort to provide a safe backup solution for multipart-aof backups.\r\n\r\n------------------\r\n\r\nNote, it was eventually decided not to include a backup script in the redis repository and instead document how to safely backup AOF files, see: https://github.com/redis/redis-doc/pull/1794\r\n\r\nOriginal top-comment before backup script was removed from this PR:\r\n\r\nAOF backup script defined in #10063.\r\n\r\nIn Redis 7 we introduced multi-part AOFs (#9788) based on a manifest file that indicates which files are part of Redis's AOF persistence. Our updated documentation about backup tells the user to simply copy the directory where all the AOF files (and manifest) reside.\r\n\r\nThis is wrong and bad advice. The reason is that during the copy files in the directory might change. During a rewrite we might end up copying an old manifest file and new base and increment files. We'll end up with a useless backup.\r\n\r\nSo this PR adds a backups script that performs the following:\r\n1. Verify the server isn't performing a rewrite.\r\n2. Create hard links to files in the directory.\r\n3. Verify no rewrite started or happened since (1). If it did delete the hard links and go back to (1).\r\n4. Copy/gzip the hard links.\r\n5. Delete the hard links.\r\n\r\nOther changes:\r\nInclude stat counters for aof rewrites and bgsaves:\r\nUseful in general but also practical if we want to make sure a certain operation was performed without any aof rewrite in the middle.\r\n\r\n- [x] fix docs aof backup doc PR accordingly. \r\n- [x] Decide if we want to use a lock file, a mem flag or just use the existing `auto-aof-rewrite-percentage` (and fail backup if rewrite is in progress) config to disable rewrites.\r\n- [x] If we create a new way to abort and disable rewrites then decide if we want it to be a generic way to to abort all forks coupled with `auto-aof-rewrite-percentage` or a specific command for rewrites.\r\n",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-25T07:59:01Z",
        "closed_at": "2022-01-26T07:59:53Z",
        "merged_at": "2022-01-26T07:59:53Z",
        "body": "This is an attempt to fix some of the issues with the cluster mode tests we are seeing in the daily run. Discovered here: https://github.com/redis/redis/runs/4930082458?check_suite_focus=true\r\n\r\nThe test is trying to incrementally adds a bunch of publish messages, expecting that eventually one of them will overflow. The tests stops one of the processes, so it expects that just that one Redis node will overflow. I think the test is flaky because under certain circumstances multiple links are getting disconnected, not just the one that is stalled. \r\n\r\nHere is a test run for this change https://github.com/redis/redis/actions/runs/1744078367",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-01-25T07:04:57Z",
        "closed_at": "2022-01-25T10:24:06Z",
        "merged_at": "2022-01-25T10:24:06Z",
        "body": "Set _commands.c_'s merge driver to `binary` so when it conflicts during a merge git will leave the local version unmodified. This way our _Makefile_ will rebuild it based on _src/commands/*.json_ before trying to compile it. Otherwise the file gets modified with merge conflict markers and gets the same timestamp as the _*.json_ files, so the _Makefile_ doesn't attempt to rebuild it before compiling and we get a compilation error.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-01-24T22:58:54Z",
        "closed_at": "2022-01-25T15:15:58Z",
        "merged_at": "2022-01-25T15:15:58Z",
        "body": "As before, a map is rendered as `key => value`, but if `value` is multiline (e.g. if it's another map with > 1 element), a linebreak is inserted after `=>` to align the nested value's first line with the remaining lines.\r\n\r\nBefore:\r\n\r\n```\r\n127.0.0.1:6379> command info get\r\n1)  1) \"get\"\r\n    2) (integer) 2\r\n    3) 1~ readonly\r\n       2~ fast\r\n    4) (integer) 1\r\n    5) (integer) 1\r\n    6) (integer) 1\r\n    7) 1~ @read\r\n       2~ @string\r\n       3~ @fast\r\n    8) (empty set)\r\n    9) 1~ 1# \"flags\" => 1~ RO\r\n             2~ access\r\n          2# \"begin_search\" => 1# \"type\" => \"index\"\r\n             2# \"spec\" => 1# \"index\" => (integer) 1\r\n          3# \"find_keys\" => 1# \"type\" => \"range\"\r\n             2# \"spec\" => 1# \"lastkey\" => (integer) 0\r\n                2# \"keystep\" => (integer) 1\r\n                3# \"limit\" => (integer) 0\r\n   10) (empty set)\r\n```\r\n\r\nAfter:\r\n\r\n```\r\n127.0.0.1:6379> command info get\r\n1)  1) \"get\"\r\n    2) (integer) 2\r\n    3) 1~ readonly\r\n       2~ fast\r\n    4) (integer) 1\r\n    5) (integer) 1\r\n    6) (integer) 1\r\n    7) 1~ @read\r\n       2~ @string\r\n       3~ @fast\r\n    8) (empty set)\r\n    9) 1~ 1# \"flags\" =>\r\n             1~ RO\r\n             2~ access\r\n          2# \"begin_search\" =>\r\n             1# \"type\" => \"index\"\r\n             2# \"spec\" => 1# \"index\" => (integer) 1\r\n          3# \"find_keys\" =>\r\n             1# \"type\" => \"range\"\r\n             2# \"spec\" =>\r\n                1# \"lastkey\" => (integer) 0\r\n                2# \"keystep\" => (integer) 1\r\n                3# \"limit\" => (integer) 0\r\n   10) (empty set)\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 84,
        "deletions": 11,
        "changed_files": 7,
        "created_at": "2022-01-24T18:41:20Z",
        "closed_at": "2022-01-25T07:55:30Z",
        "merged_at": "2022-01-25T07:55:30Z",
        "body": "This PR aims to improve the flags associated with some commands and adds various tests around these cases. Specifically, it's concerned with commands which declare keys but have no ACL flags (think `EXISTS`), the user needs either read or write permission to access this type of key.\r\n\r\nThis change is primarily concerned around commands in three categories:\r\n\r\n# General keyspace commands\r\nThese commands are agnostic to the underlying data outside of side channel attacks, so they are not marked as ACCESS.\r\n* TOUCH\r\n* EXISTS\r\n* TYPE\r\n* OBJECT 'all subcommands'\r\n\r\nNote that TOUCH is not a write command, it could be a side effect of either a read or a write command.\r\n\r\n# Length and cardinality commands\r\nThese commands are marked as NOT marked as ACCESS since they don't return actual user strings, just metadata.\r\n* LLEN\r\n* STRLEN\r\n* SCARD\r\n* HSTRLEN\r\n\r\n# Container has member commands\r\nThese commands return information about the existence or metadata about the key. These commands are NOT marked as ACCESS since the check of membership is used widely in write commands (e.g. the response of HSET). \r\n* SISMEMBER\r\n* HEXISTS\r\n\r\n# Intersection cardinality commands\r\nThese commands are marked as ACCESS since they process data to compute the result.\r\n* PFCOUNT\r\n* ZCOUNT\r\n* ZINTERCARD\r\n* SINTERCARD\r\n\r\nPrevious discussions on that subject: #10040, #10159",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-01-24T16:48:59Z",
        "closed_at": "2022-01-24T20:02:42Z",
        "merged_at": "2022-01-24T20:02:42Z",
        "body": "For some complex data types, server.dirty actually counts\r\nthe number of elements that have been changed.\r\nAnd in FLUSHDB or FLUSHALL, we count the number of keys.\r\n\r\nSo the word \"key\" is not strictly correct and is outdated.\r\nSome discussion can be seen at #8140.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 22,
        "changed_files": 1,
        "created_at": "2022-01-24T09:35:07Z",
        "closed_at": "2022-01-24T12:55:24Z",
        "merged_at": "2022-01-24T12:55:24Z",
        "body": "issue link: https://github.com/redis/redis/pull/9788#issuecomment-1019428620   ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 116,
        "deletions": 33,
        "changed_files": 5,
        "created_at": "2022-01-23T03:00:52Z",
        "closed_at": "2022-02-24T05:55:36Z",
        "merged_at": null,
        "body": "Fixes #10155 \r\n\r\nThis PR improves the consistency of \"cluster nodes\" output such that, regardless of whether there exists a replica for the failed primary node, \"cluster nodes\" executed on any remaining node in the cluster now contains the slot info for the failed primary node. \r\n\r\n[Before the change]\r\n\r\n...\r\n72f66f4ecbcfc3055faae01a68bbeadc5079bb92 127.0.0.1:30001@40001 master,fail - 1642667704665 1642667703661 1 disconnected\r\n...\r\n\r\nWith the change\r\n\r\n...\r\n72f66f4ecbcfc3055faae01a68bbeadc5079bb92 127.0.0.1:30001@40001 master,fail - 1642667704665 1642667703661 1 disconnected **0-5460**\r\n...\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 58,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2022-01-21T14:30:48Z",
        "closed_at": "2022-02-03T09:57:51Z",
        "merged_at": "2022-02-03T09:57:51Z",
        "body": "### **What problem does this PR solve?**\r\nReference this issue:: #9993\r\nCurrently, there is no check min-slave-* config when evaluating Lua script.\r\n\r\nBefore the fix:\r\n```\r\n127.0.0.1:7000> eval \"return redis.call('set','P','value')\" 0\r\nOK\r\n``` \r\nAfter the fix:\r\n```\r\n127.0.0.1:7000> eval \"return redis.call('set','P','value')\" 0\r\n(error) ERR Error running script (call to d05e85801446f5c3b3262aa9928ebf556baf9d64): @user_script:1: @user_script: 1: -NOREPLICAS Not enough good replicas to write.\r\n``` \r\n### **Check List**\r\nTests\r\n\r\n- Tests added in `integration-4.tcl`.\r\n\r\nCode changes\r\n\r\nAdd check enough good slaves for write command when evaluating scripts.\r\nThis check is made before the script is executed, if we have function flags, and per redis command if we don't.\r\n\r\nTo Do\r\n- [x] in functions or eval with shebang, unless no-writes flags is set prevent the script from running in the first place",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2022-01-21T06:41:01Z",
        "closed_at": "2022-01-25T15:14:06Z",
        "merged_at": null,
        "body": "Introduced by #9974 #10122, some special commands (i.e. don't touch value like `EXISTS` `HLEN`) miss ACL key check, for example:\r\n\r\n```\r\n127.0.0.1:6379> acl setuser test on nopass +@all resetkeys %W~a\r\nOK\r\n127.0.0.1:6379> auth test xxx\r\nOK\r\n127.0.0.1:6379> exists a\r\n(integer) 1\r\n```\r\n\r\nUser test only have write permission on key `a`, but read command `EXISTS` is still executed successfully.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 23,
        "changed_files": 1,
        "created_at": "2022-01-20T21:38:24Z",
        "closed_at": "2022-01-24T01:28:33Z",
        "merged_at": "2022-01-24T01:28:32Z",
        "body": "Fix two flaky tests introduced in PR https://github.com/redis/redis/pull/9774\r\n\r\n### For test \"Each node has two links with each peer\"\r\n\r\n```\r\n00:39:21> Each node has two links with each peer: FAILED: Expected 19*2 eq 37 (context: type eval line 11 cmd {assert {$num_peers*2 eq $num_links}} proc ::foreach_instance_id)\r\n(Jumping to next unit after error)\r\n```\r\n\r\nThis test seems to be flaky because cluster cluster is not stable and sometimes a node doesn't have both inbound and outbound connections established with every peer.\r\n\r\nThis failure is more rare than the next one. The fix is to add retries.\r\n\r\n### For test \"Disconnect link when send buffer limit reached\"\r\n\r\nThere were two sources of failure.\r\n\r\n1.\r\n\r\n```\r\n 00:47:22> Disconnect link when send buffer limit reached: error writing \"sock802fbc590\": broken pipe\r\n    while executing\r\n\"$primary1 publish channel [prepare_value [expr 30*1024*1024]]\"\r\n```\r\n\r\nRedis getting OOM killed by kernel due to out of swap. In the test, I'm allowing cluster link buffers to grow up to 32MB. There are 20 Redis nodes running in parallel in cluster tests. That proved to be too much for the FreeBSD test environment used by the daily runs.\r\n\r\nExample failure link: https://github.com/redis/redis/runs/4733591841?check_suite_focus=true\r\n\r\nFix is to use smaller cluster link buffer limits and fill it up by repeatedly sending smallish messages. This approach should be adaptive to different test environments. \r\n\r\n2.\r\n\r\n```\r\n00:46:57> Disconnect link when send buffer limit reached: FAILED: Expected [get_info_field [::redis::redisHandle1876 cluster info] total_cluster_links_buffer_limit_exceeded] eq 1 (context: type eval line 36 cmd {assert {[get_info_field [$primary1 cluster info] total_cluster_links_buffer_limit_exceeded] eq 1}} proc ::test)\r\n```\r\n\r\nI'm assuming as soon as I send a large PUBLISH command to fill up a cluster link, the link will be freed. But in reality the link will only get freed in the next clusterCron run whenever that happens. My test is not accounting for this race condition. \r\n\r\nExample failure link: https://github.com/redis/redis/runs/4829401183?check_suite_focus=true#step:9:630\r\n\r\nFix is to wait for 0.5s before checking if link has been freed.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 18,
        "changed_files": 2,
        "created_at": "2022-01-19T19:49:44Z",
        "closed_at": "2022-02-03T11:20:35Z",
        "merged_at": "2022-02-03T11:20:35Z",
        "body": "When performing `SENTINEL SET`, Sentinel updates the local configuration file. Before this commit, failure to update the file would still result with an `+OK` reply. Now, a `-ERR Failed to save config file` error will be returned.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 173,
        "deletions": 22,
        "changed_files": 7,
        "created_at": "2022-01-19T16:06:15Z",
        "closed_at": "2022-01-26T19:03:22Z",
        "merged_at": "2022-01-26T19:03:22Z",
        "body": "SET is a R+W command, because it can also do `GET` on the data.\r\nSET without GET is a write-only command.\r\nSET with GET is a read+write command.\r\n\r\nIn #9974, we added ACL to let users define write-only access.\r\nSo when the user uses SET with GET option, and the user doesn't\r\nhave the READ permission on the key, we need to reject it,\r\nbut we rather not reject users with write-only permissions from using\r\nthe SET command when they don't use GET.\r\n\r\nIn this commit, we add a `getkeys_proc` function to control key\r\nflags in SET command. We also add a new key spec flag (VARIABLE_FLAGS)\r\nmeans that some keys might have different flags depending on arguments.\r\n\r\nWe also handle BITFIELD command, add a `bitfieldGetKeys` function.\r\nBITFIELD GET is a READ ONLY command.\r\nBITFIELD SET or BITFIELD INCR are READ WRITE commands.\r\n\r\nOther changes:\r\n1. SET GET was added in 6.2, add the missing since in set.json\r\n2. Added tests to cover the changes in acl-v2.tcl\r\n3. Fix some typos in server.h and cleanups in acl-v2.tcl",
        "comments": 23
    },
    {
        "merged": true,
        "additions": 58,
        "deletions": 22,
        "changed_files": 4,
        "created_at": "2022-01-19T15:07:54Z",
        "closed_at": "2022-01-22T12:09:40Z",
        "merged_at": "2022-01-22T12:09:40Z",
        "body": "Fix #10132\r\nRecently we added extensive support for sub-commands in for redis 7.0, this meant that the old ACL mechanism for\r\nsub-commands wasn't needed, or actually was improved (to handle both include and exclude control, like for commands), but only for real sub-commands.\r\nThe old mechanism in ACL was renamed to first-arg, and was able to match the first argument of any\r\ncommand (including sub-commands).\r\nWe now realized that we might wanna completely delete that first-arg feature some day, so the first step\r\nwas not to give it new capabilities in 7.0 and it didn't have before.\r\n\r\nChanges:\r\n1. ACL: Block the first-arg mechanism on subcommands (we keep if in non-subcommands for backward compatibility)\r\n2. COMMAND: When looking up a command, insist the command name doesn't contain extra words.\r\n   Example: When a user issues `GET key` we want `lookupCommand` to return `getCommand` but\r\n   when if COMMAND calls `lookupCommand` with `get|key` we want it to fail.\r\n\r\nOther changes:\r\n1. ACLSetUser: prevent a redundant command lookup",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2022-01-19T14:41:58Z",
        "closed_at": "2022-01-29T19:00:30Z",
        "merged_at": "2022-01-29T19:00:29Z",
        "body": "Sentinel tries to resolve instances hostname to IP only during registration. \r\nIt might be that the instance is unavailable during that time, such as \r\nleader crashed and failover took place. Yet, promoted replica must support:\r\n - Register leader, even if it fails to resolve its hostname during failover\r\n - Try later to resolve it, if instance is disconnected. Note that\r\n   this condition also support ip-change of an instance.\r\n\r\nResolves #8540, Resolves #9103\r\nTest plan available [here](https://tinyurl.com/4erfy3zz)",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 45,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-01-19T09:13:04Z",
        "closed_at": "2022-01-19T19:21:43Z",
        "merged_at": "2022-01-19T19:21:43Z",
        "body": "Function PR was merged without AOF rw support because we thought this feature was going to be removed on Redis 7. As I understood it was eventually decided to keep and so adding AOF rw support for functions.\r\n\r\nTests was added on aofrw.tcl\r\nOther existing aofrw tests where slow due to unwanted rdb-key-save-delay",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 25,
        "changed_files": 1,
        "created_at": "2022-01-19T05:40:31Z",
        "closed_at": "2022-01-20T06:17:42Z",
        "merged_at": "2022-01-20T06:17:42Z",
        "body": "was merged to 5.0.11 by mistake, probably a bad merge conflict resolution",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2022-01-19T04:20:23Z",
        "closed_at": "2022-01-20T03:30:33Z",
        "merged_at": "2022-01-20T03:30:33Z",
        "body": "Renamenx right now double declares the same keys twice in keyspecs. I believe the intention was as follows. I found this through some automated testing.\r\n\r\nI think the flags are also wrong? I would expect the 2nd key flags to be INSERT instead of UPDATE, since this is the NX variant, but I am more interested in the wrong keys. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 3,
        "changed_files": 6,
        "created_at": "2022-01-18T19:11:17Z",
        "closed_at": "2022-01-24T13:02:08Z",
        "merged_at": "2022-01-24T13:02:08Z",
        "body": "The keyspec API is not yet released and there is a plan to change it\r\nin #10108, which is going to be included in RC2. Therefore, we hide\r\nit in RC1 to avoid introducing a breaking change in RC2.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2022-01-18T07:26:38Z",
        "closed_at": "2022-01-18T08:29:52Z",
        "merged_at": "2022-01-18T08:29:52Z",
        "body": "Following discussion on: https://github.com/redis/redis/issues/9899#issuecomment-1014689385\r\nRaise error if unknown parameter is given to `FUNCTION LOAD`.\r\n\r\nBefore the fix:\r\n```\r\n127.0.0.1:6379> function load LUA lib2 foo bar \"local function test1() return 5 end redis.register_function('test1', test1)\"\r\nOK\r\n```\r\n\r\nAfter the fix:\r\n```\r\n127.0.0.1:6379> function load LUA lib2 foo bar \"local function test1() return 5 end redis.register_function('test1', test1)\"\r\n(error) ERR Unknown option given: foo\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 50,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-17T22:06:02Z",
        "closed_at": "2022-02-03T02:22:47Z",
        "merged_at": "2022-02-03T02:22:47Z",
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 562,
        "deletions": 260,
        "changed_files": 55,
        "created_at": "2022-01-17T17:23:44Z",
        "closed_at": "2022-01-23T08:05:07Z",
        "merged_at": "2022-01-23T08:05:07Z",
        "body": "Summary of changes:\r\n1. Rename `redisCommand->name` to `redisCommand->declared_name`, it is a const char * for native commands and SDS for module commands.\r\n2. Store the [sub]command fullname in `redisCommand->fullname` (sds).\r\n3. List subcommands in `ACL CAT`\r\n4. List subcommands in `COMMAND LIST`\r\n5. `moduleUnregisterCommands` now will also free the module subcommands.\r\n6. RM_GetCurrentCommandName returns full command name\r\n\r\nOther changes:\r\n1. Add `addReplyErrorArity` and `addReplyErrorExpireTime`\r\n2. Remove `getFullCommandName` function that now is useless.\r\n3. Some cleanups about `fullname` since now it is SDS.\r\n4. Delete `populateSingleCommand` function from server.h that is useless.\r\n5. Added tests to cover this change.\r\n6. Add some module unload tests and fix the leaks\r\n7. Make error messages uniform, make sure they always contain the full command name and that it's quoted.\r\n7. Fixes some typos\r\n\r\nsee the history in #9504, fixes #10124",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 369,
        "deletions": 117,
        "changed_files": 15,
        "created_at": "2022-01-17T14:16:36Z",
        "closed_at": "2022-01-24T14:50:03Z",
        "merged_at": "2022-01-24T14:50:03Z",
        "body": "In #10025 we added a mechanism for flagging certain properties for Redis Functions. This lead us to think we'd like to \"port\" this mechanism to Redis Scripts (`EVAL`) as well. \r\n\r\nOne good reason for this, other than the added functionality is because it addresses the poor behavior we currently have in `EVAL` in case the script performs a (non DENY_OOM) write operation during OOM state. See #8478 (And a previous attempt to handle it via #10093) for details. Note that in Redis Functions **all** write operations (including DEL) will return an error during OOM state unless the function is flagged as `allow-oom` in which case no OOM checking is performed at all.\r\n\r\nThis PR:\r\n- Enables setting `EVAL` (and `SCRIPT LOAD`) script flags as defined in #10025.\r\n- Provides a syntactical framework via [shebang](https://en.wikipedia.org/wiki/Shebang_(Unix)) for additional script annotations and even engine selection (instead of just lua) for scripts.\r\n- Provides backwards compatibility so scripts without the new annotations will behave as they did before.\r\n- Appropriate tests.\r\n- Changes `EVAL[SHA]/_RO` to be flagged as `STALE` commands. This makes it possible to flag individual scripts as `allow-stale` or not flag them as such. In backwards compatibility mode these commands will return the `MASTERDOWN` error as before.\r\n- Changes `SCRIPT LOAD` to be flagged as a `STALE` command. This is mainly to make it logically compatible with the change to `EVAL` in the previous point. It enables loading a script on a stale server which is technically okay it doesn't relate directly to the server's dataset. Running the script does, but that won't work unless the script is explicitly marked as `allow-stale`.\r\n\r\nEven though not strictly required this should replace #10093. We also suggest closing #8478: using the shebang will cause the script to change its default behavior so it'll deny any writes during OOM state.\r\n\r\nUsage example:\r\n```lua\r\n#!lua flags=no-writes,allow-stale\r\nlocal x = redis.call('get','x')\r\nreturn x\r\n```\r\nNote that even though the LUA syntax doesn't support hash tag comments `.lua` files do support a shebang tag on the top so they can be executed on Unix systems like any shell script. LUA's `luaL_loadfile` handles this as part of the LUA library. In the case of `luaL_loadbuffer`, which is what Redis uses, I needed to fix the input script in case of a shebang manually. I did this the same way `luaL_loadfile` does, by replacing the first line with a single line feed character.\r\n\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-17T02:09:44Z",
        "closed_at": "2022-01-17T07:02:10Z",
        "merged_at": "2022-01-17T07:02:10Z",
        "body": "Use `am` instead of using `server.aof_manifest` directly to call `getBaseAndIncrAppendOnlyFilesSize`.\r\n\r\n@oranagra It was a carelessness at the time, sorry.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 758,
        "deletions": 459,
        "changed_files": 201,
        "created_at": "2022-01-16T21:06:59Z",
        "closed_at": "2022-01-18T14:00:01Z",
        "merged_at": "2022-01-18T14:00:01Z",
        "body": "The new ACL key based permissions in #9974 require the key-specs (#8324) to have more explicit flags rather than just READ and WRITE. See discussion in #10040\r\n\r\nThis PR defines two groups of flags:\r\nOne about how redis internally handles the key (mutually-exclusive).\r\nThe other is about the logical operation done from the user's point of view (3 mutually exclusive write flags, and one read flag, all optional).\r\nIn both groups, if we can't explicitly flag something as explicit read-only, delete-only, or insert-only, we flag it as `RW` or `UPDATE`.\r\nhere's the definition from the code:\r\n```\r\n/* Key-spec flags *\r\n * -------------- */\r\n/* The following refer what the command actually does with the value or metadata\r\n * of the key, and not necessarily the user data or how it affects it.\r\n * Each key-spec may must have exaclty one of these. Any operation that's not\r\n * distinctly deletion, overwrite or read-only would be marked as RW. */\r\n#define CMD_KEY_RO (1ULL<<0)     /* Read-Only - Reads the value of the key, but\r\n                                  * doesn't necessarily returns it. */\r\n#define CMD_KEY_RW (1ULL<<1)     /* Read-Write - Modifies the data stored in the\r\n                                  * value of the key or its metadata. */\r\n#define CMD_KEY_OW (1ULL<<2)     /* Overwrite - Overwrites the data stored in\r\n                                  * the value of the key. */\r\n#define CMD_KEY_RM (1ULL<<3)     /* Deletes the key. */\r\n/* The follwing refer to user data inside the value of the key, not the metadata\r\n * like LRU, type, cardinality. It refers to the logical operation on the user's\r\n * data (actual input strings / TTL), being used / returned / copied / changed,\r\n * It doesn't refer to modification or returning of metadata (like type, count,\r\n * presence of data). Any write that's not INSERT or DELETE, would be an UPADTE.\r\n * Each key-spec may have one of the writes with or without access, or none: */\r\n#define CMD_KEY_ACCESS (1ULL<<4) /* Returns, copies or uses the user data from\r\n                                  * the value of the key. */\r\n#define CMD_KEY_UPDATE (1ULL<<5) /* Updates data to the value, new value may\r\n                                  * depend on the old value. */\r\n#define CMD_KEY_INSERT (1ULL<<6) /* Adds data to the value with no chance of,\r\n                                  * modification or deletion of existing data. */\r\n#define CMD_KEY_DELETE (1ULL<<7) /* Explicitly deletes some content\r\n                                  * from the value of the key. */\r\n```\r\n\r\nUnrelated changes:\r\n- generate-command-code.py is only compatible with python3 (modified the shabang)\r\n- generate-command-code.py print file on json parsing error\r\n- rename `shard_channel` key-spec flag to just `channel`.\r\n- add INCOMPLETE flag in input spec of SORT and SORT_RO",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2022-01-16T15:58:43Z",
        "closed_at": "2022-01-17T08:42:13Z",
        "merged_at": "2022-01-17T08:42:13Z",
        "body": "These two tests have a high probability of failure\r\non MacOS. Or it takes many retries to succeed.\r\nKeys often expire before we can access them.\r\n\r\nSo this time we try to avoid this by reducing the time\r\nof the first `after`, or removeing the first `after`.\r\n\r\nThe results of doing `20/81` and `0/101` are:\r\n- PEXPIRE (20/81): 1069/1949\r\n- PEXPIREAT (20/81): 1093/1949\r\n\r\n- PEXPIRE (0/101): 31936 / 31936\r\n- PEXPIREAT (0/101): 31936 / 31936\r\n\r\nThe first number is the number of times that the\r\ntest succeeded without any retries.\r\nThe second number is the total number of executions.\r\n\r\nAnd we can see that `0/101` doesn't even need an extra\r\nretries. Also reduces the time required for testing.\r\nSo in the end we chose `0/100`, i.e. remove the first `after`.\r\n\r\nAs for `PEXPIREAT`, there is no failure, but we still changed\r\nit together, using `0/201`, after 2W tests, none of them failed.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2022-01-15T13:12:38Z",
        "closed_at": "2022-01-15T17:57:24Z",
        "merged_at": "2022-01-15T17:57:24Z",
        "body": "# Description\r\nFixes a typo to replace \"Pub/Sun\" -> \"Pub/Sub\". This can be seen in https://redis.io/commands.json",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-01-15T10:26:11Z",
        "closed_at": "2022-01-16T08:03:10Z",
        "merged_at": "2022-01-16T08:03:10Z",
        "body": "Seems like the previous implementation was broken (always returning 0)\r\n\r\nsince kinfo_proc2 is used the KERN_PROC2 sysctl oid is more appropriate\r\nand also the query's length was not necessarily accurate (6 here).",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-15T03:52:04Z",
        "closed_at": "2022-01-15T07:13:53Z",
        "merged_at": "2022-01-15T07:13:53Z",
        "body": "Fixes cluster test introduced in #10066.\r\n```\r\nFunction no-cluster flag: ERR Error registering functions: @user_function: 1: wrong number of arguments to redis.register_function\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 60,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2022-01-14T19:02:40Z",
        "closed_at": "2022-01-19T09:57:51Z",
        "merged_at": "2022-01-19T09:57:51Z",
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-14T02:17:48Z",
        "closed_at": "2022-01-14T11:54:22Z",
        "merged_at": "2022-01-14T11:54:22Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-01-13T20:34:55Z",
        "closed_at": "2022-01-14T19:48:22Z",
        "merged_at": null,
        "body": "1. close one file before system exits\r\n2. fix 2 potential memory leak issues",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 32,
        "changed_files": 7,
        "created_at": "2022-01-13T20:01:16Z",
        "closed_at": "2022-01-18T10:52:27Z",
        "merged_at": "2022-01-18T10:52:27Z",
        "body": "This extends the previous fix (#10049) to address any form of\r\nnon-printable or whitespace character (including newlines, quotes,\r\nnon-printables, etc.)\r\n\r\nAlso, removes the limitation on appenddirname, to align with the way\r\nfilenames are handled elsewhere in Redis.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-01-13T14:23:32Z",
        "closed_at": "2022-01-15T18:39:06Z",
        "merged_at": "2022-01-15T18:39:06Z",
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1281,
        "deletions": 367,
        "changed_files": 12,
        "created_at": "2022-01-13T10:57:39Z",
        "closed_at": "2022-02-04T19:09:37Z",
        "merged_at": "2022-02-04T19:09:37Z",
        "body": "Adds RM_SetCommandInfo, allowing modules to provide the following command info:\r\n\r\n* summary\r\n* complexity\r\n* since\r\n* history\r\n* hints\r\n* arity\r\n* key specs\r\n* args\r\n\r\nThis information affects the output of `COMMAND`, `COMMAND INFO` and `COMMAND DOCS`, Cluster, ACL and is used to filter commands with the wrong number of arguments before the call reaches the module code.\r\n\r\nThe recently added API functions for key specs (never released) are removed.\r\n\r\nThis work is based on some commits in #9656 and then rewritten to the declarative style as discussed in #9944.\r\n\r\nFixes #9944.\r\n\r\nA minimalist example would look like so:\r\n```c\r\n    RedisModuleCommand *mycmd = RedisModule_GetCommand(ctx,\"mymodule.mycommand\");\r\n    RedisModuleCommandInfo mycmd_info = {\r\n        .version = REDISMODULE_COMMAND_INFO_VERSION,\r\n        .arity = -5,\r\n        .summary = \"some description\",\r\n    };\r\n    if (RedisModule_SetCommandInfo(mycmd, &mycmd_info) == REDISMODULE_ERR)\r\n        return REDISMODULE_ERR;\r\n````\r\n\r\nNotes:\r\n* All the provided information (including strings) is copied, not keeping references to the API input data.\r\n* The version field is actually a static struct that contains the sizes of the the structs used in arrays, so we can extend these in the future and old version will still be able to take the part they can support.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 38,
        "changed_files": 3,
        "created_at": "2022-01-12T16:01:54Z",
        "closed_at": "2022-01-12T18:05:14Z",
        "merged_at": "2022-01-12T18:05:14Z",
        "body": "Use `getFullCommandName` to get the full name of the command.\r\nIt can also get the full name of the subcommand, like \"script|help\".\r\n\r\nBefore:\r\n```\r\n> SCRIPT HELP\r\n(error) NOPERM this user has no permissions to run the 'help' command or its subcommand\r\n\r\n> ACL LOG\r\n    7) \"object\"\r\n    8) \"help\"\r\n```\r\n\r\nAfter:\r\n```\r\n> SCRIPT HELP\r\n(error) NOPERM this user has no permissions to run the 'script|help' command\r\n\r\n> ACL LOG\r\n    7) \"object\"\r\n    8) \"script|help\"\r\n```\r\n\r\nFix #10094",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1830,
        "deletions": 1310,
        "changed_files": 107,
        "created_at": "2022-01-12T15:16:09Z",
        "closed_at": "2022-01-20T09:32:11Z",
        "merged_at": "2022-01-20T09:32:11Z",
        "body": "Fix #9876\r\n\r\nAdding command tips (see https://redis.io/topics/command-tips / [topics/command-tips.md](https://github.com/redis/redis-doc/pull/1697/files#diff-874e6857b660c278f8b8829a72058a2562a32514edddf5037ecc261ccd021545)) to commands.\r\n\r\nBreaking changes:\r\n1. Removed the \"random\" and \"sort_for_script\" flags. They are now command tips. (this isn't affecting redis behavior since #9812, but could affect some client applications that's relying on COMMAND command flags)\r\n\r\nSummary of changes:\r\n1. add BLOCKING flag (new flag) for all commands that could block. The ACL category with the same name is now implicit.\r\n2. move RANDOM flag to a `nondeterministic_output` tip\r\n3. move SORT_FOR_SCRIPT flag to `nondeterministic_output_order` tip\r\n3. add REQUEST_POLICY and RESPONSE_POLICY where appropriate as documented in the [tips](https://github.com/redis/redis-doc/pull/1697/files#diff-874e6857b660c278f8b8829a72058a2562a32514edddf5037ecc261ccd021545)\r\n4. deprecate (ignore) the `random` flag for RM_CreateCommand\r\n\r\nOther notes:\r\n1. Proxies need to send `RANDOMKEY` to all shards and then select one key randomly. The other option is to pick a random shard and transfer `RANDOMKEY `to it, but that scheme fails if this specific shard is empty\r\n2. Remove CMD_RANDOM from `XACK` (i.e. XACK does not have RANDOM_OUTPUT)\r\n   It was added in 9e4fb96ca12476b1c7468b143efca86b478bfb4a, I guess by mistake.\r\n   Also from `(P)EXPIRETIME` (new command, was flagged \"random\" by mistake).\r\n3. Add `nondeterministic_output` to `OBJECT ENCODING` (for the same reason `XTRIM` has it:\r\n   the reply may differ depending on the internal representation in memory)\r\n4. RANDOM on `HGETALL` was wrong (there due to a limitation of the old script sorting logic), now it's`nondeterministic_output_order`\r\n5. Unrelated: Hide CMD_PROTECTED from COMMAND",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 100,
        "deletions": 39,
        "changed_files": 4,
        "created_at": "2022-01-12T07:54:13Z",
        "closed_at": "2022-01-17T10:32:32Z",
        "merged_at": "2022-01-17T10:32:32Z",
        "body": "since `info commandstats` already shows sub-commands, we should do the same in `info latencystats`.\r\nsimilarly, the LATENCY HISTOGRAM command now shows sub-commands (with their full name) when:\r\n* asking for all commands\r\n* asking for a specific container command\r\n* asking for a specific sub-command)",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 83,
        "deletions": 9,
        "changed_files": 7,
        "created_at": "2022-01-11T14:11:34Z",
        "closed_at": "2022-01-13T06:49:26Z",
        "merged_at": "2022-01-13T06:49:26Z",
        "body": "See issue #9794 \r\n\r\nForce create a BASE file (use a foreground `rewriteAppendOnlyFile`) when redis starts from an empty data set and  `appendonly` is  yes.\r\n\r\nThe reasoning is that normally, after redis is running for some time, and the AOF has gone though a few rewrites, there's always a base rdb file. and the scenario where the base file is missing, is kinda rare (happens only at empty startup), so this change normalizes it.\r\nBut more importantly, there are or could be some complex modules that are started with some configuration, when they create persistence they write that configuration to RDB AUX fields, so that can can always know with which configuration the persistence file they're loading was created (could be critical). there is (was) one scenario in which they could load their persisted data, and that configuration was missing, and this change fixes it.\r\n\r\nAdd a new module event: REDISMODULE_SUBEVENT_PERSISTENCE_SYNC_AOF_START, similar to\r\nREDISMODULE_SUBEVENT_PERSISTENCE_AOF_START which is async.\r\n\r\nNote: This PR initially also disabled BGREWRITEAOF when appendonly config is no, but that part was later removed to be discussed separately.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2022-01-11T12:06:36Z",
        "closed_at": "2022-01-13T09:36:04Z",
        "merged_at": "2022-01-13T09:36:04Z",
        "body": "Added RM_MonotonicMicroseconds(). Modules can use monotonic timestamp counter for measurements.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2022-01-11T09:40:28Z",
        "closed_at": "2022-01-24T20:31:35Z",
        "merged_at": "2022-01-24T20:31:35Z",
        "body": " CI link:  https://github.com/redis/redis/runs/4769005256?check_suite_focus=true#step:4:7328\r\n\r\nModifications:\r\n1. Refactor EVAL timeout test\r\n2. since the test used `r config set appendonly yes` which generates a rewrite, it missed it's purpose\r\n2. Fix the bug that `start_server` returns before redis starts ready, which affects when multiple tests share the same dir.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2022-01-10T17:31:42Z",
        "closed_at": "2022-06-25T16:02:20Z",
        "merged_at": "2022-06-25T16:02:20Z",
        "body": "In 6.2.0 with the introduction of the REV subcommand in ZRANGE, there was a semantic shift in the arguments of ZRANGE when the REV sub-command is executed. Without the sub-command `min` and `max` (the old names of the arguments) are appropriate because if you put the min value and the max value in everything works fine.\r\n\r\n```bash\r\n127.0.0.1:6379> ZADD myset 0 foo\r\n(integer) 1\r\n127.0.0.1:6379> ZADD myset 1 bar\r\n(integer) 1\r\n127.0.0.1:6379> ZRANGE myset 0 inf BYSCORE\r\n1) \"foo\"\r\n2) \"bar\"\r\n``` \r\n\r\nHowever - if you add the `REV` subcommand, ordering the arguments `min` `max` breaks the command:\r\n\r\n```bash\r\n127.0.0.1:6379> ZRANGE myset 0 inf BYSCORE REV\r\n(empty array)\r\n```\r\n\r\nwhy? because `ZRANGE` with the `REV` sub-command is expecting the `max` first and the `min` second (because it's a reverse range like `ZREVRANGEBYSCORE`):\r\n\r\n```bash\r\n127.0.0.1:6379> ZRANGE myset 0 inf BYSCORE REV\r\n(empty array)\r\n```\r\n\r\nI spoke to @itamarhaber and he agreed that it might make sense to make  `min`->`start` and `max`->`stop` to be more generic than `min` `max`",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 68,
        "deletions": 33,
        "changed_files": 5,
        "created_at": "2022-01-10T17:27:59Z",
        "closed_at": "2022-01-11T20:43:18Z",
        "merged_at": "2022-01-11T20:43:18Z",
        "body": "This commit adds some tests that the test cases will\r\naccess the keys with expiration time set in the script call.\r\nThere was no test case for this part before. See #10080\r\n\r\nAlso there is a test will cover #1525. we block the time so\r\nthat the key can not expire in the middle of the script execution.\r\n\r\nOther changes:\r\n1. Delete `evalTimeSnapshot` and just use `scriptTimeSnapshot` in it's place.\r\n2. Some cleanups to scripting.tcl.\r\n3. better names for tests that run in a loop to make them distinctable ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2022-01-10T16:51:11Z",
        "closed_at": "2022-01-11T12:26:13Z",
        "merged_at": "2022-01-11T12:26:13Z",
        "body": "It used to return `$-1` in RESP2, now we will return `*-1`.\r\nThis is a bug in redis 6.2 when COUNT was added, the `COUNT`\r\noption was introduced in #8179. Fix #10089.\r\n\r\nthe documentation of [LPOP](https://redis.io/commands/lpop) says\r\n```\r\nWhen called without the count argument:\r\nBulk string reply: the value of the first element, or nil when key does not exist.\r\n\r\nWhen called with the count argument:\r\nArray reply: list of popped elements, or nil when key does not exist.\r\n```",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 87,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2022-01-10T15:39:35Z",
        "closed_at": "2022-01-18T08:20:34Z",
        "merged_at": null,
        "body": "See #8478 \r\nAllow configurable behavior for OOM during scripting:\r\n - Deny all writes (default).\r\n - Allow writes not flagged as CMD_DENYOOM and all writes if dataset is dirty (previous behavior).\r\n - Allow writes not flagged as CMD_DENYOOM and if dataset is dirty other writes will fail and break atomicity.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 102,
        "deletions": 56,
        "changed_files": 16,
        "created_at": "2022-01-10T13:50:11Z",
        "closed_at": "2022-01-17T12:11:12Z",
        "merged_at": "2022-01-17T12:11:12Z",
        "body": "1. enable diskless replication by default\r\n2. add a new config named repl-diskless-sync-max-replicas that enables\r\n   replication to start before the full repl-diskless-sync-delay was\r\n   reached.\r\n3. put replica online sooner on the master (see below)\r\n4. test suite uses repl-diskless-sync-delay of 0 to be faster\r\n5. a few tests that use multiple replica on a pre-populated master, are\r\n   now using the new repl-diskless-sync-max-replicas\r\n6. fix possible timing issues in a few cluster tests (see below)\r\n\r\nput replica online sooner on the master \r\n----------------------------------------------------\r\nthere were two tests that failed because they needed for the master to\r\nrealize that the replica is online, but the test code was actually only\r\nwaiting for the replica to realize it's online, and in diskless it could\r\nhave been before the master realized it.\r\n\r\nchanges include two things:\r\n1. the tests wait on the right thing\r\n2. issues in the master, putting the replica online in two steps.\r\n\r\nthe master used to put the replica as online in 2 steps. the first\r\nstep was to mark it as online, and the second step was to enable the\r\nwrite event (only after getting ACK), but in fact the first step didn't\r\ncontains some of the tasks to put it online (like updating good slave\r\ncount, and sending the module event). this meant that if a test was\r\nwaiting to see that the replica is online form the point of view of the\r\nmaster, and then confirm that the module got an event, or that the\r\nmaster has enough good replicas, it could fail due to timing issues.\r\n\r\nso now the full effect of putting the replica online, happens at once,\r\nand only the part about enabling the writes is delayed till the ACK.\r\n\r\nfix cluster tests \r\n--------------------\r\nI added some code to wait for the replica to sync and avoid race\r\nconditions.\r\nlater realized the sentinel and cluster tests where using the original 5\r\nseconds delay, so changed it to 0.\r\n\r\nthis means the other changes are probably not needed, but i suppose\r\nthey're still better (avoid race conditions)",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-01-10T00:45:20Z",
        "closed_at": "2022-01-11T00:21:06Z",
        "merged_at": "2022-01-11T00:21:05Z",
        "body": "I believe this should fix the issue outlined here:\r\nhttps://github.com/redis/redis/runs/4733590577?check_suite_focus=true\r\nhttps://github.com/redis/redis/runs/4745050597?check_suite_focus=true\r\n\r\nThis test is verifying that when a node is joining a cluster, the hostname is shown only when we've exchanged ping/pong messages (I.E. we now know the full state of the node) and just when we learn about a node through gossip. The test was previously assuming that if we had fully connected to 2 other nodes we would have at least reached out to a third node. Although this is highly likely, it's not guaranteed, so now we are waiting on both events. We've learned about every node in the cluster AND we've fully established connections with two particular nodes that have hostnames.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2022-01-09T09:40:00Z",
        "closed_at": "2022-01-09T11:06:52Z",
        "merged_at": "2022-01-09T11:06:52Z",
        "body": "The following error commands will crash redis-server:\r\n```\r\n> get|\r\nError: Server closed the connection\r\n> get|set\r\nError: Server closed the connection\r\n> get|other\r\n```\r\n\r\nThe reason is in #9504, we use `lookupCommandBySds` for find the\r\ncontainer command. And it split the command (argv[0]) with `|`.\r\nIf we input something like `get|other`, after the split, `get`\r\nwill become a valid command name, pass the `ERR unknown command`\r\ncheck, and finally crash in `addReplySubcommandSyntaxError`\r\n\r\nIn this case we do not need to split the command name with `|`\r\nand just look in the commands dict to find if `argv[0]` is a\r\ncontainer command.\r\n\r\nSo this commit introduce a new function call `isContainerCommandBySds`\r\nthat it will return true if a command name is a container command.\r\n\r\nAlso with the old code, there is a incorrect error message:\r\n```\r\n> config|get set\r\n(error) ERR Unknown subcommand or wrong number of arguments for 'set'. Try CONFIG|GET HELP.\r\n```\r\n\r\nThe crash was reported in #10070.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 34,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2022-01-09T07:35:09Z",
        "closed_at": "2022-01-10T06:21:17Z",
        "merged_at": "2022-01-10T06:21:17Z",
        "body": "The following steps will crash redis-server:\r\n```\r\n[root]# cat crash\r\nPSYNC replicationid -1\r\nSLOWLOG GET\r\nGET key\r\n[root]# nc 127.0.0.1 6379 < crash\r\n```\r\n\r\nThis one following #10020 and the crash was reported in #10076.\r\nAdd a new common function name `logInvalidUseAndFreeClient`\r\nthat will log the errors and free the client in async way.\r\n\r\nOther changes about the output info:\r\n1. Cmd with a full name by using `getFullCommandName`, now it will print the right subcommand name like `slowlog|get`.\r\n2. Print the full client info by using `catClientInfoString`, the info is also valuable.\r\n\r\nNow the log will like this:\r\n```\r\n31518:M 09 Jan 2022 18:35:19.849 # Replica generated a reply to command slowlog|get, disconnecting it: id=4 addr=127.0.0.1:45208 laddr=127.0.0.1:6379 fd=8 name= age=0 idle=0 flags=S db=0 sub=0 psub=0 multi=-1 qbuf=42 qbuf-free=20432 argv-mem=10 multi-mem=0 obl=0 oll=0 omem=0 tot-mem=40986 events=r cmd=slowlog|get user=default redir=-1 resp=2\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-09T07:08:59Z",
        "closed_at": "2022-01-11T05:56:31Z",
        "merged_at": "2022-01-11T05:56:31Z",
        "body": "fix :https://github.com/redis/redis/issues/10079 \r\n\r\nAnd I passed my test in https://github.com/redis/redis/issues/10079 \r\n\r\nThis is a recent regression from the Redis Functions commits",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 17,
        "changed_files": 7,
        "created_at": "2022-01-08T00:54:10Z",
        "closed_at": "2022-01-10T01:04:19Z",
        "merged_at": "2022-01-10T01:04:19Z",
        "body": "I found it slightly verbose that latency histogram info values are written out to full precision. So I changed that. I also removed setting the default value for tracking, which is not needed since it's set by the config infra. \r\n\r\nprevious\r\n```\r\nlatency_percentiles_usec_sadd:p50.000000=0.001,p99.000000=1.003,p99.900000=3.007\r\n```\r\n\r\nnow\r\n```\r\nlatency_percentiles_usec_sadd:p50=0.001,p99=1.003,p99.9=3.007\r\n``",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 91,
        "deletions": 91,
        "changed_files": 13,
        "created_at": "2022-01-07T16:57:22Z",
        "closed_at": "2022-01-07T19:31:56Z",
        "merged_at": null,
        "body": "This is a follow up PR for https://github.com/redis/redis/pull/9959\r\n\r\nThanks for @guybe7 mentioning the json format",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-07T15:22:56Z",
        "closed_at": "2022-01-09T06:08:55Z",
        "merged_at": "2022-01-09T06:08:55Z",
        "body": "Fixes minor typo in `redis.conf`\r\n\r\n- Origin: shard traffic while `the the` cluster\r\n- Changed: shard traffic while `the` cluster",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2022-01-07T08:50:56Z",
        "closed_at": "2022-01-07T13:31:06Z",
        "merged_at": "2022-01-07T13:31:06Z",
        "body": "Fix #9410\r\n\r\nCrucial for the ms and sequence deltas, but I changed all\r\ncalls, just in case (e.g. \"flags\")\r\n\r\nBefore this commit:\r\n`ms_delta` and `seq_delta` could have overflown, causing `currid` to be wrong,\r\nwhich in turn would cause `streamTrim` to trim the entire rax node (see new test)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 648,
        "deletions": 80,
        "changed_files": 16,
        "created_at": "2022-01-06T16:07:45Z",
        "closed_at": "2022-01-14T12:02:03Z",
        "merged_at": "2022-01-14T12:02:02Z",
        "body": "# Redis Functions Flags\r\n\r\nFollowing the discussion on https://github.com/redis/redis/issues/10025 Added Functions Flags support. The PR is divided to 2 sections:\r\n* Add named argument support to `redis.register_function` API.\r\n* Add support for function flags\r\n\r\n## `redis.register_function` named argument support\r\n\r\nThe first part of the PR adds support for named argument on `redis.register_function`, example:\r\n```\r\nredis.register_function{\r\n    function_name='f1',\r\n    callback=function()\r\n        return 'hello'\r\n    end,\r\n    description='some desc'\r\n}\r\n```\r\n\r\nThe positional arguments is also kept, which means that it still possible to write:\r\n```\r\nredis.register_function('f1', function() return 'hello' end)\r\n```\r\n\r\nBut notice that it is no longer possible to pass the optional description argument on the positional argument version. Positional argument was change to allow passing only the mandatory arguments (function name and callback). To pass more arguments the user must use the named argument version.\r\n\r\nAs with positional arguments, the `function_name` and `callback` is mandatory and an error will be raise if those are missing. Also, an error will be raise if an unknown argument name is given or the arguments type is wrong.\r\n\r\nTests was added to verify the new syntax.\r\n\r\n## Functions Flags\r\n\r\nThe second part of the PR is adding functions flags support. Flags are given to Redis when the engine calls `functionLibCreateFunction`, supported flags are:\r\n\r\n* `no-writes` - indicating the function perform no writes which means that it is OK to run it on:\r\n   * read-only replica\r\n   * Using FCALL_RO\r\n   * If disk error detected\r\n   \r\n   It will not be possible to run a function in those situations unless the function turns on the `no-writes` flag\r\n\r\n* `allow-oom` - indicate that its OK to run the function even if Redis is in OOM state, if the function will not turn on this flag it will not be possible to run it if OOM reached. If this flag is set, any command will be allow on OOM (even those that is marked with CMD_DENYOOM). The assumption is that this flag is for advance users that knows its meaning and understand what they are doing, and Redis trust them to not increase the memory usage. (e.g. it could be an INCR or a modification on an existing key, or a DEL command)\r\n**Note**: starting with redis 7.0.1, the `no-writes` flag also implies `allow-oom`, so read-only scripts (including ones used by FCALL_RO and EVAL_RO) can run in OOM state without explicitly specifying this flag. see #10699\r\n\r\n* `allow-state` - indicate that its OK to run the function on stale replica, in this case we will also make sure the function is only perform `stale` commands and raise an error if not.\r\n\r\n* `no-cluster` - indicate to disallow running the function if cluster is enabled.\r\n\r\nDefault behaviure of functions (if no flags is given):\r\n1. Allow functions to read and write\r\n2. Do not run functions on OOM\r\n3. Do not run functions on stale replica\r\n4. Allow functions on cluster\r\n\r\n### Lua API for functions flags\r\n\r\nOn Lua engine, it is possible to give functions flags as `flags` named argument:\r\n\r\n```\r\nredis.register_function{function_name='f1', callback=function() return 1 end, flags={'no-writes', 'allow-oom'}, description='description'}\r\n```\r\n\r\nThe function flags argument must be a Lua table that contains all the requested flags, The following will result in an error:\r\n* Unknown flag\r\n* Wrong flag type\r\n\r\nDefault behaviour is the same as if no flags are used.\r\n\r\nTests were added to verify all flags functionality\r\n\r\n## Additional changes\r\n* mark FCALL and FCALL_RO with CMD_STALE flag (unlike EVAL), so that they can run if the function was registered with the `allow-stale` flag.\r\n* Verify `CMD_STALE` on `scriptCall` (`redis.call`), so it will not be possible to call commands from script while stale unless the command is marked with the `CMD_STALE` flags. so that even if the function is allowed while stale we do not allow it to bypass the `CMD_STALE` flag of commands.\r\n* Flags section was added to `FUNCTION LIST` command to provide the set of flags for each function:\r\n```\r\n> FUNCTION list withcode\r\n1)  1) \"library_name\"\r\n    2) \"test\"\r\n    3) \"engine\"\r\n    4) \"LUA\"\r\n    5) \"description\"\r\n    6) (nil)\r\n    7) \"functions\"\r\n    8) 1) 1) \"name\"\r\n          2) \"f1\"\r\n          3) \"description\"\r\n          4) (nil)\r\n          5) \"flags\"\r\n          6) (empty array)\r\n    9) \"library_code\"\r\n   10) \"redis.register_function{function_name='f1', callback=function() return 1 end}\"\r\n```\r\n* Added API to get Redis version from within a script, The redis version can be provided using:\r\n   1. `redis.REDIS_VERSION` - string representation of the redis version in the format of MAJOR.MINOR.PATH\r\n   2. `redis.REDIS_VERSION_NUM` - number representation of the redis version in the format of `0x00MMmmpp` (`MM` - major, `mm` - minor,  `pp` - patch). The number version can be used to check if version is greater or less another version. The string version can be used to return to the user or print as logs.\r\n\r\n   This new API is provided to eval scripts and functions, it also possible to use this API during functions loading phase.\r\n\r\n## Considerations\r\n* After discussion with @oranagra, @yossigo and @guybe7  we decided **User experience should be smooth even at the cost of not matching functions flags to modules and commands flags**. This is why we see different flags for functions on this PR.\r\n\r\ntodo:\r\n- [ ] Decide if we want to allow `EVAL\\_RO`, `EVALSHA\\_RO` to be allowed while stale and in worst case, if the script performs a command which is not allowed while stale, an error will be raised. \r\n   * An argument against it is that scripts that publish some data and then read some data from the key space will fail when trying to read data and after already performed the publish command. This can be considered as breaking change and maybe even break atomicity? \r\n   * An argument in favor is that we already have such issue with `write` commands.\r\n- [x] Add functions flags to `function list`\r\n- [x] redis.version API",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-06T15:05:06Z",
        "closed_at": "2022-01-06T16:09:36Z",
        "merged_at": null,
        "body": "Allow jemalloc configure flags in the makefile\r\n\r\nSigned-off-by: Terry Howe <tlhowe@amazon.com>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 31,
        "changed_files": 8,
        "created_at": "2022-01-06T12:32:17Z",
        "closed_at": "2022-01-18T13:55:21Z",
        "merged_at": "2022-01-18T13:55:21Z",
        "body": "When I used C++ to develop a redis module. i  used `string.data()` as the second parameter `ele`\r\nof  `RedisModule_DigestAddStringBuffer`, but there is a warning, since we never change the `ele`,\r\ni think we should use `const char` for it.\r\n\r\nThis PR adds const to just a handful of module APIs that required it, all not very widely used.\r\nThe implication is a breaking change in terms of compilation error that's easy to resolve, and no ABI impact.\r\nThe affected APIs are around Digest, Info injection, and Cluster bus messages.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 75,
        "deletions": 18,
        "changed_files": 7,
        "created_at": "2022-01-06T09:33:31Z",
        "closed_at": "2022-05-31T05:07:33Z",
        "merged_at": "2022-05-31T05:07:33Z",
        "body": "Hi guys,\r\n\r\nI find that the amount of `server.stat_net_output_bytes/server.stat_net_input_bytes` is actually the sum of replication flow and users' data flow. \r\nIt may cause confusions like this, \"Why does my server get such a large output_bytes while I am doing nothing? \". \r\n\r\nAfter discussions and revisions, now here is the change about what this PR brings (final version before merge):\r\n- 2 server variables to count the network bytes during replication, including fullsync and propagate bytes.\r\n     - `server.stat_net_repl_output_bytes`/`server.stat_net_repl_input_bytes`\r\n- 3 info fields to print the input and output of repl bytes and instantaneous value of total repl bytes.\r\n     - `total_net_repl_input_bytes` / `total_net_repl_output_bytes`\r\n     - `instantaneous_repl_total_kbps`\r\n- 1 new API `rioCheckType()` to check the type of rio. So we can use this to distinguish between diskless and diskbased replication\r\n- 2 new counting items to keep network statistics consistent between master and slave\r\n    - rdb portion during diskless replica. in `rdbLoadProgressCallback()`\r\n    - first line of the full sync payload. in `readSyncBulkPayload()`\r\n\r\n------\r\nHere is the result\r\n## Steps\r\n1. start a new master node 30001\r\n2. using `redis-benchmark` to write to 30001\r\n     `./src/redis-benchmark -t set -r 10000000 -d 1 -n 100000000 -p 30001`\r\n3. after a while, start a new slave node 30002, run `slaveof 127.0.0.1 30001`\r\n4. run `info stats` to check the flow count of the master and slave\r\n## Result\r\n- for disk-based replication\r\n  - master\r\n  ```\r\n  # Stats\r\n  total_net_input_bytes:21534385\r\n  total_net_output_bytes:24777743\r\n  total_net_repl_input_bytes:0\r\n  total_net_repl_output_bytes:22270070\r\n  ```\r\n  - slave\r\n  ```\r\n  # Stats\r\n  total_net_input_bytes:22270213\r\n  total_net_output_bytes:4020\r\n  total_net_repl_input_bytes:22270070\r\n  total_net_repl_output_bytes:0\r\n  ```\r\n\r\n- for diskless loading (by setting `repl-diskless-load` to `swapdb` in `redis.conf`)\r\n  - master\r\n  ```\r\n   # Stats\r\n  total_net_input_bytes:8984484\r\n  total_net_output_bytes:26999927\r\n  total_net_repl_input_bytes:0\r\n  total_net_repl_output_bytes:25945330\r\n  ```\r\n  - slave\r\n  ```\r\n  # Stats\r\n  total_net_input_bytes:25945448\r\n  total_net_output_bytes:2769\r\n  total_net_repl_input_bytes:25945330\r\n  total_net_repl_output_bytes:0\r\n  ```",
        "comments": 25
    },
    {
        "merged": true,
        "additions": 573,
        "deletions": 170,
        "changed_files": 4,
        "created_at": "2022-01-06T04:53:13Z",
        "closed_at": "2022-02-17T06:13:28Z",
        "merged_at": "2022-02-17T06:13:28Z",
        "body": "issue link :  https://github.com/redis/redis/pull/9788#issuecomment-1004133640\r\n\r\nModifications of this PR:\r\n1. Support the verification of `Multi Part AOF`, while still maintaining support for the old-style `AOF/RDB-preamble`. `redis-check-aof` will automatically choose which mode to use according to the incoming file format.\r\n   \r\n    `Usage: redis-check-aof [--fix|--truncate-to-timestamp $timestamp] <AOF/manifest>`\r\n \r\n2. Refactor part of the code to make it easier to understand\r\n3. Currently only supports truncate  (`--fix` or `--truncate-to-timestamp`) the last AOF file (may be `BASE` or `INCR`)\r\n\r\nThe reasons for 3 above:\r\n- for `--fix`: Only the last AOF may be truncated, this is guaranteed by redis\r\n- for `--truncate-to-timestamp`:  Normally, we only have `BASE` + `INCR` files at most, and `BASE` cannot be truncated(It only contains a timestamp annotation at the beginning of the file), so only `INCR` can be truncated. If we have a `BASE+INCR1+INCR2` file (meaning we have an interrupted AOFRW), Only `INCR2` files can be truncated at this time. If we still insist on truncate `INCR1`, we need to manually delete `INCR2` and update the manifest file, then re-run `redis-check-aof`\r\n- If we want to support truncate any file, we need to add very complicated code to support the atomic modification of multiple file deletion and update manifest, I think this is unnecessary\r\n\r\n\r\n",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2022-01-05T21:46:01Z",
        "closed_at": "2022-01-06T07:54:22Z",
        "merged_at": "2022-01-06T07:54:22Z",
        "body": "Callers of redisFork() are logging `strerror(errno)` on failure. `errno` is not set when there is already a child process, causing printing current value of errno which was set before `redisFork()` call. \r\n\r\nSetting errno to EEXIST on this failure to provide more meaningful error message. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 395,
        "deletions": 249,
        "changed_files": 10,
        "created_at": "2022-01-05T08:14:13Z",
        "closed_at": "2022-01-11T15:16:16Z",
        "merged_at": "2022-01-11T15:16:16Z",
        "body": "Syntax:\r\n`COMMAND DOCS [<command name> ...]`\r\n\r\nBackground:\r\nApparently old version of hiredis (and thus also redis-cli) can't\r\nsupport more than 7 levels of multi-bulk nesting.\r\n\r\nThe solution is to move all the doc related metadata from COMMAND to a\r\nnew COMMAND DOCS sub-command.\r\n\r\nThe new DOCS sub-command returns a map of commands (not an array like in COMMAND),\r\nAnd the same goes for the `subcommands` field inside it (also contains a map)\r\n\r\nBesides that, the remaining new fields of COMMAND (hints, key-specs, and\r\nsub-commands), are placed in the outer array rather than a nested map.\r\nthis was done mainly for consistency with the old format.\r\n\r\nOther changes:\r\n---\r\n* Allow COMMAND INFO with no arguments, which returns all commands, so that we can some day deprecated\r\n  the plain COMMAND (no args)\r\n\r\n* Reduce the amount of deferred replies from both COMMAND and COMMAND\r\n  DOCS, especially in the inner loops, since these create many small\r\n  reply objects, which lead to many small write syscalls and many small\r\n  TCP packets.\r\n  To make this easier, when populating the command table, we count the\r\n  history, args, and hints so we later know their size in advance.\r\n  Additionally, the movablekeys flag was moved into the flags register.\r\n* Update generate-commands-json.py to take the data from both commands, it\r\n  now executes redis-cli directly, instead of taking input from stdin.\r\n* Sub-commands in both COMMAND (and COMMAND INFO), and also COMMAND DOCS, show their full name. i.e. CONFIG \r\n  GET will be shown as `config|get` rather than just `get`.\r\n  This will be visible both when asking for `COMMAND INFO config` and `COMMAND INFO config|get`, but is especially\r\n  important for the later.\r\n  i.e. imagine someone doing `COMMAND INFO slowlog|get config|get` not being able to distinguish between the two items in\r\n  the array response.\r\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-01-04T06:15:49Z",
        "closed_at": "2022-01-04T12:45:10Z",
        "merged_at": "2022-01-04T12:45:10Z",
        "body": "In monitor/pubsub mode, if the server closes the connection,\r\nfor example, use `CLIENT KILL`, redis-cli will exit directly\r\nwithout printing any error messages.\r\n\r\nThis commit ensures that redis-cli will try to print the\r\nerror messages before exiting. Also there is a minor cleanup\r\nfor restart, see the example below.\r\n\r\nbefore:\r\n```\r\n127.0.0.1:6379> monitor\r\nOK\r\n[root@ redis]#\r\n\r\n127.0.0.1:6379> subscribe channel\r\nReading messages... (press Ctrl-C to quit)\r\n1) \"subscribe\"\r\n2) \"channel\"\r\n3) (integer) 1\r\n[root@ redis]#\r\n\r\n127.0.0.1:6379> restart\r\n127.0.0.1:6379> get keyUse 'restart' only in Lua debugging mode.\r\n(nil)\r\n```\r\n\r\nafter:\r\n```\r\n127.0.0.1:6379> monitor\r\nOK\r\nError: Server closed the connection\r\n[root@ redis]#\r\n\r\n127.0.0.1:6379> subscribe channel\r\nReading messages... (press Ctrl-C to quit)\r\n1) \"subscribe\"\r\n2) \"channel\"\r\n3) (integer) 1\r\nError: Server closed the connection\r\n[root@ redis]#\r\n\r\n127.0.0.1:6379> restart\r\nUse 'restart' only in Lua debugging mode.\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 66,
        "deletions": 13,
        "changed_files": 5,
        "created_at": "2022-01-04T01:14:53Z",
        "closed_at": "2022-01-10T07:09:40Z",
        "merged_at": "2022-01-10T07:09:40Z",
        "body": "fix [issue](https://github.com/redis/redis/pull/9788#issuecomment-1004114535)  .\r\n\r\n1. Ban whitespace characters in `appenddirname`   \r\n2. Handle the case where `appendfilename` contains spaces (for backwards compatibility)",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2022-01-03T12:43:29Z",
        "closed_at": "2022-01-04T15:24:29Z",
        "merged_at": "2022-01-04T15:24:29Z",
        "body": "Fixing https://github.com/redis/redis/pull/9954#issuecomment-1003914049\r\n\r\nAbout RESP3 an ordered collection of key-value pairs, keys and value can\r\nbe any other RESP3 type, but a key should be string in JSON spec.\r\n\r\nAS IS\r\n```\r\n./src/redis-cli -3 --json latency histogram | jq\r\nparse error: Object keys must be strings at line 1, column 39\r\n\r\n./src/redis-cli -3 --json latency histogram\r\n{\"zadd\":{\"calls\":3,\"histogram_usec\":{8:1,16:2,33:3}},\"info\":{\"calls\":1,\"histogram_usec\":{264:1}},\"hset\":{\"calls\":1,\"histogram_usec\":{16:1}},\"zrange\":{\"calls\":1,\"histogram_usec\":{66:1}},\"command\":{\"calls\":1,\"histogram_usec\":{8454:1}},\"hello\":{\"calls\":22,\"histogram_usec\":{4:8,8:22}},\"hgetall\":{\"calls\":3,\"histogram_usec\":{4:1,8:3}},\"scan\":{\"calls\":2,\"histogram_usec\":{8:1,33:2}}}\r\n```\r\n\r\nTO BE\r\n```\r\n./src/redis-cli -3 --json latency histogram | jq\r\n{\r\n  \"zadd\": {\r\n    \"calls\": 3,\r\n    \"histogram_usec\": {\r\n      \"8\": 1,\r\n      \"16\": 2,\r\n      \"33\": 3\r\n    }\r\n  },\r\n  \"info\": {\r\n    \"calls\": 1,\r\n    \"histogram_usec\": {\r\n      \"264\": 1\r\n    }\r\n  },\r\n  \"hset\": {\r\n    \"calls\": 1,\r\n    \"histogram_usec\": {\r\n      \"16\": 1\r\n    }\r\n  },\r\n  \"zrange\": {\r\n    \"calls\": 1,\r\n    \"histogram_usec\": {\r\n      \"66\": 1\r\n    }\r\n  },\r\n  \"command\": {\r\n    \"calls\": 1,\r\n    \"histogram_usec\": {\r\n      \"8454\": 1\r\n    }\r\n  },\r\n  \"hello\": {\r\n    \"calls\": 13,\r\n    \"histogram_usec\": {\r\n      \"4\": 5,\r\n      \"8\": 13\r\n    }\r\n  }\r\n}\r\n```\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 15,
        "changed_files": 5,
        "created_at": "2022-01-03T12:28:40Z",
        "closed_at": "2022-01-04T15:09:23Z",
        "merged_at": "2022-01-04T15:09:23Z",
        "body": "This makes `redis-cli --replica` much faster and reduces COW/fork risks on server side.\r\nThis commit also improves the RDB filtering via `REPLCONF rdb-filter-only` to support no \"include\" specifiers at all.\r\n\r\nCame up as part of the discussion here: https://github.com/redis/redis/pull/9742#issuecomment-1001558028",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 353,
        "deletions": 44,
        "changed_files": 3,
        "created_at": "2022-01-03T10:51:25Z",
        "closed_at": "2022-02-05T14:54:17Z",
        "merged_at": "2022-02-05T14:54:17Z",
        "body": "This is a followup to #9656 and implements the following step mentioned in that PR:\r\n\r\n* When possible, extract all the help and completion tips from COMMAND DOCS (Redis 7.0 and up)\r\n* If COMMAND DOCS fails, use the static help.h compiled into redis-cli.\r\n* Supplement additional command names from COMMAND (pre-Redis 7.0)\r\n\r\nThe last step is needed to add module command and other non-standard commands.\r\n\r\nThis PR does not change the interactive hinting mechanism, which still uses only the param strings to provide somewhat unreliable and inconsistent command hints (see #8084). That task is left for a future PR. ",
        "comments": 30
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-01-02T08:20:27Z",
        "closed_at": "2022-01-02T11:58:22Z",
        "merged_at": "2022-01-02T11:58:22Z",
        "body": "This commit adds DUMP RESTORES tests for the -x and -X options.\r\nI wanted to add it in #9980 which introduce the -X option, but\r\nback then i failed due to some errors (related to redis-cli call).",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2022-01-01T23:37:18Z",
        "closed_at": "2022-01-06T15:59:37Z",
        "merged_at": "2022-01-06T15:59:37Z",
        "body": "With this rule, the script to generate commands.c from JSON runs whenever commands.o is built if any of commands/*.json are modified. Without such rule, it's easy to forget to run the script when updating the JSON files.\r\n\r\nIt's a follow-up on #9656 and #9951.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2022-01-01T14:29:33Z",
        "closed_at": "2022-01-01T15:42:12Z",
        "merged_at": null,
        "body": "annoted of replicationCron should update",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2022-01-01T10:25:25Z",
        "closed_at": "2022-01-23T11:54:50Z",
        "merged_at": "2022-01-23T11:54:50Z",
        "body": "A test failure was reported in Daily CI (test-centos7-tls).\r\n`CKQUORUM detects failover authorization cannot be reached`.\r\n\r\n```\r\nCKQUORUM detects failover authorization cannot be reached: FAILED:\r\nExpected 'invalid command name \"OK 4 usable Sentinels. Quorum and failover authorization can be reached\"' to match '*NOQUORUM*'\r\n```\r\n\r\nIt seems that current sentinel does not confirm that the other\r\nsentinels are actually `down`, and then check the quorum.\r\nIt at least take 3 seconds on my machine, and we can see there\r\nwill be a timing issue with the hard code `after 5000`.\r\n\r\nIn this commit, we simply wait_for_condition on `S 0 SENTINEL CKQUORUM mymaster`\r\nto be equal *NOQUORUM*. Give it a few more chances.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 106,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-12-31T13:48:15Z",
        "closed_at": "2022-01-04T19:48:50Z",
        "merged_at": "2022-01-04T19:48:50Z",
        "body": "The purpose of this commit is to add some tests to\r\ncover #5299, which was fixed in #5300 but without tests.\r\n\r\nThis commit should close #5306 and #5299.\r\n\r\nAlso add `wait_for_blocked_clients_count` for some test cases in\r\n`stream-cgroups` to prevent timing issues.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2021-12-31T10:25:51Z",
        "closed_at": "2022-01-09T09:13:32Z",
        "merged_at": "2022-01-09T09:13:32Z",
        "body": "Ref: https://github.com/redis/redis-doc/pull/1722#pullrequestreview-841201222",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-12-31T07:39:25Z",
        "closed_at": "2022-01-04T12:05:00Z",
        "merged_at": "2022-01-04T12:05:00Z",
        "body": "Older versions of GNU Make (<4.3) required quoting of number signs to\r\navoid them being treated as a comment. Newer versions will treat this\r\nquote as a literal.\r\n\r\nThis issue and a proposed solution is discussed here:\r\nhttps://lists.gnu.org/archive/html/info-gnu/2020-01/msg00004.html",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-12-31T01:08:57Z",
        "closed_at": "2021-12-31T12:08:04Z",
        "merged_at": "2021-12-31T12:08:04Z",
        "body": "Now if redis is still loading when we receive sigterm, we will wait for the loading to reach the event\r\nloop (once in 2mb) before actually shutting down. See #10003.\r\n\r\nThis change caused valgrind CI to fail.\r\nSee https://github.com/redis/redis/runs/4662901673?check_suite_focus=true\r\n\r\nThis pr is mainly to solve the problem that redis process cannot be exited normally.\r\nWhen the master is disconnected, if repl is processing diskless loading and using `connRead` to read data from master,\r\nit may enter an infinite retry state, which does not handle `connRead` returning 0(master connection disconnected).",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2021-12-30T12:52:43Z",
        "closed_at": "2021-12-30T15:52:29Z",
        "merged_at": null,
        "body": "use Bit operation will be faster than while",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 39,
        "deletions": 9,
        "changed_files": 4,
        "created_at": "2021-12-29T13:46:14Z",
        "closed_at": "2021-12-30T03:29:02Z",
        "merged_at": null,
        "body": "If we revoke the default user privileges, the Master/slave synchronization by default user will break the chain and resynchronize, resulting in the Master coredump; And same other case reply to slave will result in the same result",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-12-29T13:25:26Z",
        "closed_at": "2021-12-30T09:34:41Z",
        "merged_at": "2021-12-30T09:34:41Z",
        "body": "Add blank before lists, so that they will be rendered as lists.\r\n\r\nCommands RM_GetCommand and RM_CreateSubcommand were added in #9656.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 31,
        "changed_files": 1,
        "created_at": "2021-12-29T07:16:27Z",
        "closed_at": "2022-06-06T05:09:21Z",
        "merged_at": null,
        "body": "After aofrewrite is completed, if the `server.aof_state` is `AOF_OFF`, the `backgroundRewriteDoneHandler` will try to open `server.aof_filename`. I think this operation is unnecessary.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-12-29T04:23:01Z",
        "closed_at": "2021-12-31T17:32:33Z",
        "merged_at": "2021-12-31T17:32:33Z",
        "body": "Cluster node names are not null terminated, so they need to be constrained when printing them out.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-12-28T10:25:55Z",
        "closed_at": "2022-01-02T09:18:43Z",
        "merged_at": "2022-01-02T09:18:43Z",
        "body": "Since #9166 we have an assertion [here](https://github.com/redis/redis/blob/266d95066da8082ab569207765bea674fa297404/src/networking.c#L1013) to make sure replica clients don't write anything to their buffer.\r\nBut in reality a replica may attempt write data to it's buffer simply by sending a command on the replication link. This command in most cases will be rejected since #8868 but it'll still generate an error.\r\nActually the only valid command to send on a replication link is 'REPCONF ACK` which generates [**no** response](https://github.com/redis/redis/blob/266d95066da8082ab569207765bea674fa297404/src/replication.c#L1164).\r\n\r\nWe want to keep the design so that replicas can send commands but we need to avoid any situation where we start putting data in their response buffers, especially since they aren't used anymore. This PR makes sure to disconnect a rogue client which generated a write on the replication link that cause something to be written to the response buffer.\r\n\r\nTo recreate the bug this fixes simply connect via _telnet_ to a redis server and write `sync\\r\\n` wait for the the payload to be written and then write any command (valid or invalid), such as `ping\\r\\n` on the _telnet_ connection. It'll crash the server.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 9,
        "changed_files": 4,
        "created_at": "2021-12-28T09:11:23Z",
        "closed_at": "2022-01-04T11:08:10Z",
        "merged_at": "2022-01-04T11:08:10Z",
        "body": "This would mean that the effects of `CONFIG SET maxmemory` may not be visible once the command returns.\r\nThat could anyway happen since incremental eviction was added in redis 6.2 (see #7653)\r\n\r\nWe do this to fix one of the propagation bugs about eviction see #9890 and #10014.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2021-12-28T08:02:01Z",
        "closed_at": "2022-02-21T06:06:58Z",
        "merged_at": "2022-02-21T06:06:58Z",
        "body": "Consider the following example:\r\n1. geoadd k1 -0.15307903289794921875 85 n1 0.3515625 85.00019260486917005437 n2.\r\n2. geodist k1 n1 n2 returns  \"4891.9380\"\r\n3. but GEORADIUSBYMEMBER k1 n1 4891.94 m only returns n1.\r\nn2 is in the  boundingbox but out of search areas.So we let  search areas contain boundingbox to get n2.",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-12-28T06:44:21Z",
        "closed_at": "2022-01-02T08:40:00Z",
        "merged_at": "2022-01-02T08:40:00Z",
        "body": "Report the cluster slots to keys map size in MEMORY STATS.\r\nAlso, include that overhead in MEMORY USAGE.\r\n\r\nslots to keys map is implemented as a linked list. \r\nTwo pointers, prev and next, are used to implement the map as a linked list.\r\n\r\nAddresses the memory usage reporting part of issue https://github.com/redis/redis/issues/9939\r\n\r\n\r\nMEMORY STATS output\r\n```\r\n./src/redis-cli -p 30001 memory stats\r\n 1) \"peak.allocated\"\r\n 2) (integer) 6844896\r\n 3) \"total.allocated\"\r\n 4) (integer) 6279648\r\n 5) \"startup.allocated\"\r\n 6) (integer) 1711968\r\n 7) \"replication.backlog\"\r\n 8) (integer) 1048592\r\n 9) \"clients.slaves\"\r\n10) (integer) 17384\r\n11) \"clients.normal\"\r\n12) (integer) 0\r\n13) \"aof.buffer\"\r\n14) (integer) 1536\r\n15) \"lua.caches\"\r\n16) (integer) 0\r\n17) \"functions.caches\"\r\n18) (integer) 200\r\n19) \"db.0\"\r\n20) 1) \"overhead.hashtable.main\"\r\n    2) (integer) 1376344\r\n    3) \"overhead.hashtable.expires\"\r\n    4) (integer) 0\r\n    5) \"overhead.hashtable.slot-to-keys\"\r\n    6) (integer) 445680\r\n21) \"overhead.total\"\r\n22) (integer) 4601704\r\n23) \"keys.count\"\r\n24) (integer) 27855\r\n25) \"keys.bytes-per-key\"\r\n26) (integer) 163\r\n27) \"dataset.bytes\"\r\n28) (integer) 1677944\r\n29) \"dataset.percentage\"\r\n30) \"36.735149383544922\"\r\n31) \"peak.percentage\"\r\n32) \"91.742050170898438\"\r\n33) \"allocator.allocated\"\r\n34) (integer) 6250448\r\n35) \"allocator.active\"\r\n36) (integer) 9583616\r\n37) \"allocator.resident\"\r\n38) (integer) 9583616\r\n39) \"allocator-fragmentation.ratio\"\r\n40) \"1.5332686901092529\"\r\n41) \"allocator-fragmentation.bytes\"\r\n42) (integer) 3333168\r\n43) \"allocator-rss.ratio\"\r\n44) \"1\"\r\n45) \"allocator-rss.bytes\"\r\n46) (integer) 0\r\n47) \"rss-overhead.ratio\"\r\n48) \"1.0039534568786621\"\r\n49) \"rss-overhead.bytes\"\r\n50) (integer) 37888\r\n51) \"fragmentation\"\r\n52) \"1.5393302440643311\"\r\n53) \"fragmentation.bytes\"\r\n54) (integer) 3371056\r\n```\r\n\r\nMEMORY USAGE output\r\n```\r\nCMD\r\nyzhaon@3c22fb5d816a redis % ./src/redis-cli set foo foo\r\nOK\r\nyzhaon@3c22fb5d816a redis % ./src/redis-cli memory usage foo\r\n(integer) 72\r\n\r\nCME\r\nyzhaon@3c22fb5d816a redis % ./src/redis-cli -p 30001 set foo foo\r\n(error) MOVED 12182 127.0.0.1:30003\r\nyzhaon@3c22fb5d816a redis % ./src/redis-cli -p 30003 set foo foo\r\nOK\r\nyzhaon@3c22fb5d816a redis % ./src/redis-cli -p 30003 memory usage foo\r\n(integer) 88\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 451,
        "deletions": 211,
        "changed_files": 64,
        "created_at": "2021-12-27T22:07:28Z",
        "closed_at": "2021-12-29T19:57:40Z",
        "merged_at": "2021-12-29T19:57:40Z",
        "body": "Add missing information about commands, mainly from reviewing redis-doc and removing the metadata from it (https://github.com/redis/redis-doc/pull/1722)\r\n\r\n* Reintroduces CLUSTER S****S (supported by Redis) but missing from the JSON / docs (related? #9675).\r\n  Note that without that json file, the command won't work (breaking change)\r\n* Adds the `replicas` argument (exists in Redis) to `CLIENT KILL`.\r\n* Adds `history` entries to several commands based on redis-doc's man pages.\r\n* Adds `since` to applicable command arguments based on `history` (this basically makes some of `history` redundant - perhaps at a later stage).\r\n* Uses proper semantic versioning in all version references.\r\n* Also removes `geoencodeCommand` and `geodecodeCommand` header\r\ndeclarations per b96af595a5fddbbdcbf78ed3c51acd60976416f4.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 114,
        "deletions": 11,
        "changed_files": 11,
        "created_at": "2021-12-27T12:29:36Z",
        "closed_at": "2022-01-04T11:37:48Z",
        "merged_at": "2022-01-04T11:37:48Z",
        "body": "Creating fork (or even a foreground SAVE) during a transaction breaks the atomicity of the transaction.\r\nIn addition to that, it could mess up the propagated transaction to the AOF file.\r\n\r\nThis change blocks SAVE, PSYNC, SYNC and SHUTDOWN from being executed inside MULTI-EXEC.\r\nIt does that by adding a command flag, so that modules can flag their commands with that flag too.\r\n\r\nBesides it changes BGSAVE, BGREWRITEAOF, and CONFIG SET appendonly, to turn the\r\nscheduled flag instead of forking righ taway.\r\n\r\nOther changes:\r\n* expose `protected`, `no-async-loading`, and `no_multi` flags in COMMAND command\r\n* add a test to validate propagation of FLUSHALL inside a transaction.\r\n* add a test to validate how CONFIG SET that errors reacts in a transaction",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-12-27T10:27:54Z",
        "closed_at": "2021-12-28T08:19:58Z",
        "merged_at": "2021-12-28T08:19:58Z",
        "body": "Preventing COFIG SET maxmemory from propagating is just the tip of the iceberg.\r\nModule that performs a write operation in a notification can cause any\r\ncommand to be propagated, based on server.dirty\r\n\r\nWe need to come up with a better solution.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 21,
        "changed_files": 3,
        "created_at": "2021-12-27T08:50:32Z",
        "closed_at": "2021-12-27T19:37:21Z",
        "merged_at": "2021-12-27T19:37:21Z",
        "body": "It turns out that libc malloc can return an allocation of a different size on requests of the same size.\r\nthis means that matching MEMORY USAGE of one key to another copy of the same data can fail.\r\n\r\nSolution:\r\nKeep running the test that calls MEMORY USAGE, but ignore the response.\r\nWe do that by introducing a new utility function to get the memory usage, which always returns 1 when the allocator is not jemalloc.\r\n\r\nOther changes:\r\nSome formatting for datatype2.tcl\r\n\r\nFor detailed discussion, please see:  https://github.com/redis/redis/pull/8999#issuecomment-1001428093",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2021-12-27T03:18:47Z",
        "closed_at": "2021-12-27T13:18:17Z",
        "merged_at": "2021-12-27T13:18:17Z",
        "body": "PR #9890 may have introduced a problem.\r\nThere are tests that use MULTI-EXEC to make sure two BGSAVE / BGREWRITEAOF are executed together.\r\nBut now it's not valid to run run commands that create a snapshot inside a transaction (gonna be blocked soon)\r\nThis PR modifies the test not to rely on MULTI-EXEC.\r\n\r\nOld description \r\n----\r\n\r\nlet me describe the phenomenon.\r\n\r\nstart redis server: `./src/redis-server --appendonly yes --aof-use-rdb-preamble no`\r\n\r\nstart a redis-cli\r\n\r\nthen execute the following sequence commands:\r\n```\r\n127.0.0.1:6379> multi\r\nOK\r\n127.0.0.1:6379(TX)> lpush listkey e1\r\nQUEUED\r\n127.0.0.1:6379(TX)> lpush listkey e2\r\nQUEUED\r\n127.0.0.1:6379(TX)> BGREWRITEAOF\r\nQUEUED\r\n127.0.0.1:6379(TX)> exec\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) Background append only file rewriting started\r\n```\r\nThen I get the following AOF:\r\n```\r\n*2\r\n$6\r\nSELECT\r\n$1\r\n0\r\n*4\r\n$5\r\nRPUSH\r\n$7\r\nlistkey\r\n$2\r\ne2\r\n$2\r\ne1\r\n*2\r\n$6\r\nSELECT\r\n$1\r\n0\r\n*1\r\n$5\r\nMULTI\r\n*3\r\n$5\r\nlpush\r\n$7\r\nlistkey\r\n$2\r\ne1\r\n*3\r\n$5\r\nlpush\r\n$7\r\nlistkey\r\n$2\r\ne2\r\n*1\r\n$4\r\nEXEC\r\n```\r\nWe can see that `e1` and `e2` have been added twice, restart redis and load AOF:\r\n```\r\n127.0.0.1:6379> llen listkey\r\n(integer) 4\r\n```\r\n\r\nThen  i tested the version before the PR #9890 \r\nI get the following AOF:\r\n```\r\n*2\r\n$6\r\nSELECT\r\n$1\r\n0\r\n*4\r\n$5\r\nRPUSH\r\n$7\r\nlistkey\r\n$2\r\ne2\r\n$2\r\ne1\r\n*2\r\n$6\r\nSELECT\r\n$1\r\n0\r\n*1\r\n$4\r\nexec\r\n```\r\nObviously we have got an incomplete transaction record. restart redis server:\r\n```\r\n22656:M 27 Dec 2021 11:09:29.514 # Server initialized\r\n22656:M 27 Dec 2021 11:09:29.514 # == CRITICAL == This server is sending an error to its AOF-loading-client: 'EXEC without MULTI' after processing the command 'exec'\r\n22656:M 27 Dec 2021 11:09:29.515 * DB loaded from append only file: 0.000 seconds\r\n22656:M 27 Dec 2021 11:09:29.515 * Ready to accept connections\r\n```\r\nBut we can still get the correct data\uff1a\r\n```\r\n127.0.0.1:6379> llen listkey\r\n(integer) 2\r\n```\r\n\r\nSo, i simply modified the code(without corresponding test), I am not sure if this modification will cause other problems. At least I think that allowing `bgsave/bgrewriteaof/save` commands to be executed in a transaction is not a wise choice.\r\n\r\nSo I think `bgsave/bgrewriteaof/save` should not be executed in a transaction, they should be executed immediately, and the unfinished data should not be persisted to disk.\r\n\r\n",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 41,
        "changed_files": 4,
        "created_at": "2021-12-26T18:16:58Z",
        "closed_at": "2021-12-27T06:45:40Z",
        "merged_at": null,
        "body": "Remove redundant declarations in geo.c",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1348,
        "deletions": 490,
        "changed_files": 22,
        "created_at": "2021-12-26T14:44:43Z",
        "closed_at": "2022-01-06T11:39:40Z",
        "merged_at": "2022-01-06T11:39:39Z",
        "body": "# Redis Function Libraries\r\n\r\nThis PR implements Redis Functions Libraries as describe on: https://github.com/redis/redis/issues/9906.\r\n\r\nLibraries purpose is to provide a better code sharing between functions by allowing to create multiple functions in a single command. Functions that were created together can safely share code between each other without worrying about compatibility issues and versioning.\r\n\r\nCreating a new library is done using 'FUNCTION LOAD' command (full API is described below)\r\n\r\nThis PR introduces a new struct called libraryInfo, libraryInfo holds information about a library:\r\n* name - name of the library\r\n* engine - engine used to create the library\r\n* code - library code\r\n* description - library description\r\n* functions - the functions exposed by the library\r\n\r\nWhen Redis gets the `FUNCTION LOAD` command it creates a new empty libraryInfo. Redis passes the `CODE` to the relevant engine alongside the empty libraryInfo. As a result, the engine will create one or more functions by calling 'libraryCreateFunction'. The new funcion will be added to the newly created libraryInfo. So far Everything is happening locally on the libraryInfo so it is easy to abort the operation (in case of an error) by simply freeing the libraryInfo. After the library info is fully constructed we start the joining phase by which we will join the new library to the other libraries currently exist on Redis. The joining phase make sure there is no function collision and add the library to the librariesCtx (renamed from functionCtx). LibrariesCtx is used all around the code in the exact same way as functionCtx was used (with respect to RDB loading, replicatio, ...). The only difference is that apart from function dictionary (maps function name to functionInfo object), the librariesCtx contains also a libraries dictionary that maps library name to libraryInfo object.\r\n\r\n## New API\r\n### FUNCTION LOAD\r\n`FUNCTION LOAD <ENGINE> <LIBRARY NAME> [REPLACE] [DESCRIPTION <DESCRIPTION>] <CODE>`\r\nCreate a new library with the given parameters:\r\n* ENGINE - REPLACE Engine name to use to create the library.\r\n* LIBRARY NAME - The new library name.\r\n* REPLACE - If the library already exists, replace it.\r\n* DESCRIPTION - Library description.\r\n* CODE - Library code.\r\n\r\nReturn \"OK\" on success, or error on the following cases:\r\n* Library name already taken and REPLACE was not used\r\n* Name collision with another existing library (even if replace was uses)\r\n* Library registration failed by the engine (usually compilation error)\r\n\r\n## Changed API\r\n### FUNCTION LIST\r\n`FUNCTION LIST [LIBRARYNAME <LIBRARY NAME PATTERN>] [WITHCODE]`\r\nCommand was modified to also allow getting libraries code (so `FUNCTION INFO` command is no longer needed and removed). In addition the command gets an option argument, `LIBRARYNAME` allows you to only get libraries that match the given `LIBRARYNAME` pattern. By default, it returns all libraries.\r\n\r\n### INFO MEMORY\r\nAdded number of libraries to `INFO MEMORY`\r\n\r\n### Commands flags\r\n`DENYOOM` flag was set on `FUNCTION LOAD` and `FUNCTION RESTORE`. We consider those commands as commands that add new data to the dateset (functions are data) and so we want to disallows to run those commands on OOM.\r\n\r\n## Removed API\r\n* FUNCTION CREATE - Decided on https://github.com/redis/redis/issues/9906\r\n* FUNCTION INFO - Decided on https://github.com/redis/redis/issues/9899\r\n\r\n## Lua engine changes\r\nWhen the Lua engine gets the code given on `FUNCTION LOAD` command, it immediately runs it, we call this run the loading run. Loading run is not a usual script run, it is not possible to invoke any Redis command from within the load run. Instead there is a new API provided by `library` object. The new API's: \r\n* `redis.log` - behave the same as `redis.log`\r\n* `redis.register_function` - register a new function to the library\r\n\r\nThe loading run purpose is to register functions using the new `redis.register_function` API. Any attempt to use any other API will result in an error. In addition, the load run is has a time limit of 500ms, error is raise on timeout and the entire operation is aborted.\r\n\r\n### `redis.register_function`\r\n`redis.register_function(<function_name>, <callback>, [<description>])`\r\nThis new API allows users to register a new function that will be linked to the newly created library. This API can only be called during the load run (see definition above). Any attempt to use it outside of the load run will result in an error. The parameters pass to the API are:\r\n* function_name - Function name (must be a Lua string)\r\n* callback - Lua function object that will be called when the function is invokes using fcall/fcall_ro\r\n* description - Function description, optional (must be a Lua string).\r\n\r\n### Example\r\nThe following example creates a library called `lib` with 2 functions, `f1` and `f1`, returns 1 and 2 respectively:\r\n```\r\nlocal function f1(keys, args)\r\n\u00a0 \u00a0 return 1\r\nend\r\n\r\nlocal function f2(keys, args)\r\n\u00a0 \u00a0 return 2\r\nend\r\n\r\nredis.register_function('f1', f1)\r\nredis.register_function('f2', f2)\r\n```\r\n\r\nNotice: Unlike `eval`, functions inside a library get the KEYS and ARGV as arguments to the functions and not as global.\r\n\r\n### Technical Details\r\n\r\nOn the load run we only want the user to be able to call a white list on API's. This way, in the future, if new API's will be added, the new API's will not be available to the load run unless specifically added to this white list. We put the while list on the `library` object and make sure the `library` object is only available to the load run by using [lua_setfenv](https://www.lua.org/manual/5.1/manual.html#lua_setfenv) API. This API allows us to set the `globals` of a function (and all the function it creates). Before starting the load run we create a new fresh Lua table (call it `g`) that only contains the `library` API (we make sure to set global protection on this table just like the general global protection already exists today), then we use [lua_setfenv](https://www.lua.org/manual/5.1/manual.html#lua_setfenv) to set `g` as the global table of the load run. After the load run finished we update `g` metatable and set `__index` and `__newindex` functions to be `_G` (Lua default globals), we also pop out the `library` object as we do not need it anymore. This way, any function that was created on the load run (and will be invoke using `fcall`) will see the default globals as it expected to see them and will not have the `library` API anymore.\r\n\r\nAn important outcome of this new approach is that now we can achieve a distinct global table for each library (it is not yet like that but it is very easy to achieve it now). In the future we can decide to remove global protection because global on different libraries will not collide or we can chose to give different API to different libraries base on some configuration or input.\r\n\r\nNotice that this technique was meant to prevent errors and was not meant to prevent malicious user from exploit it. For example, the load run can still save the `library` object on some local variable and then using in `fcall` context. To prevent such a malicious use, the C code also make sure it is running in the right context and if not raise an error.\r\n\r\n- [x] Implement `FUNCTION LIST` pattern matching\r\n- [x] Count libraries memory overhead\r\n- [x] Functions flags, decide if we want it and which flags we want - delayed to followup PR and will be discussed on https://github.com/redis/redis/issues/10025",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-12-26T10:01:56Z",
        "closed_at": "2021-12-28T11:25:56Z",
        "merged_at": "2021-12-28T11:25:56Z",
        "body": "on the signal handler we are enabling server.shutdown_asap flag instead of just doing `exit()`,\r\nand then catch it on the whileBlockedCron() where we prepare for shutdown correctly.\r\n\r\nthis is a more ore organized and safe termination, the old approach was missing these for example:\r\n1. removal of the pidfile\r\n2. shutdown event to modules",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 591,
        "deletions": 35,
        "changed_files": 10,
        "created_at": "2021-12-26T02:23:26Z",
        "closed_at": "2022-01-18T11:10:07Z",
        "merged_at": "2022-01-18T11:10:07Z",
        "body": "Added event loop support to the module API. \r\n\r\nModules can now register sockets/pipe to the Redis main thread event loop and do network operations asynchronously. Previously, modules had to maintain an event loop and another thread for asynchronous network operations. Also, if module is calling API functions after doing some network operations, module had to synchronize its event loop thread's access with Redis main thread by locking the GIL, causing contention on the lock. After this commit, no synchronization is needed as module can operate in Redis main thread context. So, this commit may improve the performance for some use cases. \r\n\r\nAdded three functions to the module API: \r\n- RedisModule_EventLoopAdd(int fd, int mask, RedisModuleEventLoopFunc func, void *user_data)\r\n- RedisModule_EventLoopDel(int fd, int mask) \r\n- RedisModule_EventLoopAddOneShot(RedisModuleEventLoopOneShotFunc func, void *user_data) - This function can be called from other threads to trigger callback on Redis main thread. Callback will be triggered only once. If Redis main thread is sleeping, this call will wake up the Redis main thread.\r\n\r\nEvent loop callbacks are called by Redis main thread after locking the GIL. Inside callbacks, modules can operate as if they are holding the GIL. \r\n\r\nAdded REDISMODULE_EVENT_EVENTLOOP event with two subevents:\r\n- REDISMODULE_SUBEVENT_EVENTLOOP_BEFORE_SLEEP\r\n- REDISMODULE_SUBEVENT_EVENTLOOP_AFTER_SLEEP\r\n\r\nThese events are for modules that want to participate in the before and after sleep action. e.g It might be useful to implement batching : Read data from the network, write all to a file in one go on BEFORE_SLEEP event.",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 56,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-12-22T18:06:30Z",
        "closed_at": "2022-01-11T12:33:39Z",
        "merged_at": "2022-01-11T12:33:39Z",
        "body": "We must fail RM_SubscribeToServerEvent in case a module, that\r\nwas compiled with a new redismodule.h, tries to subscribe to an\r\nevent that doesn't exist on an old redis-server",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 24,
        "changed_files": 13,
        "created_at": "2021-12-22T16:21:29Z",
        "closed_at": "2021-12-28T08:11:59Z",
        "merged_at": null,
        "body": "If we try to link statically the binaries with TLS support, (we only\r\nhave the static archive libcrypto.a, libssl.a and libz.a), we have\r\nmultiple definition of the symbol `zcalloc`.\r\n\r\n```\r\n/lib/gcc/x86_64-1a-linux-gnu/6.5.0/../../../../x86_64-1a-linux-gnu/bin/ld:\r\n/home/docker/development/opensource-pack-builder/components/redis-server/DEPS/root/lib/libz.a(zutil.o):\r\nin function `zcalloc':\r\nzutil.c:309: multiple definition of `zcalloc';\r\nzmalloc.o:/redis-6.0.9/src/zmalloc.c:135: first defined here\r\ncollect2: error: ld returned 1 exit status\r\nmake[1]: *** [Makefile:318: redis-cli] Error 1\r\nmake[1]: *** Waiting for unfinished jobs....\r\n/lib/gcc/x86_64-1a-linux-gnu/6.5.0/../../../../x86_64-1a-linux-gnu/bin/ld:\r\n/home/docker/development/opensource-pack-builder/components/redis-server/DEPS/root/lib/libz.a(zutil.o):\r\nin function `zcalloc':\r\nzutil.c:309: multiple definition of `zcalloc';\r\nzmalloc.o:/redis-6.0.9/src/zmalloc.c:135: first defined here\r\n```\r\n\r\nThis PR fix that error but I think some symbols should be prefixed to\r\navoid any future naming collisions.",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-12-22T16:16:19Z",
        "closed_at": "2021-12-27T11:10:43Z",
        "merged_at": null,
        "body": "The current behaviour is to run pkg-config to get the right link flags.\r\nThis is good for systems that can rely upon the this.\r\n\r\nOn some, we cannot rely on it. In that case, the Makefile add the\r\nhardcoded `-lcrypto` and `-lssl` but the `-lz` is missing.\r\n\r\nMoreover, it does not help to set it in the `REDIS_LDFLAGS`.\r\n\r\nThe idea is that, if we need to add openssl link flags, let's add zlib\r\ntoo.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-12-22T15:23:26Z",
        "closed_at": "2021-12-26T06:37:24Z",
        "merged_at": "2021-12-26T06:37:24Z",
        "body": "Use case insensitive string comparison for function names (like we do for commands and configs)\r\nIn addition, add verification that the functions only use the following characters: `[a-zA-Z0-9_]`\r\n\r\nBefore this fix:\r\n```\r\n> FUNCTION CREATE LUA TEST \"return 1\"\r\nOK\r\n> FCALL TEST 0\r\n1\r\n> FCALL test 0\r\nERR Function not found\r\n```\r\n\r\nAfter this fix:\r\n```\r\n> FUNCTION CREATE LUA TEST \"return 1\"\r\nOK\r\n> FCALL TEST 0\r\n1\r\n> FCALL test 0\r\n1\r\n```\r\n\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 21,
        "changed_files": 15,
        "created_at": "2021-12-22T15:13:23Z",
        "closed_at": "2021-12-30T10:17:22Z",
        "merged_at": "2021-12-30T10:17:22Z",
        "body": "Many API functions have existed since Redis 4 and it makes no sense that they're flagged as experimental.\r\n\r\nThe API reference doesn't even mention that some of them are experimental. It makes them hard to use without source code debugging (which shouldn't be necessary with good docs).\r\n\r\nHere I simply make all of them non-experimental, but if some of them are new enough and should remain experimental, just say so.\r\n\r\nThis is thing no. 4 in #6860 and \"Graduate from experimental API some of the stuff\" in #8157.",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 123,
        "deletions": 23,
        "changed_files": 2,
        "created_at": "2021-12-22T10:11:42Z",
        "closed_at": "2021-12-30T10:10:04Z",
        "merged_at": "2021-12-30T10:10:04Z",
        "body": "There are two changes in this commit:\r\n\r\n1. Add -X option to redis-cli.\r\nCurrently `-x` can only be used to provide the last argument,\r\nso you can do `redis-cli dump keyname > key.dump`,\r\nand then do `redis-cli -x restore keyname 0 < key.dump`.\r\n\r\nBut what if you want to add the replace argument (which comes last?).\r\noran suggested adding such usage:\r\n`redis-cli -X <tag> restore keyname <tag> replace < key.dump`\r\n\r\ni.e. you're able to provide a string in the arguments that's gonna be\r\nsubstituted with the content from stdin.\r\n\r\nNote that the tag name should not conflict with others non-replaced args.\r\nAnd the -x and -X options are conflicting.\r\n\r\nSome usages:\r\n```\r\n[root]# echo mypasswd | src/redis-cli -X passwd_tag mset username myname password passwd_tag                                                   OK\r\n[root]# echo username > username.txt\r\n[root]# head -c -1 username.txt | src/redis-cli -X name_tag mget name_tag password\r\n1) \"myname\"\r\n2) \"mypasswd\\n\"\r\n```\r\n\r\n2. Handle the combination of both `-x` and `--cluster` or `-X` and `--cluster`\r\nExtend the broadcast option to receive the last arg or <tag> arg from the stdin.\r\n\r\nNow we can use `redis-cli -x --cluster call <host>:<port> cmd`,\r\nor `redis-cli -X <tag> --cluster call <host>:<port> cmd <tag>`.\r\n(support part of #9899)\r\n\r\nExample one, calling PING:\r\n```\r\n[root]# echo myping > ping.txt\r\n\r\n[root]# head -c -1 ping.txt | src/redis-cli -x --cluster call 127.0.0.1:6379 ping\r\n>>> Calling ping myping\r\n127.0.0.1:6379: myping\r\n127.0.0.1:6380: myping\r\n127.0.0.1:6383: myping\r\n127.0.0.1:6384: myping\r\n127.0.0.1:6382: myping\r\n127.0.0.1:6381: myping\r\n\r\n[root]# head -c -1 ping.txt | src/redis-cli -X ping_tag --cluster call 127.0.0.1:6379 ping ping_tag\r\n>>> Calling ping myping\r\n127.0.0.1:6379: myping\r\n127.0.0.1:6381: myping\r\n127.0.0.1:6380: myping\r\n127.0.0.1:6383: myping\r\n127.0.0.1:6384: myping\r\n127.0.0.1:6382: myping\r\n```\r\n\r\nExample two, calling FUNCTION CREATE and FUNCTION DELETE\r\n```\r\n[root]# echo \"return 'hello world'\" > hello.lua\r\n[root]# head -c -1 hello.lua | src/redis-cli -X func_tag --cluster call 127.0.0.1:6379 function create lua test func_tag\r\n>>> Calling function create lua test return 'hello world'\r\n127.0.0.1:6379: ERR Can not create a function on a read only replica\r\n\r\n127.0.0.1:6381: OK\r\n127.0.0.1:6380: OK\r\n127.0.0.1:6383: ERR Can not create a function on a read only replica\r\n\r\n127.0.0.1:6384: OK\r\n127.0.0.1:6382: ERR Can not create a function on a read only replica\r\n\r\n--------------------------------------------------------------------\r\n\r\n[root]# echo delete > opt.txt\r\n[root]# head -c -1 opt.txt | src/redis-cli -X opt_tag --cluster call 127.0.0.1:6379 function opt_tag test\r\n>>> Calling function delete test\r\n127.0.0.1:6379: ERR Can not delete a function on a read only replica\r\n\r\n127.0.0.1:6381: OK\r\n127.0.0.1:6380: OK\r\n127.0.0.1:6383: ERR Can not delete a function on a read only replica\r\n\r\n127.0.0.1:6384: OK\r\n127.0.0.1:6382: ERR Can not delete a function on a read only replica\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2021-12-21T07:55:24Z",
        "closed_at": "2021-12-22T10:06:30Z",
        "merged_at": "2021-12-22T10:06:30Z",
        "body": "# Intro\r\nIf a test fails at `wait_for_blocked_clients_count` after the `PAUSE` command, It won't send `UNPAUSE` to server, leading to the server hanging until timeout, which is bad and hard to debug sometimes when developing.\r\nThis PR tries to fix this.\r\n# Changes\r\nTimeout in `CLIENT PAUSE` shortened from 1e5 seconds(extremely long) to 50~100 seconds.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1809,
        "deletions": 614,
        "changed_files": 21,
        "created_at": "2021-12-21T06:42:13Z",
        "closed_at": "2022-01-20T21:05:27Z",
        "merged_at": "2022-01-20T21:05:27Z",
        "body": "Implementation based off of #7291\r\n\r\n## Implementation details\r\n\r\nThis feature adds two major features to ACLs:\r\n\r\n### Key based permissions\r\n\r\nWe will introduce three sub-permissions on keys, read + write. The goal here is to make it easy to restrict specific types of operations on key values and to allow defining permissions that work with modules. This functionality also lets you reason about \"future proofing operations\". Key based permissions do not replace command permissions.\r\n\r\n`~<pattern>` is just a shorthand for full permission on the pattern.  \r\n`%(R|W)~<pattern>` will grant the specified permissions for that pattern.  \r\nTo reset, use `resetkeys` just as before. Patterns are merged within a selector.  \r\n`%(R|W~*)` is valid syntax, and maps to granting the specified access to the entire keyspace.\r\n\r\n### Multiple selectors\r\n\r\nA selector is a set of allowed commands + first args + a set of key patterns that match against Redis commands. Each user can now have 1 or more selectors, and as long as 1 selector matches against the user, the command will be allowed. We are introducing the concept of the \"root permissions\" which is the selector that is applied to users when they are created. This selector is mutable in order to be maximally backwards compatible.\r\n\r\nAll new selectors by default are `-@ALL` and `resetkeys`. Selectors are not currently deduplicated if they are identical, but could be in the future.\r\n\r\nAll created selectors are immutable after initialization. The only way to update them is with a new keyword, `clearselectors`. Therefor to make minor changes requires fully resetting them and adding a new ones. Calling `clearselectors` does not remove permissions from the root segment. This dramatically reduces the chance of accidentally updating a selector to something you don't expect, since you are being explicit about the new state.\r\n\r\nWith selectors, you still only need to have channel access to maintain access for subscribe. We aggregate all of the allowed channels across all selectors when doing verification you can still access the channel.\r\n\r\nA selector is defined by a set of ACL operations surrounded by parentheses. (e.g. `(+@all ~*)`. The canonical form of a selector is that it should be a single string that starts with ( and ends with ) with no spacing between the boundary characters and the ACL operators. This is how it will be written out in `ACL LIST`. However, while testing I found it was easy to mess up, and it's a little weird in the ACL file. So Redis will try to transform selectors into the canonical form by stripping whitespace and searching for valid selectors across arguments. This applies to startup args, ACL list, config file, and `ACL SETUSER`. This code isn't strictly needed, but I think it's a nice usability issue. \r\n\r\n### ACL DRYRUN command\r\n\r\nA new dryrun command was added that allows users to \"test\" whether or not a given user will be able to execute a command. I was using this mostly for testing, but Itamer suggested I commit it. It also makes some tests much easier to orchestrate.\r\n\r\nSyntax\r\n```\r\nACL DRYRUN <username> <command> [<arg> ...]\r\n```\r\n\r\nReturns:\r\nOK: The command can be executed.\r\nBulk String: The error code for the command.\r\nErrors on invalid command or user names.\r\n\r\n### Module APIs\r\n\r\nThe RM_CheckKeyPermission API now also takes in a flag argument describing how the key is going to be modified.\r\n\r\n### ACL List\r\n\r\nNothing specifically interesting here, key permissions and selectors are included naturally here.\r\n\r\n### ACL GETUSER\r\nTwo changes have been made to the getuser response. The first is the new selector field, which was mentioned earlier. The second is that the GETUSER now responds with DSL for keys and channels. This was a required decision for keys, since they now had more precise definitions. I also updated channels since now everything is using DSL as opposed to channels being the one thing that wasn't.\r\nNote that there's a breaking change here!\r\n\r\nExample:\r\n```\r\n>acl list\r\n1) \"user foo off %R~foo1 %W~bar2 ~whatever resetchannels &channel1 -@all +get (%R~selector1 &* -@all +get)\"\r\n> acl getuser foo\r\n 1) \"flags\"\r\n 2) 1) \"off\"\r\n 3) \"passwords\"\r\n 4) (empty array)\r\n 5) \"commands\"\r\n 6) \"-@all +get\"\r\n 7) \"keys\"\r\n 8) \"%R~foo1 %W~bar2 ~whatever\"\r\n 9) \"channels\"\r\n10) \"&channel1\"\r\n11) \"selectors\"\r\n12) 1) 1) \"commands\"\r\n       2) \"-@all +get\"\r\n       3) \"keys\"\r\n       4) \"%R~selector1\"\r\n       5) \"channels\"\r\n       6) \"&*\"\r\n    2) 1) \"commands\"\r\n       2) \"-@all\"\r\n       3) \"keys\"\r\n       4) \"\"\r\n       5) \"channels\"\r\n       6) \"&*\"\r\n```\r\n### Keyspecs and key references\r\n\r\nThe getKeySpecs functionality returns a keyReference instead of just a position. This is used so that the ACLs can fetch the flags corresponding to the key.\r\n\r\n## The set/get problem\r\n\r\nSet is currently marked as R+W command, because it can also do GET on the data. This pops in a couple of other commands that have both `RW` flags even though they normally only do one, this was the result of us building complex commands. I just wanted to raise this as a downside. This seems like a gap, but my guess is it's not worth fixing now.\r\n\r\nI don't believe strongly this needs to be solved right now. It would require another big push into updating the keyspecs to have multiple different variants with different keys. This is being investigated in #10040. \r\n\r\n## Related conversations\r\n\r\n- [x] [Sort out the mess with read keys on keyspecs #10040](https://github.com/redis/redis/issues/10040)\r\n- [X] Implement a dryrun command for testing\r\n- [ ] [Change ACL permissions for sort/sort_ro](https://github.com/redis/redis/issues/10106)\r\n- [ ] Probably worth doing some performance testing\r\n- [ ] create redis-doc PR (including new sub-command) https://github.com/redis/redis-doc/pull/1756\r\n- [ ] Modules + channel keyspecs (https://github.com/redis/redis/issues/10156)",
        "comments": 20
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-12-20T21:49:12Z",
        "closed_at": "2021-12-28T07:20:10Z",
        "merged_at": "2021-12-28T07:20:10Z",
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-12-20T16:33:52Z",
        "closed_at": "2021-12-21T12:11:18Z",
        "merged_at": "2021-12-21T12:11:18Z",
        "body": "Let's you see which version of redis this tool is part of (similarly to redis-cli and redis-benchmark)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 288,
        "deletions": 133,
        "changed_files": 12,
        "created_at": "2021-12-20T14:08:51Z",
        "closed_at": "2022-01-02T07:39:01Z",
        "merged_at": "2022-01-02T07:39:01Z",
        "body": "This is needed in order to ease the deployment of functions for ephemeral cases, where user needs to spin up a server with functions pre-loaded.\r\nSee https://github.com/redis/redis/issues/9899#issuecomment-991886099 and following comments.\r\n\r\n#### Details:\r\n\r\n* Added `--functions-rdb` option to _redis-cli_.\r\n* Functions only rdb via `REPLCONF rdb-filter-only functions`. This is a placeholder for a space separated inclusion filter for the RDB. In the future can be `REPLCONF rdb-filter-only \"functions db:3 key-patten:user*\"` and a complementing `rdb-filter-exclude` `REPLCONF` can also be added.\r\n* Handle \"slave requirements\" specification to RDB saving code so we can use the same RDB when different slaves express the same requirements (like functions-only) and not share the RDB when their requirements differ. This is currently just a flags `int`, but can be extended to a more complex structure with various filter fields.\r\n* make sure to support filters only in diskless replication mode (not to override the persistence file), we do that by forcing diskless (even if disabled by config)\r\n\r\nother changes:\r\n* some refactoring in rdb.c (extract portion of a big function to a sub-function)\r\n* rdb_key_save_delay used in AOFRW too\r\n\r\n#### TODO:\r\n- [x] tests.\r\n- [x] handles TODOs in the PR code.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2021-12-20T08:37:20Z",
        "closed_at": "2021-12-20T10:31:13Z",
        "merged_at": "2021-12-20T10:31:13Z",
        "body": "Recent PRs have introduced some failures, this commit\r\ntry to fix these CI failures. Here are the changes:\r\n\r\n1. Enable debug-command in sentinel test.\r\n```\r\nMaster reboot in very short time: ERR DEBUG command not allowed. If the\r\nenable-debug-command option is set to \"local\", you can run it from a\r\nlocal connection, otherwise you need to set this option in the\r\nconfiguration file, and then restart the server.\r\n```\r\n\r\n2. Enable protected-config in sentinel test.\r\n```\r\nSDOWN is triggered by misconfigured instance replying with errors: ERR\r\nCONFIG SET failed (possibly related to argument 'dir') - can't set\r\nprotected config\r\n```\r\n\r\n3. Enable debug-command in cluster test.\r\n```\r\nVerify slaves consistency: ERR DEBUG command not allowed. If the\r\nenable-debug-command option is set to \"local\", you can run it from a\r\nlocal connection, otherwise you need to set this option in the\r\nconfiguration file, and then restart the server.\r\n```\r\n\r\n4. quicklist fill should be signed int.\r\nThe reason for the modification is to eliminate the warning.\r\nModify `int fill: QL_FILL_BITS` to `signed int fill: QL_FILL_BITS`\r\n\r\nThe first three were introduced at #9920 (same issue).\r\nAnd the last one was introduced at #9962.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 129,
        "deletions": 80,
        "changed_files": 18,
        "created_at": "2021-12-19T10:52:59Z",
        "closed_at": "2021-12-19T15:41:51Z",
        "merged_at": "2021-12-19T15:41:51Z",
        "body": "- add needs:debug flag for some tests\r\n- disable \"save\" in external tests (speedup?)\r\n- use debug_digest proc instead of debug command directly so it can be skipped\r\n- use OBJECT ENCODING instead of DEBUG OBJECT to get encoding\r\n- add a proc for OBJECT REFCOUNT so it can be skipped\r\n- move a bunch of tests in latency_monitor tests to happen later so that latency monitor has some values in it\r\n- add missing close_replication_stream calls\r\n- make sure to close the temp client if DEBUG LOG fails\r\n\r\nthis is a followup PR for #9920 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 493,
        "deletions": 96,
        "changed_files": 31,
        "created_at": "2021-12-19T09:51:57Z",
        "closed_at": "2022-01-20T07:05:53Z",
        "merged_at": "2022-01-20T07:05:53Z",
        "body": "issue : https://github.com/redis/redis/issues/9655\r\n\r\nSome modules might perform a long-running logic in different stages of Redis lifetime, for example:\r\n* command execution\r\n* RDB loading\r\n* thread safe context\r\n\r\nDuring this long-running logic Redis is not responsive.\r\n\r\nThis PR offers \r\n1. An API to process events while a busy command is running (`RM_Yield`)\r\n2. A new flag (`ALLOW_BUSY`) to mark the commands that should be handled during busy\r\n  jobs which can also be used by modules (`allow-busy`)\r\n3. In slow commands and thread safe contexts, this flag will start rejecting commands with -BUSY only\r\n  after `busy-reply-threshold`\r\n4. During loading (`rdb_load` callback), it'll process events right away (not wait for `busy-reply-threshold`),\r\n  but either way, the processing is throttled to the server hz rate.\r\n5. Allow modules to Yield to redis background tasks, but not to client commands\r\n\r\n\r\n* rename `script-time-limit` to `busy-reply-threshold` (an alias to the pre-7.0 `lua-time-limit`)",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 9,
        "changed_files": 5,
        "created_at": "2021-12-19T07:17:43Z",
        "closed_at": "2021-12-19T15:52:23Z",
        "merged_at": "2021-12-19T15:52:23Z",
        "body": "1. Local variable 's' hides a parameter of the same name.\r\n```\r\nint anetTcpAccept(char *err, int s, char *ip, size_t ip_len, int *port) {\r\n    if ((fd = anetGenericAccept(err,s,(struct sockaddr*)&sa,&salen)) == ANET_ERR){}\r\n}\r\n```\r\nChange the parameter name from `s` to `serversock`,\r\nalso unified with the header file definition.\r\n\r\n2. Comparison is always false because i <= 48.\r\n```\r\nfor (i = 0; i < DICT_STATS_VECTLEN-1; i++) {  // i < 49\r\n    (i == DICT_STATS_VECTLEN-1)?\">= \":\"\",  // i == 49\r\n}\r\n```\r\n`i == DICT_STATS_VECTLEN-1` always result false, it is a dead code.\r\n\r\n3. Empty block without comment.\r\n`else if (!strcasecmp(opt,\"ch\")) {}`, add a comment to avoid warnings.\r\n\r\n4. Bit field fill of type int should have explicitly unsigned integral, explicitly signed integral, or enumeration type.\r\nModify `int fill: QL_FILL_BITS;` to `unsigned int fill: QL_FILL_BITS;`\r\n\r\n5. The result of this call to reconnectingRedisCommand is not checked for null, but 80% of calls to reconnectingRedisCommand check for null.\r\nJust a cleanup job like others.\r\n\r\nLGTM ref: https://lgtm.com/projects/g/redis/redis/?mode=list",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 20,
        "changed_files": 3,
        "created_at": "2021-12-17T16:31:49Z",
        "closed_at": "2021-12-18T07:00:43Z",
        "merged_at": "2021-12-18T07:00:43Z",
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 121,
        "deletions": 7,
        "changed_files": 4,
        "created_at": "2021-12-17T14:03:10Z",
        "closed_at": "2021-12-27T17:31:13Z",
        "merged_at": "2021-12-27T17:31:13Z",
        "body": "Following #9656, this script generates a \"commands.json\" file from the output of the new `COMMAND`.\r\n\r\nThe output of this script is used in https://github.com/redis/redis-doc/pull/1714 and by https://github.com/redis/redis-io/pull/259\r\n\r\nThis also converts a couple of rogue dashes (in 'key-specs' and 'multiple-token' flags) to underscores (continues #9959)\r\n\r\nUPDATE: with the possible introduction of #9954, this can be reduced even further.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 89,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2021-12-16T19:00:05Z",
        "closed_at": "2021-12-23T08:21:19Z",
        "merged_at": "2021-12-23T08:21:18Z",
        "body": "Introduce `redis-cli --json` option.\r\nCSV doesn't support Map type, then parsing SLOWLOG or HMSET with multiple args are not helpful.\r\n\r\nBy default the `--json` implies RESP3, which makes it much more useful, and a `-2` option was added to force RESP2.\r\nWhen `HELLO 3` fails, it prints a warning message (which can be silenced with `-2`).\r\nIf a user passed `-3` explicitly, the non-interactive mode will also exit with error without running the command, while in interactive session it'll keep running after printing the warning.\r\n\r\nJSON output would be helpful to parse Redis replies with other tools like jq.\r\n\r\n```\r\nredis-cli --json slowlog get | jq\r\n\r\n[\r\n  [\r\n    1,\r\n    1639677545,\r\n    322362,\r\n    [\r\n      \"HMSET\",\r\n      \"dummy-key\",\r\n      \"field1\",\r\n      \"123,456,789... (152 more bytes)\",\r\n      \"field2\",\r\n      \"111,222,333... (140 more bytes)\",\r\n      \"field3\",\r\n      \"... (349 more arguments)\"\r\n    ],\r\n    \"127.0.0.1:49312\",\r\n    \"\"\r\n  ]\r\n]\r\n\r\n```",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 135,
        "deletions": 70,
        "changed_files": 19,
        "created_at": "2021-12-16T15:37:41Z",
        "closed_at": "2021-12-21T14:13:30Z",
        "merged_at": "2021-12-21T14:13:30Z",
        "body": "The issue with MAY_REPLICATE is that all automatic mechanisms to handle write commands will not work. This require have a special treatment for:\r\n* Not allow those commands to be executed on RO replica.\r\n* Allow those commands to be executed on RO replica from primary connection.\r\n* Allow those commands to be executed on the RO replica from AOF.\r\n\r\nBy setting those commands as WRITE commands we are getting all those properties from Redis.\r\nTest was added to verify that those properties work as expected.\r\n\r\nIn addition, rearrange when and where functions are flushed. Before this PR functions were flushed manually on `rdbLoadRio` and cleaned manually on failure. This contradicts the assumptions that functions are data and need to be created/deleted alongside with the data. A side effect of this, for example, `debug reload noflush` did not flush the data but did flush the functions, `debug loadaof` flush the data but not the functions.\r\nThis PR move functions deletion into `emptyDb`. `emptyDb` (renamed to `emptyData`) will now accept an additional flag, `NOFUNCTIONS` which specifically indicate that we do not want to flush the functions (on all other cases, functions will be flushed). Used the new flag on FLUSHALL and FLUSHDB only! Tests were added to `debug reload` and `debug loadaof` to verify that functions behave the same as the data.\r\n\r\nNotice that because now functions will be deleted along side with the data we can not allow `CLUSTER RESET` to be called from within a function (it will cause the function to be released while running), this PR adds `NO_SCRIPT` flag to `CLUSTER RESET`  so it will not be possible to be called from within a function. The other cluster commands are allowed from within a function (there are use-cases that uses `GETKEYSINSLOT` to iterate over all the keys on a given slot). Tests was added to verify `CLUSTER RESET` is denied from within a script.\r\n\r\nAnother small change on this PR is that `RDBFLAGS_ALLOW_DUP` is also applicable on functions. When loading functions, if this flag is set, we will replace old functions with new ones on collisions. \r\n\r\nTodo:\r\n- [ ] Set write commands instead of maybe_replicate on: [FUNCTION RESTORE](https://github.com/redis/redis/pull/9938) - will be done on `FUNCTION DUMP/RESTORE` PR.\r\n- [x] CLUSTER RESET should not be allowed from inside a function, now its even risky because it cleans the functions (while a function is running)\r\n- [x] Add commands description to functions commands json",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 187,
        "deletions": 117,
        "changed_files": 4,
        "created_at": "2021-12-14T08:28:28Z",
        "closed_at": "2022-01-11T17:00:57Z",
        "merged_at": "2022-01-11T17:00:57Z",
        "body": "Added a pool for temporary client objects to reuse in module operations. By reusing temporary clients, we are avoiding expensive createClient()/freeClient() calls and improving performance of RM_BlockClient() and  RM_GetThreadSafeContext() calls. \r\n\r\nThis commit contains two optimizations: \r\n\r\n1 - RM_BlockClient() and RM_GetThreadSafeContext() calls create temporary clients and they are freed in RM_UnblockClient() and RM_FreeThreadSafeContext() calls respectively. Creating/destroying client object takes quite time. To avoid that, added a pool of temporary clients. Pool expands when more clients are needed. Also, added a cron function to shrink the pool and free unused clients after some time. Pool starts with zero clients in it. It does not have max size and can grow unbounded as we need it. We will keep minimum of 8 temporary clients in the pool once created. Keeping small amount of clients to avoid client allocation costs if temporary clients are required after some idle period.\r\n\r\n2 - After unblocking a client (RM_UnblockClient()), one byte is written to pipe to wake up Redis main thread. If there are many clients that will be unblocked, each operation requires one write() call which is quite expensive. Changed code to avoid subsequent calls if possible. \r\n\r\nThere are a few more places that need temporary client objects (e.g RM_Call()). These are now using the same temporary client pool to make things more centralized. ",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 511,
        "deletions": 33,
        "changed_files": 11,
        "created_at": "2021-12-13T15:28:21Z",
        "closed_at": "2021-12-26T07:03:37Z",
        "merged_at": "2021-12-26T07:03:37Z",
        "body": "Follow the conclusions here: https://github.com/redis/redis/issues/9899\r\nThis PR handles point 1,2 on the action items list: https://github.com/redis/redis/issues/9899#issuecomment-991886099\r\n\r\nAdded 2 new FUNCTION sub-commands:\r\n1. `FUNCTION DUMP` - dump a binary payload representation of all the functions.\r\n2. `FUNCTION RESTORE <PAYLOAD> [FLUSH|APPEND|REPLACE]` - give the binary payload extracted using `FUNCTION DUMP`, restore all the functions on the given payload. Restore policy can be given to control how to handle existing functions (default is APPEND):\r\n   * FLUSH: delete all existing functions.\r\n   * APPEND: appends the restored functions to the existing functions. On collision, abort.\r\n   * REPLACE: appends the restored functions to the existing functions. On collision, replace the old function with the new function.\r\n\r\nModify `redis-cli --cluster add-node` to use `FUNCTION DUMP` to get existing functions from one of the nodes in the cluster, and `FUNCTION RESTORE` to load the same set of functions to the new node. `redis-cli` will execute this step before sending the `CLUSTER MEET` command to the new node. If `FUNCTION DUMP` returns an error, assume the current Redis version do not support functions and skip `FUNCTION RESTORE`. If `FUNCTION RESTORE` fails, abort and do not send the `CLUSTER MEET` command. If the new node already contains functions (before the `FUNCTION RESTORE` is sent), abort and do not add the node to the cluster. Test was added to verify `redis-cli --cluster add-node` works as expected. \r\n\r\ntodo:\r\n- [x] Set write command instead of maybe_replicate on: FUNCTION RESTORE\r\n- [x] Update commands description on commands json files.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 165,
        "deletions": 1,
        "changed_files": 9,
        "created_at": "2021-12-13T08:50:04Z",
        "closed_at": "2021-12-16T15:58:26Z",
        "merged_at": "2021-12-16T15:58:25Z",
        "body": "Base on conversion on https://github.com/redis/redis/pull/9923, added `FUNCTION FLUSH` command. The new sub-command allows delete all the functions. An optional `[SYNC|ASYNC]` argument can be given to control whether or not to flush the functions synchronously or asynchronously. if not given the default flush mode is chosen by `lazyfree-lazy-user-flush` configuration values.\r\n\r\nAdd the missing `functions.tcl` test to the list of tests that are executed in test_helper.tcl, and call FUNCTION FLUSH in between servers in external mode",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2021-12-13T06:21:58Z",
        "closed_at": "2021-12-13T16:39:05Z",
        "merged_at": "2021-12-13T16:39:05Z",
        "body": "If someone tries to create a user with spaces, the command will fail, but the arguments won't be redacted so they might be logged. \r\n\r\nRelated to this PR: https://github.com/redis/redis/pull/8859/files which didn't place the redaction code correctly. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 156,
        "deletions": 28,
        "changed_files": 4,
        "created_at": "2021-12-12T10:46:07Z",
        "closed_at": "2022-02-22T12:00:37Z",
        "merged_at": "2022-02-22T12:00:37Z",
        "body": "There are scenarios where it results in many small objects in the reply list,\r\nsuch as commands heavily using deferred array replies (`addReplyDeferredLen`).\r\nE.g. what COMMAND command and CLUSTER SLOTS used to do (see #10056, #7123),\r\nbut also in case of a transaction or a pipeline of commands that use just one deferred array reply.\r\n\r\nWe used to have to run multiple loops along with multiple calls to `write()` to send data back to peer based on the current code, but by means of `writev()`, we can gather those scattered objects in reply list and include the static reply buffer as well, then send it by one system call, that ought to achieve higher performance.\r\n\r\nIn the case of TLS,  we simply check and concatenate buffers into one big buffer and send it away by one call to `connTLSWrite()`, if the amount of all buffers exceeds `NET_MAX_WRITES_PER_EVENT`, then invoke `connTLSWrite()` multiple times to avoid a huge massive of memory copies.\r\n\r\nNote that aside from reducing system calls, this change will also reduce the number of small TCP packets sent.",
        "comments": 69
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-12-11T13:08:08Z",
        "closed_at": "2021-12-21T06:59:45Z",
        "merged_at": null,
        "body": "make BITFIELD with only GET subcommand reject OVERFLOW subcommand.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-12-10T16:21:06Z",
        "closed_at": "2021-12-10T18:35:51Z",
        "merged_at": "2021-12-10T18:35:51Z",
        "body": "A test failure was reported in Daily CI (FreeBSD).\r\n`XREAD: XADD + DEL should not awake client`\r\n\r\n```\r\n*** [err]: XREAD: XADD + DEL should not awake client in tests/unit/type/stream.tcl\r\nExpected [lindex  0 0] eq {s1} (context: type eval line 11 cmd {assert {[lindex $res 0 0] eq {s1}}} proc ::test)\r\n```\r\n\r\nIt seems that `r` is executed before `rd` enters the blocking\r\nstate. And ended up getting a empty reply by timeout.\r\n\r\nWe use `wait_for_blocked_clients_count` to wait for the\r\nblocking client to be ready and avoid this situation.\r\nAlso fixed other test cases that may have the same issue.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-12-09T20:46:23Z",
        "closed_at": "2021-12-15T07:46:32Z",
        "merged_at": "2021-12-15T07:46:32Z",
        "body": "In the Sentinel, we have clear error message to clients what happens in the Sentinel Config Set as below:\r\n\r\n**127.0.0.1:26381> sentinel set mymaster failover-timeoutd 777\r\n(error) ERR Unknown option or number of arguments for SENTINEL SET 'failover-timeoutd'\r\n127.0.0.1:26381>\r\n127.0.0.1:26381>\r\n127.0.0.1:26381> sentinel set mymaster failover-timeout 777x\r\n(error) ERR Invalid argument '777x' for SENTINEL SET 'failover-timeout'**\r\n\r\n\r\nBecause we have new feature for Config Set command, we should have the same clear error message as Sentinel part\r\nThus, in this PR, we have following improved error message: \r\n\r\n**127.0.0.1:6381>\r\n127.0.0.1:6381> config set maxmemorya 12m\r\n(error) ERR Unknown option or number of arguments for CONFIG SET - 'maxmemorya'\r\n127.0.0.1:6381>\r\n127.0.0.1:6381>\r\n127.0.0.1:6381> config set maxmemory aaa\r\n(error) ERR argument 'maxmemory' for CONFIG SET failed - argument must be a memory value\r\n127.0.0.1:6381>\r\n127.0.0.1:6381>\r\n127.0.0.1:6381> config set mammemory 10000001 maxmemory-clients 200% client-query-buffer-limit invalid\r\n(error) ERR Unknown option or number of arguments for CONFIG SET - 'mammemory'\r\n127.0.0.1:6381>\r\n127.0.0.1:6381> config set maxmemory 10000001 maxmemory-clients 200% client-query-buffer-limit invalid\r\n(error) ERR argument 'maxmemory-clients' for CONFIG SET failed - percentage argument must be less or equal to 100\r\n127.0.0.1:6381>\r\n127.0.0.1:6381> config set maxmemory 10000001 port 26381 client-query-buffer-limit invalid\r\n(error) ERR argument 'client-query-buffer-limit' for CONFIG SET failed - argument must be a memory valu**e\r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 44,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2021-12-09T11:22:47Z",
        "closed_at": "2021-12-13T07:36:50Z",
        "merged_at": null,
        "body": "The issue this PR comes to solve is the ability to flush the entire data set (including functions which was introduce on https://github.com/redis/redis/pull/9780).\r\n\r\n### Why flushing Redis functions can not be added to `FLUSHALL`?\r\nFlushing the functions on `FLUSHALL` command can be dangerous because `FLUSHALL` can be called from within a function and cause the function structures to be freed while the function is running. In addition, Disallow `FLUSHALL` inside a function will differentiate functions and eval.\r\n\r\n### The solution\r\nIntroducing `FLUSHEVERYTHING` command that flushes the entire data set, including functions and eval cache. This command is not allowed inside a script (function or eval). The command arguments is just like the `FLUSHALL` command (`sync` or `async`).",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-12-09T04:40:07Z",
        "closed_at": "2021-12-12T07:44:33Z",
        "merged_at": null,
        "body": "in same place aeBeforeSleepProc func name should change to aeBeforeSleepProc ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 148,
        "deletions": 31,
        "changed_files": 15,
        "created_at": "2021-12-08T19:49:02Z",
        "closed_at": "2021-12-19T08:46:16Z",
        "merged_at": "2021-12-19T08:46:16Z",
        "body": "following issue #9687, this commit introduce 3 new immutable configs:\r\n\r\n* `enable-protected-configs` - block modification of configs with the new `PROTECTED_CONFIG` flag.\r\n   Currently we add this flag to `dbfilename`, and `dir` configs,\r\n   all of which are non-mutable configs that can set a file redis will write to.\r\n* `enable-debug-command` - block the `DEBUG` command\r\n* `enable-module-command` - block the `MODULE` command\r\n\r\nThese have a default value set to `no`, so that these features are not\r\nexposed by default to client connections, and can only be set by modifying the config file.\r\n\r\nUsers can change each of these to either `yes` (allow all access), or `local` (allow access from local TCP connections and unix domain connections)\r\n\r\nNote that this is a **breaking change** (specifically the part about MODULE command being disabled by default).\r\nI.e. we don't consider DEBUG command being blocked as an issue (people shouldn't have been using it), and the few configs we protected are unlikely to have been set at runtime anyway.\r\nOn the other hand, it's likely to assume some users who use modules, load them from the config file anyway.\r\nNote that's the whole point of this PR, for redis to be more secure by default and reduce the attack surface on innocent users, so secure defaults will necessarily mean a breaking change.\r\n\r\n[still missing in this PR]\r\n- [x] support the unitsocket option for these new configs\r\n- [x] documentation of these configs in redis.conf",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-12-08T18:19:41Z",
        "closed_at": "2021-12-09T14:37:06Z",
        "merged_at": "2021-12-09T14:37:06Z",
        "body": "When clients enter multiply parameters in the CONFIG SET command,  the server side log displays the wrong config parameters.\r\n\r\nIn the following example, one sentinel instance runs on port 26381.  \r\nWhen clients enter different config set commands, the server side log display different log. \r\nPlease check the following 4 examples:\r\n\r\nExample 1:\r\n\r\nconfig set maxmemory 10000001 client-query-buffer-limit 10m port 26381\r\n![image](https://user-images.githubusercontent.com/51993843/145263734-6178712d-eaa3-4667-9efe-cbdb2089d3ea.png)\r\n\r\nServer side log:\r\n![image](https://user-images.githubusercontent.com/51993843/145263770-7d97c2e0-7abd-4b71-9169-7b806d81a800.png)\r\n\r\nYou can see: the server side report:  Failed applying new client-query-buffer-limit configuration, restoring previous setting.\r\n But in fact, it should report \"Failed applying new port configuration\"\r\n\r\n\r\nExample 2\uff1a\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/145264030-d2d9baa1-0502-4185-a094-745e91137e7a.png)\r\n\r\nServer side log:\r\n![image](https://user-images.githubusercontent.com/51993843/145264067-c6e1cb6b-4458-42c6-b4b2-f8c989a42cb8.png)\r\n\r\nThe server side should report \"\"Failed applying new port configuration\"\" too \r\n\r\nExample 3:\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/145264187-421eeb6a-f2e0-40bb-bee2-6f6774a3b0da.png)\r\n\r\nServer side log:\r\n![image](https://user-images.githubusercontent.com/51993843/145264222-7466eb7d-7e1c-404d-94ed-f80a99fc6a32.png)\r\n\r\nThe server side display the correct message.\r\n\r\nExample 4\uff1a\r\n\r\nIf we just move port 26381 to the tail of the command as below:\r\n![image](https://user-images.githubusercontent.com/51993843/145264397-a2de6bac-4dc2-4bf1-a8f0-c5296b857725.png)\r\n\r\nServer side log:\r\n![image](https://user-images.githubusercontent.com/51993843/145264431-bdf1430b-3834-4648-986c-77300e5e55da.png)\r\n\r\nThe server side still need to report \"\"Failed applying new port configuration\"\"\r\n\r\n \r\nThis PR fix this bug, and server side always display the correct information according to the above 4 example command:\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/145264641-aab4484d-6075-4b65-a5c7-e414a9aa6efd.png)\r\n\r\nserver side always display \"Failed applying new port configuration\"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 125,
        "deletions": 107,
        "changed_files": 7,
        "created_at": "2021-12-08T14:20:11Z",
        "closed_at": "2021-12-13T19:16:26Z",
        "merged_at": "2021-12-13T19:16:26Z",
        "body": "This caused a crash when adding elements larger than 2GB to a set (same goes for hash keys). See #8455.\r\n\r\n----------------\r\nDetails:\r\n* The fix makes the dict hash functions receive a `size_t` instead of an `int`. In practice the dict hash functions call siphash which receives a `size_t` and the callers to the hash function pass a `size_t` to it so the fix is trivial. Details about the bug's history are in this comment: https://github.com/redis/redis/issues/8455#issuecomment-989645382.\r\n* The issue was recreated by attempting to add a >2gb value to a set. Appropriate tests were added where I create a set with large elements and check basic functionality on it (SADD, SCARD, SPOP, etc...).\r\n* When I added the tests I also refactored a bit all the tests code which is run under the `--large-memory` flag. This removed code duplication for the test framework's `write_big_bulk` and `read_big_bulk` code and also takes care of not allocating the test frameworks helper huge string used by these tests when not run under `--large-memory`.\r\n* I also added the _violoations.tcl_ unit tests to be part of the entire test suite and cleaned up non relevant list related tests that were in there. This was done in this PR because most of the _violations_ tests are \"large memory\" tests.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 56,
        "deletions": 43,
        "changed_files": 4,
        "created_at": "2021-12-08T11:43:04Z",
        "closed_at": "2021-12-16T07:01:13Z",
        "merged_at": "2021-12-16T07:01:13Z",
        "body": "Support doing `CONFIG GET <x> <y> <z>`, each of them can also be a pattern with wildcards.\r\n\r\nThis avoids duplicates in the result by looping over the configs and for each once checking all the patterns, once a match is found for a pattern we move on to the next config.\r\n\r\nSee #9871.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 119,
        "deletions": 21,
        "changed_files": 8,
        "created_at": "2021-12-07T10:17:14Z",
        "closed_at": "2021-12-08T12:07:16Z",
        "merged_at": null,
        "body": "Initializes the dictionary hash function seed with a pre-determined\r\nvalue, in order to make all hash based operations deterministic.\r\n\r\nThe hash-seed configuration is immutable and basically intended to\r\nsupport special use cases where Redis needs to be more deterministic,\r\nlike executing certain tests, supporting [RedisRaft](https://github.com/redislabs/redisraft), etc.\r\n\r\nThis commit also introduces an additional set of preset-seed hash\r\nfunctions that are used by dictionaries that need to be initialized and\r\npopulated early on, before configuration is loaded and the hash-seed\r\nparameter can be inspected and applied.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 16,
        "changed_files": 1,
        "created_at": "2021-12-07T08:30:01Z",
        "closed_at": "2021-12-07T10:02:59Z",
        "merged_at": "2021-12-07T10:02:59Z",
        "body": "A test failure was reported in Daily CI.\r\n`Crash report generated on SIGABRT` with FreeBSD.\r\n\r\n```\r\n*** [err]: Crash report generated on SIGABRT in tests/integration/logging.tcl\r\nExpected [string match *crashed by signal* ### Starting...(logs) in tests/integration/logging.tcl]\r\n```\r\n\r\nIt look like `tail -1000` was executed too early, before it\r\nprinted out all the crash logs. We can give it a few more\r\nchances by using `wait_for_log_messages`.\r\n\r\nOther changes:\r\n1. In `Server is able to generate a stack trace on selected systems`,\r\nuse `wait_for_log_messages`to reduce the lines of code. And if it\r\nfails, there are more detailed logs that can be printed.\r\n\r\n2. In `Crash report generated on DEBUG SEGFAULT`, we also use\r\n`wait_for_log_messages` to avoid possible timing issues.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2021-12-07T08:28:36Z",
        "closed_at": "2021-12-08T06:59:02Z",
        "merged_at": "2021-12-08T06:59:02Z",
        "body": "For `SENTINEL SET`, we can use in these ways:\r\n1. SENTINEL SET mymaster quorum 3\r\n2. SENTINEL SET mymaster quorum 5 parallel-syncs 1\r\n\r\nFor `SENTINEL SIMULATE-FAILURE`, although it is only used for testing:\r\n1. SENTINEL SIMULATE-FAILURE CRASH-AFTER-ELECTION\r\n2. SENTINEL SIMULATE-FAILURE CRASH-AFTER-ELECTION CRASH-AFTER-PROMOTION\r\n\r\nsee https://github.com/redis/redis/pull/9504#pullrequestreview-824123777",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-12-07T08:14:51Z",
        "closed_at": "2021-12-07T19:14:18Z",
        "merged_at": "2021-12-07T19:14:18Z",
        "body": "change _dictReset function annotate,",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 85,
        "deletions": 19,
        "changed_files": 4,
        "created_at": "2021-12-06T14:16:53Z",
        "closed_at": "2021-12-07T14:05:52Z",
        "merged_at": "2021-12-07T14:05:52Z",
        "body": "When disabling redis oom-score-adj managment we restore the base value read before enabling oom-score-adj management.\r\n\r\nThis fixes an issue introduced in #9748 where updating `oom-score-adj-values` while `oom-score-adj` was set to `no` would write the base oom score adj value read on startup to `/proc`. This is a bug since while `oom-score-adj` is disabled we should never write to proc and let external processes manage it.\r\n\r\nAdded appropriate test.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-12-06T05:57:22Z",
        "closed_at": "2021-12-06T08:14:13Z",
        "merged_at": "2021-12-06T08:14:13Z",
        "body": "Just a minor typo I noticed while working on ACL V2, sanitation vs sanitization. ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-12-05T15:34:33Z",
        "closed_at": "2022-07-12T19:00:40Z",
        "merged_at": null,
        "body": "optimize the code of dictFind (from #9838) and fix some styling issue",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2021-12-05T07:31:33Z",
        "closed_at": "2021-12-08T09:25:57Z",
        "merged_at": "2021-12-08T09:25:57Z",
        "body": "In 666b343, we modified the default value of protected-mode\r\nfrom yes to no.\r\n\r\nHowever, this change is not mentioned in sentinel.conf.\r\nLooking at the sentinel.conf alone, it is easy to make\r\npeople think that in sentinel mode, we hava truned on\r\nthe protected-mode.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-12-05T07:08:47Z",
        "closed_at": "2021-12-08T01:56:35Z",
        "merged_at": null,
        "body": "\u2026 idx type same with it, because their range is same but -1.and table type i range 0 to 1 so it's type no need to be uint64_t.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1004,
        "deletions": 393,
        "changed_files": 23,
        "created_at": "2021-12-03T12:52:26Z",
        "closed_at": "2021-12-22T22:03:48Z",
        "merged_at": "2021-12-22T22:03:48Z",
        "body": "The mess:\r\nSome parts use alsoPropagate for late propagation, others using an immediate one (propagate()), causing edge cases, ugly/hacky code, and the tendency for bugs\r\n\r\nThe basic idea is that all commands are propagated via alsoPropagate (i.e. added to a list) and the top-most call() is responsible for going over that list and actually propagating them (and wrapping them in MULTI/EXEC if there's more than one command). This is done in the new function, propagatePendingCommands.\r\n\r\nCallers to propagatePendingCommands:\r\n1. top-most call() (we want all nested call()s to add to the also_propagate array and just the top-most one to propagate them) - via `afterCommand`\r\n2. handleClientsBlockedOnKeys: it is out of call() context and it may propagate stuff - via `afterCommand`. \r\n3. handleClientsBlockedOnKeys edge case: if the looked-up key is already expired, we will propagate the expire but will not unblock any client so `afterCommand` isn't called. in that case, we have to propagate the deletion explicitly.\r\n4. cron stuff: active-expire and eviction may also propagate stuff\r\n5. modules: the module API allows to propagate stuff from just about anywhere (timers, keyspace notifications, threads). I could have tried to catch all the out-of-call-context places but it seemed easier to handle it in one place: when we free the context. in the spirit of what was done in call(), only the top-most freeing of a module context may cause propagation.\r\n6. modules: when using a thread-safe ctx it's not clear when/if the ctx will be freed. we do know that the module must lock the GIL before calling RM_Replicate/RM_Call so we propagate the pending commands when releasing the GIL.\r\n\r\nA \"known limitation\", which were actually a bug, was fixed because of this commit (see propagate.tcl):\r\nWhen using a mix of RM_Call with `!` and RM_Replicate, the command would propagate out-of-order: first all the commands from RM_Call, and then the ones from RM_Replicate\r\n\r\nAnother thing worth mentioning is that if, in the past, a client would issue a MULTI/EXEC with just one write command the server would blindly propagate the MULTI/EXEC too, even though it's redundant. not anymore.\r\n\r\nThis commit renames propagate() to propagateNow() in order to cause conflicts in pending PRs.\r\npropagatePendingCommands is the only caller of propagateNow, which is now a static, internal helper function.\r\n\r\nOptimizations:\r\n1. alsoPropagate will not add stuff to also_propagate if there's no AOF and replicas\r\n2. alsoPropagate reallocs also_propagagte exponentially, to save calls to memmove\r\n\r\nBugfixes:\r\n1. CONFIG SET can create evictions, sending notifications which can cause to dirty++ with modules.\r\n   we need to prevent it from propagating to AOF/replicas\r\n2. We need to set current_client in RM_Call. buggy scenario:\r\n   - CONFIG SET maxmemory, eviction notifications, module hook calls RM_Call\r\n   - assertion in lookupKey crashes, because current_client has CONFIG SET, which isn't CMD_WRITE\r\n3. minor: in eviction, call propagateDeletion after notification, like active-expire and all commands\r\n   (we always send a notification before propagating the command)",
        "comments": 22
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-12-03T12:10:15Z",
        "closed_at": "2021-12-04T14:43:08Z",
        "merged_at": "2021-12-04T14:43:08Z",
        "body": "When an invalid listpack entry starts with EOF, we will skip it when we verify it in the loop.\r\nFollowing are the steps to reproduce it before modifying:\r\n\r\n```sh\r\nconfig set sanitize-dump-payload yes\r\nrestore _list 0 \"\\x12\\x01\\x02\\x0b\\x0b\\x00\\x00\\x00\\x01\\x00\\x81\\x61\\x02\\xff\\xff\\x0a\\x00\\x7e\\xd8\\xde\\x5b\\x0d\\xd7\\x70\\xb8\"\r\nrpush _list a\r\nlrange _list 0 -1\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2021-12-03T10:29:53Z",
        "closed_at": "2021-12-08T10:44:11Z",
        "merged_at": "2021-12-08T10:44:11Z",
        "body": "Added `HIDDEN_CONFIG` to hide debug / dev / testing configs from CONFIG GET when it is used with a wildcard.\r\nThese are not documented in redis.conf so now CONFIG GET only works when they are explicitly specified.\r\n\r\nThe current configs are: \r\n```\r\nkey-load-delay\r\nloading-process-events-interval-bytes\r\nrdb-key-save-delay\r\nuse-exit-on-panic\r\nwatchdog-period\r\n```\r\n\r\nSee #9684.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-12-03T04:42:10Z",
        "closed_at": "2021-12-08T01:54:16Z",
        "merged_at": null,
        "body": "Save a 6 bit len, Currently RDB_6BITLEN is 0 ,no need to operate buf[0]",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 3080,
        "changed_files": 2,
        "created_at": "2021-12-03T04:17:39Z",
        "closed_at": "2021-12-08T01:44:21Z",
        "merged_at": null,
        "body": "Currently RDB_6BITLEN is 0 ,no need to operate buf[0] with `|(RDB_6BITLEN<<6)`, we can write in annote in case we change the value in  futrue",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2021-12-02T07:41:28Z",
        "closed_at": "2021-12-02T16:18:18Z",
        "merged_at": "2021-12-02T16:18:18Z",
        "body": "After the introduction of `Multiparam config set` in #9748,\r\nthere are two tests cases failed.\r\n\r\n```\r\n[exception]: Executing test client: ERR Config set failed - Failed to set current oom_score_adj. Check server logs..\r\nERR Config set failed - Failed to set current oom_score_adj. Check server logs.\r\n```\r\n\r\n`CONFIG sanity` test failed on the `config set oom-score-adj-values`\r\nwhich is a \"special\" config that does not catch no-op changes.\r\nAnd then it will update `oom-score-adj` which not supported in\r\nMacOs. We solve it by adding `oom-score*` to the `skip_configs` list.\r\n\r\n```\r\n*** [err]: CONFIG SET rollback on apply error in tests/unit/introspection.tcl\r\nExpected an error but nothing was caught\r\n```\r\n\r\n`CONFIG SET rollback on apply error` test failed on the\r\n`config set port $used_port`. In theory, it should throw the\r\nerror `Unable to listen on this port*`. But it failed on MacOs.\r\nWe solve it by adding `-myaddr 127.0.0.1` to the socket call.\r\n\r\nIn MacOS, tcl8.5, both socket calls are successful:\r\n```\r\n$ tclsh8.5\r\n% socket -server dummy_accept 8080\r\nsock5\r\n% socket -server dummy_accept -myaddr 127.0.0.1 8080\r\nsock6\r\n\r\n$ lsof -i:8080\r\nCOMMAND   PID   USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME\r\ntclsh8.5 2030 binbin    5u  IPv4 0xb67acb243ed62471      0t0  TCP *:http-alt (LISTEN)\r\ntclsh8.5 2030 binbin    6u  IPv4 0xb67acb245cae8211      0t0  TCP localhost:http-alt (LISTEN)\r\n```\r\n\r\nIn CentOS7, tcl8.5, will throw an `address already in use` error:\r\n```\r\n[root@binblog redis]# tclsh8.5\r\n% socket -server dummy_accept 8080\r\nsock5\r\n\r\n[root@binblog redis]# lsof -i:8080\r\nCOMMAND    PID USER   FD   TYPE     DEVICE SIZE/OFF NODE NAME\r\ntclsh8.5 30648 root    5u  IPv4 1353843801      0t0  TCP *:webcache (LISTEN)\r\n\r\n--------------------------------------------------------------------------------------\r\n\r\n[root@binblog redis]# tclsh8.5\r\n% socket -server dummy_accept -myaddr 127.0.0.1 8080\r\nsock5\r\n% socket -server dummy_accept 8080\r\ncouldn't open socket: address already in use\r\n\r\n[root@binblog redis]# lsof -i:8080\r\nCOMMAND    PID USER   FD   TYPE     DEVICE SIZE/OFF NODE NAME\r\ntclsh8.5 30726 root    5u  IPv4 1353844175      0t0  TCP VM-0-10-centos:webcache (LISTEN)\r\n```\r\n\r\nIn tests/assets/default.conf there's a default bind address for all tests: \"127.0.0.1\".\r\nOn MacOs it seems that it will bind to \"*\" then it doesn't conflict with binding to \"127.0.0.1\".\r\n\r\nsee https://github.com/redis/redis/pull/9748#pullrequestreview-820252460 for more details",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 129,
        "deletions": 35,
        "changed_files": 32,
        "created_at": "2021-12-01T17:53:30Z",
        "closed_at": "2021-12-22T12:11:17Z",
        "merged_at": "2021-12-22T12:11:17Z",
        "body": "## background\r\nTill now CONFIG SET was blocked during loading.\r\n(In the not so distant past, GET was disallowed too)\r\n\r\nWe recently (not released yet) added an async-loading mode, see #9323, and during that time it'll serve CONFIG SET and any other command.\r\nAnd now we realized (#9770) that some configs, and commands are dangerous during async-loading.\r\n\r\n## changes\r\n* Allow most CONFIG SET during loading (both on async-loading and normal loading)\r\n* Allow CONFIG REWRITE and CONFIG RESETSTAT during loading\r\n* Block a few config during loading (`appendonly`, `repl-diskless-load`, and `dir`)\r\n* Block a few commands during loading (list below)\r\n\r\n## the blocked commands:\r\n* SAVE - obviously we don't wanna start a foregreound save during loading 8-)\r\n* BGSAVE - we don't mind to schedule one, but we don't wanna fork now\r\n* BGREWRITEAOF - we don't mind to schedule one, but we don't wanna fork now\r\n* MODULE - we obviously don't wanna unload a module during replication / rdb loading (MODULE HELP and MODULE LIST are not blocked)\r\n* SYNC / PSYNC - we're in the middle of RDB loading from master, must not allow sync requests now.\r\n* REPLICAOF / SLAVEOF - we're in the middle of replicating, maybe it makes sense to let the user abort it, but he couldn't do that so far, i don't wanna take any risk of bugs due to odd state.\r\n* CLUSTER - only allow [HELP, SLOTS, NODES, INFO, MYID, LINKS, KEYSLOT, COUNTKEYSINSLOT, GETKEYSINSLOT, RESET, REPLICAS, COUNT_FAILURE_REPORTS], for others, preserve the status quo\r\n\r\n## other fixes\r\n* processEventsWhileBlocked had an issue when being nested, this could happen with a busy script during async loading (new), but also in a busy script during AOF loading (old). this lead to a crash in the scenario described in #6988\r\n\r\n## todo\r\n- [x] convert the hard coded list of commands to block to command flags\r\n- [x] remove SENTINEL (it's not allowed in loading anyway)\r\n- [x] make sure to add the flag to both SLAVEOF and REPLICAOF\r\n- [x] block most CLUSTER sub-commands, only allow the ones listed above.\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-12-01T10:15:23Z",
        "closed_at": "2021-12-02T08:41:50Z",
        "merged_at": "2021-12-02T08:41:50Z",
        "body": "When we use monitor in redis-cli but encounter an error reply,\r\nwe will get stuck until we press Ctrl-C to quit.\r\n\r\nThis is a harmless bug. It might be useful if we add parameters\r\nto monitor in the future, suck as monitoring only selected db.\r\n\r\nbefore:\r\n```\r\n127.0.0.1:6379> monitor wrong\r\n(error) ERR wrong number of arguments for 'monitor' command or subcommand\r\n^C(9.98s)\r\n127.0.0.1:6379>\r\n```\r\n\r\nafter:\r\n```\r\n127.0.0.1:6379> monitor wrong\r\n(error) ERR wrong number of arguments for 'monitor' command or subcommand\r\n127.0.0.1:6379>\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 688,
        "deletions": 96,
        "changed_files": 18,
        "created_at": "2021-11-30T15:47:22Z",
        "closed_at": "2022-01-02T07:50:15Z",
        "merged_at": "2022-01-02T07:50:15Z",
        "body": "To avoid data loss, this commit adds a grace period for lagging replicas to\r\ncatch up the replication offset.\r\n\r\nDone:\r\n\r\n* Wait for replicas when shutdown is triggered by SIGTERM and SIGINT.\r\n\r\n* Wait for replicas when shutdown is triggered by the SHUTDOWN command. A new\r\n  blocked client type BLOCKED_SHUTDOWN is introduced, allowing multiple clients\r\n  to call SHUTDOWN in parallel.\r\n  Note that they don't expect a response unless an error happens and shutdown is aborted.\r\n\r\n* Log warning for each replica lagging behind when finishing shutdown.\r\n\r\n* CLIENT_PAUSE_WRITE while waiting for replicas.\r\n\r\n* Configurable grace period 'shutdown-timeout' in seconds (default 10).\r\n\r\n* New flags for the SHUTDOWN command:\r\n\r\n    - NOW disables the grace period for lagging replicas.\r\n\r\n    - FORCE ignores errors writing the RDB or AOF files which would normally\r\n      prevent a shutdown.\r\n\r\n    - ABORT cancels ongoing shutdown. Can't be combined with other flags.\r\n\r\n* New field in the output of the INFO command: 'shutdown_in_milliseconds'. The\r\n  value is the remaining maximum time to wait for lagging replicas before\r\n  finishing the shutdown. This field is present in the Server section **only**\r\n  during shutdown.\r\n\r\nNot directly related:\r\n\r\n* When shutting down, if there is an AOF saving child, it is killed **even** if AOF\r\n  is disabled. This can happen if BGREWRITEAOF is used when AOF is off.\r\n\r\n* Client pause now has end time and type (WRITE or ALL) per purpose. The\r\n  different pause purposes are *CLIENT PAUSE command*, *failover* and\r\n  *shutdown*. If clients are unpaused for one purpose, it doesn't affect client\r\n  pause for other purposes. For example, the CLIENT UNPAUSE command doesn't\r\n  affect client pause initiated by the failover or shutdown procedures. A completed\r\n  failover or a failed shutdown doesn't unpause clients paused by the CLIENT\r\n  PAUSE command.\r\n\r\nNotes:\r\n\r\n* DEBUG RESTART doesn't wait for replicas.\r\n\r\n* We already have a warning logged when a replica disconnects. This means that\r\n  if any replica connection is lost during the shutdown, it is either logged as\r\n  disconnected or as lagging at the time of exit.\r\n\r\nFixes #9693",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-30T08:14:42Z",
        "closed_at": "2021-12-08T01:41:11Z",
        "merged_at": "2021-12-08T01:41:11Z",
        "body": "Change the order of these statements form\r\n```\r\n        $rd SET FOO BAR\r\n        r client PAUSE 100000000 WRITE\r\n        assert_equal [$rd read] \"QUEUED\"\r\n```\r\nto\r\n``` \r\n        $rd SET FOO BAR\r\n        assert_equal [$rd read] \"QUEUED\"\r\n        r client PAUSE 100000000 WRITE\r\n```\r\nThis ensures `r client PAUSE 100000000 WRITE` is  executed after `$rd SET FOO BAR`.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-11-30T05:47:14Z",
        "closed_at": "2021-12-01T12:48:52Z",
        "merged_at": null,
        "body": "Currently, `current_db` and `resize_db` are both `unsigned int`. When updating in each loop, the value for them may overflow `unsigned int`. We need to reset `current_db` and `resize_db` when needed.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-30T03:20:49Z",
        "closed_at": "2021-11-30T12:10:49Z",
        "merged_at": "2021-11-30T12:10:49Z",
        "body": "now rdbSaveInfo used in both way, so i think we should delete previous notes, in case of misleading",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-11-29T21:23:13Z",
        "closed_at": "2022-01-26T15:43:52Z",
        "merged_at": null,
        "body": "It is benefit to add sentinel and cluster test in CI part, we could avoid some running error in Daily schedule. \r\nThere is no too much CI time added.\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/144093516-2f21016a-c4d3-477f-8248-2b4f8c59e6ce.png)\r\n\r\n\r\nFor sentinel part, it takes 3 mins to test.\r\nFor cluster part, it takes less than 9 mins.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-11-29T12:58:24Z",
        "closed_at": "2021-12-08T01:54:59Z",
        "merged_at": null,
        "body": "rdbSaveLzfStringObject function only called by rdbSaveRawString.in rdbSaveRawString has the condition len > 20, so Judgment length < 4 is no need;\r\n![image](https://user-images.githubusercontent.com/60819151/143872234-ed3e2144-4261-4d53-84c1-fc571a618570.png)\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-11-29T10:48:11Z",
        "closed_at": "2021-11-29T14:09:20Z",
        "merged_at": "2021-11-29T14:09:20Z",
        "body": "caused a build warning in linenoise since glibc 2.20:\r\n```\r\nMAKE linenoise\r\ncd linenoise && make\r\nmake[3]: Entering directory '/home/yoav/redis/deps/linenoise'\r\ncc  -Wall -Os -g  -c linenoise.c\r\nIn file included from /usr/include/termios.h:25,\r\n                 from linenoise.c:107:\r\n/usr/include/features.h:187:3: warning: #warning \"_BSD_SOURCE and _SVID_SOURCE are deprecated, use _DEFAULT_SOURCE\" [-Wcpp]\r\n  187 | # warning \"_BSD_SOURCE and _SVID_SOURCE are deprecated, use _DEFAULT_SOURCE\"\r\n      |   ^~~~~~~\r\nmake[3]: Leaving directory '/home/yoav/redis/deps/linenoise'\r\nMAKE lua\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-29T01:48:28Z",
        "closed_at": "2021-12-03T04:09:55Z",
        "merged_at": null,
        "body": "in dict struct , rehashidx's type is long, i think we should keep the idx type same  with it,  because their range is same but -1.and table type i range 0 to 1 so it's type no need to be uint64_t.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 9,
        "changed_files": 10,
        "created_at": "2021-11-28T15:56:36Z",
        "closed_at": "2022-01-04T14:19:28Z",
        "merged_at": "2022-01-04T14:19:28Z",
        "body": "This setup dependabot to check weekly updates for pip and github-actions dependencies. If it finds an update it will create a PR to update the dependency. More information can be found [here](https://help.github.com/github/administering-a-repository/configuration-options-for-dependency-updates)\r\n\r\nIt includes the update of : \r\n* vmactions/freebsd-vm from 0.1.4 to 0.1.5\r\n* codespell from 2.0.0 to 2.1.0\r\n\r\nIt fixes the spelling errors found by the latest version of codespell. I created a dedicated folder for codespell folder so dependabot can read a requirements.txt file and every files dedicated to codespell can be grouped in the same place ",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-11-28T11:49:36Z",
        "closed_at": "2021-11-28T13:37:36Z",
        "merged_at": "2021-11-28T13:37:35Z",
        "body": "In order to test the situation where multiple clients are\r\nblocked, we set up multiple clients to execute some blocking\r\ncommands. These tests depend on the order of command processing.\r\n\r\nThose tests are based on the wrong assumption that the command\r\nsend first will be executed by the server first, which is obviously\r\nwrong in some network delyas.\r\n\r\nThis commit ensures orderly execution of commands by waiting\r\nand judging the number of blocked clients each time.\r\n\r\nFix #9850",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-11-27T12:15:32Z",
        "closed_at": "2022-07-13T03:32:15Z",
        "merged_at": "2022-07-13T03:32:15Z",
        "body": "Validating inputs ahead of time, to give the end user a slightly more useful error.\r\n\r\nbefore:\r\n```\r\n[root@binblog redis]# src/redis-benchmark -p 888888\r\nCould not connect to Redis at 127.0.0.1:888888: Connection refused\r\nWARNING: Could not fetch server CONFIG\r\n^C\r\n[root@binblog redis]# src/redis-cli -p 688888\r\nCould not connect to Redis at 127.0.0.1:688888: Connection refused\r\nnot connected>\r\n\r\n[root@binblog redis]# src/redis-benchmark -u redis://127.0.0.1:80000/10\r\nCould not connect to Redis at 127.0.0.1:80000: Connection refused\r\nWARNING: Could not fetch server CONFIG\r\n^C\r\n[root@binblog redis]# src/redis-cli -u redis://127.0.0.1:80000/10\r\nCould not connect to Redis at 127.0.0.1:80000: Connection refused\r\nnot connected>\r\n```\r\n\r\nafter:\r\n```\r\n[root@binblog redis]# src/redis-benchmark -p -100\r\nInvalid server port.\r\n[root@binblog redis]# src/redis-cli -p 688888\r\nInvalid server port.\r\n\r\n[root@binblog redis]# src/redis-cli -u redis://127.0.0.1:80000/10\r\nInvalid server port.\r\n[root@binblog redis]# src/redis-benchmark -u redis://127.0.0.1:80000/10\r\nInvalid server port.\r\n```\r\n\r\nsee https://github.com/redis/redis/pull/5545",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-11-27T11:29:29Z",
        "closed_at": "2021-11-29T21:35:36Z",
        "merged_at": "2021-11-29T21:35:36Z",
        "body": "CLIENT KILL with ID argument should only kill the client\r\nwith the provided ID. In old code, CLIENT KILL with id 0\r\nwill kill all the connected clients.\r\n\r\nThis commit adds an input check to avoid this situation.\r\nAdds some tests to ensure that the behavior is as expected.\r\n\r\nRefresh #3235",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 28,
        "changed_files": 4,
        "created_at": "2021-11-27T10:11:29Z",
        "closed_at": "2021-11-28T07:02:39Z",
        "merged_at": "2021-11-28T07:02:39Z",
        "body": "Remove `lcsGetKeys` to clean up the remaining `Stralgo` after #9733.\r\n\r\nBefore:\r\n```sh\r\n$ command getkeys lcs keys a b\r\n1) \"a\"\r\n2) \"b\"\r\n\r\n$ command getkeys lcs a b\r\n(error) ERR Invalid arguments specified for command\r\n```\r\n\r\nAfter:\r\n```\r\n$ command getkeys lcs keys a b\r\n1) \"keys\"\r\n2) \"a\"\r\n\r\n$ command getkeys lcs a b\r\n1) \"a\"\r\n2) \"b\"\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 33,
        "changed_files": 4,
        "created_at": "2021-11-27T07:59:04Z",
        "closed_at": "2021-11-27T22:47:51Z",
        "merged_at": "2021-11-27T22:47:51Z",
        "body": "`sentinelAddFlagsToDictOfRedisInstances` and `sentinelDelFlagsToDictOfRedisInstances` were never used.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 398,
        "deletions": 284,
        "changed_files": 4,
        "created_at": "2021-11-26T02:54:43Z",
        "closed_at": "2021-11-29T05:57:01Z",
        "merged_at": "2021-11-29T05:57:01Z",
        "body": "This pr is following #9779 .\r\n\r\n## Describe of feature\r\nNow when we turn on the `list-compress-depth` configuration, the list will compress the ziplist between `[list-compress-depth, -list-compress-depth]`.\r\nWhen we need to use the compressed data, we will first decompress it, then use it, and finally compress it again.\r\nIt's controlled by `quicklistNode->recompress`, which is designed to avoid the need to re-traverse the entire quicklist for compression after each decompression, we only need to recompress the quicklsitNode being used.\r\nIn order to ensure the correctness of recompressing, we should normally let quicklistDecompressNodeForUse and quicklistCompress appear in pairs, otherwise, it may lead to the head and tail being compressed or the middle ziplist not being compressed correctly, which is exactly the problem this pr needs to solve.\r\n\r\n## Solution\r\n1. Reset `quicklistIter` after insert and replace.\r\n    The quicklist node will be compressed in `quicklistInsertAfter`, `quicklistInsertBefore`, `quicklistReplaceAtIndex`, so we can safely reset the quicklistIter to avoid it being used again\r\n2. `quicklistIndex` will return an iterator that can be used to recompress the current node after use.\r\n    \r\n## Test\r\n1. In the `Stress Tester for #3343-Similar Errors` test, when the server crashes or when `valgrind` or `asan` error is detected, print violating commands.\r\n2. Add a crash test due to wrongly recompressing after `lrem`.\r\n3. Remove `insert before with 0 elements` and `insert after with 0 elements`, \r\n    Now we forbid any operation on an NULL quicklistIter.\r\n# TODO\r\n- [x] benchmark\r\n- [ ] more test",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2021-11-23T16:45:35Z",
        "closed_at": "2021-12-06T12:13:06Z",
        "merged_at": null,
        "body": "optimze the code of dictFind()",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2021-11-23T11:15:09Z",
        "closed_at": "2021-11-28T09:33:09Z",
        "merged_at": "2021-11-28T09:33:09Z",
        "body": "`msgpack` lib missed using `lua_checkstack` and so on rare cases overflow the stack by at most 2 elements. This is a violation of the Lua C API. Notice that Lua allocates additional 5 more elements on top of `lua->stack_last` so Redis does not access an invalid memory. But it is an API violation and we should avoid it.\r\n\r\nThis PR also added a new Lua compilation option. The new option can be enable using environment variable called `LUA_DEBUG`. If set to `yes` (by default `no`), Lua will be compiled without optimisations and with debug symbols (`-O0 -g`). Moreover, in this new mode, Lua will be compiled with the `-DLUA_USE_APICHECK` flag that enables extended Lua C API validations.\r\n\r\nIn addition, set `LUA_DEBUG=yes` on daily sanitizer UB flow so we will be able to catch Lua C API violations in the future.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-11-23T10:36:09Z",
        "closed_at": "2021-11-23T12:56:53Z",
        "merged_at": "2021-11-23T12:56:53Z",
        "body": "If the last bytes in ziplist are corrupt and we decode from tail to head, we may reach slightly outside the ziplist.\r\n\r\nFound by ASAN on daily CI.\r\nhttps://github.com/redis/redis/runs/4293567869?check_suite_focus=true\r\n\r\n```\r\n==23961==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x6030000603ba at pc 0x0000005659a5 bp 0x7ffc328755f0 sp 0x7ffc328755e8\r\nREAD of size 1 at 0x6030000603ba thread T0\r\n    #0 0x5659a4 in ziplistIndex /home/runner/work/redis/redis/src/ziplist.c:1163:13\r\n    #1 0x4fb3a0 in quicklistPopCustom /home/runner/work/redis/redis/src/quicklist.c:1590:9\r\n    #2 0x5ff4e6 in listTypePop /home/runner/work/redis/redis/src/t_list.c:66:13\r\n\r\n0x6030000603ba is located 1 bytes to the right of 25-byte region [0x6030000603a0,0x6030000603b9)\r\nallocated by thread T0 here:\r\n    #0 0x4bfa1d in malloc (/home/runner/work/redis/redis/src/redis-server+0x4bfa1d)\r\n    #1 0x54bd26 in ztrymalloc_usable /home/runner/work/redis/redis/src/zmalloc.c:108:17\r\n    #2 0x54bd26 in ztrymalloc /home/runner/work/redis/redis/src/zmalloc.c:133:17\r\n    #3 0x5da6a9 in rdbGenericLoadStringObject /home/runner/work/redis/redis/src/rdb.c:534:29\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 587,
        "deletions": 544,
        "changed_files": 9,
        "created_at": "2021-11-23T05:23:05Z",
        "closed_at": "2021-11-26T02:55:14Z",
        "merged_at": null,
        "body": "This pr is following #9779 .\r\n\r\n## Describe of feature\r\nNow when we turn on the `list-compress-depth` configuration, the list will compress the ziplist between `[list-compress-depth, -list-compress-depth]`.\r\nWhen we need to use the compressed data, we will first decompress it, then use it, and finally compress it again.\r\nIt's controlled by `quicklistNode->recompress`, which is designed to avoid the need to re-traverse the entire quicklist for compression after each decompression, we only need to recompress the quicklsitNode being used.\r\nIn order to ensure the correctness of recompressing, we should normally let quicklistDecompressNodeForUse and quicklistCompress appear in pairs, otherwise, it may lead to the head and tail being compressed or the middle ziplist not being compressed correctly, which is exactly the problem this pr needs to solve.\r\n\r\n## Solution\r\n1. Reset `quicklistIter` after insert and replace.\r\n    The compression will be updated correctly in quicklistInsertAfter,\r\n    quicklistInsertBefore, quicklistReplaceAtIndex, so we can safely reset the quicklistIter to avoid it being used again\r\n2. `quicklistIndex` will return an iterator that can be used to recompress the current node after use.\r\n    \r\n## Test\r\n1. In the `Stress Tester for #3343-Similar Errors` test, when the server crashes or when `valgrind` or `asan` error is detected, print violating commands.\r\n2. Add a crash test due to wrongly recompressing after `lrem`.\r\n3. Remove `insert before with 0 elements` and `insert after with 0 elements`, \r\n    Now we forbid any operation on an NULL quicklistIter.\r\n# TODO\r\n- [ ] benchmark\r\n- [ ] more test",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-11-22T21:02:54Z",
        "closed_at": "2021-11-25T13:23:16Z",
        "merged_at": "2021-11-25T13:23:16Z",
        "body": "Currently, the watching clients are marked as dirty when a watched key is touched, but we continue watching the keys for no reason. Then, when the same key is touched again, we iterate again on the watching clients list and mark all clients as dirty again. Only when the exec/unwatch command is issued will the client be removed from the key->watching_clients list. The same applies when a dirty client calls the WATCH command. The key will be added to be watched by the client even if it has no effect.\r\n\r\nIn the field, no performance degradation was observed as a result of the current implementation;  it is merely a cleanup with possible memory and performance gains in some situations.\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2021-11-22T16:05:32Z",
        "closed_at": "2021-11-29T08:30:35Z",
        "merged_at": "2021-11-29T08:30:35Z",
        "body": "Update CI so that warnings cause build failures.\r\n\r\nAlso fix a warning in `test-sanitizer-address`:\r\n```\r\nIn function \u2018strncpy\u2019,\r\n   inlined from \u2018clusterUpdateMyselfIp\u2019 at cluster.c:545:13:\r\n\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:10:\r\nerror: \u2018__builtin_strncpy\u2019 specified bound 46 equals destination size [-Werror=stringop-truncation]\r\n\r\n  106 |   return __builtin___strncpy_chk (__dest, __src, __len, __bos (__dest));\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncc1: all warnings being treated as errors\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/22811481/142894958-345a8e98-290b-4392-ae3e-d89669329e77.png)\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-11-22T12:22:01Z",
        "closed_at": "2021-12-16T05:38:45Z",
        "merged_at": "2021-12-16T05:38:45Z",
        "body": "This is in order to fix the issue originally started by this PR: \r\nhttps://github.com/redis/redis/pull/5219",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-11-22T08:34:05Z",
        "closed_at": "2021-11-22T10:30:07Z",
        "merged_at": "2021-11-22T10:30:07Z",
        "body": "TCL8.5 can't handle cases where part of the string is escaped and part of it isn't,\r\nif there's a single char that needs escaping, we need to escape the whole string.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 155,
        "deletions": 17,
        "changed_files": 8,
        "created_at": "2021-11-22T06:47:47Z",
        "closed_at": "2022-02-22T09:19:39Z",
        "merged_at": "2022-02-22T09:19:39Z",
        "body": "Current implementation simple idle client which serves no traffic still\r\nuse ~17Kb of memory. this is mainly due to a fixed size reply buffer\r\ncurrently set to 16kb here: https://github.com/redis/redis/blob/unstable/src/server.h#L157\r\n\r\nWe have encountered some cases in which the server operates in a low memory environments.\r\nIn such cases a user who wishes to create large connection pools to support potential burst period, \r\nwill exhaust a large amount of memory  to maintain connected Idle clients.\r\nSome users may choose to \"sacrifice\" performance in order to save memory.\r\n\r\nThis commit introduce a dynamic mechanism to shrink and expend the client reply buffer based on periodic observed peak.\r\nthe algorithm works as follows:\r\n1. each time a client reply buffer has been fully written, the last recorded peak is updated: \r\nnew peak = MAX( last peak, current written size)\r\n2. during clients cron we check for each client if the last observed peak was:\r\n     a. matching the current buffer size - in which case we expend (resize) the buffer size by 100%\r\n     b. less than half the buffer size - in which case we shrink the buffer size by 50%\r\n3. In any case we will **not** resize the buffer in case:\r\n    a. the current buffer peak is less then the current buffer usable size and higher than 1/2 the current buffer usable size\r\n    b. the value of (current buffer usable size/2) is less than 1Kib\r\n    c. the value of  (current buffer usable size*2) is larger than 16Kib\r\n4. the peak value is reset to the current buffer position once every **5** seconds. we maintain a new field in the client structure (buf_peak_last_reset_time) which is used to keep track of how long it passed since the last buffer peak reset.\r\n\r\n### **Interface changes:**\r\n**CIENT LIST** - now contains 2 new extra fields:\r\nrbs= < the current size in bytes of the client reply buffer >\r\nrbp=< the current value in bytes of the last observed buffer peak position >\r\n\r\n**INFO STATS** - now contains 2 new statistics:\r\nreply_buffer_shrinks = < total number of buffer shrinks performed >\r\nreply_buffer_expends = < total number of buffer expends performed >\r\n\r\n### **Results:**\r\nThe main concern in this case was the potential performance degradation as a result of buffer dereferencing.\r\nIn order to verify the change we have performed multiple benchmarking tests.\r\nall tests performed on:\r\n\r\nm5.2xlarge instance\r\n3 io-threads (main+2 threads)\r\n468750 keys with 4K values\r\n15 c5n.2xlarge client machines running single threaded redis-benchmark.\r\neach redis benchmark running 50 clients with only get requests and run for 900 seconds \r\n\r\ncomparing the  results of the static vs dynamic case:\r\n<html xmlns:o=\"urn:schemas-microsoft-com:office:office\"\r\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\r\nxmlns=\"http://www.w3.org/TR/REC-html40\">\r\n\r\n<head>\r\n\r\n<meta name=ProgId content=Excel.Sheet>\r\n<meta name=Generator content=\"Microsoft Excel 15\">\r\n<link id=Main-File rel=Main-File\r\nhref=\"file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\r\n<link rel=File-List\r\nhref=\"file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\r\n\r\n<!--table\r\n\t{mso-displayed-decimal-separator:\"\\.\";\r\n\tmso-displayed-thousand-separator:\"\\,\";}\r\n@page\r\n\t{margin:.75in .7in .75in .7in;\r\n\tmso-header-margin:.3in;\r\n\tmso-footer-margin:.3in;}\r\ntr\r\n\t{mso-height-source:auto;}\r\ncol\r\n\t{mso-width-source:auto;}\r\nbr\r\n\t{mso-data-placement:same-cell;}\r\ntd\r\n\t{padding-top:1px;\r\n\tpadding-right:1px;\r\n\tpadding-left:1px;\r\n\tmso-ignore:padding;\r\n\tcolor:black;\r\n\tfont-size:12.0pt;\r\n\tfont-weight:400;\r\n\tfont-style:normal;\r\n\ttext-decoration:none;\r\n\tfont-family:Calibri, sans-serif;\r\n\tmso-font-charset:0;\r\n\tmso-number-format:General;\r\n\ttext-align:general;\r\n\tvertical-align:bottom;\r\n\tborder:none;\r\n\tmso-background-source:auto;\r\n\tmso-pattern:auto;\r\n\tmso-protection:locked visible;\r\n\twhite-space:nowrap;\r\n\tmso-rotate:0;}\r\n.xl65\r\n\t{font-weight:700;}\r\n-->\r\n\r\n</head>\r\n\r\n<body link=\"#0563C1\" vlink=\"#954F72\">\r\n\r\n\r\n\r\n\u00a0 | DYNAMIC | STATIC\r\n-- | -- | --\r\nTPS | 164353 | 164667\r\nP90 Latency(ms) | 6.1 | 6.1\r\n\r\n\r\n\r\n</body>\r\n\r\n</html>\r\n \r\nAlso looking into **cache-misses** stats using perf verified the TLB and L1 cache misses was increased ~1%.\r\n\r\n`sudo perf stat -a -g --pid 'pidof redis-servert' -e cache-misses -- sleep 60`\r\n\r\nStatic Buf:\r\n> 3,313,413,581      cache-misses\r\n\r\nDynamic Buffer:\r\n> 3,351,794,484      cache-misses\r\n\r\n![cache-misses-fg](https://user-images.githubusercontent.com/88133677/147092458-3459be1b-1a6b-44da-b3e1-3f551a8dcc0d.PNG)\r\n\r\n",
        "comments": 39
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-11-21T19:15:52Z",
        "closed_at": "2021-11-22T13:30:00Z",
        "merged_at": "2021-11-22T13:30:00Z",
        "body": "Issue found by corrupt-dump-fuzzer test with ASAN.\r\nThe problem was that lpSkip and lpGetWithSize could read the next listpack entry without validating that it's in range.\r\nSimilarly even the memcmp in lpFind could do that and possibly crash on segfault and now they'll crash on assert first.\r\n\r\nThe naive fix of using lpAssertValidEntry every time, resulted in 30% degradation in the lpFind benchmark of the unit test.\r\nThe final fix with the condition at the bottom has no performance implications.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-21T11:42:02Z",
        "closed_at": "2021-11-21T16:27:55Z",
        "merged_at": "2021-11-21T16:27:55Z",
        "body": "Don't use accurate option with ASAN unit tests to prevent extreme slow down.\r\n\r\nStress tests and benchmarks in ziplist.c and listpack.c with \"accurate\" flag causes extreme slow down in the address sanitizer. \r\n\r\nFound here : https://github.com/redis/redis/runs/4232082551?check_suite_focus=true\r\nIntroduced by : https://github.com/redis/redis/pull/9792\r\n\r\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-11-21T10:07:49Z",
        "closed_at": "2021-11-21T12:30:21Z",
        "merged_at": "2021-11-21T12:30:21Z",
        "body": "LCS can allocate immense amount of memory (sizes of two inputs multiplied by each other).\r\nIn the past this caused some possible security issues due to overflows, which we solved\r\nand also added use of `trymalloc` to return \"Insufficient memory\" instead of OOM panic zmalloc.\r\n\r\nBut in case overcommit is enabled, it could be that we won't get the OOM panic, and zmalloc\r\nwill succeed, and then we can get OOM killed by the kernel.\r\n\r\nThe solution here is to prevent LCS from allocating transient memory that's bigger than\r\n`proto-max-bulk-len` config.\r\nThis config is not directly related to transient memory, but using a hard coded value ad well as\r\nintroducing a specific config seems wrong.\r\n\r\nThis comes to solve an error in the corrupt-dump-fuzzer test that started in the daily CI see #9799",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2021-11-21T09:59:35Z",
        "closed_at": "2021-11-21T16:47:11Z",
        "merged_at": "2021-11-21T16:47:11Z",
        "body": "Leak found by the corrupt-dump-fuzzer when using GCC ASAN, which seems\r\nto falsely report leaks on pointers kept only on the stack when calling exit.\r\nInstead we now use _exit on panic / assert to skip these leak checks.\r\n\r\nAdditionally, check for sanitizer warnings in the corrupt-dump-fuzzer between iterations,\r\nso that when something is found we know which test to relate it too (and it prints reproduction command list)\r\n\r\nFound here:\r\nhttps://github.com/redis/redis/runs/4245339451?check_suite_focus=true\r\nand then reproduced by @sundb\r\n\r\n@tezc FYI",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-11-19T23:15:40Z",
        "closed_at": "2021-11-20T03:56:45Z",
        "merged_at": "2021-11-20T03:56:45Z",
        "body": "We started depending on a value that wasn't properly initialized in cluster init, so adding that. This just causes a Valgrind warning but should other wise be correct since static initialization would have set it to 0 (NULL) anyways.\r\n\r\nhttps://github.com/redis/redis/actions/runs/1483081713\r\nhttps://github.com/redis/redis/actions/runs/1483085335",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-19T20:59:25Z",
        "closed_at": "2021-11-19T22:48:08Z",
        "merged_at": "2021-11-19T22:48:08Z",
        "body": "Auto memory management is not enabled, so we need to manually free the call reply. See details in #9800 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 201,
        "deletions": 577,
        "changed_files": 24,
        "created_at": "2021-11-19T10:20:21Z",
        "closed_at": "2021-12-21T06:32:43Z",
        "merged_at": "2021-12-21T06:32:42Z",
        "body": "# Background\r\n\r\nThe main goal of this PR is to remove relevant logics on Lua script verbatim replication, only keeping effects replication logic, which has been set as default since Redis 5.0. As a result, Lua in Redis 7.0 would be acting the same as Redis 6.0 with default configuration from users' point of view.\r\n\r\nTo get you back on track if you are not keeping eyes on it, please refer to these issues/PRs: [PR#4966](https://github.com/redis/redis/pull/4966), [Issue#5292](https://github.com/redis/redis/issues/5292), [Issue#8370](https://github.com/redis/redis/issues/8370) .\r\n\r\nThere are lots of reasons to remove verbatim replication. Antirez has listed some of the benefits in [Issue#5292](https://github.com/redis/redis/issues/5292):\r\n\r\n>1. No longer need to explain to users side effects into scripts. They can do whatever they want.\r\n>2. No need for a cache about scripts that we sent or not to the slaves.\r\n>3. No need to sort the output of certain commands inside scripts (SMEMBERS and others): this both simplifies and gains speed.\r\n>4. No need to store scripts inside the RDB file in order to startup correctly.\r\n>5. No problems about evicting keys during the script execution.\r\n\r\nWhen looking back at Redis 5.0, antirez and core team decided to set the config `lua-replicate-commands yes` by default instead of removing verbatim replication directly, in case some bad situations happened. 3 years later now before Redis 7.0, it's time to remove it formally.\r\n\r\n# Changes\r\n\r\n- configuration for lua-replicate-commands removed\r\n  - created config file stub for backward compatibility\r\n- Replication script cache removed\r\n  - this is useless under script effects replication\r\n  - relevant statistics also removed\r\n- script persistence in RDB files is also removed\r\n- Propagation of SCRIPT LOAD and SCRIPT FLUSH to replica / AOF removed\r\n- Deterministic execution logic in scripts removed (i.e. don't run write commands after random ones, and sorting output of commands with random order)\r\n  - the flags indicating which commands have non-deterministic results are kept as hints to clients.\r\n- `redis.replicate_commands()` & `redis.set_repl()` changed\r\n  - now `redis.replicate_commands()` does nothing and return an 1\r\n  - ...and then `redis.set_repl()` can be issued before `redis.replicate_commands()` now\r\n- Relevant TCL cases adjusted\r\n- DEBUG lua-always-replicate-commands removed\r\n\r\n# Other changes\r\n- Fix a recent bug comparing CLIENT_ID_AOF to original_client->flags instead of id. (introduced in #9780)\r\n\r\n# Thoughts & Discussions\r\n\r\n- Now verbatim replication is removed, is there any necessary to keep scripts persisting?\r\n  - Case 1: Persisting.\r\n    - Redis has no SLA on persisting scripts. So users have to handle `NOSCRIPT` error. \r\n  - Case 2: Not Persisting.\r\n    - Users have to handle `NOSCRIPT` error too.\r\n  - Users may benefit if redis persists their scripts when something like failover or else happens, but I think this reason is not strong enough to convince me keeping script persisting logics.\r\n  - Conclusion: I think NO\r\n- More?",
        "comments": 30
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-11-18T21:23:36Z",
        "closed_at": "2021-11-28T09:59:39Z",
        "merged_at": "2021-11-28T09:59:39Z",
        "body": "This commit 0f8b634cd (CVE-2021-32626 released in 6.2.6, 6.0.16, 5.0.14) fixes an invalid memory write issue by using `lua_checkstack` API to make sure the Lua stack is not overflow. This fix was added on 3 places:\r\n1. `luaReplyToRedisReply`\r\n2. `ldbRedis`\r\n3. `redisProtocolToLuaType`\r\n\r\nOn the first 2 functions, `lua_checkstack` is handled gracefully while the last is handled with an assert and a statement that this situation can not happened (only with misbehave module):\r\n> the Redis reply might be deep enough to explode the LUA stack (notice that currently there is no such command in Redis that returns such a nested reply, but modules might do it)\r\n\r\nThe issue that was discovered is that user arguments is also considered part of the stack, and so the following script (for example) make the assertion reachable:\r\n```\r\nlocal a = {}\r\nfor i=1,7999 do\r\n    a[i] = 1\r\nend \r\nreturn redis.call(\"lpush\", \"l\", unpack(a))\r\n```\r\nThis is a regression because such a script would have worked before and now its crashing Redis.\r\nThe solution is to clear the function arguments from the Lua stack which makes the original assumption true and the assertion unreachable.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 221,
        "deletions": 175,
        "changed_files": 5,
        "created_at": "2021-11-18T20:45:07Z",
        "closed_at": "2021-12-28T19:04:03Z",
        "merged_at": null,
        "body": "redisProtocolToLuaType was changed to assert upon failing lua_checkstack\r\nas part of CVE-2021-32626 to avoid heap overflow vulnerability. The server\r\ncrash is not a good experience for users of redis server and can be painful.\r\nThis patch gracefully bails out and avoids server crash in redisProtocolToLuaType\r\nupon failing lua_checkstack.",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-11-18T16:54:14Z",
        "closed_at": "2021-11-18T21:01:57Z",
        "merged_at": "2021-11-18T21:01:57Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 13,
        "changed_files": 2,
        "created_at": "2021-11-18T16:38:23Z",
        "closed_at": "2021-11-21T13:54:15Z",
        "merged_at": "2021-11-21T13:54:15Z",
        "body": "With dynamically growing argc (#9528), it is necessary to initialize\r\nargv_len. Normally createClient() handles that, but in the case of a\r\nmodule shared_client, this needs to be done explicitly.\r\n\r\nThis also addresses an issue with rewriteClientCommandArgument() which\r\ndoesn't properly handle the case where the new element extends beyond\r\nargc but not beyond argv_len.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 97,
        "changed_files": 5,
        "created_at": "2021-11-17T14:31:40Z",
        "closed_at": "2021-11-18T08:47:49Z",
        "merged_at": "2021-11-18T08:47:49Z",
        "body": "Fix #9744\r\n\r\nDrop the STRALGO command, now LCS is a command of its own and it only works on keys (not input strings).\r\nThe motivation is that STRALGO's syntax was really messed-up...\r\n- assumes all (future) string algorithms will take similar arguments\r\n- mixes command that takes keys and one that doesn't in the same command.\r\n- make it nearly impossible to expose the right key spec in COMMAND INFO (issues cluster clients)\r\n- hard for cluster clients to determine the key names (firstkey, lastkey, etc)\r\n- hard for ACL / flags (is it a read command?)\r\n\r\nThis is a breaking change.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 25,
        "changed_files": 6,
        "created_at": "2021-11-17T13:46:47Z",
        "closed_at": "2021-11-23T08:38:26Z",
        "merged_at": "2021-11-23T08:38:26Z",
        "body": "Some people complain that QUIT is missing from help/command table.\r\nNot appearing in COMMAND command, command stats, ACL, etc.\r\nand instead, there's a hack in processCommand with a comment that looks outdated.\r\nNote that it is [documented](https://redis.io/commands/quit)\r\n\r\nAt the same time, HOST: and POST are there in the command table although these are not real commands.\r\nThey would appear in the COMMAND command, and even in commandstats.\r\n\r\nOther changes:\r\n1. Initialize the static logged_time static var in securityWarningCommand\r\n2. add `no-auth` flag to RESET so it can always be executed.\r\n\r\nFix #9793",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 53,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-11-17T13:06:07Z",
        "closed_at": "2021-11-18T02:44:38Z",
        "merged_at": null,
        "body": "On Tencent Cloud, when multiple master nodes fail simultaneously,\r\nthe cluster cannot recover within the default effective time (160 seconds)\r\non the nodes. We tested 128 shards, 63 nodes failed simultaneously.\r\nAmong 900 tests, the cluster recovered for only three times.\r\nThe main reason is that the vote is without ranking among multiple slave\r\nnodes, which case too many conflicts. Therefore, Tencent Cloud introduced\r\ninto ranking based on the master node name. If the ranking is stucked, a\r\nround of maximum voting time will be 500*(failed nodes). After this optimization,\r\n900 tests were all successful.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2021-11-16T16:45:22Z",
        "closed_at": "2021-11-16T19:00:13Z",
        "merged_at": "2021-11-16T19:00:13Z",
        "body": "Add --accurate to unit tests (new feature recently added)\r\nAdd --no-latency to valgrind run (was present only for modules)\r\nadd --no-latency to macos and freebsd runs (was not present for modules)\r\nadd --timeout to freebsd (same one we have for valgrind)",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 14498,
        "deletions": 3933,
        "changed_files": 435,
        "created_at": "2021-11-16T15:26:58Z",
        "closed_at": "2022-01-20T22:32:35Z",
        "merged_at": null,
        "body": "This is the command mentioned in https://github.com/redis/redis/issues/9764.\r\n\r\nThe following are the ways this command can be used:\r\n\r\n1) SENTINEL get\r\n![image](https://user-images.githubusercontent.com/51993843/150418237-9a995ff1-af50-41ca-9084-53feb123965b.png)\r\n\r\n2) SENTINEL get mymaster\r\n![image](https://user-images.githubusercontent.com/51993843/142014017-b579767c-1aae-4c64-b3db-0463779b7578.png)\r\n\r\n3) SENTINEL get mymaster down-after-milliseconds\r\n![image](https://user-images.githubusercontent.com/51993843/142014195-5f849a9d-98b8-4e67-8971-6b12ff938cb8.png)\r\n\r\n4) SENTINEL get down-after-milliseconds\r\n![image](https://user-images.githubusercontent.com/51993843/150418315-46eec308-43ee-4f94-ba1d-4822b9af6a98.png)\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-16T13:52:48Z",
        "closed_at": "2021-11-16T15:03:25Z",
        "merged_at": "2021-11-16T15:03:25Z",
        "body": "in `scan 0 match \"\"` case,  pat is empty sds(patlen is 0), I don't think should access the first character directly in this case(even though the first character is ' \\0 '), for the  code readability, I switch the two positions of judgment logic. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2567,
        "deletions": 643,
        "changed_files": 25,
        "created_at": "2021-11-16T13:28:30Z",
        "closed_at": "2022-01-03T17:14:14Z",
        "merged_at": "2022-01-03T17:14:14Z",
        "body": "Implement Multi-Part AOF mechanism to avoid overheads during AOFRW.\r\nIntroducing a folder with multiple AOF files tracked by a manifest file.\r\n\r\nThis PR implements the Meta-File Based Multi-Part AOF mechanism described in [9539](https://github.com/redis/redis/pull/9539#issuecomment-964737334).\r\n\r\nThe main issues with the the original AOFRW mechanism are:\r\n* buffering of commands that are processed during rewrite (consuming a lot of RAM)\r\n* freezes of the main process when the AOFRW completes to drain the remaining part of the buffer and fsync it.\r\n* double disk IO for the data that arrives during AOFRW (had to be written to both the old and new AOF files)\r\n\r\nThe main modifications of this PR:\r\n1. Remove the AOF rewrite buffer and related code.\r\n2. Divide the AOF into multiple files, they are classified as two types, one is the the `BASE` type, it represents the full amount of data (Maybe AOF or RDB format) after each AOFRW, there is only one `BASE` file at most. The second is `INCR` type, may have more than one. They represent the incremental commands since the last AOFRW.\r\n3. Use a AOF manifest file to record and manage these AOF files mentioned above.\r\n4. The original configuration of `appendfilename` will be the base part of the new file name, for example: `appendonly.aof.1.base.rdb` and `appendonly.aof.2.incr.aof`\r\n5. Add manifest-related TCL tests, and modified some existing tests that depend on the `appendfilename`\r\n6. Remove the `aof_rewrite_buffer_length` field in info.\r\n7. Add `aof-disable-auto-gc` configuration. By default we're automatically deleting HISTORY type AOFs. It also gives users the opportunity to preserve the history AOFs. just for testing use now.\r\n8. Add AOFRW limiting measure. When the AOFRW failures reaches the threshold (3 times now), we will delay the execution \r\nof the next AOFRW by 1 minute. If the next AOFRW also fails, it will be delayed by 2 minutes. The next is 4, 8, 16, the maximum delay is 60 minutes (1 hour). During the limit period, we can still use the 'bgrewriteaof' command to execute AOFRW immediately.\r\n9. Support upgrade (load) data from old version redis.\r\n10. Add `appenddirname` configuration, as the directory name of the append only files. All AOF files and manifest file will be placed in this directory.\r\n11. Only the last AOF file (BASE or INCR) can be truncated. Otherwise redis will exit even if `aof-load-truncated` is enabled.\r\n\r\nTo Do:\r\n- [x] On upgrades from old redis versions, we need to detect the old AOF file and rename it / create a meta file.\r\n- [x]  At present, if AOFRW fails, redis will automatically retry. If it  continues to fail, we may get a lot of very small INCR files. So we need an AOFRW limiting measure.",
        "comments": 50
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-16T03:29:54Z",
        "closed_at": "2021-11-16T06:21:23Z",
        "merged_at": "2021-11-16T06:21:23Z",
        "body": "The client flags is a 64 bit integer, but the temporary cached value on the stack of call() is 32 bit.\r\nluckily this doesn't lead to any bugs since the only flags used against this variables are below 32 bit.\r\n\r\nfix #9648 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5260,
        "deletions": 3259,
        "changed_files": 31,
        "created_at": "2021-11-14T18:23:13Z",
        "closed_at": "2021-12-02T19:41:59Z",
        "merged_at": "2021-12-02T19:41:58Z",
        "body": "# Redis Function\r\nThis PR added the Redis Functions capabilities that were suggested on https://github.com/redis/redis/issues/8693. The PR also introduce a big refactoring to the current Lua implementation (i.e `scripting.c`). The main purpose of the refactoring is to have better code sharing between the Lua implementation that exists today on Redis (`scripting.c`) and the new Lua engine that is introduced on this PR. The refactoring includes code movements and file name changes as well as some logic changes that need to be carefully reviewed. To make the review easier, the PR was split into multiple commits. Each commit is deeply described later on but the main concept is that some commits are just moving code around without making any logical changes, those commits are less likely to cause any issues or regressions and can be reviewed fast. Other commits, which perform code and logic changes, need to be reviewed carefully, but those commits were created after the code movements so it's pretty easy to see what was changed. To sum up, it is highly recommended to review this PR commit by commit as it will be easier to see the changes, it is also recommended to read each commit description (written below) to understand what was changed on the commit and whether or not it's just a huge code movement or a logic changes.\r\n\r\n## Terminology\r\nCurrently, the terminology in Redis is not clearly defined. Scripts refer to Lua scripts and eval also refers only to Lua. Introducing Redis Function requires redefining those terms to be able to clearly understand what is been discussed on each context.\r\n* eval - legacy Lua script implementation.\r\n* Function - new scripting implementation (currently implemented in Lua but in the future, it might be other languages like javascript).\r\n* Engine - the component that is responsible for executing functions.\r\n* Script - Function or legacy Lua (executed with `eval` or `evalsha`)\r\n\r\n## Refactoring New Structure\r\nToday, the entire scripting logic is located on `scripting.c`. This logic can be split into 3 main groups:\r\n1. Script management - responsible for storing the scripts that were sent to Redis and retrieving them when they need to be run (base on the script sha on the current implementation).\r\n2. Script invocation - invoke the script given on `eval` or `evalsha` command (this part includes finding the relevant script, preparing the arguments, ..)\r\n3. Interact back with Redis (command invocation)\r\n\r\nThose 3 groups are tightly coupled on `scripting.c`. Redis Functions also need to use those groups logics, for example,  to interact back with Redis or to execute Lua code. The refactoring attempts to split those 3 groups and define APIs so that we can reuse the code both on legacy Lua scripts and Redis Functions.\r\n\r\nIn order to do so we define the following units:\r\n1. script.c: responsible for interaction with Redis from within a script.\r\n2. script_lua.c: responsible to execute Lua code, uses `script.c` to interact with Redis from within the Lua code.\r\n3. function_lua.c: contains the Lua engine implementation, uses `script_lua.c` to execute the Lua code.\r\n4. functions.c: Contains Redis Functions implementation (`FUNCTION` command,), uses `functions_lua.c` if the function it wants to invoke needs the Lua engine.\r\n4. eval.c: the original `scripting.c` contains the Lua legacy implementation and was refactored to use `script_lua.c` to invoke the Lua code.\r\n\r\n## Commits breakdown\r\nNotice: Some small commits are omitted from this list as they are small and insignificant (for example build fixes)\r\n### First commit - code movements\r\nThis commit rename `scripting.c` -> `eval.c` and introduce the new `script_lua.c` unit. The commit moves relevant code from `eval.c` (`scripting.c`) to `script_lua.c`, the purpose of moving the code is so that later we will be able to re-use the code on the Lua engine (`function_lua.c`). The commit only moves the code without modifying even a single line, so there is a very low risk of breaking anything and it also makes it much easier to see the changes on the following commits.\r\nBecause the commit does not change the code (only moves it), it does not compile. But we do not care about it as the only purpose here is to make the review processes simpler.\r\n\r\n### Second commit - move legacy Lua variables into `eval.c`\r\nToday, all Lua-related variables are located on the server struct. The commit attempt to identify those variable and take them out from the server struct, leaving only script related variables (variables that later need to be used also by engines)\r\nThe following variable where renamed and left on the server struct:\r\n   * lua_caller \t\t\t-> script_caller\r\n   * lua_time_limit \t\t-> script_time_limit\r\n   * lua_timedout \t\t-> script_timedout\r\n   * lua_oom \t\t\t-> script_oom\r\n   * lua_disable_deny_script \t-> script_disable_deny_script\r\n   * in_eval\t\t\t-> in_script\r\n\r\nThe following variables where moved to lctx under eval.c\r\n   * lua\r\n   * lua_client\r\n   * lua_cur_script\r\n   * lua_scripts\r\n   * lua_scripts_mem\r\n   * lua_replicate_commands\r\n   * lua_write_dirty\r\n   * lua_random_dirty\r\n   * lua_multi_emitted\r\n   * lua_repl\r\n   * lua_kill\r\n   * lua_time_start\r\n   * lua_time_snapshot\r\n   \r\nThis commit is in a low risk of introducing any issues and it is just moving variables around and not changing any logic.\r\n\r\n### Third commit - introducing script unit\r\nThis commit introduces the `script.c` unit. Its purpose (as described above) is to provide an API for scripts to interact with Redis. Interaction includes mostly executing commands, but also other functionalities. The interaction is done using a `ScriptRunCtx` object that needs to be created by the user and initialized using `scriptPrepareForRun`. A detailed list of functionalities expose by the unit:\r\n1. Calling commands (including all the validation checks such as\r\n   acl, cluster, read only run, ...)\r\n2. Set Resp\r\n3. Set Replication method (AOF/REPLICATION/NONE)\r\n4. Call Redis back on long-running scripts to allow Redis to reply to clients and perform script kill\r\n\r\nThe commit introduces the new unit and uses it on eval commands to interact with Redis.\r\n\r\n### Fourth commit - Moved functionality of invoke Lua code to `script_lua.c`\r\nThis commit moves the logic of invoking the Lua code into `script_lua.c` so later it can be used also by Lua engine (`function_lua.c`). The code is located on `callFunction` function and assumes the Lua function already located on the top of the Lua stack. This commit also change `eval.c` to use the new functionality to invoke Lua code.\r\n\r\n### Fith commit - Added Redis Functions unit (`functions.c`) and Lua engine (`function_lua.c`)\r\nAdded Redis Functions unit under `functions.c`, included:\r\n1. FUNCTION command:\r\n     * FUNCTION CREATE\r\n     * FUNCTION CALL\r\n     * FUNCTION DELETE\r\n     * FUNCTION KILL\r\n     * FUNCTION INFO\r\n     * FUNCTION STATS\r\n2. Register engines\r\n\r\nIn addition, this commit introduces the first engine that uses the Redis Functions capabilities, the Lua engine (`function_lua.c`)\r\n\r\n## API Changes\r\n### `lua-time-limit` \r\nconfiguration was renamed to `script-time-limit` (keep `lua-time-limit` as alias for backward compatibility).\r\n\r\n### Error log changes\r\nWhen integrating with Redis from within a Lua script, the `Lua` term was removed from all the error messages and instead we write only `script`. For example:\r\n`Wrong number of args calling Redis command From Lua script` -> `Wrong number of args calling Redis command From script`\r\n\r\n### `info memory` changes:\r\nBefore stating all the changes made to memory stats we will try to explain the reason behind them and what we want to see on those metrics:\r\n* memory metrics should show both totals (for all scripting frameworks), as well as a breakdown per framework / vm.\r\n* The totals metrics should have \"human\" metrics while the breakdown shouldn't.\r\n* We did try to maintain backward compatibility in some way, that said we did make some repurpose to existing metrics where it looks reasonable.\r\n* We separate between memory used by the script framework (part of redis's used_memory), and memory used by the VM (not part of redis's used_memory)\r\n\r\nA full breakdown of `info memory` changes:\r\n* `used_memory_lua` and `used_memory_lua_human` was deprecated, `used_memory_vm_eval` has the same meaning as `used_memory_lua`\r\n* `used_memory_scripts` was renamed to `used_memory_scripts_eval`\r\n* `used_memory_scripts` and `used_memory_scripts_human` were repurposed and now return the total memory used by functions and eval (not including vm memory, only code cache, and structs).\r\n* `used_memory_vm_function` was added and represents the total memory used by functions vm's\r\n* `used_memory_functions` was added and represents the total memory by functions (not including vm memory, only code cache, and structs)\r\n* `used_memory_vm_total` and `used_memory_vm_total_human` was added and represents the total memory used by vm's (functions and eval combined)\r\n\r\n### `functions.caches`\r\n`functions.caches` field was added to `memory stats`, representing the memory used by engines that are not functions (this memory includes data structures like dictionaries, arrays, ...)\r\n\r\n## New API\r\n### FUNCTION CREATE\r\n\r\nUsage: FUNCTION CREATE `ENGINE` `NAME` `[REPLACE]` `[DESC <DESCRIPTION>]`  `<CODE>`\r\n\r\n* `ENGINE` - The name of the engine to use to create the script.\r\n* `NAME` - the name of the function that can be used later to call the function using `FUNCTION CALL` command.\r\n* `REPLACE` - if given, replace the given function with the existing function (if exists).\r\n* `DESCRIPTION` - optional argument describing the function and what it does\r\n* `CODE` - function code.\r\n\r\nThe command will return `OK` if created successfully or error in the following cases:\r\n* The given engine name does not exist\r\n* The function name is already taken and `REPLACE` was not used.\r\n* The given function failed on the compilation.\r\n\r\n### FCALL and FCALL_RO\r\n\r\nUsage: FCALL/FCALL_RO `NAME` `NUM_KEYS key1 key2` \u2026 ` arg1 arg2`\r\n\r\nCall and execute the function specified by `NAME`. The function will receive all arguments given after `NUM_KEYS`. The return value from the function will be returned to the user as a result.\r\n\r\n* `NAME` - Name of the function to run.\r\n* The rest is as today with EVALSHA command.\r\n\r\nThe command will return an error in the following cases:\r\n* `NAME` does not exist\r\n* The function itself returned an error.\r\n\r\nThe `FCALL_RO` is equivalent to `EVAL_RO` and allows only read-only commands to be invoked from the script.\r\n\r\n### FUNCTION DELETE\r\n\r\nUsage: FUNCTION DELETE `NAME`\r\n\r\nDelete a function identified by `NAME`. Return `OK` on success or error on one of the following:\r\n* The given function does not exist\r\n\r\n### FUNCTION INFO\r\n\r\nUsage: FUNCTION INFO `NAME` [WITHCODE]\r\n\r\nReturn information about a function by function name:\r\n* Function name\r\n* Engine name\r\n* Description \r\n* Raw code (only if WITHCODE argument is given)\r\n\r\n### FUNCTION LIST\r\n\r\nUsage: FUNCTION LIST\r\n\r\nReturn general information about all the functions:\r\n* Function name\r\n* Engine name\r\n* Description\r\n\r\n### FUNCTION STATS\r\n\r\nUsage: FUNCTION STATS\r\n\r\nReturn information about the current running function:\r\n* Function name\r\n* Command that was used to invoke the function\r\n* Duration in MS that the function is already running\r\n\r\nIf no function is currently running, this section is just a RESP nil.\r\n\r\nAdditionally, return a list of all the available engines.\r\n\r\n### FUNCTION KILL\r\n\r\nUsage: `FUNCTION KILL`\r\n\r\nKill the currently executing function. The command will fail if the function already initiated a write command.\r\n\r\n## TODO\r\n- [x] Handle short read (add tests)\r\n- [x] Improve testing\r\n\r\n## Community requests to handle on following PR's\r\n* Allow loading functions from an external file on startup - https://github.com/redis/redis/issues/9899\r\n* Define and implement Function arguments parsing (named arguments/positional arguments)\r\n* Define and implement code reuse between functions - https://github.com/redis/redis/issues/9906\r\n* Add JS engine\r\n\r\n## Community requests handled on this PR's\r\n* FUNCTION RUNNING command, to check to current running function: https://github.com/redis/redis/issues/8693#issuecomment-859957474\r\nHandled with FUCTION STATS command\r\n\r\n* Get the source code of a function: https://github.com/redis/redis/issues/8693#issuecomment-859957474\r\nHandled with FUCTION INFO command\r\n\r\n## Community requests to consider for future planning\r\n* Memory limit on functions engines.\r\n* Trigger functions on events: https://github.com/redis/redis/issues/8693#issuecomment-812481386\r\n* Blocking operations inside functions: https://github.com/redis/redis/issues/8693#issuecomment-815007678\r\n* Background function execution: https://github.com/redis/redis/issues/8693#issuecomment-859957474\r\n* Rollback on timeouts: https://github.com/redis/redis/issues/8693#issuecomment-816630862\r\n\r\n## Notes\r\n\r\nNote: Function creation/deletion is replicated to AOF  but AOFRW is not implemented sense its going to be removed: https://github.com/redis/redis/issues/9794",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2021-11-14T16:24:56Z",
        "closed_at": "2021-11-18T16:09:30Z",
        "merged_at": "2021-11-18T16:09:30Z",
        "body": "Recently we started using list-compress-depth in tests (was completely untested till now).\r\nTurns this triggered test failures with the external mode, since the tests left the setting enabled and then it was used in other tests (specifically the fuzzer named \"Stress tester for #3343-alike bugs\").\r\n\r\nThis PR fixes the issue of the `recompress` flag being left set by mistake, which caused the code to later to compress the head or tail nodes (which should never be compressed)\r\n\r\nThe solution is to reset the recompress flag when it should have been (when it was decided not to compress).\r\n\r\nAdditionally we're adding some assertions and improve the tests so in order to catch other similar bugs.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 16,
        "changed_files": 2,
        "created_at": "2021-11-14T16:12:14Z",
        "closed_at": "2021-11-21T11:35:40Z",
        "merged_at": "2021-11-21T11:35:39Z",
        "body": "Background:\r\nFollowing the upgrade to jemalloc 5.2, there was a test that used to be flaky and started failing consistently (on 32bit), so we disabled it \u200b(see #9645).\r\n\r\nThis is a test that i introduced in #7289 when i attempted to solve a rare stagnation problem, and it later turned out i failed to solve it, ans what's more i added a test that caused it to be not so rare, and as i mentioned, now in jemalloc 5.2 it became consistent on 32bit.\r\n\r\nStagnation can happen when all the slabs of the bin are equally utilized, so the decision to move an allocation from a relatively empty slab to a relatively full one, will never happen, and in that test all the slabs are at 50% utilization, so the defragger could just keep scanning the keyspace and not move anything.\r\n\r\nWhat this PR changes:\r\n* First, finally in jemalloc 5.2 we have the count of non-full slabs, so when we compare the utilization of the current slab, we can compare it to the average utilization of the non-full slabs in our bin, instead of the total average of our bin. this takes the full slabs out of the game, since they're not candidates for migration (neither source nor target).\r\n* Secondly, We add some 12% (100/8) to the decision to defrag an allocation, this is the part that aims to avoid stagnation, and it's especially important since the above mentioned change can get us closer to stagnation.\r\n* Thirdly, since jemalloc 5.2 adds sharded bins, we take into account all shards (something that's missing from the original PR that merged it), this isn't expected to make any difference since anyway there should be just one shard.\r\n\r\nHow this was benchmarked.\r\nWhat i did was run the memefficiency test unit with `--verbose` and compare the defragger hits and misses the tests reported.\r\nAt first, when i took into consideration only the non-full slabs, it got a lot worse (i got into stagnation, or just got a lot of misses and a lot of hits), but when i added the 10% i got back to results that were slightly better than the ones of the jemalloc 5.1 branch. i.e. full defragmentation was achieved with fewer hits (relocations), and fewer misses (keyspace scans).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 103,
        "deletions": 10,
        "changed_files": 10,
        "created_at": "2021-11-14T13:20:07Z",
        "closed_at": "2021-11-15T09:07:44Z",
        "merged_at": "2021-11-15T09:07:44Z",
        "body": "Two issues:\r\n1. In many tests we simply forgot to close the connections we created, which doesn't matter for normal tests where the server is killed, but creates a leak on external server tests.\r\n2. When calling `start_server` on external test we create a fresh connection instead of really starting a new server, but never clean it at the end.\r\n\r\nWhat happened in the past is that when running a complete test suite against an external server we wound up having hundreds of connections to that server. For any test that assumes a limited number of connections or iterates over the connection list this can potentially create a problem and a gap between external and normal test cases. Specifically this used to cause issues when running the \"query buffer resized correctly when not idle\" test, which, on certain platform used to fail because the `clientsCron` took too long.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 109,
        "deletions": 17,
        "changed_files": 6,
        "created_at": "2021-11-13T09:41:25Z",
        "closed_at": "2021-11-16T11:12:25Z",
        "merged_at": "2021-11-16T11:12:25Z",
        "body": "## Describe of feature\r\nRedis supports inserting data over 4GB into string (and recently for lists too, see #9357), But LZF compression used in RDB files (see `rdbcompression` config), and in quicklist (see `list-compress-depth` config) does not support compress/decompress data over UINT32_MAX, which will result in corrupting the rdb after compression.\r\n\r\n## Internal changes\r\n1. Modify the `unsigned int` parameter of `lzf_compress/lzf_decompress` to `size_t`.\r\n2. Modify the variable types in `lzf_compress` involving offsets and lengths to `size_t`.\r\n3. Set LZF_USE_OFFSETS to 0.\r\n    When LZF_USE_OFFSETS is 1, lzf store offset into `LZF_HSLOT`(32bit). \r\n    Even in 64-bit, `LZF_USE_OFFSETS` defaults to 1, because lzf assumes that it only compresses and decompresses data smaller than UINT32_MAX.\r\n    But now we need to make lzf support 64-bit, turning on `LZF_USE_OFFSETS` will make it impossible to store 64-bit\r\n    offsets or pointers.\r\n    BTW, disable LZF_USE_OFFSETS also brings a few performance improvements.\r\n\r\n## Test\r\n4. Add test for compress/decompress string large than UINT32_MAX.\r\n5. Add unittest for compress/decompress quicklistNode.\r\n\r\nImplements #9732.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-13T02:03:13Z",
        "closed_at": "2021-11-13T05:55:49Z",
        "merged_at": "2021-11-13T05:55:49Z",
        "body": "I have seen this CI failure twice on MacOS:\r\n\r\n```\r\n# https://github.com/redis/redis/runs/4195718065?check_suite_focus=true#step:5:1328\r\nsub-second expire test attempts: 30\r\n*** [err]: PEXPIRE/PSETEX/PEXPIREAT can set sub-second expires in tests/unit/expire.tcl\r\nExpected 'somevalue {} somevalue {} somevalue {}' to equal or match '{} {} {} {} somevalue {}'\r\n```\r\n\r\nI did some loop test in my own daily CI, the results show that is\r\nnot particularly stable. Change the threshold from 30 to 50.\r\n\r\nsee: https://github.com/redis/redis/pull/7791#issuecomment-950800838 for more details",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 421,
        "deletions": 32,
        "changed_files": 13,
        "created_at": "2021-11-12T23:54:03Z",
        "closed_at": "2021-12-17T05:56:59Z",
        "merged_at": "2021-12-17T05:56:59Z",
        "body": "This PR introduced memory management on cluster bus link send buffers, to address issues raised in https://github.com/redis/redis/issues/9688.\r\n\r\nThe interface changes are (need core-team consensus):\r\n * Introduced a new `cluster-link-sendbuf-limit` config that caps memory usage of cluster bus link send buffers. \r\n   * Its default is chosen to be 0 (aka infinite).\r\n * Introduced a new `CLUSTER LINKS` command that displays current TCP links to/from peers as a RESP3 map.\r\n * Added `mem_cluster_links` field under the `Memory` section of `INFO` command output, which displays the overall memory usage by all current cluster links. \r\n   * Value of `mem_cluster_links` is also deducted from `used_memory_dataset` to better isolate the memory usage of real key values.\r\n   * Memory usage of cluster links are still counted toward the `maxmemory` limit, so over consumption by cluster links can still cause evictions or `OOM` errors (no behavior change).\r\n * Added `total_cluster_links_buffer_limit_exceeded` field under the `CLUSTER INFO` command output, which displays the accumulated count of cluster links freed due to `cluster-link-sendbuf-limit`.\r\n\r\n\r\nOn an implementation level:\r\n * In `clusterNode` struct, added an `inbound_link` field pointing to the inbound link accepted from this `node`. Before this change, inbound links are not associated with the corresponding `clusterNode` and are loosely managed.",
        "comments": 35
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 16,
        "changed_files": 1,
        "created_at": "2021-11-11T11:32:48Z",
        "closed_at": "2021-11-11T12:39:20Z",
        "merged_at": "2021-11-11T12:39:20Z",
        "body": "I cannot run daily CI from my fork due to \"redis/redis\" repo check. Let's disable that check for manual triggers. ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 67,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-11-11T08:16:26Z",
        "closed_at": "2021-12-15T13:21:32Z",
        "merged_at": null,
        "body": "PR #9323 introduces some bugs, since now we allow almost all commands during async loading, but execute some commands like `config` `eval` is very dangerous, for example disable diskless-load during async loading can lead to data inconsistency(if replication succeeds) and memory leak(if replication fails) and crash(free master client after full resync), because the global config is modified, you can use the new test case in this PR to reproduce it.\r\n\r\na simple way to fix it is revert the `async_loading` check in `processCommand()`, or we need more work to improve this feature, maybe refactor the rdb load mechanism to make it independent.\r\n\r\nping @oranagra @eduardobr ",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2021-11-10T16:22:41Z",
        "closed_at": "2021-11-10T17:38:58Z",
        "merged_at": "2021-11-10T17:38:58Z",
        "body": "We saw some tests sporadically time out on valgrind (namely the ones\r\nfrom #9323).\r\n\r\nIncreasing valgrind timeout from 20 mins to 40 mins in CI.\r\nAnd fixing an outdated help message.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-10T12:59:00Z",
        "closed_at": "2021-11-11T06:33:09Z",
        "merged_at": "2021-11-11T06:33:09Z",
        "body": "This fix makes it possible to run `create-cluster` from any dir, not only from the dir of the script.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2021-11-10T08:25:45Z",
        "closed_at": "2021-11-11T11:04:02Z",
        "merged_at": "2021-11-11T11:04:02Z",
        "body": "On test failure store the external redis server logs as CI artifacts so we can review them.\r\n\r\nThis was very helpful for me to debug external redis test failures.\r\n\r\nThe PR also includes writing the test name to the external-server's log file for easier debugging.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 17,
        "changed_files": 3,
        "created_at": "2021-11-09T18:23:37Z",
        "closed_at": "2021-11-09T20:37:18Z",
        "merged_at": "2021-11-09T20:37:18Z",
        "body": "In both tests, \"diskless loading short read\" and \"diskless loading short read with module\",\r\nthe timeout of waiting for the replica to respond to a short read and log it, is too short.\r\n\r\nThe test fails to find one of the results of the replication in the replica log:\r\n```\r\n[wait_for_log_messages -1 {\"*Internal error in RDB*\" \"*Finished with success*\" \"*Successful partial resynchronization*\"} $loglines 500 10]\r\n```\r\n\r\nLog from Replica:\r\n```\r\n15757:S 08 Nov 2021 00:46:22.139 * MASTER <-> REPLICA sync: Loading DB in memory\r\n15757:S 08 Nov 2021 00:46:22.140 * Loading RDB produced by version 255.255.255\r\n15757:S 08 Nov 2021 00:46:22.140 * RDB age 0 seconds\r\n15757:S 08 Nov 2021 00:46:22.141 * RDB memory usage when created 69.97 Mb\r\n15757:S 08 Nov 2021 00:46:22.863 # Short read or OOM loading DB. Unrecoverable error, aborting now.\r\n15757:S 08 Nov 2021 00:46:22.863 # Internal error in RDB reading offset 0, function at rdb.c:2952 -> Unexpected EOF reading RDB file. Failure loading rdb format from socket, assuming connection error, resuming operation.\r\n15757:S 08 Nov 2021 00:46:22.864 # Failed trying to load the MASTER synchronization DB from socket: Success\r\n15757:S 08 Nov 2021 00:46:22.864 * Reconnecting to MASTER 127.0.0.1:21216 after failure\r\n15757:S 08 Nov 2021 00:46:22.864 * MASTER <-> REPLICA sync started\r\n15757:S 08 Nov 2021 00:46:22.865 * MASTER <-> REPLICA sync: Discarding temporary DB in background\r\n15757:S 08 Nov 2021 00:46:22.866 * Non blocking connect for SYNC fired the event.\r\n15757:S 08 Nov 2021 00:46:22.868 * Master replied to PING, replication can continue...\r\n15757:S 08 Nov 2021 00:46:22.870 * Trying a partial resynchronization (request 694d7ebd1f0f1b04a2fe50d4c072696025a4d9d5:1).\r\n15757:S 08 Nov 2021 00:46:22.879 * Full resync from master: 5361195be938e956f5d6455dee7b6935902c4aa0:0\r\n15757:S 08 Nov 2021 00:46:22.957 * MASTER <-> REPLICA sync: receiving streamed RDB from master with EOF to parser\r\n15757:S 08 Nov 2021 00:46:22.958 * MASTER <-> REPLICA sync: Loading DB in memory\r\n15757:S 08 Nov 2021 00:46:22.959 * Loading RDB produced by version 255.255.255\r\n15757:S 08 Nov 2021 00:46:22.959 * RDB age 0 seconds\r\n15757:S 08 Nov 2021 00:46:22.959 * RDB memory usage when created 69.97 Mb\r\n15757:signal-handler (1636332391) Received shutdown signal during loading, exiting now.\r\n```\r\nWe can see that at the first iteration, it took 720 [ms] from where the replica started loading DB in memory (00:46:22.139) till it fails on short read and log the failure (00:46:22.863).\r\nBut on the next iteration, we can see that the replica starts to load the DB in memory (00:46:22.958) but since we have a timeout of 1000 [ms] the test terminate the replica.\r\n\r\nAlso, add --dump-logs in runtest-moduleapi for valgrind runs.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 36,
        "changed_files": 2,
        "created_at": "2021-11-09T05:58:20Z",
        "closed_at": "2022-06-03T16:30:28Z",
        "merged_at": "2022-06-03T16:30:28Z",
        "body": "Saw this earlier https://github.com/redis/redis/pull/5198/files, remembered that I had this checked out. We have a fixed size of SHA was have to compute against, so this code can be much simpler than it currently is.\r\n\r\nAlso made the function a little bit more generic.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2021-11-08T22:07:52Z",
        "closed_at": "2021-11-18T08:53:17Z",
        "merged_at": "2021-11-18T08:53:17Z",
        "body": "As discussed here https://github.com/redis/redis/issues/3172#issuecomment-962593315\r\nCurrently PING returns different status when server is not serving data, for example when `LOADING` or `BUSY`.\r\nBut same is not true for `MASTERDOWN`\r\nThis PR makes PING reply with `MASTERDOWN` when replica-serve-stale-data=no and link is MASTER is down.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2021-11-08T19:30:34Z",
        "closed_at": "2021-11-09T09:46:10Z",
        "merged_at": "2021-11-09T09:46:10Z",
        "body": "As noted by @oranagra here: https://github.com/redis/redis/pull/9323#discussion_r744605630\r\nDuring diskless replication, the check for broken EOF mark is misplaced and should be earlier.\r\nNow we do not swap db, we do proper cleanup and correctly raise module events on this kind of failure.\r\n\r\nThis issue existed prior to #9323, but before, the side effect was not restoring backup and not raising the correct module events on this failure.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2021-11-08T11:23:35Z",
        "closed_at": "2021-11-08T14:09:33Z",
        "merged_at": "2021-11-08T14:09:33Z",
        "body": "An alternative fix for #9749:\r\n\r\n* Clean up `EINTR` handling so `EINTR` will not change connection state to begin with.\r\n* On TLS, catch `EINTR` and return it as-is before going through OpenSSL error handling (which seems to not distinguish it from `EAGAIN`).",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-11-07T14:38:34Z",
        "closed_at": "2021-11-08T12:19:35Z",
        "merged_at": null,
        "body": "Test introduced in #9629 fails in TLS mode.\r\n```\r\n*** [err]: replica can handle EINTR if use diskless load in tests/integration/replication.tcl\r\nExpected !1 (context: type eval line 28 cmd {assert ![log_file_matches [srv -1 stdout] \"*Reconnecting to MASTER*\"]            } proc ::start_server)\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 532,
        "deletions": 453,
        "changed_files": 7,
        "created_at": "2021-11-07T12:17:14Z",
        "closed_at": "2021-12-01T08:15:11Z",
        "merged_at": "2021-12-01T08:15:11Z",
        "body": "# Multiparam config set - see #8349\r\n\r\nWe can now do: `config set maxmemory 10m repl-backlog-size 5m`\r\n\r\n## Basic algorithm to support \"transaction like\" config sets:\r\n\r\n1. Backup all relevant current values (via get).\r\n2. Run \"verify\" and \"set\" on everything, if we fail run \"restore\".\r\n3. Run \"apply\" on everything (optional optimization: skip functions already run). If we fail run \"restore\".\r\n4. Return success.\r\n\r\n### restore\r\n1. Run set on everything in backup. If we fail log it and continue (this puts us in an undefined state but we decided it's better than the alternative of panicking). This indicates either a bug or some unsupported external state.\r\n2. Run apply on everything in backup (optimization: skip functions already run). If we fail log it (see comment above).\r\n3. Return error.\r\n\r\n## Implementation/design changes:\r\n* Apply function are idempotent (have no effect if they are run more than once for the same config).\r\n* No indication in set functions if we're reading the config or running from the `CONFIG SET` command (removed `update` argument).\r\n* Set function should set some config variable and assume an (optional) apply function will use that later to apply. If we know this setting can be safely applied immediately and can always be reverted and doesn't depend on any other configuration we can apply immediately from within the set function (and not store the setting anywhere). This is the case of this `dir` config, for example, which has no apply function. No apply function is need also in the case that setting the variable in the `server` struct is all that needs to be done to make the configuration take effect. Note that the original concept of `update_fn`, which received the old and new values was removed and replaced by the optional apply function.\r\n* Apply functions use settings written to the `server` struct and don't receive any inputs.\r\n* I take care that for the generic (non-special) configs if there's no change I avoid calling the setter (possible optimization: avoid calling the apply function as well).\r\n* Passing the same config parameter more than once to `config set` will fail. You can't do `config set my-setting value1 my-setting value2`.\r\n\r\nNote that getting `save` in the context of the conf file parsing to work here as before was a pain. The conf file supports an aggregate `save` definition, where each `save` line is added to the server's save params. This is unlike any other line in the config file where each line overwrites any previous configuration. Since we now support passing multiple save params in a single line (see top comments about `save` in https://github.com/redis/redis/pull/9644) we should deprecate the aggregate nature of this config line and perhaps reduce this ugly code in the future.\r\n\r\n## Todo:\r\n- [x] Missing tests.\r\n- [x] Handle all todo's in the code (optimizations). \r\nnote: I decided to leave the comments for the two potential optimizations for removing polynomial complexity O(n<sup>2</sup>) and perhaps handle them in the future. Since we assume in most cases we won't have more than 10 configs passed to `config set` and in the worst case less than 200 configs will be passed we can assume the O(n<sup>2</sup>) complexity won't cause too much of an issue (a worst case of a 40,000 loop is minimal compared to some of the operations that are performed in `config set`'s apply functions).\r\n- [x] Add variadic `config get` too? No, decide to open a separate issue for it (#9871).\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-05T10:00:33Z",
        "closed_at": "2021-11-15T15:22:25Z",
        "merged_at": null,
        "body": "[root@node]# src/redis-**sentinel** -v\r\nRedis **server** v=255.255.255 sha=00000000:0 malloc=jemalloc-5.1.0 bits=64 build=d788edfc546625c9",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 59,
        "deletions": 11,
        "changed_files": 3,
        "created_at": "2021-11-05T07:32:34Z",
        "closed_at": "2022-01-04T15:10:37Z",
        "merged_at": null,
        "body": "As what we want usually, master will send back a big rdb if PSYNC\r\nturns out to be a full resynchronization. This really helps if\r\nreplica wants the whole same dataset as master. In some another\r\nuncommon situation, however, replica only needs stream of commands\r\nthat change dataset, such as redis-cli --replica. This contributes\r\nmaster performance drops caused by BGSAVE.\r\n\r\nBy sending REPLCONF BUF-ONLY 1 to master before PSYNC, An empty\r\nRDB is received if partial resynchronization failed, and\r\nmaster is forkless and diskless. More redis-cli clients replicate\r\nmaster, more this option helps.\r\n\r\nAlthough REPLCONF and PSYNC are internal commands, some end user\r\ncould talk replication handshake protocol whith master on a clean\r\nconnection to get a modification command stream. This using case\r\nshould drop master performance as little as possible, and BUF-ONLY\r\noption helps much.  From the view point of master, those clients are\r\nnot real replica at all.\r\n",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 508,
        "deletions": 362,
        "changed_files": 17,
        "created_at": "2021-11-05T06:11:41Z",
        "closed_at": "2021-11-24T11:34:14Z",
        "merged_at": "2021-11-24T11:34:13Z",
        "body": "Part three of implementing #8702, following #8887 and #9366 .\r\n\r\n## Description of the feature\r\n1. Replace the ziplist container of quicklist with listpack.\r\n2. Convert existing quicklist ziplists on RDB loading time. an O(n) operation.\r\n\r\n## Interface changes\r\n1. New `list-max-listpack-size` config is an alias for `list-max-ziplist-size`.\r\n2. Replace `debug ziplist` command with `debug listpack`.\r\n\r\n## Internal changes\r\n1. Add `lpMerge` to merge two listpacks . (same as `ziplistMerge`)\r\n2. Add `lpRepr` to print info of listpack which is used in debugCommand and `quicklistRepr`. (same as `ziplistRepr`)\r\n3. Replace `QUICKLIST_NODE_CONTAINER_ZIPLIST` with `QUICKLIST_NODE_CONTAINER_PACKED`(following #9357 ).\r\n    It represent that a quicklistNode is a packed node, as opposed to a plain node.\r\n4. Remove `createZiplistObject` method, which is never used.\r\n5. Calculate listpack entry size using overhead overestimation in `quicklistAllowInsert`.\r\n    We prefer an overestimation, which would at worse lead to a few bytes below the lowest limit of 4k.\r\n\r\n## Improvements\r\n1. Calling `lpShrinkToFit` after converting Ziplist to listpack, which was missed at #9366.\r\n2. Optimize `quicklistAppendPlainNode` to avoid memcpy data.\r\n\r\n## Bugfix\r\n1. Fix crash in `quicklistRepr` when ziplist is compressed, introduced from #9366.\r\n\r\n## Test\r\n1. Add unittest for `lpMerge`.\r\n2. Modify the old quicklist ziplist corrupt dump test.\r\n\r\n## TODO\r\n- [x] Comment typos.\r\n- [x] Benchmark.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-05T02:53:27Z",
        "closed_at": "2021-11-05T05:50:25Z",
        "merged_at": "2021-11-05T05:50:25Z",
        "body": "There is a warning when compiling on the alphine platform\r\n\r\n```\r\n# https://github.com/redis/redis/runs/4111889932?check_suite_focus=true#step:5:494\r\n/usr/include/sys/errno.h:1:2: error: #warning redirecting incorrect #include <sys/errno.h> to <errno.h> [-Werror=cpp]\r\n    1 | #warning redirecting incorrect #include <sys/errno.h> to <errno.h>\r\n```\r\n\r\nIntroduced in https://github.com/redis/redis/pull/9629\r\n\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2021-11-04T19:24:14Z",
        "closed_at": "2022-04-29T20:33:57Z",
        "merged_at": null,
        "body": "Wait for https://github.com/redis/redis/pull/9788\r\n\r\nThis PR adds the filenames of the rdb and aof files to the logs in the following cases:\r\n\r\n1. If no aof file the disk, and config file has appendonly yes\r\n![image](https://user-images.githubusercontent.com/51993843/140405921-6bb27770-1481-4157-9550-1c2eb5eedf7f.png)\r\n\r\n2. When redis server startup, and loading data from rdb file\r\n![image](https://user-images.githubusercontent.com/51993843/140406157-ee87b376-ee2f-4c20-85e0-85ec7af8ca1f.png)\r\n\r\n3. When redis server startup, and loading data from aof file\r\n![image](https://user-images.githubusercontent.com/51993843/140406278-3ad71b86-7371-4927-ba44-20990ea06696.png)\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2021-11-03T08:04:08Z",
        "closed_at": "2021-11-03T09:46:06Z",
        "merged_at": null,
        "body": "When change a replica to master, we need release the `slaveKeysWithExpire` dict to free memory no longer used.\r\n\r\nAnd now we use an async way to release the `slaveKeysWithExpire`, include `emptyDb` and `replicationUnsetMaster`.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-11-02T21:52:27Z",
        "closed_at": "2021-11-02T23:07:52Z",
        "merged_at": "2021-11-02T23:07:52Z",
        "body": "Another attempt to solve the problem described in #9722\r\n\r\nThe issue was that setting maxmemory to used_memory and expecting\r\neviction is insufficient, since we need to take\r\nmem_not_counted_for_evict into consideration.\r\n\r\nThis test got broken by #9166",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2021-11-02T13:36:27Z",
        "closed_at": "2021-11-02T14:42:54Z",
        "merged_at": "2021-11-02T14:42:53Z",
        "body": "external tests started failing recently for unclear reason:\r\n```\r\n*** [err]: Tracking invalidation message of eviction keys should be before response in tests/unit/tracking.tcl\r\nExpected '0' to be equal to 'invalidate volatile-key' (context: type eval line 21 cmd {assert_equal $res {invalidate volatile-key}} proc ::test)\r\n```\r\n\r\nI suspect the issue is that the used_memory sample is taken while a lazy free is still being processed.\r\nsee discussion in #9422",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-02T08:10:12Z",
        "closed_at": "2021-11-04T06:44:18Z",
        "merged_at": "2021-11-04T06:44:18Z",
        "body": "there was a chance that by the time the assertion is executed,\r\nthe replica already manages to reconnect.\r\n\r\nnow we make sure the replica is unable to re-connect to the master.\r\n\r\nadditionally, we wait for some gossip from the disconnected replica,\r\nto see that it doesn't mess things up.\r\n\r\nunrelated: fix a typo when trying to exhaust the backlog, one that\r\ndidn't have any harmful implications",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 21,
        "changed_files": 3,
        "created_at": "2021-11-02T06:58:34Z",
        "closed_at": "2021-11-02T08:53:52Z",
        "merged_at": "2021-11-02T08:53:52Z",
        "body": "After PR #9166 , replication backlog is not a real block of memory, just contains a reference points to replication buffer's block and the blocks index (to accelerate search offset when partial sync), so we need update both replication buffer's block's offset and replication backlog blocks index's offset when master restart from RDB, since the `server.master_repl_offset` is changed.\r\nThe implications of this bug was just a slow search, but not a replication failure.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-11-02T05:45:02Z",
        "closed_at": "2021-11-02T08:32:01Z",
        "merged_at": "2021-11-02T08:32:01Z",
        "body": "```\r\nhttps://github.com/redis/redis/runs/4028904426?check_suite_focus=true\r\n*** [err]: diskless no replicas drop during rdb pipe in tests/integration/replication.tcl\r\nlog message of '\"*Diskless rdb transfer, done reading from pipe, 2 replicas still up*\"' not found in ./tests/tmp/server.6124.69/stdout after line: 52 till line: 52\r\n```\r\nSome observations made with @oranagra:\r\n\r\n```\r\ncontent of ./tests/tmp/server.6124.69/stdout from line: 52:\r\n17992:M 28 Oct 2021 00:11:45.137 * Streamed RDB transfer with replica 127.0.0.1:24146 succeeded (socket). Waiting for REPLCONF ACK from slave to enable streaming\r\n\r\n49 17992:M 28 Oct 2021 00:11:45.129 # Diskless rdb transfer, done reading from pipe, 2 replicas still up.\r\n50 17992:M 28 Oct 2021 00:11:45.137 * Background RDB transfer terminated with success\r\n51 17992:M 28 Oct 2021 00:11:45.137 * Streamed RDB transfer with replica 127.0.0.1:24147 succeeded (socket). Waiting for REPLCONF ACK from slave to enable streaming\r\n52 17992:M 28 Oct 2021 00:11:45.137 * Streamed RDB transfer with replica 127.0.0.1:24146 succeeded (socket). Waiting for REPLCONF ACK from slave to enable streaming\r\n```\r\n\r\nLook at the logs, we can see `Diskless rdb transfer, done reading from pipe, 2 replicas still up`\r\nwas printed in line 49. But the test was looking for it from line 52.\r\n\r\nSo it looks like sampling `set loglines [count_log_lines -2]` was\r\nexecuted too late, and the replication managed to complete before that.\r\n\r\nChange:\r\n1. when we search the master log file, we start to search from before we sent the REPLICAOF command,\r\n  to prevent a race in which the replication completed before we sampled the log line count.\r\n2. we don't need to sample the replica loglines sine it's a fresh resplica that's just been started, so the message we're looking for is the\r\n  first occurrence in the log, we can start search from 0.\r\n\r\n![image](https://user-images.githubusercontent.com/22811481/139792811-a87982a3-44b2-4dce-9c9b-5c2c9a502bb6.png)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-11-01T14:22:08Z",
        "closed_at": "2021-11-02T09:04:12Z",
        "merged_at": "2021-11-02T09:04:12Z",
        "body": "Since the loop in incrementalTrimReplicationBacklog checks the size of histlen, we cannot afford to update it only when the loop exits, this may cause deleting much more replication blocks, and replication backlog may be less than setting size.\r\n\r\nintroduce in #9166 \r\n\r\nthanks @sundb finding this bug.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-11-01T12:34:34Z",
        "closed_at": "2021-11-01T14:07:08Z",
        "merged_at": "2021-11-01T14:07:08Z",
        "body": "Test failed on freebsd: https://github.com/redis/redis/runs/4028903962?check_suite_focus=true\r\n```\r\n*** [err]: Make the old master a replica of the new one and check conditions in tests/integration/psync2-pingoff.tcl\r\nExpected '162' to be equal to '176' (context: type eval line 18 cmd {assert_equal [status $R(0) master_repl_offset] [status $R(1) master_repl_offset]} proc ::test)\r\n```\r\nSome observations made with @oranagra:\r\nThere are two possible race conditions in the test.\r\n\r\n1. The code waits for sync_full to increment, and assumes that means the\r\nmaster did the fork. But in fact there are cases the master will increment\r\nthat sync_full counter (after replica asks for sync), but will see that\r\nthere's already a fork running and will delay the fork creation.\r\n\r\nIn this case the INCR will be executed before the fork happens, so it'll\r\nnot be in the command stream. Solve that by waiting for `master_link_status: up`\r\non the replica before the INCR.\r\n\r\n2. The repl-ping-replica-period is still high (1 second), so there's a chance the\r\nmaster will send an additional PING between the two calls to INFO (the line that\r\nfails is the one that samples INFO from both servers). So there's a chance one of\r\nthem will have an incremented offset due to PING and the other won't have it yet.\r\n\r\nIn theory we can wait for the repl_offset to match, but then we risk facing a\r\nsituation where that race will hide an offset mis-match. so instead, i think we\r\nshould just change repl-ping-replica-period to prevent further pings from being pushed.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 63,
        "deletions": 36,
        "changed_files": 7,
        "created_at": "2021-11-01T03:28:45Z",
        "closed_at": "2021-11-18T08:13:17Z",
        "merged_at": "2021-11-18T08:13:17Z",
        "body": "Moves ZPOP ... 0 fast exit path after type check to reply with\r\nWRONGTYPE. In the past it will return an empty array.\r\n\r\nAlso now count is not allowed to be negative.\r\n\r\nsee #9680\r\n\r\nbefore:\r\n```\r\n127.0.0.1:6379> set zset str\r\nOK\r\n127.0.0.1:6379> zpopmin zset 0\r\n(empty array)\r\n127.0.0.1:6379> zpopmin zset -1\r\n(empty array)\r\n```\r\n\r\nafter:\r\n```\r\n127.0.0.1:6379> set zset str\r\nOK\r\n127.0.0.1:6379> zpopmin zset 0\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n127.0.0.1:6379> zpopmin zset -1\r\n(error) ERR value is out of range, must be positive\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 19,
        "changed_files": 4,
        "created_at": "2021-10-31T10:24:16Z",
        "closed_at": "2021-11-01T11:41:35Z",
        "merged_at": "2021-11-01T11:41:35Z",
        "body": "The module test in reply.tcl was introduced by #8521 but didn't run until recently (see #9639)\r\nand then it started failing with valgrind.\r\nThis is because valgrind uses 64 bit long double (unlike most other platforms that have at least 80 bits)\r\nBut besides valgrind, the tests where also incompatible with ARM32, which also uses 64 bit long doubles.\r\n\r\nWe now use appropriate value to avoid issues with either valgrind or ARM32\r\n\r\nIn all the double tests, i use 3.141, which is safe since since addReplyDouble uses\r\n`%.17Lg` which is able to represent this value without adding any digits due to precision loss. \r\n\r\nIn the long double, since we use `%.17Lf` in ld2string, it preserves 17 significant\r\ndigits, rather than 17 digit after the decimal point (like in `%.17Lg`).\r\nSo to make these similar, i use value lower than 1 (no digits left of\r\nthe period)\r\n\r\nLastly, we have the same issue with TCL (no long doubles) so we read\r\nraw protocol in that test.\r\n\r\nNote that the only error before this fix (in both valgrind and ARM32 is this:\r\n```\r\n*** [err]: RM_ReplyWithLongDouble: a float reply in tests/unit/moduleapi/reply.tcl\r\nExpected '3.141' to be equal to '3.14100000000000001' (context: type eval line 2 cmd {assert_equal 3.141 [r rw.longdouble 3.141]} proc ::test)\r\n```\r\nso the changes to debug.c and scripting.tcl aren't really needed, but i consider them a cleanup\r\n(i.e. scripting.c validated a different constant than the one that's sent to it from debug.c).\r\n\r\nAnother unrelated change is to add the RESP version to the repeated tests in reply.tcl",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 14,
        "changed_files": 4,
        "created_at": "2021-10-31T08:49:51Z",
        "closed_at": "2021-10-31T17:22:22Z",
        "merged_at": "2021-10-31T17:22:22Z",
        "body": "Fix failures introduced by #9695 which was an attempt to solve failures introduced by #9679.\r\nAn alternative to #9703 (i didn't like the extra argument to kill_instance).\r\n\r\nReverting #9695.\r\nInstead of stopping AOF on all terminations, stop it only on the two which need it.\r\nDo it as part of the test rather than the infra (it was add that kill_instance used `R`\r\nto communicate to the instance)\r\n\r\nNote that the original purpose of these tests was to trigger a crash, but that upsets\r\nvalgrind so in redis 6.2 i changed it to use SIGTERM, so i now rename the tests\r\n(remove \"kill\" and \"crash\").\r\n\r\nAlso add some colors to failures, and the word \"FAILED\" so that it's searchable.\r\n\r\nAnd solve a semi-related race condition in 14-consistency-check.tcl",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-10-31T07:53:35Z",
        "closed_at": "2021-10-31T13:59:49Z",
        "merged_at": "2021-10-31T13:59:49Z",
        "body": "This will only cause an actual leak when repl-diskless-load is being used.\r\n\r\nFixes #9704",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2021-10-31T04:20:35Z",
        "closed_at": "2021-10-31T17:19:06Z",
        "merged_at": null,
        "body": "Two tests were getting delayed since there were waiting for the command to disable AOF to go through while it was being killed. However, the node was paused since the next part of the test relied on the node being unreachable from the replica, so the waiting to execute the AOF command broke the test timing.\r\n\r\nhttps://github.com/redis/redis/actions/runs/1403850622",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-10-31T02:32:53Z",
        "closed_at": "2021-10-31T14:01:54Z",
        "merged_at": "2021-10-31T14:01:54Z",
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2021-10-31T01:41:49Z",
        "closed_at": "2021-10-31T14:10:30Z",
        "merged_at": "2021-10-31T14:10:29Z",
        "body": "The previous code did not check whether COUNT is set.\r\nSo we can use `lmpop 2 key1 key2 left count 1 count 2`.\r\n\r\nThis situation can occur in LMPOP/BLMPOP/ZMPOP/BZMPOP commands.\r\nLMPOP/BLMPOP introduced in #9373, ZMPOP/BZMPOP introduced in #9484.\r\n\r\nbefore:\r\n```\r\n127.0.0.1:6379> lmpop 2 key1 key2 left count 1\r\n(nil)\r\n127.0.0.1:6379> lmpop 2 key1 key2 left count 1 count 2\r\n(nil)\r\n127.0.0.1:6379> lmpop 2 key1 key2 left count 1 count 2 count 3\r\n(nil)\r\n\r\n127.0.0.1:6379> zmpop 2 key1 key2 min count 1\r\n(nil)\r\n127.0.0.1:6379> zmpop 2 key1 key2 min count 1 count 2\r\n(nil)\r\n127.0.0.1:6379> zmpop 2 key1 key2 min count 1 count 2 count 3\r\n(nil)\r\n```\r\n\r\nafter:\r\n```\r\n127.0.0.1:6379> lmpop 2 key1 key2 left count 1 count 2 count 3\r\n(error) ERR syntax error\r\n127.0.0.1:6379> zmpop 2 key1 key2 min count 2 count 2 count 3\r\n(error) ERR syntax error\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-10-28T01:17:57Z",
        "closed_at": "2021-10-31T07:59:35Z",
        "merged_at": null,
        "body": "Now invalidation messages are always after command replies. But hiredis\r\nonly handles invalidation messages before replies. redis-cli will\r\ndisplay output wrong. This commit do a best-efforts fix to it. After\r\nreply, we check buffer to see whether there are invalidation messages.\r\n\r\n```\r\n127.0.0.1:6383> client tracking on\r\nOK\r\n127.0.0.1:6383> get hello\r\n\"1\"\r\n127.0.0.1:6383> set hello world\r\nOK\r\n127.0.0.1:6383> get hello\r\n1) \"invalidate\"\r\n2) 1) \"hello\"\r\n127.0.0.1:6383> a\r\n\"world\"\r\n```\r\n\r\nfix https://github.com/redis/redis/issues/8923",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2021-10-27T20:03:40Z",
        "closed_at": "2021-11-04T07:43:08Z",
        "merged_at": "2021-11-04T07:43:08Z",
        "body": "Fixes #9680.\r\n\r\nIntroduced in #8179, this fixes the command's replies in the 0 count edge case.\r\n\r\n* [BREAKING] changes the reply type when count is 0 to an empty array (instead of nil)\r\n* Moves LPOP ... 0 fast exit path after type check to reply with WRONGTYPE\r\n\r\nTOOD: \r\n\r\n* [ ] Update docs with reply type",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2021-10-27T10:27:07Z",
        "closed_at": "2021-10-27T11:48:08Z",
        "merged_at": "2021-10-27T11:48:08Z",
        "body": "We now use git subtree for deps/jemalloc, updating\r\njemalloc is detailed in deps/README.md\r\n\r\nsee #9623 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-10-26T05:33:03Z",
        "closed_at": "2021-10-26T10:02:31Z",
        "merged_at": "2021-10-26T10:02:31Z",
        "body": "The first test exited before all the memory was reclaimed, so when the second test sampled used_memory, it was too early.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-10-25T11:27:36Z",
        "closed_at": "2021-10-26T05:34:31Z",
        "merged_at": "2021-10-26T05:34:31Z",
        "body": "I recently started seeing a lot of empty valgrind reports in the daily CI.\r\ni.e. prints showing valgrind header but no leak report, which causes the tests to fail\r\nhttps://github.com/redis/redis/runs/3991335416?check_suite_focus=true\r\n\r\nThis commit change 2 things:\r\n* first, considering valgrind is just slow, we used to give processes 60 seconds timeout on shutdown instead of 10 seconds we give normally. this commit changes that to 120.\r\n* secondly, when we reach the timeout, we first try to use SIGSEGV so that maybe we'll get a stack trace indicating where redis is hang, and we only resort to SIGKILL if double that time passed.\r\n\r\nnote that if there are indeed hang processes, we will normally not see that in the non-valgrind runs, since the tests didn't use to detect any failure in that case, and now they will since `crashlog_from_file` is run after `kill_server`.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 33,
        "changed_files": 4,
        "created_at": "2021-10-25T08:30:47Z",
        "closed_at": "2022-04-11T08:08:39Z",
        "merged_at": "2022-04-11T08:08:39Z",
        "body": "Durability of database is a big and old topic, in this regard Redis use AOF to support it, and `appendfsync=alwasys` policy is the most strict level, guarantee all data is both written and synced on disk before reply success to client.\r\n\r\nBut there are some cases have been overlooked, and could lead to durability broken.\r\n\r\n1. The most clear one is about threaded-io mode\r\n    we should also set client's write handler with `ae_barrier` in `handleClientsWithPendingWritesUsingThreads`, or the write handler would be called after read handler in the next event loop, it means the write command result could be replied to client before flush to AOF.\r\n2. About blocked client (mostly by module)\r\n    in `beforeSleep()`, `handleClientsBlockedOnKeys()` should be called before `flushAppendOnlyFile()`, in case the unblocked clients modify data without persistence but send reply.\r\n3. When handling `ProcessingEventsWhileBlocked`\r\n    normally it takes place when lua/function/module timeout, and we give a chance to users to kill the slow operation, but we should call `flushAppendOnlyFile()` before `handleClientsWithPendingWrites()`, in case the other clients in the last event loop get acknowledge before data persistence.\r\n    for a instance:\r\n    ```\r\n    in the same event loop\r\n    client A executes set foo bar\r\n    client B executes eval \"for var=1,10000000,1 do end\" 0\r\n    ```\r\n    after the script timeout, client A will get `OK` but lose data after restart (kill redis when timeout) if we don't flush the write command to AOF.\r\n4. A more complex case about `ProcessingEventsWhileBlocked`\r\n    it is lua timeout in transaction, for example `MULTI; set foo bar; eval \"for var=1,10000000,1 do end\" 0; EXEC`, then client will get set command's result before the whole transaction done, that breaks atomicity too.\r\n    fortunately, it's already fixed by #5428 (although it's not the original purpose just a side effect : )), but module timeout should be fixed too.\r\n\r\ncase 1, 2, 3 are fixed in this PR, the module issue in case 4 needs a followup PR.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 7,
        "created_at": "2021-10-25T06:51:29Z",
        "closed_at": "2021-10-25T09:00:43Z",
        "merged_at": "2021-10-25T09:00:43Z",
        "body": "REDISMODULE_POSTPONED_ARRAY_LEN is deprecated, use REDISMODULE_POSTPONED_LEN instead",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-10-25T02:42:48Z",
        "closed_at": "2021-11-18T05:28:14Z",
        "merged_at": "2021-11-18T05:28:14Z",
        "body": "Closes https://github.com/redis/redis/issues/9618. The basic idea is that when cluster failover is initiated manually, the master pauses itself for 10 seconds regardless of outcome. This now unpauses the master as soon as reset is called, either on success or failover, so commands can resume.\r\n\r\nNote that the conditional was changed since mf_end is used by both primary and replica, while mf_slave is only used by the primary.  \r\n\r\nCluster test run: https://github.com/redis/redis/runs/3992088669. There are already tests for manual failover (and they run ~8 seconds faster now!)",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2021-10-24T08:24:45Z",
        "closed_at": "2021-10-25T10:01:26Z",
        "merged_at": "2021-10-25T10:01:26Z",
        "body": "Improve code doc for allowed_firstargs (used to be allowed_commands before #9504.\r\nI don't think the text in the code needs to refer to the history (it's not there just for backwards compatibility).\r\ninstead it should just describe what it does.\r\nref https://github.com/redis/redis/pull/9504#discussion_r728327508",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-10-23T14:13:21Z",
        "closed_at": "2021-10-24T09:32:53Z",
        "merged_at": "2021-10-24T09:32:53Z",
        "body": "Introduced via typo in #9504. Reported by @sazzad16.\r\n\r\nAlso adds a sanity test for coverage.\r\n\r\nAdditional seemingly uncovered `CLUSTER` subcommands: `BUMPEPOCH`, `COUNT-FAILURE-REPORTS`, `FORGET`, `FLUSHSLOTS`, `KEYSLOT`, `SAVECONFIG`",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-10-22T14:00:28Z",
        "closed_at": "2021-11-02T15:03:08Z",
        "merged_at": "2021-11-02T15:03:08Z",
        "body": "In the file redis-check-aof.c, we have the following [code](https://github.com/redis/redis/blob/761d7d27711edfbf737def41ff28f5b325fb16c8/src/redis-check-aof.c#L78):\r\n```\r\nint readString(FILE *fp, char** target) {\r\n    long len;\r\n    *target = NULL;\r\n    if (!readLong(fp,'$',&len)) {\r\n        return 0;\r\n    }\r\n\r\n    /* Increase length to also consume \\r\\n */\r\n    len += 2;\r\n    *target = (char*)zmalloc(len);\r\n    if (!readBytes(fp,*target,len)) {\r\n        return 0;\r\n    }\r\n    ...\r\n}\r\n```\r\n\r\nThe variable `len` is read from the file. It could be a large value (e.g., `LONG_MAX`) such that `len += 2` may result in integer overflow. Moreover, since signed overflow is undefined behavior in C, it should be avoided anyway.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-10-22T09:04:38Z",
        "closed_at": "2021-10-24T09:53:06Z",
        "merged_at": null,
        "body": "this pr is not necessary, it's free to be closed.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2021-10-21T13:15:41Z",
        "closed_at": "2021-11-07T11:53:57Z",
        "merged_at": "2021-11-07T11:53:57Z",
        "body": "Use 'socket -server' instead of 'socket' to rule out port on TIME_WAIT.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-10-21T12:14:35Z",
        "closed_at": "2021-10-25T08:33:37Z",
        "merged_at": "2021-10-25T08:33:37Z",
        "body": "I think I missed this one in my review, but it shouldn't be admin as it applies only to the current connection.\r\n\r\nTODO:\r\n- [x] when we merge this, update the summary in #9504 for the release notes.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 26,
        "changed_files": 3,
        "created_at": "2021-10-21T07:45:58Z",
        "closed_at": "2021-10-21T09:50:59Z",
        "merged_at": "2021-10-21T09:50:59Z",
        "body": "Bugs introduced in https://github.com/redis/redis/pull/9504",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-10-20T18:33:08Z",
        "closed_at": "2021-10-24T07:15:32Z",
        "merged_at": "2021-10-24T07:15:32Z",
        "body": "This issue has come up in the cross-compilation setup in the redis-debian repository, and addressed by a [patch](https://github.com/redis/redis-debian/blob/master/debian/patches/jemalloc_crosscompile.diff).\r\n\r\nThis patch would be broken by the recent jemalloc upgrade, so it's a good opportunity to apply it here.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 23984,
        "deletions": 2388,
        "changed_files": 392,
        "created_at": "2021-10-20T09:53:05Z",
        "closed_at": "2021-12-15T19:23:15Z",
        "merged_at": "2021-12-15T19:23:15Z",
        "body": "This is the final fix of https://github.com/redis/redis/issues/9359, except https://github.com/redis/redis/issues/9845, and https://github.com/redis/redis/issues/9876, and https://github.com/redis/redis/issues/9944 which will be handled separately.\r\n\r\nDelete the hardcoded command table and replace it with an auto-generated table, based\r\non a JSON file that describes the commands (each command must have a JSON file).\r\n\r\nThese JSON files are the SSOT of everything there is to know about Redis commands,\r\nand it is reflected fully in COMMAND INFO.\r\n\r\nThese JSON files are used to generate commands.c (using a python script), which is then committed to the repo and compiled.\r\n\r\nThe purpose is:\r\n* Clients and proxies will be able to get much more info from redis, instead of relying on hard coded logic.\r\n* drop the dependency between Redis-user and the commands.json in redis-doc.\r\n* delete help.h and have redis-cli learn everything it needs to know just by issuing COMMAND (will be done in a separate PR)\r\n* redis.io should stop using commands.json and learn everything from Redis (ultimately one of the release artifacts should be a large JSON, containing all the information about all of the commands, which will be generated from COMMAND's reply)\r\n* the byproduct of this is:\r\n  * module commands will be able to provide that info and possibly be more of a first-class citizens\r\n  * in theory, one may be able to generate a redis client library for a strictly typed language, by using this info.\r\n\r\n### Interface changes\r\n\r\n#### COMMAND INFO's reply change (and arg-less COMMAND)\r\n\r\nBefore this commit the reply at index 7 contained the key-specs list\r\nand reply at index 8 contained the sub-commands list (Both unreleased).\r\nNow, reply at index 7 is a map of:\r\nNOTE, many of these where later moved to COMMAND DOCS, see: #10056)\r\n- summary - short command description\r\n- since - debut version\r\n- group - command group\r\n- complexity - complexity string\r\n- doc-flags - flags used for documentation (e.g. \"deprecated\")\r\n- deprecated-since - if deprecated, from which version?\r\n- replaced-by - if deprecated, which command replaced it?\r\n- history - a list of (version, what-changed) tuples\r\n- hints - a list of strings, meant to provide hints for clients/proxies. see https://github.com/redis/redis/issues/9876\r\n- arguments - an array of arguments. each element is a map, with the possibility of nesting (sub-arguments)\r\n- key-specs - an array of keys specs (already in unstable, just changed location)\r\n- subcommands - a list of sub-commands (already in unstable, just changed location)\r\n- reply-schema - will be added in the future (see https://github.com/redis/redis/issues/9845)\r\n\r\nmore details on these can be found in https://github.com/redis/redis-doc/pull/1697\r\n\r\nonly the first three fields are mandatory \r\n\r\n#### API changes (unreleased API obviously) - now they take RedisModuleCommand opaque pointer instead of looking up the command by name.\r\nNote: these where later moved, see: #10108)\r\n\r\n- RM_CreateSubcommand\r\n- RM_AddCommandKeySpec\r\n- RM_SetCommandKeySpecBeginSearchIndex\r\n- RM_SetCommandKeySpecBeginSearchKeyword\r\n- RM_SetCommandKeySpecFindKeysRange\r\n- RM_SetCommandKeySpecFindKeysKeynum\r\n\r\nCurrently, we did not add module API to provide additional information about their commands because we couldn't agree on how the API should look like, see https://github.com/redis/redis/issues/9944.\r\n\r\n### Somehow related changes\r\n1. Literals should be in uppercase while placeholder in lowercase. Now all the GEO* command will be documented with M|KM|FT|MI and can take both lowercase and uppercase\r\n\r\n### Unrelated changes\r\n1. Bugfix: no_madaory_keys was absent in COMMAND's reply\r\n2. expose CMD_MODULE as \"module\" via COMMAND\r\n3. have a dedicated uint64 for ACL categories (instead of having them in the same uint64 as command flags)\r\n\r\n### TODO\r\n- [x] Some information is missing from the JSON files (mostly summary and complexity info) - will go over them manually\r\n- [x] Open a GH issue: define and document general hints for RM_SetCommandHints - https://github.com/redis/redis/issues/9876\r\n- [x] Create a PR for redis-doc, describing all the changes in this PR (namely the changes in COMMAND) - https://github.com/redis/redis-doc/pull/1697\r\n- [x] Add test: per command, all key-specs are referred to, and every key arg reference appears at least once in the key-specs array - tested manually, I don't see a value in writing an automatic test for this.\r\n- [x] Write the argument array where missing:\r\n 'fcall',\r\n 'fcall_ro',\r\n 'georadius_ro',\r\n 'georadiusbymember_ro',\r\n 'substr',\r\n 'xsetid'\r\n internal command that take arguments (but we will not bother to add them to the jsons)\r\n 'pfdebug',\r\n 'replconf',\r\n 'restore-asking',\r\n- [x] Generate the arg array of COMMAND LIST\r\n- [x] Come up with a way to prevent people from using the jsons directly, and force them to use COMMAND: all flag and acl cats will be uppercase in the jsons, the script to generate commands.c will lower them\r\n- [x] Create jsons for new function command(s) + the new form of CONFIG SET\r\n- [x] fix moduleAPI for arguments (\"value\" is gone from c file but not from h. fix test module)\r\n- [x] Yossi's comments\r\n- [x] add CMD_MODULE to COMMAND INFO\r\n\r\n### Open issues:\r\n\r\n#### Argument's \"value\" and \"name\"\r\n\r\nhow should we call what is now called \u201cvalue\u201d (of an argument)?\r\nit is one of the following:\r\n - a string to display when rendering the syntax\r\n - an array of sub-arguments\r\n \r\nalso, do we want to get rid of \u201cname\u201d ?it is indeed redundant (args are ordered so we can identify one by its nested index)\r\n\r\nupdate: this is what we decided:\r\n1. \u201cname\u201d is mandatory. if the arg doesn\u2019t have subargs this is the string we use for rendering the syntax\r\n2. get rid of \u201cvalue\u201d. if the command has subargs they will appear under \u201carguments\u201d\r\n\r\nbasically, the players in syntax rendering are \u201cname\u201d (if arg doesn\u2019t have subargs) , \u201ctoken\u201d (if exists), and \u201carguments\u201d (if exists)\r\n\r\n#### Usage of versions in module API\r\n\r\nWhen a module sets \"since\", should it be the debut Redis version or the module version?\r\n\r\nupdate: for now, the code suggests it's the module version\r\n\r\n#### Optional args interchangeability \r\n\r\nthe rule is that all optional args, at the same nesting level, may interchange as long as there isn\u2019t a non-optional arg between them\r\ne.g.\r\n```\r\nSORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern ...]] [ASC|DESC] [ALPHA] [STORE destination]\r\n```\r\nall optionals can interchange\r\n\r\n```\r\nXPENDING key group [[IDLE min-idle-time] start end count [consumer]]\r\n```\r\nIDLE min-idle-time and consumer can\u2019t interchange because there\u2019s a non-optional between them\r\n\r\n\r\nthe only exception is MIGRATE, which actually has two forms:\r\neither\r\n```\r\nMIGRATE host port key destination-db timeout [COPY] [REPLACE] [AUTH password] [AUTH2 username password]\r\n```\r\nor\r\n```\r\nMIGRATE host port \"\" destination-db timeout [COPY] [REPLACE] [AUTH password] [AUTH2 username password] KEYS key [key ...]\r\n```\r\nwe have two options:\r\n1. say f*ck it, it\u2019s only one exception, the cli hints will be wrong\r\n2. rearrange the command json file so that it\u2019ll look like `MIGRATE host port (key destination-db timeout [COPY] [REPLACE] [AUTH password] [AUTH2 username password] | \"\" destination-db timeout [COPY] [REPLACE] [AUTH password] [AUTH2 username password] KEYS key [key ...])`\r\n3. add an ad-hoc feature in the JSON that says \"this arg is optional, but only if (another arg) is not (value)\"\r\n\r\nupdate: for now we've decided to go with (1)",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-10-19T17:41:21Z",
        "closed_at": "2021-10-26T10:13:12Z",
        "merged_at": "2021-10-26T10:13:12Z",
        "body": "Before this fix, processing a `sentinel auth-pass` command would dump the new password in the log which does not follow best practices of redacting secrets.\r\n\r\nRelated to #9605",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 7,
        "changed_files": 4,
        "created_at": "2021-10-19T12:21:18Z",
        "closed_at": "2021-10-24T13:52:45Z",
        "merged_at": "2021-10-24T13:52:44Z",
        "body": "I moved a bunch of stats in redisFork to be executed only on successful\r\nfork, since they seem wrong to be done when it failed.\r\nI guess when fork fails it does that immediately, no latency spike.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 367,
        "deletions": 419,
        "changed_files": 4,
        "created_at": "2021-10-18T08:59:23Z",
        "closed_at": "2021-11-07T11:40:09Z",
        "merged_at": "2021-11-07T11:40:09Z",
        "body": "See issue #8349.\r\nCurrently this refactors the code based on initial work by @bjosv so all `CONFIG SET`s and conf file loading arguments go through the generic config handling interface.\r\n\r\nRefactoring changes:\r\n- All config params go through the `standardConfig` interface (some stuff which is only related to the config file and not the `CONFIG` command still has special handling for rewrite/config file parsing, `loadmodule`, for example.) .\r\n- Added `MULTI_ARG_CONFIG` flag for configs to signify they receive a variable number of arguments instead of a single argument. This is used to break up space separated arguments to `CONFIG SET` so the generic setter interface can pass multiple arguments to the setter function. When parsing the config file we also break up anything after the config name into multiple arguments to the setter function.\r\n\r\nInterface changes:\r\n- A side effect of the above interface is that the `bind` argument in the config file can be empty (no argument at all) this is treated the same as passing an single empty string argument (same as `save` already used to work, see https://github.com/redis/redis/pull/9644#discussion_r740199778).\r\n- Support rewrite and setting `watchdog-period` from config file (was only supported by the CONFIG command till now).\r\n- Another side effect is that the `save T X` config argument now supports multiple Time-Changes pairs in a single line like its `CONFIG SET` counterpart. So in the config file you can either do:\r\n  ```\r\n  save 3600 1\r\n  save 600 10\r\n  ```\r\n  or do\r\n  ```\r\n  save 3600 1 600 10\r\n  ```\r\nPotentially breaking changes:\r\n- Another change to the `save` behavior is that we used to ignore invalid input and reset the `save` to no saving when reading the config file or parsing command line. So running `redis-sever --save invalid` would start the server with save disabled. But we now treat this generically and fail to start the server like any other invalid config.\r\n\r\nFuture PRs should:\r\n- Design generic validation mechanism.\r\n- Add multi param support.\r\n- Tests.\r\n- Depricate support for multiline `save` in config file and document the variadic version.",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-10-17T18:48:08Z",
        "closed_at": "2021-10-24T13:02:39Z",
        "merged_at": null,
        "body": "cppcheck report:\r\nsrc/redis-cli.c:6322:35: warning: Either the condition '!table' is redundant or there is possible null pointer dereference: table. [nullPointerRedundantCheck]",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-10-17T12:19:47Z",
        "closed_at": "2021-10-17T14:15:27Z",
        "merged_at": "2021-10-17T14:15:27Z",
        "body": "the RedisModule_ReplyWithPush prototype was merged by mistake (no such API yet)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 27,
        "changed_files": 9,
        "created_at": "2021-10-17T11:57:41Z",
        "closed_at": "2021-11-03T12:12:33Z",
        "merged_at": "2021-11-03T12:12:33Z",
        "body": "When using SETNX and SETXX we could end up doing key lookup twice.\r\nThis presents a small inefficiency price.\r\nAlso once we have statistics of write hit and miss they'll be wrong (recording the same key hit twice) ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 69,
        "deletions": 14,
        "changed_files": 6,
        "created_at": "2021-10-17T08:02:02Z",
        "closed_at": "2021-10-25T08:31:20Z",
        "merged_at": "2021-10-25T08:31:20Z",
        "body": "Let modules use additional type of RESP3 response (unused by redis so far)\r\n\r\nAlso fix tests that where introduced in #8521 but didn't actually run.\r\n",
        "comments": 15
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2021-10-17T05:58:35Z",
        "closed_at": "2022-08-29T11:47:45Z",
        "merged_at": null,
        "body": "Metrics simliar to https://github.com/redis/redis/pull/9377 about active expire.\r\n\r\n`current_expire_timelimit_time` if greater than 0, means how much time has\r\npassed since active expire exits because of time limit.  If activie expire exits\r\nnormally, this metric is reset to 0. `total_expire_timelimit_time` means total time \r\nactive expire exits because of time limit till to exits normally.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-10-16T17:04:32Z",
        "closed_at": "2021-10-16T21:07:28Z",
        "merged_at": "2021-10-16T21:07:28Z",
        "body": "* Fix test modules linking on macOS 11.x.\r\n* Use macOS 10.x for FreeBSD VM as VirtualBox is not yet supported on\r\n  11.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-10-15T07:38:54Z",
        "closed_at": "2021-10-21T11:01:11Z",
        "merged_at": "2021-10-21T11:01:11Z",
        "body": "currently when we create a blocked client and a thread safe context for it, the new context's client doesn't carry through the original's resp version.\r\n\r\nthis copies it through to the new client",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-10-13T11:35:48Z",
        "closed_at": "2021-10-14T06:53:46Z",
        "merged_at": "2021-10-14T06:53:46Z",
        "body": "Verbatim Stings in RESP3 have a type/extension.  The existing redismoule reply function, hard coded it to \"txt\".",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-10-12T13:00:40Z",
        "closed_at": "2021-10-12T17:16:29Z",
        "merged_at": "2021-10-12T17:16:29Z",
        "body": "Cherry pick a more complete fix to 0215324a6 that also doesn't leak\r\nmemory from latest hiredis.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-10-12T12:02:11Z",
        "closed_at": "2021-11-04T07:09:28Z",
        "merged_at": "2021-11-04T07:09:28Z",
        "body": "When repl-diskless-load is enabled, the connection is set to the blocking state.\r\nThe connection may be interrupted by a signal during a system call.\r\nThis would have resulted in a disconnection and possibly a reconnection loop.",
        "comments": 20
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 24,
        "changed_files": 4,
        "created_at": "2021-10-12T00:50:05Z",
        "closed_at": "2021-10-13T07:06:54Z",
        "merged_at": "2021-10-13T07:06:54Z",
        "body": "There was a couple of issues with the cluster replica test test:\r\n1. It was populating about 1GB of data in order to make sure there was enough time to capture the loading. Now we're using less memory and using configuration to slow down the sync.\r\n2. One of the asserts was racy, since it was asserting that the master will remove the replica from cluster slots as soon as the replica is in the loading state. This is not the case, as the replica still needs to broadcast it's repl-offset on the cluster bus first. This was the assert that was failing the daily tests. \r\n3. A lot of the timeouts were generally very high, so I lowered a couple.\r\n\r\nNote I added a config parameter, this is intended to be used only for debugging so I didn't add documentation for it.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-10-11T03:56:13Z",
        "closed_at": "2021-10-11T05:58:43Z",
        "merged_at": "2021-10-11T05:58:42Z",
        "body": "Supplement to #8096 , in case config inconsistency between master and replicas.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-10-10T10:45:49Z",
        "closed_at": "2021-10-12T20:00:50Z",
        "merged_at": "2021-10-12T20:00:49Z",
        "body": "bigkeys sleep is defined each 100 scanned keys, and it is checked it only between scan cycles.\r\nIn cases that scan does not return exactly 10 keys it will never sleep.\r\nIn addition the comment was sleep each 100 SCANs but it was 100 scanned keys.\r\n\r\nThe descriptions said:\r\n> /* Status message */\r\n    printf(\"\\n# Scanning the entire keyspace to find hot keys as well as\\n\");\r\n    printf(\"# average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec\\n\");\r\n    printf(\"# per 100 SCAN commands (not usually needed).\\n\\n\");",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 10735,
        "deletions": 2817,
        "changed_files": 154,
        "created_at": "2021-10-10T10:09:20Z",
        "closed_at": "2021-10-18T09:45:11Z",
        "merged_at": "2021-10-18T09:45:11Z",
        "body": "Upgraded to jemalloc 5.2.1 from 5.1.0. Cherry picked all relevant fixes (by diffing our 5.1.0 to upstream 5.10 and finding relevant commits). Details of what was done:\r\n\r\n**[cherry-picked]** fd7d51c35 2021-05-03 Resolve nonsense static analysis warnings (Oran Agra)\r\n**[cherry-picked]** 448c435b1 2020-09-29 Fix compilation warnings in Lua and jemalloc dependencies (#7785) (YoongHM)\r\n**[skipped - already in upstream]** 9216b96b4 2020-09-21 Fix compilation warning in jemalloc's malloc_vsnprintf (#7789) (YoongHM)\r\n**[cherry-picked]** 88d71f479 2020-05-20 fix a rare active defrag edge case bug leading to stagnation (Oran Agra)\r\n**[skipped - already in upstream]** 2fec7d9c6 2019-05-30 Jemalloc: Avoid blocking on background thread lock for stats.\r\n**[cherry-picked]** 920158ec8 2018-07-11 Active defrag fixes for 32bit builds (again) (Oran Agra)\r\n**[cherry-picked]** e8099cabd 2018-06-26 add defrag hint support into jemalloc 5 (Oran Agra)\r\n**[re-done]** 4e729fcda 2018-05-24 Generate configure for Jemalloc. (antirez)\r\n\r\nAdditionally had to do this:\r\n7727cc248 2021-10-10 Fix defrag to support sharded bins in arena (added in v5.2.1) (Yoav Steinberg)\r\n\r\nWhen reviewing please look at all except the first commit which is just replacing 5.1.0 with 5.2.1 sources.\r\nAlso I think we should **merge this without squashing** to preserve the changes we did to to jemalloc.\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-10-09T09:03:10Z",
        "closed_at": "2021-10-10T05:17:54Z",
        "merged_at": "2021-10-10T05:17:54Z",
        "body": "When appendfsync is set to 'everysec', we will call time(NULL) once per second, but it's of little use",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 71,
        "deletions": 77,
        "changed_files": 1,
        "created_at": "2021-10-09T08:39:43Z",
        "closed_at": "2021-10-11T02:56:52Z",
        "merged_at": null,
        "body": "Move SSL options into a block to make it easy to read, and change\r\nglobal rules to make it easy to maintain. For the further step, it\r\ngets extensible to add another type.\r\n\r\nSigned-off-by: zhenwei pi <pizhenwei@bytedance.com>",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 73,
        "deletions": 73,
        "changed_files": 9,
        "created_at": "2021-10-07T21:22:47Z",
        "closed_at": "2021-10-12T20:35:54Z",
        "merged_at": null,
        "body": "In our codes, the following config parameters are not consistent with their mapping variable. \r\nIn this pr, update all of them to keep them consistent.\r\n\r\nThe list of config parameters in redis config file:\r\n\r\n**cluster-replica-no-failover\r\nreplica-lazy-flush\r\nreplica-serve-stale-data\r\nreplica-read-only\r\nreplica-ignore-maxmemory\r\nreplica-announce-ip\r\ncluster-replica-validity-factor\r\nreplica-priority\r\nreplica-announce-port\r\nrepl-ping-replica-periods\r\nmin-replicas-to-write\r\nmin-replicas-max-lag**",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 13,
        "changed_files": 5,
        "created_at": "2021-10-07T11:36:21Z",
        "closed_at": "2021-10-17T14:31:06Z",
        "merged_at": "2021-10-17T14:31:06Z",
        "body": "This is useful for approximating size computation of complex module types.\r\nNote that the mem_usage2 callback is new and has not been released yet, which is why we can modify it.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 34,
        "changed_files": 1,
        "created_at": "2021-10-07T09:31:16Z",
        "closed_at": "2021-10-07T12:43:48Z",
        "merged_at": "2021-10-07T12:43:48Z",
        "body": "obuf based eviction tests run until eviction occurs instead of assuming a certain amount of writes will fill the obuf enough for eviction to occur. This handles the kernel buffering written data and emptying the obuf even though no one actualy reads from it.\r\n\r\nThe tests have a new timeout of 20sec: if the test doesn't pass after 20 sec it'll fail. Hopefully this enough for our slow CI targets.\r\n\r\nThis also eliminates the need to skip some tests in TLS.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 24,
        "changed_files": 3,
        "created_at": "2021-10-06T07:21:38Z",
        "closed_at": "2021-10-19T06:58:52Z",
        "merged_at": "2021-10-19T06:58:52Z",
        "body": "This also corrects a string conversion issue.\r\nA faulty octal number in the config `unixsocketperm 778` would result in an accepted config file, but the unixsocket would get a permission similar to: `unixsocketperm 077`\r\n`strtol` parses `778` as `77` since 8 is an invalid octal number and accepts that (but its `char** endptr` will point to 8)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 59,
        "deletions": 26,
        "changed_files": 6,
        "created_at": "2021-10-05T19:33:51Z",
        "closed_at": "2021-11-26T20:43:08Z",
        "merged_at": null,
        "body": "This PR adds the feature of custom hashing support in clusters as mentioned in https://github.com/redis/redis/issues/8948. A new file is added \"custom_hash.c\" where the custom hash can be added and if \"cluster-custom-hash yes\" is added to the conf file then the custom hashing mode is enabled otherwise the default hashing function is used.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-10-05T14:24:48Z",
        "closed_at": "2021-10-06T08:39:10Z",
        "merged_at": "2021-10-06T08:39:10Z",
        "body": "When queuing a multi command we duplicated the argv (meaning an alloc and a memcpy). This isn't needed since we can use the previously allocated argv and just reset the client objects argv to NULL. This should saves some memory and is a minor optimization in heavy MULTI/EXEC traffic, especially if there are lots of arguments.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 326,
        "deletions": 54,
        "changed_files": 29,
        "created_at": "2021-10-05T12:23:59Z",
        "closed_at": "2021-11-11T11:51:34Z",
        "merged_at": "2021-11-11T11:51:34Z",
        "body": "- Added sanitizer support. `address`, `undefined` and `thread` sanitizers are available.  \r\n- To build Redis with desired sanitizer : `make SANITIZER=undefined`\r\n- There were some sanitizer findings, cleaned up codebase\r\n- Added tests with address and undefined behavior sanitizers to daily CI.\r\n- Added tests with address sanitizer to the per-PR CI (smoke out mem leaks sooner).\r\n\r\nBasically, there are three types of issues : \r\n\r\n**1- Unaligned load/store** : Most probably, this issue may cause a crash on a platform that does not support unaligned access. Redis does unaligned access only on supported platforms.\r\n\r\n**2- Signed integer overflow.** Although, signed overflow issue can be problematic time to time and change how compiler generates code, current findings mostly about signed shift or simple addition overflow. For most platforms Redis can be compiled for, this wouldn't cause any issue as far as I can tell (checked generated code on godbolt.org).\r\n\r\n **3 -Minor leak** (redis-cli), **use-after-free**(just before calling exit());\r\n\r\nUB means nothing guaranteed and risky to reason about program behavior but I don't think any of the fixes here worth backporting. As sanitizers are now part of the CI, preventing new issues will be the real benefit. \r\n\r\n[EDIT] It seems that gcc 12.2.1 present on Alpine produces a bug in BITFIELD overflow detection.\r\nlooks like in this case, the fix here is not just to silence a warning, it actually fixes a bug.",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2021-10-05T12:20:34Z",
        "closed_at": "2021-10-18T19:21:19Z",
        "merged_at": "2021-10-18T19:21:19Z",
        "body": "The LRU of the key is not touched. Logically expired keys are\r\nlogically not existing, so they're treated as such.\r\n\r\nNo. 11 in #6860",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-10-05T08:46:56Z",
        "closed_at": "2021-10-11T10:09:19Z",
        "merged_at": "2021-10-11T10:09:18Z",
        "body": "When calling `XADD` with a predefined id (instead of `*`) there's no need to run the code which replaces the supplied id with itself. Only when we pass a wildcard id we need to do this. For apps which always supply their own id this is a slight optimization.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2021-10-05T06:50:46Z",
        "closed_at": "2021-10-08T05:32:40Z",
        "merged_at": "2021-10-08T05:32:40Z",
        "body": "Known changed behaviors:\r\n- When `cluster-config-file` is configured its now also included in the output of `CONFIG GET *`.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-10-04T19:10:45Z",
        "closed_at": "2021-10-06T07:15:03Z",
        "merged_at": "2021-10-06T07:15:03Z",
        "body": "**Describe the bug**\r\n1. client block on command `XREAD BLOCK 0 STREAMS mystream  $`\r\n2. in a module, calling `XADD mystream * field value` via lua from a timer callback\r\n3. client will receive response after some latency up to 100ms\r\n\r\n**Reason**\r\nWhen `XADD` signal the key `mystream` as ready, `beforeSleep` in next eventloop will call `handleClientsBlockedOnKeys` to unblock the client and add pending data to write but not actually install a write handler, so next redis will block in `aeApiPoll` up to 100ms given `hz` config as default 10, pending data will be sent in another next eventloop by `handleClientsWithPendingWritesUsingThreads`.\r\n\r\nCalling `handleClientsBlockedOnKeys` before `handleClientsWithPendingWritesUsingThreads` in `beforeSleep` solves the problem.\r\n\r\nRelated issues #8125 #7880 #7903",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 20,
        "changed_files": 3,
        "created_at": "2021-10-04T09:53:54Z",
        "closed_at": "2021-10-08T05:33:09Z",
        "merged_at": "2021-10-08T05:33:09Z",
        "body": "Add the config `logfile` to the generic config infra.\r\nBy initiating the generic configs early we make sure their default values are set from start.\r\nSome initiation functions, like `populateCommandTable()` could call `serverPanic()` or `serverAssert()` without an initialized `crash-log-enabled` default set to `1`. \r\n\r\nAlso removed some duplicated initializations that are already done in the generic config infra.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 136,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-10-04T09:48:54Z",
        "closed_at": "2021-10-04T12:17:51Z",
        "merged_at": "2021-10-04T12:17:51Z",
        "body": "When LUA call our C code, by default, the LUA stack has room for 10\r\nelements. In most cases, this is more than enough but sometimes it's not\r\nand the caller must verify the LUA stack size before he pushes elements.\r\n\r\nOn 3 places in the code, there was no verification of the LUA stack size.\r\nOn specific inputs this missing verification could have lead to invalid\r\nmemory write:\r\n1. On 'luaReplyToRedisReply', one might return a nested reply that will\r\n   explode the LUA stack.\r\n2. On 'redisProtocolToLuaType', the Redis reply might be deep enough\r\n   to explode the LUA stack (notice that currently there is no such\r\n   command in Redis that returns such a nested reply, but modules might\r\n   do it)\r\n3. On 'ldbRedis', one might give a command with enough arguments to\r\n   explode the LUA stack (all the arguments will be pushed to the LUA\r\n   stack)\r\n\r\nThis commit is solving all those 3 issues by calling 'lua_checkstack' and\r\nverify that there is enough room in the LUA stack to push elements. In\r\ncase 'lua_checkstack' returns an error (there is not enough room in the\r\nLUA stack and it's not possible to increase the stack), we will do the\r\nfollowing:\r\n1. On 'luaReplyToRedisReply', we will return an error to the user.\r\n2. On 'redisProtocolToLuaType' we will exit with panic (we assume this\r\n   scenario is rare because it can only happen with a module).\r\n3. On 'ldbRedis', we return an error.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2021-10-04T07:30:23Z",
        "closed_at": "2021-10-04T09:14:13Z",
        "merged_at": "2021-10-04T09:14:13Z",
        "body": "The protocol parsing on 'ldbReplParseCommand' (LUA debugging)\r\nAssumed protocol correctness. This means that if the following\r\nis given:\r\n*1\r\n$100\r\ntest\r\nThe parser will try to read additional 94 unallocated bytes after\r\nthe client buffer.\r\nThis commit fixes this issue by validating that there are actually enough\r\nbytes to read. It also limits the amount of data that can be sent by\r\nthe debugger client to 1M so the client will not be able to explode\r\nthe memory.\r\n\r\nFound and fixed by @MeirShpilraien ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 352,
        "deletions": 61,
        "changed_files": 14,
        "created_at": "2021-10-04T07:29:08Z",
        "closed_at": "2021-10-04T09:11:03Z",
        "merged_at": "2021-10-04T09:11:03Z",
        "body": "- fix possible heap corruption in ziplist and listpack resulting by trying to\r\n  allocate more than the maximum size of 4GB.\r\n- prevent ziplist (hash and zset) from reaching size of above 1GB, will be\r\n  converted to HT encoding, that's not a useful size.\r\n- prevent listpack (stream) from reaching size of above 1GB.\r\n- XADD will start a new listpack if the new record may cause the previous\r\n  listpack to grow over 1GB.\r\n- XADD will respond with an error if a single stream record is over 1GB\r\n- List type (ziplist in quicklist) was truncating strings that were over 4GB,\r\n  now it'll respond with an error.\r\n\r\nCo-authored-by: sundb <sundbcn@gmail.com>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 7,
        "changed_files": 4,
        "created_at": "2021-10-04T07:25:13Z",
        "closed_at": "2021-10-04T09:10:31Z",
        "merged_at": "2021-10-04T09:10:31Z",
        "body": "This change sets a low limit for multibulk and bulk length in the\r\nprotocol for unauthenticated connections, so that they can't easily\r\ncause redis to allocate massive amounts of memory by sending just a few\r\ncharacters on the network.\r\nThe new limits are 10 arguments of 16kb each (instead of 1m of 512mb)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-10-04T07:23:23Z",
        "closed_at": "2021-10-04T09:10:18Z",
        "merged_at": "2021-10-04T09:10:17Z",
        "body": "The redis-cli command line tool and redis-sentinel service may be vulnerable\r\nto integer overflow when parsing specially crafted large multi-bulk network\r\nreplies. This is a result of a vulnerability in the underlying hiredis\r\nlibrary which does not perform an overflow check before calling the calloc()\r\nheap allocation function.\r\n\r\nThis issue only impacts systems with heap allocators that do not perform their\r\nown overflow checks. Most modern systems do and are therefore not likely to\r\nbe affected. Furthermore, by default redis-sentinel uses the jemalloc allocator\r\nwhich is also not vulnerable.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2021-10-04T07:20:30Z",
        "closed_at": "2021-10-04T09:09:26Z",
        "merged_at": "2021-10-04T09:09:26Z",
        "body": "The vulnerability involves changing the default set-max-intset-entries\r\nconfiguration parameter to a very large value and constructing specially\r\ncrafted commands to manipulate sets",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 496,
        "deletions": 59,
        "changed_files": 24,
        "created_at": "2021-10-04T07:11:39Z",
        "closed_at": "2021-10-04T10:58:44Z",
        "merged_at": "2021-10-04T10:58:44Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2021-41099) Integer to heap buffer overflow handling certain string\r\n  commands and network payloads, when proto-max-bulk-len is manually configured\r\n  to a non-default, very large value [reported by yiyuaner].\r\n* (CVE-2021-32762) Integer to heap buffer overflow issue in redis-cli and\r\n  redis-sentinel parsing large multi-bulk replies on some older and less common\r\n  platforms [reported by Microsoft Vulnerability Research].\r\n* (CVE-2021-32687) Integer to heap buffer overflow with intsets, when\r\n  set-max-intset-entries is manually configured to a non-default, very large\r\n  value [reported by Pawel Wieczorkiewicz, AWS].\r\n* (CVE-2021-32675) Denial Of Service when processing RESP request payloads with\r\n  a large number of elements on many connections.\r\n* (CVE-2021-32672) Random heap reading issue with Lua Debugger [reported by\r\n  Meir Shpilraien].\r\n* (CVE-2021-32628) Integer to heap buffer overflow handling ziplist-encoded\r\n  data types, when configuring a large, non-default value for\r\n  hash-max-ziplist-entries, hash-max-ziplist-value, zset-max-ziplist-entries\r\n  or zset-max-ziplist-value [reported by sundb].\r\n* (CVE-2021-32627) Integer to heap buffer overflow issue with streams, when\r\n  configuring a non-default, large value for proto-max-bulk-len and\r\n  client-query-buffer-limit [reported by sundb].\r\n* (CVE-2021-32626) Specially crafted Lua scripts may result with Heap buffer\r\n  overflow [reported by Meir Shpilraien].",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 591,
        "deletions": 86,
        "changed_files": 29,
        "created_at": "2021-10-04T07:09:47Z",
        "closed_at": "2021-10-04T10:59:20Z",
        "merged_at": "2021-10-04T10:59:20Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2021-41099) Integer to heap buffer overflow handling certain string\r\n  commands and network payloads, when proto-max-bulk-len is manually configured\r\n  to a non-default, very large value [reported by yiyuaner].\r\n* (CVE-2021-32762) Integer to heap buffer overflow issue in redis-cli and\r\n  redis-sentinel parsing large multi-bulk replies on some older and less common\r\n  platforms [reported by Microsoft Vulnerability Research].\r\n* (CVE-2021-32687) Integer to heap buffer overflow with intsets, when\r\n  set-max-intset-entries is manually configured to a non-default, very large\r\n  value [reported by Pawel Wieczorkiewicz, AWS].\r\n* (CVE-2021-32675) Denial Of Service when processing RESP request payloads with\r\n  a large number of elements on many connections.\r\n* (CVE-2021-32672) Random heap reading issue with Lua Debugger [reported by\r\n  Meir Shpilraien].\r\n* (CVE-2021-32628) Integer to heap buffer overflow handling ziplist-encoded\r\n  data types, when configuring a large, non-default value for\r\n  hash-max-ziplist-entries, hash-max-ziplist-value, zset-max-ziplist-entries\r\n  or zset-max-ziplist-value [reported by sundb].\r\n* (CVE-2021-32627) Integer to heap buffer overflow issue with streams, when\r\n  configuring a non-default, large value for proto-max-bulk-len and\r\n  client-query-buffer-limit [reported by sundb].\r\n* (CVE-2021-32626) Specially crafted Lua scripts may result with Heap buffer\r\n  overflow [reported by Meir Shpilraien].\r\n\r\nOther bug fixes:\r\n* Fix appendfsync to always guarantee fsync before reply, on MacOS and FreeBSD (kqueue) (#9416)\r\n* Fix the wrong mis-detection of sync_file_range system call, affecting performance (#9371)\r\n* Fix replication issues when repl-diskless-load is used (#9280)\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1423,
        "deletions": 229,
        "changed_files": 53,
        "created_at": "2021-10-04T07:05:47Z",
        "closed_at": "2021-10-04T10:59:40Z",
        "merged_at": "2021-10-04T10:59:40Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues.\r\n\r\nSecurity Fixes:\r\n* (CVE-2021-41099) Integer to heap buffer overflow handling certain string\r\n  commands and network payloads, when proto-max-bulk-len is manually configured\r\n  to a non-default, very large value [reported by yiyuaner].\r\n* (CVE-2021-32762) Integer to heap buffer overflow issue in redis-cli and\r\n  redis-sentinel parsing large multi-bulk replies on some older and less common\r\n  platforms [reported by Microsoft Vulnerability Research].\r\n* (CVE-2021-32687) Integer to heap buffer overflow with intsets, when\r\n  set-max-intset-entries is manually configured to a non-default, very large\r\n  value [reported by Pawel Wieczorkiewicz, AWS].\r\n* (CVE-2021-32675) Denial Of Service when processing RESP request payloads with\r\n  a large number of elements on many connections.\r\n* (CVE-2021-32672) Random heap reading issue with Lua Debugger [reported by\r\n  Meir Shpilraien].\r\n* (CVE-2021-32628) Integer to heap buffer overflow handling ziplist-encoded\r\n  data types, when configuring a large, non-default value for\r\n  hash-max-ziplist-entries, hash-max-ziplist-value, zset-max-ziplist-entries\r\n  or zset-max-ziplist-value [reported by sundb].\r\n* (CVE-2021-32627) Integer to heap buffer overflow issue with streams, when\r\n  configuring a non-default, large value for proto-max-bulk-len and\r\n  client-query-buffer-limit [reported by sundb].\r\n* (CVE-2021-32626) Specially crafted Lua scripts may result with Heap buffer\r\n  overflow [reported by Meir Shpilraien].\r\n\r\nBug fixes that involve behavior changes:\r\n* GEO* STORE with empty source key deletes the destination key and return 0 (#9271)\r\n  Previously it would have returned an empty array like the non-STORE variant.\r\n* PUBSUB NUMPAT replies with number of patterns rather than number of subscriptions (#9209)\r\n  This actually changed in 6.2.0 but was overlooked and omitted from the release notes.\r\n\r\nBug fixes that are only applicable to previous releases of Redis 6.2:\r\n* Fix CLIENT PAUSE, used an old timeout from previous PAUSE (#9477)\r\n* Fix CLIENT PAUSE in a replica would mess the replication offset (#9448)\r\n* Add some missing error statistics in INFO errorstats (#9328)\r\n\r\nOther bug fixes:\r\n* Fix incorrect reply of COMMAND command key positions for MIGRATE command (#9455)\r\n* Fix appendfsync to always guarantee fsync before reply, on MacOS and FreeBSD (kqueue) (#9416)\r\n* Fix the wrong mis-detection of sync_file_range system call, affecting performance (#9371)\r\n\r\nCLI tools:\r\n* When redis-cli received ASK response, it didn't handle it (#8930)\r\n\r\nImprovements:\r\n* Add latency monitor sample when key is deleted via lazy expire (#9317)\r\n* Sanitize corrupt payload improvements (#9321, #9399)\r\n* Delete empty keys when loading RDB file or handling a RESTORE command (#9297, #9349)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-10-04T05:59:33Z",
        "closed_at": "2021-10-04T09:17:22Z",
        "merged_at": "2021-10-04T09:17:22Z",
        "body": "Recently merged PR introduced a leak when loading AOF files.\r\nThis was because argv_len wasn't set, so rewriteClientCommandArgument\r\nwould shrink the argv array and updating argc to a small value.\r\n\r\nThe change at the bottom of loadAppendOnlyFile is just cleanup.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 53,
        "deletions": 34,
        "changed_files": 3,
        "created_at": "2021-10-03T12:28:28Z",
        "closed_at": "2021-11-08T02:56:04Z",
        "merged_at": "2021-11-08T02:56:03Z",
        "body": "Currently in cluster `myself->ip` and `myself->flags` are updated by `clusterCron` periodically. \r\nThey can only be modified by `CONFIG SET`. I think it's better that we set them in `CONFIG SET`.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-10-02T10:02:04Z",
        "closed_at": "2021-10-03T06:38:06Z",
        "merged_at": "2021-10-03T06:38:05Z",
        "body": "See #725\r\n\r\nNote that this breaks compatibility because in the past doing:\r\n`DECRBY x -9223372036854775808` \r\nwould succeed (and create an invalid result) and now this returns an error.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 51,
        "changed_files": 2,
        "created_at": "2021-09-30T16:16:18Z",
        "closed_at": "2021-10-01T12:49:33Z",
        "merged_at": "2021-10-01T12:49:33Z",
        "body": "Just a cleanup to make the code easier to maintain and reduce the risk of something being overlooked.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 307,
        "deletions": 154,
        "changed_files": 25,
        "created_at": "2021-09-30T16:14:33Z",
        "closed_at": "2021-11-28T09:26:29Z",
        "merged_at": "2021-11-28T09:26:29Z",
        "body": "Writable replicas now no longer use the values of expired keys. Expired keys are\r\ndeleted when lookupKeyWrite() is used, even on a writable replica. Previously,\r\nwritable replicas could use the value of an expired key in write commands such\r\nas INCR, SUNIONSTORE, etc..\r\n\r\nThis commit also sorts out the mess around the functions lookupKeyRead() and\r\nlookupKeyWrite() so they now indicate what we intend to do with the key and\r\nare not affected by the command calling them.\r\n\r\nMulti-key commands like SUNIONSTORE, ZUNIONSTORE, COPY and SORT with the\r\nstore option now use lookupKeyRead() for the keys they're reading from (which will\r\nnot allow reading from logically expired keys).\r\n\r\nThis commit also fixes a bug where PFCOUNT could return a value of an\r\nexpired key.\r\n\r\nTest modules commands have their readonly and write flags updated to correctly\r\nreflect their lookups for reading or writing. Modules are not required to\r\ncorrectly reflect this in their command flags, but this change is made for\r\nconsistency since the tests serve as usage examples.\r\n\r\nFixes #6842. Fixes #7475.",
        "comments": 41
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2021-09-29T16:32:55Z",
        "closed_at": "2021-09-30T12:51:19Z",
        "merged_at": "2021-09-30T12:51:19Z",
        "body": "While the original issue was on Linux, this should work for other\r\nplatforms as well.\r\n\r\nFixes #9565",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 220,
        "deletions": 87,
        "changed_files": 9,
        "created_at": "2021-09-29T14:16:11Z",
        "closed_at": "2023-06-18T04:16:52Z",
        "merged_at": "2023-06-18T04:16:52Z",
        "body": "This PR adds the a human readable name to a node in clusters that are visible as part of error logs. This is useful so that admins and operators of Redis cluster have better visibility into failures without having to cross-reference the generated ID with some logical identifier (such as pod-ID or EC2 instance ID). This is mentioned in https://github.com/redis/redis/issues/8948.  Specific nodenames can be set by using the variable `cluster-announce-human-nodename`. The nodename is gossiped using the clusterbus extension in https://github.com/redis/redis/pull/9530.\r\n\r\n### Dependent PRs\r\nPR https://github.com/redis/redis/pull/9530 is done\r\nPR https://github.com/redis/redis/pull/10536 is done\r\n\r\n```\r\nRelease notes\r\nInclude a new configuration, `cluster-announce-human-nodename`, which allows configuring a unique identifier for a node that is be used in logs for debugging.\r\n```\r\n",
        "comments": 26
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-29T12:05:25Z",
        "closed_at": "2021-09-29T14:10:06Z",
        "merged_at": "2021-09-29T14:10:06Z",
        "body": "See https://github.com/redis/redis/runs/3726678420?check_suite_focus=true\r\n```\r\n*** [err]: eviction due to output buffers of pubsub, client eviction: false in tests/unit/maxmemory.tcl\r\nExpected '0' to be more than '0' (context: type source line 41 file /__w/redis/redis/tests/unit/maxmemory.tcl cmd {assert_moret\r\n```\r\nFor some reason the maxmemory pubsub obuf publish tests which should fill the output buffer and cause key eviction fails sometimes with TLS turned on. Added debug print to verify `publish` commands aren't failing.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-09-29T11:58:08Z",
        "closed_at": "2021-10-04T07:32:26Z",
        "merged_at": "2021-10-04T07:32:26Z",
        "body": "Since we measure the COW size in this test by changing some keys and reading\r\nthe reported COW size, we need to ensure that the \"dismiss mechanism\" (#8974)\r\nwill not free memory and reduce the COW size.\r\n\r\nFor that, this commit changes the size of the keys to 512B (less than a page).\r\nand because some keys may fall into the same page, we are modifying ten keys\r\non each iteration and check for at least 50% change in the COW size.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-09-27T06:13:51Z",
        "closed_at": "2021-10-04T08:11:10Z",
        "merged_at": "2021-10-04T08:11:10Z",
        "body": "An integer overflow bug in the underlying string library can be used to corrupt the heap and potentially result with denial of service or remote code execution.\r\n\r\nThe vulnerability involves changing the default proto-max-bulk-len configuration parameter to a very large value and constructing specially crafted network payloads or commands.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-09-26T07:16:43Z",
        "closed_at": "2021-09-26T15:46:23Z",
        "merged_at": "2021-09-26T15:46:23Z",
        "body": "This was recently broken in #9321 when we validated stream IDs to be\r\nintegers but did that after to the stepping next record instead of before.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 27,
        "changed_files": 9,
        "created_at": "2021-09-25T12:05:57Z",
        "closed_at": "2021-09-26T14:45:02Z",
        "merged_at": "2021-09-26T14:45:02Z",
        "body": "Fixing CI test issues introduced in #8687\r\n- valgrind warnings in readQueryFromClient when client was freed by processInputBuffer\r\n- adding `DEBUG pause-cron [1|0]` for tests not to be time dependent.\r\n- skipping a test that depends on socket buffers / events not compatible with TLS\r\n- making sure client got subscribed before publishing to it by not using deferring client",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-24T17:33:05Z",
        "closed_at": "2021-09-24T19:03:19Z",
        "merged_at": "2021-09-24T19:03:19Z",
        "body": "Hostname is freed here, expects a sds string : \r\n\r\nhttps://github.com/redis/redis/blob/9967a53f4c8e9f50a4e018aee469bf8c5a4647e9/src/redis-benchmark.c#L2010\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-09-24T16:35:16Z",
        "closed_at": "2021-10-15T05:44:25Z",
        "merged_at": "2021-10-15T05:44:25Z",
        "body": "On macOS and iOS, fsync() doesn't guarantee durability past powerfailures.\r\nHere are the description of fsync at macos:\r\n\r\nSpecifically, if the drive loses power or the OS crashes, the application may find that only some or none of their data was \r\nwritten.  The disk drive may also re-order the data so that later writes may be present, while earlier writes are not.\r\nThis is not a theoretical edge case.  This scenario is easily reproduced with real world workloads and drive power failures.\r\n\r\nFor applications that require tighter guarantees about the integrity of their data, Mac OS X provides the F_FULLFSYNC fcntl.  The F_FULLFSYNC fcntl asks the drive to flush all buffered data to permanent storage.  Applications, such as databases, that require a strict orderingof writes should use F_FULLFSYNC to ensure that their data is written in the order they expect.  Please see fcntl(2) for more detail.\r\n\r\nLinks: https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man2/fsync.2.html\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-23T14:28:35Z",
        "closed_at": "2021-09-27T06:51:33Z",
        "merged_at": "2021-09-27T06:51:33Z",
        "body": "Starting from version 11, Clang enables -fno-common by default : https://releases.llvm.org/11.0.0/tools/clang/docs/ReleaseNotes.html#modified-compiler-flags\r\n\r\n So, \"\\_\\_common\\_\\_\" attribute can be used for Clang C builds.\r\nAs Clang C++ does not allow this attribute, I added a check to enable attribute only for Clang C builds. ",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2021-09-23T10:31:02Z",
        "closed_at": "2021-09-23T12:00:37Z",
        "merged_at": "2021-09-23T12:00:37Z",
        "body": "This commit makes it possible to explicitly trim the allocation of a\r\nRedisModuleString.\r\n\r\nCurrently, Redis automatically trims strings that have been retained by\r\na module command when it returns. However, this is not thread safe and\r\nmay result with corruption in threaded modules.\r\n\r\nSupporting explicit trimming offers a backwards compatible workaround to\r\nthis problem.\r\n\r\nAddresses the issue discussed in #5834.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 484,
        "deletions": 544,
        "changed_files": 14,
        "created_at": "2021-09-23T01:06:39Z",
        "closed_at": "2021-11-16T15:24:59Z",
        "merged_at": null,
        "body": "Hi, I implemented a new AOF persistence mechanism (currently it is still in the draft stage). Its main purpose is to remove the AOF rewrite buffer, which can save memory overhead during rewrite and shorten the time consumption of rewrite.\r\n\r\nI call it the `ping-pong model` for the time being. The reason why I call it the `ping-pong model` is because it is similar to the `ping-pong buffer`. When one buffer is full, I can continue to write another buffer, and then exchange the two buffer without interrupting the entire processing flow.\r\n\r\nI divide AOF into four types (BASE, PING, PONG, TEMP). The AOF generated after rewrite is called BASE type AOF (configured by `appendfilename` configuration item), which is similar to the rdb file and represents a certain one Snapshot of redis data at the moment. That is, the AOF of the BASE type will not write incremental commands.\r\n\r\nNext is the PING type AOF (configured by the configuration item `aof-ping-filename` configuration item), which is basically the same as the current AOF concept, mainly used to store the incremental commands written, when the AOF persistence function is turned on at the time, the executed commands will be written into this type AOF one by one.\r\n\r\nWhen bgrewriteaof is triggered, a new AOF will be generated at this time, which is a PONG type AOF, which will continue to store the user's incremental commands. At the same time, the original PING type AOF will no longer write any data.\r\n\r\nWhen the child process completes the rewrite task (`temp-rewriteaof-bg.aof` is generated), in the backgroundRewriteDoneHandler, I will rename the `temp-rewriteaof-bg.aof` file to a new BASE type AOF, and at the same time, I will rename the PONG type AOF to  PING type. At this time, BASE AOF and PING AOF together constitute the current full amount of data. It can be seen that in the entire rewrite process, no rewrite buffer is used, and no data is exchanged between the parent process and the child process.\r\n\r\n![image](https://user-images.githubusercontent.com/13696140/134442429-1dde94a0-ae49-42a6-97a8-7a939425e293.png)\r\n\r\n\r\nThe above is the success of rewrite. If the rewrite fails, we will eventually have three AOF files at the same time, namely BASE, PING and PONG, which together constitute the current full amount of data. This means that these AOF files need to be loaded in turn when redis restarts. Even worse, if bgrewrite occurs again at this time, we can no longer generate PONG2, PONG3 and other files without limitation, otherwise we will not know which files we need and their order.\r\n\r\nTherefore, for the small probability event of rewrite failure, we will generate a new AOF file, which we temporarily call `temp.aof`. The next incremental command will be \"simultaneously\" double written to PONG type AOF and `temp.aof`. If this rewrite is successful, rename `temp.aof` to  PING type AOF, and both the original PING and PONG type AOFs can be deleted. If this rewrite still fails, just delete `temp.aof`, the original BASE/PING/PONG still constitute the complete data.\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/13696140/134441895-e2c4bcef-38f3-42e7-8624-f415c923d110.png)\r\n\r\n\r\nCurrently, this solution seems to have many problems (although the tests have already been run). For example, how to ensure the atomicity of multiple file renames. Since we did not record any meta informations for AOF files, we must use double writing and multiple renames to limit the number of AOF files. In fact, in our internal practice, we designed a meta file for the AOF file(s) to record the information (including the file name and sequence, etc.), so that we do not need to rename the newly generated AOF file. But this scheme is obviously inappropriate in the community, because once relying on the meta file, it means that it is no longer compatible with the previous redis version.\r\n\r\nSo, this is my current thinking, I don\u2019t know if I have expressed it clearly. I think this idea is okay, but we still have some problems to solve (such as the atomic problem of multi-file modification, etc.).  @redis/core-team  Do you have any better suggestions?\r\n\r\nTODO:  \r\n- [x] Remove rewrite buf overhead\uff0cuse ping-pong write models\r\n- [x] Remove the pipe and data exchange between the child process and the parent\r\n- [ ] The atomicity of multi-file modification\r\n- [x] More accurate size statistics for multiple AOFs\r\n- [ ] Separately add some test cases for multi aof, such as double writing\r\n- [ ] Add or modify the corresponding code comment",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2021-09-22T12:39:15Z",
        "closed_at": "2021-09-24T14:36:26Z",
        "merged_at": "2021-09-24T14:36:26Z",
        "body": "In the `HRANDFIELD`, `SRANDMEMBER` and `ZRANDMEMBER` commands,\r\nThere are some strategies that could in some rare cases return an unfair random.\r\nthese cases are where s small dict happens be be hashed unevenly.\r\n\r\nSpecifically when `count*ZRANDMEMBER_SUB_STRATEGY_MUL > size`, using `dictGetRandomKey` to randomize from a dict will result in an unfair random result.",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 15,
        "changed_files": 4,
        "created_at": "2021-09-22T11:17:18Z",
        "closed_at": "2021-09-30T08:21:33Z",
        "merged_at": "2021-09-30T08:21:33Z",
        "body": "- adding an advanced api to enable loading data that was sereialized with a specific encoding version",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-22T08:23:25Z",
        "closed_at": "2021-10-06T08:50:48Z",
        "merged_at": "2021-10-06T08:50:48Z",
        "body": "Flush db and *then* wait for the bgsave to complete.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2021-09-22T01:12:18Z",
        "closed_at": "2021-09-23T14:12:11Z",
        "merged_at": "2021-09-23T14:12:11Z",
        "body": "When `server.maxmemory` is not set, return ASAP. We don't need to count AOF and replicas buffer.",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2021-09-21T17:52:23Z",
        "closed_at": "2021-09-29T04:10:33Z",
        "merged_at": "2021-09-29T04:10:33Z",
        "body": "This PR adds the missing error checks for fstat in cluster load and save config functions, and it uses redis_fstat instead of fstat.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 609,
        "deletions": 50,
        "changed_files": 10,
        "created_at": "2021-09-21T02:19:34Z",
        "closed_at": "2022-01-03T03:48:30Z",
        "merged_at": "2022-01-03T03:48:30Z",
        "body": "This PR introduces two changes, it introduces a clusterbus extension system so that we can add additional metadata and then uses that to add hostname support. I've been very slow, and it's been sitting on my laptop for awhile, so would rather publish it with context before I get hit by a bus. I will try to iterate more this week and get the code into a better shape, but would appreciate input from @redis/core-team / @ShooterIT / @zuiderkwast.\r\n\r\n### Clusterbus extension\r\nNow we can send extra metadata after the end of the gossip information. This is a backwards compatible change, in that you can't use it between nodes of the same cluster version, but you can upgrade all the nodes in a cluster to support it and then start using it. The point of this is we want to send a consistent version of the cluster mode state along with the message, instead of introducing a separate type of message.\r\n\r\nAn alternative to this would be to add a new type of message, a hostname message. The reason I don't want to introduce this is that it adds yet another way to propagate information throughout the cluster, and introduces periods of time where one of the messages ( the MEET for example) was received but we still don't know the clusters hostname, so we might have to show the IP to incoming clients. A new message also introduces more overhead.\r\n\r\nAnother use of this extension is that I want to add display names/context names, that can be printed in place of the nodeID (the 40 character hex blob). When debugging, the 40 character hex blob is really annoying.\r\n\r\n### Hostname support\r\nI've added a new config, \"cluster-announce-hostname\", which is a hostname that an externally facing client can use to connect to this node. Using the new mechanism we will send an hostname extension to all nodes, so that eventually all nodes in the cluster will know our hostname. NOTE: This is not gossiped, we don't tell other nodes about other nodes hostname's, this is just to reduce message volume. NOTE: Nodes do not talk to each other with the hostname.\r\n\r\nYou can also add a hostname to a node in existing cluster, and it will be eventually propagated to all nodes.\r\n\r\nThis hostname will be added as the 4th field to the `CLUSTER SLOTS` output which is the primary way clients will discover it. I'm also proposing we introduce a \"cluster-preferred-endpoint-type\" option to configure what type of endpoint is shown by default.\r\n\r\nThe hostname will be committed to the cluster nodes file, appended on the end of IP/port/cport information. I think it was a done in a way that supports clients, and it's actually easier to place there then to throw it at the end of the line as a positional argument.\r\n\r\n### Considerations\r\n1. `CLUSTER SLOTS` will be considered as a first class citizen, but `CLUSTER NODES` will be able to support it if clients want to do special work.  Right now I am adding the hostname into the cluster nodes file, so that it's loaded on restart, but not considering making it terribly easy to parse. However, I know some clients try to use that to discover the topology, but I don't want to try to do anything special for them with regards to hostname support. A follow up item will be to expose a variant of `CLUSTER NODES` that is more client friendly.\r\n2. Extensions are only added to PING/PONG/MEET right now, but there is nothing blocking future implementation work to add them for other messages.\r\n\r\nOut of scope:\r\n- Intra-node DNS resolution, all of the Redis cluster nodes should be in the same network (or I haven't heard a reason why to do that otherwise) so all of the connection establishment is still done through IP.\r\n- Doing TLS verification between nodes within the cluster based off of the hostname. This might be a useful verification, but I'm not convinced as of right now.\r\n\r\nTasks punted to other PRs:\r\n- [ ] All of the tooling should support DNS resolution, especially for redis-cli --cluster stuff. Not a critical requirement for the main release.\r\n- [ ] There was a follow up ask to make a version of CLUSTER NODES that is more human readable, like `CLUSTER HEALTH` or `CLUSTER STATUS`. It should be able to show the hostnames, but not necessarily be used by clients. \r\n- [ ] A note to myself, a lot of the tests set up non-contiguous  slots, which makes `CLUSTER SLOTS` really slow. Might want to optimize this in tests.",
        "comments": 48
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-09-20T19:18:29Z",
        "closed_at": "2021-09-21T17:57:52Z",
        "merged_at": null,
        "body": "This PR is to add the hostname feature for clusters as mentioned in https://github.com/redis/redis/issues/8948. This feature is similar to the hostname feature implemented in sentinel.c. This is a work in progress, any comments/suggestions are welcome.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 8,
        "changed_files": 3,
        "created_at": "2021-09-20T09:17:06Z",
        "closed_at": "2021-10-03T06:13:10Z",
        "merged_at": "2021-10-03T06:13:09Z",
        "body": "Remove hard coded multi-bulk limit (was 1,048,576), new limit is INT_MAX.\r\nWhen client sends an m-bulk that's higher than 1024, we initially only allocate\r\nthe argv array for 1024 arguments, and gradually grow that allocation as arguments\r\nare received.\r\n\r\nSee #9023",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-09-20T07:20:14Z",
        "closed_at": "2021-09-22T06:48:44Z",
        "merged_at": "2021-09-22T06:48:44Z",
        "body": "i've seen this CI failure a couple of times on MacOS:\r\n```\r\n*** [err]: lazy free a stream with all types of metadata in tests/unit/lazyfree.tcl\r\nlazyfree isn't done\r\n```\r\nonly reason i can think of is that 500ms is sometimes not enough on slow systems.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2021-09-20T06:13:46Z",
        "closed_at": "2022-06-14T04:32:43Z",
        "merged_at": "2022-06-14T04:32:43Z",
        "body": "During a slot migration, a command with multiple keys should behave in one of three ways:\r\n1. All keys are on the node, the node can serve the command.\r\n2. None of the keys are on the node, if it's the migrating node it will return -ASK so the client sends the same command to the importing node. On the importing node, it executes the command and returns nothing.\r\n3. If some keys are present, returns TRYAGAIN. We can't serve this command since keys are split between multiple nodes.\r\n\r\nBefore this commit, when a node is in migrating state and receives multiple keys command, if some keys don't exist, the command emits an `ASK` redirection.\r\n\r\nAfter this commit, if some keys exist and some keys don't exist, the command emits a TRYAGAIN error. If all keys don't exist, the command emits an `ASK` redirection. This optimizes for a single network hop, since if we have some of the keys the target will most likely also throw a try again, so we should just return it ASAP.",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-09-19T03:43:33Z",
        "closed_at": "2021-09-20T16:30:23Z",
        "merged_at": "2021-09-20T16:30:22Z",
        "body": "Replace redis-trib to redis-cli in comment  since it is no longer available",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-09-17T06:45:03Z",
        "closed_at": "2021-09-30T13:28:26Z",
        "merged_at": "2021-09-30T13:28:26Z",
        "body": "Fixing wrong documentation for replica-serve-stale-data in redis.conf.\r\nError message didn't match with actual one returned.\r\nReported in https://github.com/redis/redis/issues/3131\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-09-16T18:02:39Z",
        "closed_at": "2021-09-20T16:32:33Z",
        "merged_at": "2021-09-20T16:32:33Z",
        "body": "Because the string literals are pass as parameters in the function test_is_selected and benchmark \r\nstring constant type should be used in both two places.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-16T11:00:51Z",
        "closed_at": "2021-09-19T09:07:05Z",
        "merged_at": "2021-09-19T09:07:05Z",
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 73,
        "deletions": 23,
        "changed_files": 7,
        "created_at": "2021-09-16T08:30:54Z",
        "closed_at": "2021-10-06T13:08:14Z",
        "merged_at": "2021-10-06T13:08:14Z",
        "body": "Implement createPipe() to combine creating pipe and setting flags, also reduce system calls by prioritizing `pipe2()` over `pipe()`.\r\n\r\nWithout `createPipe()`, we have to call `pipe()` to create a pipe and then call some functions (like anetCloexec() and anetNonBlock()) of `anet.c` to set flags respectively, which leads to some extra system calls, now we can leverage `pipe2()` to combine them and make the process of creating pipe more convergent in `createPipe()`.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-16T02:01:14Z",
        "closed_at": "2021-09-26T07:24:52Z",
        "merged_at": "2021-09-26T07:24:52Z",
        "body": "When redis executes `loadDataFromDisk` after startup, since `server.aof_fd` is still -1 at this time,`aofUpdateCurrentSize` will not be able to get the loaded AOF size and will report the following error. I think in this case we can try to use `open` to get the real size.\r\n\r\n57831:M 16 Sep 2021 09:37:25.907 # Server initialized\r\n57831:M 16 Sep 2021 09:37:25.907 # **Unable to obtain the AOF file length. stat: Bad file descriptor**\r\n57831:M 16 Sep 2021 09:37:25.907 * DB loaded from append only file: 0.000 seconds\r\n57831:M 16 Sep 2021 09:37:25.907 * Ready to accept connections",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-09-15T20:03:42Z",
        "closed_at": "2021-09-19T09:13:46Z",
        "merged_at": "2021-09-19T09:13:46Z",
        "body": "![image](https://user-images.githubusercontent.com/51993843/133501565-162be231-7cdd-46d7-9bf3-f4f219fe853a.png)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1497,
        "deletions": 409,
        "changed_files": 25,
        "created_at": "2021-09-15T10:17:40Z",
        "closed_at": "2021-10-20T08:52:57Z",
        "merged_at": "2021-10-20T08:52:57Z",
        "body": "## Intro\r\n\r\nThe purpose is to allow having different flags/ACL categories for\r\nsubcommands (Example: CONFIG GET is ok-loading but CONFIG SET isn't)\r\n\r\nWe create a small command table for every command that has subcommands\r\nand each subcommand has its own flags, etc. (same as a \"regular\" command)\r\n\r\nThis commit also unites the Redis and the Sentinel command tables\r\n\r\n## Affected commands\r\n\r\nCONFIG\r\nUsed to have \"admin ok-loading ok-stale no-script\"\r\nChanges:\r\n1. Dropped \"ok-loading\" in all except GET (this doesn't change behavior since there were checks in the code doing that)\r\n\r\nXINFO\r\nUsed to have \"read-only random\"\r\nChanges:\r\n1. Dropped \"random\" in all except CONSUMERS\r\n\r\nXGROUP\r\nUsed to have \"write use-memory\"\r\nChanges:\r\n1. Dropped \"use-memory\" in all except CREATE and CREATECONSUMER\r\n\r\nCOMMAND\r\nNo changes.\r\n\r\nMEMORY\r\nUsed to have \"random read-only\"\r\nChanges:\r\n1. Dropped \"random\" in PURGE and USAGE\r\n\r\nACL\r\nUsed to have \"admin no-script ok-loading ok-stale\"\r\nChanges:\r\n1. Dropped \"admin\" in WHOAMI, GENPASS, and CAT\r\n\r\nLATENCY\r\nNo changes.\r\n\r\nMODULE\r\nNo changes.\r\n\r\nSLOWLOG\r\nUsed to have \"admin random ok-loading ok-stale\"\r\nChanges:\r\n1. Dropped \"random\" in RESET\r\n\r\nOBJECT\r\nUsed to have \"read-only random\"\r\nChanges:\r\n1. Dropped \"random\" in ENCODING and REFCOUNT\r\n\r\nSCRIPT\r\nUsed to have \"may-replicate no-script\"\r\nChanges:\r\n1. Dropped \"may-replicate\" in all except FLUSH and LOAD\r\n\r\nCLIENT\r\nUsed to have \"admin no-script random ok-loading ok-stale\"\r\nChanges:\r\n1. Dropped \"random\" in all except INFO and LIST\r\n2. Dropped \"admin\" in ID, TRACKING, TRACKINGINFO, CACHING, GETREDIR, INFO, SETNAME, GETNAME, and REPLY\r\n\r\nSTRALGO\r\nNo changes.\r\n\r\nPUBSUB\r\nNo changes.\r\n\r\nCLUSTER\r\nChanges:\r\n1. Dropped \"admin in countkeysinslots, getkeysinslot, info, nodes, keyslot, myid, and slots\r\n\r\nSENTINEL\r\nNo changes.\r\n\r\n(note that DEBUG also fits, but we decided not to convert it since it's for\r\ndebugging and anyway undocumented)\r\n\r\n## New sub-command\r\nThis commit adds another element to the per-command output of COMMAND,\r\ndescribing the list of subcommands, if any (in the same structure as \"regular\" commands)\r\nAlso, it adds a new subcommand:\r\n```\r\nCOMMAND LIST [FILTERBY (MODULE <module-name>|ACLCAT <cat>|PATTERN <pattern>)]\r\n```\r\nwhich returns a set of all commands (unless filters), but excluding subcommands.\r\n\r\n## Module API\r\nA new module API, RM_CreateSubcommand, was added, in order to allow\r\nmodule writer to define subcommands\r\n\r\n## ACL changes:\r\n1. Now, that each subcommand is actually a command, each has its own ACL id.\r\n2. The old mechanism of allowed_subcommands is redundant\r\n(blocking/allowing a subcommand is the same as blocking/allowing a regular command),\r\nbut we had to keep it, to support the widespread usage of allowed_subcommands\r\nto block commands with certain args, that aren't subcommands (e.g. \"-select +select|0\").\r\n3. I have renamed allowed_subcommands to allowed_firstargs to emphasize the difference.\r\n4. Because subcommands are commands in ACL too, you can now use \"-\" to block subcommands\r\n(e.g. \"+client -client|kill\"), which wasn't possible in the past.\r\n5. It is also possible to use the allowed_firstargs mechanism with subcommand.\r\nFor example: `+config -config|set +config|set|loglevel` will block all CONFIG SET except\r\nfor setting the log level.\r\n  NOTE: this capability was later removed in #10147\r\n6. All of the ACL changes above required some amount of refactoring.\r\n\r\n## Misc\r\n1. There are two approaches: Either each subcommand has its own function or all\r\n   subcommands use the same function, determining what to do according to argv[0].\r\n   For now, I took the former approaches only with CONFIG and COMMAND,\r\n   while other commands use the latter approach (for smaller blamelog diff).\r\n2. Deleted memoryGetKeys: It is no longer needed because MEMORY USAGE now uses the \"range\" key spec.\r\n4. Bugfix: GETNAME was missing from CLIENT's help message.\r\n5. Sentinel and Redis now use the same table, with the same function pointer.\r\n   Some commands have a different implementation in Sentinel, so we redirect\r\n   them (these are ROLE, PUBLISH, and INFO).\r\n6. Command stats now show the stats per subcommand (e.g. instead of stats just\r\n   for \"config\" you will have stats for \"config|set\", \"config|get\", etc.)\r\n7. It is now possible to use COMMAND directly on subcommands:\r\n   COMMAND INFO CONFIG|GET (The pipeline syntax was inspired from ACL, and\r\n   can be used in functions lookupCommandBySds and lookupCommandByCString)\r\n8. STRALGO is now a container command (has \"help\")\r\n\r\n## Breaking changes:\r\n1. Command stats now show the stats per subcommand (see (5) above)\r\n\r\n## TODO\r\n- [x] need to update the acl topic in redis.io (both the categories and the sub-commands) - with some note about redis 7 and up\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 75,
        "changed_files": 10,
        "created_at": "2021-09-15T08:26:34Z",
        "closed_at": "2021-09-15T09:53:42Z",
        "merged_at": "2021-09-15T09:53:42Z",
        "body": "This is a prepartion for a future commit that treats\r\nsubcommands as commands",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2021-09-14T18:17:25Z",
        "closed_at": "2021-09-15T10:39:50Z",
        "merged_at": "2021-09-15T10:39:50Z",
        "body": "This fix is for the failing daily CI. The problem is that due to the race condition, the tests were failing in the slower systems due to which we had to adjust the sentinel ping and info period times to accomodate to those systems. ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2021-09-14T16:55:57Z",
        "closed_at": "2021-09-15T10:04:32Z",
        "merged_at": "2021-09-15T10:04:32Z",
        "body": "[root@ycz redis-unstable]# echo 'puts $tcl_version;exit 0' | tclsh\r\n8.7\r\n[root@ycz redis-unstable]# make test-modules\r\ncd src && make test-modules   \r\nmake[1]: Entering directory `/root/redis-unstable/src`   \r\nYou need tcl 8.5 or newer in order to run the Redis ModuleApi test   \r\nmake[1]: *** [test-modules] Error 1   \r\nmake[1]: Leaving directory `/root/redis-unstable/src`  \r\nmake: *** [test-modules] Error 2\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-14T10:51:47Z",
        "closed_at": "2021-09-14T12:14:09Z",
        "merged_at": "2021-09-14T12:14:09Z",
        "body": "Before #9497, before redis-server was shut down, we did not manually shut down all the clients, which would have prevented valgrind from detecting a memory leak in the client's argc.\r\n\r\nThis was exposed by the following test failure: https://github.com/redis/redis/runs/3593238549?check_suite_focus=true",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-09-13T13:59:13Z",
        "closed_at": "2021-09-13T15:16:47Z",
        "merged_at": "2021-09-13T15:16:47Z",
        "body": "To recreate the issue see: https://gist.github.com/yoav-steinberg/e6d48ff1a2958da5338368c309b53e2f\r\nThe fix:\r\n* On `kill_server` make sure we close the default `\"client\"` connection.\r\n* Don't reconnect when trying to execute the client's `close` command.\r\n* On `restart_server` make sure to remove the (closed) default `\"client\"` after killing the old server.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-12T12:54:23Z",
        "closed_at": "2022-02-06T05:04:44Z",
        "merged_at": null,
        "body": "int dictDelete(dict *ht, const void *key)=>int dictDelete(dict *d, const void *key)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-12T11:46:30Z",
        "closed_at": "2021-09-14T10:37:37Z",
        "merged_at": null,
        "body": "dbUnshareStringValue doesn't intend to replace the existing key with\r\na new one. So it should not call any module hooks, keyspace\r\nnotifications, and so on.\r\n\r\nSee https://github.com/redis/redis/pull/9406",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-10T15:13:30Z",
        "closed_at": "2021-09-11T19:54:09Z",
        "merged_at": "2021-09-11T19:54:09Z",
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 855,
        "deletions": 174,
        "changed_files": 9,
        "created_at": "2021-09-10T09:41:00Z",
        "closed_at": "2021-09-23T05:34:40Z",
        "merged_at": "2021-09-23T05:34:40Z",
        "body": "This is similar to the recent addition of LMPOP/BLMPOP (#9373), but zset.\r\n\r\nSyntax for the new ZMPOP command:\r\n`ZMPOP numkeys [<key> ...] MIN|MAX [COUNT count]`\r\n\r\nSyntax for the new BZMPOP command:\r\n`BZMPOP timeout numkeys [<key> ...] MIN|MAX [COUNT count]`\r\n\r\nSome background:\r\n- ZPOPMIN/ZPOPMAX take only one key, and can return multiple elements.\r\n- BZPOPMIN/BZPOPMAX take multiple keys, but return only one element from just one key.\r\n- ZMPOP/BZMPOP can take multiple keys, and can return multiple elements from just one key.\r\n\r\nNote that ZMPOP/BZMPOP can take multiple keys, it eventually operates on just on key.\r\nAnd it will propagate as ZPOPMIN or ZPOPMAX with the COUNT option.\r\n\r\nAs new commands, if we can not pop any elements, the response like:\r\n- ZMPOP: Return a NIL in both RESP2 and RESP3, unlike ZPOPMIN/ZPOPMAX return emptyarray.\r\n- BZMPOP: Return a NIL in both RESP2 and RESP3 when timeout is reached, like BZPOPMIN/BZPOPMAX.\r\n\r\nFor the normal response is nested arrays in RESP2 and RESP3:\r\n```\r\nZMPOP/BZMPOP\r\n1) keyname\r\n2) 1) 1) member1\r\n      2) score1\r\n   2) 1) member2\r\n      2) score2\r\n\r\nIn RESP2:\r\n1) \"myzset\"\r\n2) 1) 1) \"three\"\r\n      2) \"3\"\r\n   2) 1) \"two\"\r\n      2) \"2\"\r\n\r\nIn RESP3:\r\n1) \"myzset\"\r\n2) 1) 1) \"three\"\r\n      2) (double) 3\r\n   2) 1) \"two\"\r\n      2) (double) 2\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 367,
        "deletions": 6,
        "changed_files": 6,
        "created_at": "2021-09-10T06:03:07Z",
        "closed_at": "2021-10-19T08:50:38Z",
        "merged_at": "2021-10-19T08:50:38Z",
        "body": "Prevent clients from being blocked forever in cluster when they block with their own module command\r\nand the hash slot is migrated to another master at the same time.\r\nThese will get a redirection message when unblocked.\r\nAlso, release clients blocked on module commands when cluster is down (same as other blocked clients)",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-09-09T12:23:56Z",
        "closed_at": "2021-09-10T06:02:46Z",
        "merged_at": "2021-09-10T06:02:46Z",
        "body": null,
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-09-09T11:56:19Z",
        "closed_at": "2021-09-09T13:52:22Z",
        "merged_at": "2021-09-09T13:52:21Z",
        "body": "Then can run 'make test-cluster' like 'make test-sentinel'.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-09-09T08:24:00Z",
        "closed_at": "2021-09-09T10:44:48Z",
        "merged_at": "2021-09-09T10:44:48Z",
        "body": "A write request may be paused unexpectedly because `server.client_pause_end_time` is old.\r\n\r\n**Recreate this:**\r\nredis-cli -p 6379\r\n127.0.0.1:6379> client pause 500000000 write\r\nOK\r\n127.0.0.1:6379> client unpause\r\nOK\r\n127.0.0.1:6379> client pause 10000 write\r\nOK\r\n127.0.0.1:6379> set key value\r\n\r\nThe write request `set key value` is paused util  the timeout of 500000000 milliseconds was reached.\r\n\r\n**Fix:**\r\nreset `server.client_pause_end_time` = 0 in `unpauseClients`\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-08T13:03:15Z",
        "closed_at": "2021-09-09T07:16:31Z",
        "merged_at": "2021-09-09T07:16:31Z",
        "body": "Accommodates https://github.com/redis/redis-doc/pull/1223",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-09-08T00:01:50Z",
        "closed_at": "2021-09-08T20:22:02Z",
        "merged_at": "2021-09-08T20:22:02Z",
        "body": "I think `redismodule.h` needs to include necessary header files for `RedisModule_Assert`, such as stdlib.h where `exit` is located, otherwise a compilation warning or error will be generated\uff08If the module itself does not include this header file\uff09.\r\n\r\n\r\non linux:\r\n```\r\nnote: include \u2018<stdlib.h>\u2019 or provide a declaration of \u2018exit\u2019\r\n #define RedisModule_Assert(_e) ((_e)?(void)0 : (RedisModule__Assert(#_e,__FILE__,__LINE__),exit(1)))\r\n```\r\n\r\non macos:\r\n```\r\nerror: implicitly declaring library function 'exit' with type 'void (int) __attribute__((noreturn))' [-Werror,-Wimplicit-function-declaration]\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 72,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2021-09-07T16:29:37Z",
        "closed_at": "2022-02-06T04:35:07Z",
        "merged_at": null,
        "body": "Hello, I feel the `FAILOVER` and `CLIENT PAUSE/UNPAUSE` may conflict in some scenarios, here are some of my thoughts, please correct me\uff5e\r\n\r\n# Problem\r\n\r\n- **Topo**: master(A) and replica(B)\r\n- **Way**: \r\n    1. A: FAILOVER TO B_IP B_PORT (B is new master)\r\n    2. B: FAILOVER TO A_IP A_PORT(A is new master)\r\n    3. A: CLIENT PAUSE 1000 WRITE(**A will block all write commands until timeout, but it didn't end until the timeout was reached**)\r\n- **Another way**:\r\n     1. A: CLIENT PAUSE 1000000 WRITE (blocking for 1000s)\r\n     2. A: CLIENT UNPAUSE\r\n     3.  A: CLIENT PAUSE 1000 WRITE(**A will block all write commands until timeout, but it didn't end until the timeout was reached, it may block for about 1000s**)\r\n\r\n- **Try fix**:\r\n```c\r\nvoid pauseClients(mstime_t end, pause_type type) {\r\n    if (type > server.client_pause_type) {\r\n        server.client_pause_type = type;\r\n    }\r\n\r\n    // Old\r\n    if (end > server.client_pause_end_time) {\r\n        server.client_pause_end_time = end;\r\n    }\r\n\r\n    // New\r\n    server.client_pause_end_time = end;\r\n\r\n    ...\r\n}\r\n```\r\n\r\n# Thinking\r\n\r\nBoth `FAILOVER` and `CLIENT PAUSE/UNPAUSE` use `pauseClients` and `unpauseClients` functions to implement block client logic:\r\n\r\n- FAILOVER\r\n    - start: set `server.client_pause_end_time` to `LLONG_MAX` by `pauseClients` function, but we use `server.failover_end_time` to stop failover (try keep pauseClient simple?)\r\n    - stop: clean failover meta data, but `server.client_pause_end_time` always a maximum\r\n- CLINET PAUSE/UNPAUSE\r\n    - pause: can't update `server.client_pause_end_time` after failover\r\n    - unpause: not reset `server.client_pause_end_time` to zero\r\n\r\nAnother question is, why should we limit the value of `server.client_pause_end_time` to a larger value?\r\n\r\nEnglish is not my native language; please excuse typing errors.\r\n\r\nHope to get a reply~",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 568,
        "deletions": 6,
        "changed_files": 24,
        "created_at": "2021-09-05T23:43:30Z",
        "closed_at": "2022-01-05T12:01:06Z",
        "merged_at": "2022-01-05T12:01:05Z",
        "body": "Fixes #4707.\r\n\r\n# Short description\r\n\r\nThe Redis extended latency stats track per command latencies and enables:\r\n- exporting the per-command percentile distribution via the `INFO LATENCYSTATS` command. **( percentile distribution is not mergeable between cluster nodes ).**\r\n- exporting the per-command cumulative latency distributions via the `LATENCY HISTOGRAM` command. Using the cumulative distribution of latencies we can merge several stats from different cluster nodes to calculate aggregate metrics .\r\n\r\nBy default, the extended latency monitoring is enabled since the overhead of keeping track of the command latency is very small.\r\n \r\nIf you don't want to track extended latency metrics, you can easily disable it at runtime using the command:\r\n - `CONFIG SET latency-tracking no`\r\n\r\n\r\nBy default, the exported latency percentiles are the p50, p99, and p999.  You can alter them at runtime using the command:\r\n- `CONFIG SET latency-tracking-info-percentiles \"0.0 50.0 100.0\"`\r\n\r\n\r\n## Some details:\r\n- The total size per histogram should sit around 40 KiB. We only allocate those 40KiB when a command was called for the first time.\r\n- With regards to the WRITE overhead As seen below, there is no measurable overhead on the achievable ops/sec or full latency spectrum on the client. Including also the measured redis-benchmark for unstable vs this branch. \r\n- We track from 1 nanosecond to 1 second ( everything above 1 second is considered +Inf )\r\n\r\n----\r\n\r\n### overhead test using redis-benchmark on unstable vs commands.latency.histogram\r\n\r\nI've tested using ping and set commands ( fast ones ) to assess the largest possible overhead of histogram latency tracking. \r\nAs seen below there is no measurable overhead on the achievable ops/sec or full latency spectrum on the client. This matches the expected behaviour, given on previous microbenchmarks of the hdr_record_value (the function used) [>>see reference<<](https://github.com/HdrHistogram/HdrHistogram_c/pull/76) took on average 5-6 ns/op.\r\n \r\n#### results on unstable ( commit hash = af0b50f83a997ca3eb2e21fd9ee823ef15e12183 )\r\nUsed redis-server command\r\n```\r\ntaskset -c 0 ./src/redis-server --save \"\" --daemonize yes\r\n```\r\n\r\n```\r\ntaskset -c 1,2 redis-benchmark -c 50 --threads 2 -d 1024 -r 1000000 -n 10000000 --csv -t ping,set,get\r\n\"test\",\"rps\",\"avg_latency_ms\",\"min_latency_ms\",\"p50_latency_ms\",\"p95_latency_ms\",\"p99_latency_ms\",\"max_latency_ms\"\r\n\"PING_INLINE\",\"212512.75\",\"0.220\",\"0.040\",\"0.215\",\"0.279\",\"0.295\",\"24.383\"\r\n\"PING_MBULK\",\"220716.45\",\"0.213\",\"0.040\",\"0.215\",\"0.263\",\"0.287\",\"28.127\"\r\n\"SET\",\"175198.86\",\"0.269\",\"0.040\",\"0.263\",\"0.359\",\"0.391\",\"20.383\"\r\n\"GET\",\"179121.59\",\"0.263\",\"0.040\",\"0.263\",\"0.351\",\"0.367\",\"32.127\"\r\n```\r\n\r\n#### results on commands.latency.histogram branch with latency track disabled\r\nUsed redis-server command\r\n```\r\ntaskset -c 0 ./src/redis-server --save \"\" --latency-tracking no --daemonize yes\r\n```\r\n```\r\ntaskset -c 1,2 redis-benchmark -c 50 --threads 2 -d 1024 -r 1000000 -n 10000000 --csv -t ping,set,get\r\n\"test\",\"rps\",\"avg_latency_ms\",\"min_latency_ms\",\"p50_latency_ms\",\"p95_latency_ms\",\"p99_latency_ms\",\"max_latency_ms\"\r\n\"PING_INLINE\",\"212408.94\",\"0.221\",\"0.040\",\"0.223\",\"0.279\",\"0.295\",\"24.383\"\r\n\"PING_MBULK\",\"219375.22\",\"0.214\",\"0.032\",\"0.215\",\"0.263\",\"0.287\",\"28.367\"\r\n\"SET\",\"173644.27\",\"0.270\",\"0.040\",\"0.263\",\"0.367\",\"0.391\",\"28.207\"\r\n\"GET\",\"177619.89\",\"0.265\",\"0.040\",\"0.263\",\"0.351\",\"0.375\",\"28.127\"\r\n```\r\n\r\n#### results on commands.latency.histogram branch with latency track enabled\r\n\r\n```\r\ntaskset -c 1,2 redis-benchmark -c 50 --threads 2 -d 1024 -r 1000000 -n 10000000 --csv -t ping,set,get\r\n\"test\",\"rps\",\"avg_latency_ms\",\"min_latency_ms\",\"p50_latency_ms\",\"p95_latency_ms\",\"p99_latency_ms\",\"max_latency_ms\"\r\n\"PING_INLINE\",\"211528.28\",\"0.223\",\"0.040\",\"0.223\",\"0.279\",\"0.295\",\"24.191\"\r\n\"PING_MBULK\",\"219664.36\",\"0.214\",\"0.040\",\"0.215\",\"0.263\",\"0.287\",\"24.127\"\r\n\"SET\",\"175204.98\",\"0.271\",\"0.040\",\"0.263\",\"0.359\",\"0.383\",\"24.399\"\r\n\"GET\",\"179172.94\",\"0.263\",\"0.040\",\"0.263\",\"0.351\",\"0.375\",\"24.239\"\r\n```\r\n\r\n## `INFO LATENCYSTATS` exposition format\r\n\r\n   - Format: `latency_percentiles_usec_<CMDNAME>:p0=XX,p50....` \r\n\r\n#### latencystats output sample:\r\n\r\n```\r\nroot@hpe10:~# redis-cli INFO LATENCYSTATS\r\n# Latencystats - latency by percentile distribution\r\nlatency_percentiles_usec_zadd:p50.000000=0.001,p99.000000=1.003,p99.900000=4.015\r\nlatency_percentiles_usec_hset:p50.000000=0.001,p99.000000=1.003,p99.900000=4.015\r\nlatency_percentiles_usec_rpop:p50.000000=0.001,p99.000000=1.003,p99.900000=6.015\r\nlatency_percentiles_usec_set:p50.000000=0.001,p99.000000=1.003,p99.900000=4.015\r\nlatency_percentiles_usec_zpopmin:p50.000000=0.001,p99.000000=1.003,p99.900000=3.007\r\nlatency_percentiles_usec_spop:p50.000000=0.001,p99.000000=1.003,p99.900000=3.007\r\nlatency_percentiles_usec_lpush:p50.000000=0.001,p99.000000=1.003,p99.900000=4.015\r\nlatency_percentiles_usec_ping:p50.000000=0.001,p99.000000=1.003,p99.900000=1.003\r\nlatency_percentiles_usec_lpop:p50.000000=0.001,p99.000000=1.003,p99.900000=5.023\r\nlatency_percentiles_usec_lrange:p50.000000=14.015,p99.000000=22.015,p99.900000=56.063\r\nlatency_percentiles_usec_incr:p50.000000=0.001,p99.000000=1.003,p99.900000=4.015\r\nlatency_percentiles_usec_get:p50.000000=0.001,p99.000000=1.003,p99.900000=3.007\r\nlatency_percentiles_usec_mset:p50.000000=1.003,p99.000000=1.003,p99.900000=2.007\r\nlatency_percentiles_usec_rpush:p50.000000=0.001,p99.000000=1.003,p99.900000=4.015\r\nlatency_percentiles_usec_sadd:p50.000000=0.001,p99.000000=1.003,p99.900000=3.007\r\n```\r\n\r\n## `LATENCY HISTOGRAM [command ...]` exposition format\r\n\r\nReturn a cumulative distribution of latencies in the format of a histogram for the specified command names.\r\n\r\nThe histogram is composed of a map of time buckets:\r\n- Each representing a latency range, between 1 nanosecond and roughly 1 second.\r\n- Each bucket covers twice the previous bucket's range.\r\n- Empty buckets are not printed.\r\n- Everything above 1 sec is considered +Inf.\r\n- At max there will be log2(1000000000)=30 buckets\r\n\r\nWe reply a map for each command in the format `<command name> : { calls: <total command calls> , histogram : { <bucket 1> : latency , < bucket 2> : latency, ...  } }`\r\n\r\n\r\nRESP2:\r\n```\r\n redis-cli latency histogram set spop\r\n1) \"set\"\r\n2) 1) \"calls\"\r\n   2) (integer) 100000\r\n   3) \"histogram_usec\"\r\n   4) 1) (integer) 1\r\n      2) (integer) 99986\r\n      3) (integer) 4\r\n      4) (integer) 99991\r\n      5) (integer) 16\r\n      6) (integer) 100000\r\n3) \"spop\"\r\n4) 1) \"calls\"\r\n   2) (integer) 100000\r\n   3) \"histogram_usec\"\r\n   4) 1) (integer) 1\r\n      2) (integer) 99991\r\n      3) (integer) 4\r\n      4) (integer) 99995\r\n      5) (integer) 8\r\n      6) (integer) 99997\r\n      7) (integer) 16\r\n      8) (integer) 100000\r\n```\r\nRESP3:\r\n\r\n```\r\n# redis-cli -3 latency histogram\r\n 1# \"set\" => 1# \"calls\" => (integer) 100000\r\n    2# \"histogram_usec\" => 1# (integer) 1 => (integer) 99986\r\n       2# (integer) 4 => (integer) 99991\r\n       3# (integer) 16 => (integer) 100000\r\n 2# \"spop\" => 1# \"calls\" => (integer) 100000\r\n    2# \"histogram_usec\" => 1# (integer) 1 => (integer) 99991\r\n       2# (integer) 4 => (integer) 99995\r\n       3# (integer) 8 => (integer) 99997\r\n       4# (integer) 16 => (integer) 100000\r\n```",
        "comments": 17
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 39,
        "changed_files": 2,
        "created_at": "2021-09-03T07:27:12Z",
        "closed_at": "2021-09-03T11:57:40Z",
        "merged_at": null,
        "body": "In #6236 barrier mechanism has been moved to connection.c.\r\nAlso remove `AE_BARRIER` which is no longer in use.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 19,
        "changed_files": 3,
        "created_at": "2021-09-02T13:47:37Z",
        "closed_at": "2021-09-06T06:12:39Z",
        "merged_at": "2021-09-06T06:12:39Z",
        "body": "Until now, giving a negative index seeks from the end of a list and a\r\npositive seeks from the beginning. This change makes it seek from the\r\nnearest end, regardless of the sign of the given index.\r\n\r\nquicklistIndex is used by all list commands which operate by index.\r\n\r\nLINDEX key 999999 in a list if 1M elements is greately optimized by\r\nthis change. Latency is cut by 75%.\r\n\r\nLINDEX key -1000000 in a list of 1M elements, likewise.\r\n\r\nLRANGE key -1 -1 is affected by this, since LRANGE converts the indices\r\nto positive numbers before seeking.\r\n\r\nThe tests for corrupt dumps are updated to make sure the corrupt data\r\nis seeked in the same direction as before.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-09-02T09:52:33Z",
        "closed_at": "2021-09-09T09:47:26Z",
        "merged_at": "2021-09-09T09:47:26Z",
        "body": "For a lot of long strings which have same prefix which extends beyond\r\nhashing limit, there will be many hash collisions which result in\r\nperformance degradation using commands like KEYS\r\n\r\nAddresses https://github.com/redis/redis/issues/8077",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 75,
        "deletions": 9,
        "changed_files": 4,
        "created_at": "2021-09-02T07:14:18Z",
        "closed_at": "2021-09-08T08:07:26Z",
        "merged_at": "2021-09-08T08:07:26Z",
        "body": "When a replica paused, it would not apply any commands even if the command comes from master, if we feed the non-applied command to replication stream, the replication offset would be wrong, and data would be lost after failover (since replica's `master_repl_offset` grows but command is not applied).\r\n\r\nTo fix it, here are the changes:\r\n* Don't update replica's replication offset or propagate commands to sub-replicas when it's paused in `commandProcessed`.\r\n* Show `slave_read_repl_offset` in info reply.\r\n* Add an assert to make sure master client should never be blocked unless pause or module (some modules may use block way to do background (parallel) processing and forward original block module command to the replica, it's not a good way but it can work, so the assert excludes module now, but someday in future all modules should rewrite block command to propagate like what `BLPOP` does).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-09-02T02:29:38Z",
        "closed_at": "2021-09-09T08:03:06Z",
        "merged_at": "2021-09-09T08:03:06Z",
        "body": "When parsing an array type reply, `ctx` will be lost when recursively parsing its elements, which will cause a memory leak in automemory mode.\r\n\r\n```\r\nstatic void callReplyParseCollection(ReplyParser *parser, CallReply *rep, size_t len, const char *proto, size_t elements_per_entry) {\r\n    rep->len = len;\r\n    rep->val.array = zcalloc(elements_per_entry * len * sizeof(CallReply));\r\n    for (size_t i = 0; i < len * elements_per_entry; i += elements_per_entry) {\r\n        for (size_t j = 0 ; j < elements_per_entry ; ++j) {\r\n            // here lost the ctx\r\n            parseReply(parser, rep->val.array + i + j); \r\n            rep->val.array[i + j].flags |= REPLY_FLAG_PARSED;\r\n            rep->val.array[i + j].private_data = rep->private_data;\r\n            if (rep->val.array[i + j].flags & REPLY_FLAG_RESP3) {\r\n                /* If one of the sub-replies is RESP3, then the current reply is also RESP3. */\r\n                rep->flags |= REPLY_FLAG_RESP3;\r\n            }\r\n        }\r\n    }\r\n    rep->proto = proto;\r\n    rep->proto_len = parser->curr_location - proto;\r\n}\r\n```\r\n\r\nThis is a result of the changes in #9202",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 149,
        "deletions": 22,
        "changed_files": 2,
        "created_at": "2021-09-01T21:14:30Z",
        "closed_at": "2021-10-27T04:44:33Z",
        "merged_at": "2021-10-27T04:44:33Z",
        "body": "Now, in the cluster mode. By using command CLUSTER DELSLOTS slot [slot ...] and command CLUSTER ADDSLOTS slot [slot ...]\r\nuser can only assign or forget discrete slot for a cluster node instead of a slot range.\r\n\r\nFoe example, if user want to assign 1--5000 slot to a cluster node, user has only 2 choices:\r\noption 1: run command CLUSTER ADDSLOTS 1 2 3 .... 4999 5000\r\noption 2: assign these slot by bash shell command\r\nredis-cli -h 127.0.0.1 -p 6379 cluster addslots {0...5000}\r\n\r\nI would like to add 2 new commands:\r\n\r\nCLUSTER DELSLOTSRANGE startslot endslot [startslot endslot...]\r\nCLUSTER ADDSLOTSRANGE startslot endslot [startslot endslot...]\r\n\r\nThus, if I would like to assign 1 -- 5000 slot to current cluster node, user only need to run command:\r\n\r\ncluster addslotsrange 1 5000",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 157,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2021-08-31T20:23:25Z",
        "closed_at": "2021-11-30T16:46:15Z",
        "merged_at": "2021-11-30T16:46:15Z",
        "body": "Background Information:\r\n\r\nWe have two issues https://github.com/redis/redis/issues/1297 and https://github.com/redis/redis/issues/4561\r\nThey mentioned that Sentinel does not trigger failover in case of master node reboot in very shot time.\r\nThis PR is to fix this problem for sentinel.\r\n\r\nSentinel send PING request to master, replicas and other sentinels according to the \"down-after-milliseconds\" parameters in\r\nsentinel.conf or its own config file.\r\n\r\nIf down-after-milliseconds is greater than 1000, Sentinel will send PING every second.\r\nIf down-after-milliseconds is less than 1000, Sentinel will send PING according to the down-after-milliseconds value\r\n\r\nFor example:\r\n\r\n// Sentinel send PING request every 1 second\r\nSentinel down-after-milliseconds mymaster 60000\r\n\r\n// Sentinel send PING request every 600 milliseconds\r\nSentinel down-after-milliseconds mymaster 600\r\n\r\nSolution:\r\n\r\nIf master reboot in very short time, Sentinel will receive one \"reboot\" information.\r\nAt this time, add SRI_MASTER_REBOOT to this Sentinel instance and record the master reboot time in master_reboot_since_time variable.\r\n\r\nIf Sentinel receives \"Loading\" reply with SRI_MASTER_REBOOT flag from master for 10 * PING time,\r\nSentinel think master is subjectively down, and ready to vote and failover process.\r\n\r\nThis solution could grantee if master is reboot in very short time, failover could finish in 10 seconds.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-08-29T11:30:11Z",
        "closed_at": "2021-08-30T09:39:09Z",
        "merged_at": "2021-08-30T09:39:09Z",
        "body": "Failed on Raspberry Pi 3b where that single test took about 170 seconds",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 145,
        "deletions": 11,
        "changed_files": 7,
        "created_at": "2021-08-29T06:26:22Z",
        "closed_at": "2021-09-16T11:07:09Z",
        "merged_at": "2021-09-16T11:07:09Z",
        "body": "Implements the [LIMIT limit] variant of SINTERCARD/ZINTERCARD.\r\nNow with the LIMIT, we can stop the searching when cardinality\r\nreaching the limit, and return the cardinality ASAP.\r\n\r\nNote that in SINTERCARD, the old synatx was: `SINTERCARD key [key ...]`\r\nIn order to add a optional parameter, we must break the old synatx.\r\nSo the new syntax of SINTERCARD will be consistent with ZINTERCARD.\r\nNew syntax: `SINTERCARD numkeys key [key ...] [LIMIT limit]`.\r\n\r\nNote that this means that SINTERCARD has a different syntax than\r\nSINTER and SINTERSTORE (taking numkeys argument)\r\n\r\nAs for ZINTERCARD, we can easily add a optional parameter to it.\r\nNew syntax: `ZINTERCARD numkeys key [key ...] [LIMIT limit]`\r\n\r\nrefs: https://github.com/redis/redis/issues/9388 https://github.com/redis/redis/pull/8946",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 181,
        "deletions": 11,
        "changed_files": 6,
        "created_at": "2021-08-29T03:44:47Z",
        "closed_at": "2021-10-07T12:13:43Z",
        "merged_at": "2021-10-07T12:13:43Z",
        "body": "Tracking invalidation messages were sometimes sent in inconsistent order, before the command's reply rather than after.\r\nIn addition to that, they were sometimes embedded inside other commands responses, like MULTI-EXEC and MGET.\r\nFix https://github.com/redis/redis/issues/8206 and https://github.com/redis/redis/issues/8935",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-08-28T05:33:30Z",
        "closed_at": "2021-08-28T09:27:38Z",
        "merged_at": null,
        "body": "Fix a small error due to the unreleased lock io_threads_mutex before program exit.\r\n\r\nIt is a small, harmless error since the program exits and all of the program resources will be cleaned up by some common OS. However, other OS systems (e.g., embedded systems) do not automatically free some resources at the exit. It is a good manner for resource management. Adding the unlock statement adds symmetry, so the code looks better. Also, the debugger would not warn this case : )",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 23,
        "changed_files": 2,
        "created_at": "2021-08-27T13:39:27Z",
        "closed_at": "2021-08-29T11:31:08Z",
        "merged_at": "2021-08-29T11:31:08Z",
        "body": "A few pitfalls I discovered while testing the slot-to-key PR and some more fixes while I'm at it. Commit message below.\r\n\r\n----\r\n\r\n1. The output of --help:\r\n\r\n  * On the Usage line, just write [OPTIONS] [COMMAND ARGS...] instead listing\r\n    only a few arbitrary options and no command.\r\n  * For --cluster, describe that if the command is supplied on the command line,\r\n    the key must contain \"{tag}\". Otherwise, the command will not be sent to the\r\n    right cluster node.\r\n  * For -r, add a note that if -r is omitted, all commands in a benchmark will\r\n    use the same key. Also align the description.\r\n  * For -t, describe that -t is ignored if a command is supplied on the command\r\n    line.\r\n\r\n2. Print a warning if -t is present when a specific command is supplied.\r\n\r\n3. Print all warnings and errors to stderr.\r\n\r\n4. Remove -e from calls in redis-benchmark test suite.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2021-08-26T12:26:23Z",
        "closed_at": "2021-09-02T08:07:51Z",
        "merged_at": "2021-09-02T08:07:51Z",
        "body": "Normally we execute the read event first and then the write event. When the barrier is set, we will do it reverse.\r\nHowever, under `kqueue`, if an `fd` has both read and write events, reading the event using `kevent` will generate two events, which will result in uncontrolled read and write timing.\r\nThis also means that the guarantees of AOF `appendfsync` = `always` are not met on MacOS without this fix.\r\n\r\nThe main change to this pr is to cache the events already obtained when reading them, so that if the same `fd` occurs again, only the mask in the cache is updated, rather than a new event is generated.\r\n\r\nThis was exposed by the following test failure on MacOS:\r\n```\r\n*** [err]: AOF fsync always barrier issue in tests/integration/aof.tcl\r\nExpected 544 != 544 (context: type eval line 26 cmd {assert {$size1 != $size2}} proc ::test)\r\n```",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-08-26T07:12:01Z",
        "closed_at": "2023-09-02T12:37:05Z",
        "merged_at": "2023-09-02T12:37:05Z",
        "body": "Recently, the option of sending an argument from stdin using `-x` flag was added to redis-benchmark (this option is available in redis-cli as well). However, using the `-x` option for sending a blobs that contains null-characters doesn't work as expected - the argument is trimmed in the first occurrence of `\\X00` (unlike in redis-cli).  \r\nThis PR aims to fix this issue and add the support for every binary string input, by sending arguments length to `redisFormatCommandArgv` when processing redis-benchmark command, so we won't treat the arguments as C-strings.\r\n\r\nAdditionally, we add a simple test coverage for `-x` (without binary strings, and also remove an excessive server started in tests, and make sure to select db 0 so that `r` and the benchmark work on the same db.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 47,
        "deletions": 13,
        "changed_files": 4,
        "created_at": "2021-08-25T09:26:46Z",
        "closed_at": "2021-08-30T07:24:54Z",
        "merged_at": "2021-08-30T07:24:54Z",
        "body": "We implement incremental data sync in rio.c by call fsync, on slow disk, that may cost a lot of time,\r\nsync_file_range could provide async fsync, so we could serialize key/value and sync file data at the same time.\r\n\r\n> one tip for sync_file_range usage: http://lkml.iu.edu/hypermail/linux/kernel/1005.2/01845.html\r\n\r\nAdditionally, this change avoids a single large write to be used, which can result in a mass of dirty\r\npages in the kernel (increasing the risk of someone else's write to block).\r\n\r\nOn HDD, current solution could reduce approximate half of dumping RDB time,\r\nthis PR costs 50s for dump 7.7G rdb but unstable branch costs 93s.\r\nOn NVME SSD, this PR can't reduce much time,  this PR costs 40s, unstable branch costs 48s.\r\n\r\nMoreover, I find calling data sync every 4MB is better than 32MB.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 6,
        "changed_files": 7,
        "created_at": "2021-08-24T20:14:04Z",
        "closed_at": "2021-09-05T10:26:30Z",
        "merged_at": "2021-09-05T10:26:30Z",
        "body": "Used the sentinel debug command to reduce the overall time it takes for sentinel test cases to run.\r\nTest time reduced from 3 minutes to 1 minute 40 seconds in local environment.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 508,
        "deletions": 58,
        "changed_files": 13,
        "created_at": "2021-08-23T05:05:12Z",
        "closed_at": "2022-11-30T09:56:36Z",
        "merged_at": "2022-11-30T09:56:36Z",
        "body": "See https://github.com/redis/redis/issues/9354\r\n\r\nWe add a new module event `RedisModule_Event_Key`, this event is fired when a key is removed from the keyspace.\r\nThe event includes an open key that can be used for reading the key before it is removed.\r\nModules can also extract the key-name, and use RM_Open or RM_Call to access key from within that event, but shouldn't modify anything from within this event.\r\nThe following sub events are available:\r\n  - `REDISMODULE_SUBEVENT_KEY_DELETED`\r\n  - `REDISMODULE_SUBEVENT_KEY_EXPIRED`\r\n  - `REDISMODULE_SUBEVENT_KEY_EVICTED`\r\n  - `REDISMODULE_SUBEVENT_KEY_OVERWRITE`\r\n\r\nThe data pointer can be casted to a RedisModuleKeyInfo structure with the following fields:\r\n```\r\n     RedisModuleKey *key;    // Opened Key\r\n ```\r\n\r\n### internals\r\n\r\n* We also add two dict functions:\r\n  `dictTwoPhaseUnlinkFind` finds an element from the table, also get the plink of the entry.\r\n  The entry is returned if the element is found. The user should later call `dictTwoPhaseUnlinkFree`\r\n  with it in order to unlink and release it. Otherwise if the key is not found, NULL is returned.\r\n  These two functions should be used in pair. `dictTwoPhaseUnlinkFind` pauses rehash and\r\n  `dictTwoPhaseUnlinkFree` resumes rehash.\r\n* We change `dbOverwrite` to `dbReplaceValue` which just replaces the value of the key and\r\n  doesn't fire any events. The \"overwrite\" part (which emits events) is just when called from `setKey`,\r\n  the other places that called dbOverwrite were ones that just update the value in-place (INCR*, SPOP,\r\n  and dbUnshareStringValue). This should not have any real impact since `moduleNotifyKeyUnlink` and\r\n  `signalDeletedKeyAsReady` wouldn't have mattered in these cases anyway (i.e. module keys and\r\n  stream keys didn't have direct calls to dbOverwrite)\r\n* since we allow doing RM_OpenKey from withing these callbacks, we temporarily disable lazy expiry.\r\n* We also temporarily disable lazy expiry when we are in unlink/unlink2 callback and keyspace \r\n  notification callback.\r\n* Move special definitions to the top of redismodule.h\r\n  This is needed to resolve compilation errors with RedisModuleKeyInfoV1\r\n  that carries a RedisModuleKey member.",
        "comments": 63
    },
    {
        "merged": true,
        "additions": 62,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2021-08-20T17:04:12Z",
        "closed_at": "2021-08-22T07:20:54Z",
        "merged_at": "2021-08-22T07:20:54Z",
        "body": "In old way, we always increase server.dirty in BITSET and BITFIELD SET.\r\nEven the command doesn't really change anything. This commit make \r\nsure BITSET and BITFIELD SET only increase dirty when the value changed.\r\n\r\nBecause of that, if the value not changed, some others implications:\r\n- Avoid adding useless AOF\r\n- Reduce replication traffic\r\n- Will not trigger keyspace notifications (setbit)\r\n- Will not invalidate WATCH\r\n- Will not sent the invalidation message to the tracking client\r\n\r\nFixes #1314",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-08-20T02:50:38Z",
        "closed_at": "2021-08-20T07:37:46Z",
        "merged_at": "2021-08-20T07:37:46Z",
        "body": "This is found by `corrupt-dump-fuzzing` test.\r\nWhen using raxInsert to insert a duplicate `nodekey`, it will overwrite the old data, and finally, return 0.\r\nThis will result in both `decrRefCount(o)` and `lpFree(lp)` which will free the listpack.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 86,
        "deletions": 13,
        "changed_files": 2,
        "created_at": "2021-08-20T01:16:25Z",
        "closed_at": "2021-09-09T08:32:29Z",
        "merged_at": "2021-09-09T08:32:29Z",
        "body": "Currently, once replica need to start full synchronization with master, it will discard cached master whatever full synchronization is failed or not. I think the cached master still is valid if failing to full synchronization, since replica's data is not changed during waiting data snapshot from master. Now we discard cached master only when transferring RDB is finished  and start to change data space, this make replica could start partial resynchronization with another new master if new master is failed during full synchronization.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-08-19T15:45:02Z",
        "closed_at": "2021-08-22T10:57:18Z",
        "merged_at": "2021-08-22T10:57:18Z",
        "body": "Also make sure function can't return NULL by another assert.\r\n\r\nFixes #9383.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-08-19T13:31:50Z",
        "closed_at": "2021-08-22T02:43:18Z",
        "merged_at": "2021-08-22T02:43:18Z",
        "body": "For other configs, we show both the main config name and its alias if both match the pattern. This one is handled separately.\r\n\r\nFixes #9376",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 6,
        "changed_files": 6,
        "created_at": "2021-08-18T13:55:47Z",
        "closed_at": "2021-10-19T05:28:27Z",
        "merged_at": "2021-10-19T05:28:27Z",
        "body": "Background: \r\nCurrently, for a cluster node, cluster bus port = command port+ 10000, which is a fixed value.\r\n\r\nWe would like to make the value flexible, thus user could have more choice for the offset between command port\r\nand cluster bus port.\r\n\r\nThe parameter \"cluster-port\" can be added in redis.conf, the value could be from 1 to 65535.\r\n\r\nExample for 4 node config files in a clsuter:\r\n\r\n**node1:** \r\n\r\nport 6381\r\ncluster-enabled yes\r\ncluster-config-file nodes1-6381.conf\r\ncluster-node-timeout 15000\r\ncluster-port 10000\r\n\r\n**node2:**\r\n\r\nport 6382\r\ncluster-enabled yes\r\ncluster-config-file nodes2-6382.conf\r\ncluster-node-timeout 15000\r\ncluster-port 10001\r\n\r\n**node3:**\r\n\r\nport 6383\r\ncluster-enabled yes\r\ncluster-config-file nodes3-6383.conf\r\ncluster-node-timeout 15000\r\ncluster-port 20000\r\n\r\n**node4:**\r\n\r\nport 6384\r\ncluster-enabled yes\r\ncluster-config-file nodes4-6384.conf\r\ncluster-node-timeout 15000\r\n\r\nthe result:\r\n\r\n![image](https://user-images.githubusercontent.com/51993843/134528277-530c02c4-25a1-4678-8f96-532a4418e3d9.png)\r\n\r\n\r\n\r\n\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-08-18T10:35:31Z",
        "closed_at": "2021-08-18T12:46:11Z",
        "merged_at": "2021-08-18T12:46:11Z",
        "body": "Specifically handles build warnings on s390x",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 165,
        "deletions": 163,
        "changed_files": 38,
        "created_at": "2021-08-18T08:47:01Z",
        "closed_at": "2021-11-09T07:17:02Z",
        "merged_at": null,
        "body": "Throughout the codebase, many pointers are declared with \"char\" instead of the proper type, \"const char\". This PR resolves this by replacing \"char*\" to \"const char*\" at many locations. These locations were discovered by compiling with the warning flag `-Wdiscarded-qualifiers` enabled, and resolving many (but not all) of the warnings.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-08-18T01:34:04Z",
        "closed_at": "2021-08-19T00:47:20Z",
        "merged_at": null,
        "body": "\u2026ster.\r\n\r\nFor example, the `RESET` command calls `clusterSetNodeAsMaster` function, so when we reset a node the flag will not be deleted. Then this node which may act as a fresh slave to rejoin the cluster will not try to failover its master by default.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-08-16T15:55:44Z",
        "closed_at": "2021-08-16T17:09:09Z",
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2021-08-16T04:55:12Z",
        "closed_at": "2021-09-09T08:38:10Z",
        "merged_at": "2021-09-09T08:38:10Z",
        "body": "Add two INFO metrics:\r\n```\r\ntotal_active_defrag_time:12345\r\ncurrent_active_defrag_time:456\r\n```\r\n`current_active_defrag_time` if greater than 0, means how much time has\r\npassed since active defrag started running. If active defrag stops, this metric is reset to 0.\r\n`total_active_defrag_time` means total time the fragmentation\r\nwas over the defrag threshold since the server started.\r\n\r\nThis is a followup PR for #9031",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 694,
        "deletions": 150,
        "changed_files": 10,
        "created_at": "2021-08-14T13:42:11Z",
        "closed_at": "2021-09-09T09:02:34Z",
        "merged_at": "2021-09-09T09:02:33Z",
        "body": "We want to add COUNT option for BLPOP.\r\nBut we can't do it without breaking compatibility due to the command arguments syntax.\r\nSo this commit introduce two new commands.\r\n\r\nSyntax for the new LMPOP command:\r\n`LMPOP numkeys [<key> ...] LEFT|RIGHT [COUNT count]`\r\n\r\nSyntax for the new BLMPOP command:\r\n`BLMPOP timeout numkeys [<key> ...] LEFT|RIGHT [COUNT count]`\r\n\r\nSome background:\r\n- LPOP takes one key, and can return multiple elements.\r\n- BLPOP takes multiple keys, but returns one element from just one key.\r\n- LMPOP can take multiple keys and return multiple elements from just one key.\r\n\r\nNote that LMPOP/BLMPOP  can take multiple keys, it eventually operates on just one key.\r\nAnd it will propagate as LPOP or RPOP with the COUNT option.\r\n\r\nAs a new command, it still return NIL if we can't pop any elements.\r\nFor the normal response is nested arrays in RESP2 and RESP3, like:\r\n```\r\nLMPOP/BLMPOP \r\n1) keyname\r\n2) 1) element1\r\n   2) element2\r\n```\r\nI.e. unlike BLPOP that returns a key name and one element so it uses a flat array,\r\nand LPOP that returns multiple elements with no key name, and again uses a flat array,\r\nthis one has to return a nested array, and it does for for both RESP2 and RESP3 (like SCAN does)\r\n\r\nSome discuss can see: #766 #8824",
        "comments": 24
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-08-14T12:39:00Z",
        "closed_at": "2021-08-15T13:10:00Z",
        "merged_at": null,
        "body": "modify the comment of \"ZSKIPLIST_MAXLEVEL\",  \" 2^64 elements\" maybe modif to \"2^32 elements\" .\r\n\r\n<!-- \r\n#define ZSKIPLIST_MAXLEVEL 32 /* Should be enough for 2^64 elements */\r\n-->",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-08-13T16:23:06Z",
        "closed_at": "2021-08-14T20:52:44Z",
        "merged_at": "2021-08-14T20:52:44Z",
        "body": "If we want to check `defined(SYNC_FILE_RANGE_WAIT_BEFORE)`, we should include fcntl.h.\r\notherwise, SYNC_FILE_RANGE_WAIT_BEFORE is not defined, and there is alway not `sync_file_range` system call.\r\nIntroduced by #8532 (released in 6.0.12 and 6.2.1)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 19,
        "changed_files": 1,
        "created_at": "2021-08-13T16:16:23Z",
        "closed_at": "2022-02-07T20:40:20Z",
        "merged_at": null,
        "body": "The number of written bytes was not being tracked so short writes would cause `writeHandler` to restart writing at the beginning of the buffer. This would manifest as various protocol errors.\r\n\r\nThis fix largely reverts the `writeHandler` changes made in ae349a40dbd3f3ac929409dd0751ed34dafc3a1d which introduced TLS support to redis-benchmark (see #7959). The one piece of that change which has been retained is the use of `cliWriteConn`. The `while(1)` loop introduced in that change has been removed so that we yield control back to the event loop rather than spinning when the socket is unwritable.\r\n\r\n---\r\n\r\nTo reproduce the issue I started redis-server with `redis-server --loglevel verbose`. In another window I ran redis-benchmark as `redis-benchmark -q -c 1 -n 100000000 -P 500 ping \"$(head -c 512 < /dev/zero | tr '\\0' 'a')\"` (you may need to adjust the pipeline length and ping size to trigger the issue),\r\n\r\nredis-benchmark output,\r\n\r\n```\r\n$ redis-benchmark -q -c 1 -n 100000000 -P 500 ping \"$(head -c 512 < /dev/zero | tr '\\0' 'a')\"\r\nping aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa: rps=0.0 (overall: nan) avg_msec=nError from server: ERR Protocol error: invalid bulk length\r\n```\r\n\r\nand corresponding redis-server output,\r\n\r\n```\r\n84279:M 13 Aug 2021 12:14:04.134 - Accepted 127.0.0.1:55235\r\n84279:M 13 Aug 2021 12:14:04.135 - Protocol error (invalid bulk length) from client: id=4 addr=127.0.0.1:55235 laddr=127.0.0.1:6379 fd=8 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=16484 qbuf-free=49046 argv-mem=4 obl=15642 oll=0 omem=0 tot-mem=82964 events=r cmd=ping user=default redir=-1. Query buffer during protocol error: '$5*2..$4..ping..$512..aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' (... more 342 bytes ...) 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'\r\n```\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 688,
        "deletions": 455,
        "changed_files": 28,
        "created_at": "2021-08-12T11:41:20Z",
        "closed_at": "2021-09-09T15:18:53Z",
        "merged_at": "2021-09-09T15:18:53Z",
        "body": "Part two of implementing #8702 (zset), after #8887.\r\n\r\n## Description of the feature\r\nReplaced all uses of ziplist with listpack in t_zset, and optimized some of the code to optimize performance.\r\n\r\n## Rdb format changes\r\nNew `RDB_TYPE_ZSET_LISTPACK` rdb type.\r\n\r\n## Rdb loading improvements:\r\n1) Pre-expansion of dict for validation of duplicate data for listpack and ziplist.\r\n2) Simplifying the release of empty key objects when RDB loading.\r\n3) Unify ziplist and listpack data verify methods for zset and hash, and move code to rdb.c.\r\n\r\n## Interface changes\r\n1) New `zset-max-listpack-entries` config is an alias for `zset-max-ziplist-entries` (same with `zset-max-listpack-value`).\r\n2) OBJECT ENCODING will return listpack instead of ziplist.\r\n\r\n## Listpack improvements:\r\n1) Add `lpDeleteRange` and `lpDeleteRangeWithEntry` functions to delete a range of entries from listpack.\r\n2) Improve the performance of `lpCompare`, converting from string to integer is faster than converting from integer to string.\r\n3) Replace `snprintf` with `ll2string` to improve performance in converting numbers to strings in `lpGet()`.\r\n\r\n## Zset improvements:\r\n1) Improve the performance of `zzlFind` method, use `lpFind` instead of `lpCompare` in a loop.\r\n2) Use `lpDeleteRangeWithEntry` instead of `lpDelete` twice to delete a element of zset.\r\n\r\n## Tests\r\n1) Add some unittests for `lpDeleteRange` and `lpDeleteRangeWithEntry` function.\r\n2) Add zset RDB loading test.\r\n3) Add benchmark test for `lpCompare` and `ziplsitCompare`.\r\n4) Add empty listpack zset corrupt dump test.\r\n",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-08-12T10:15:21Z",
        "closed_at": "2022-02-06T05:53:33Z",
        "merged_at": null,
        "body": "format code , code style is important.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-08-12T04:52:14Z",
        "closed_at": "2021-08-12T21:50:09Z",
        "merged_at": "2021-08-12T21:50:09Z",
        "body": "Minor nitpick to print the human readable name for a packet while debugging cluster packets.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-08-12T04:51:30Z",
        "closed_at": "2021-08-12T21:59:17Z",
        "merged_at": "2021-08-12T21:59:17Z",
        "body": "Redis cluster SETSLOT <slot> [importing|migrating] <nodeid> is intended to migrate data from one primary to another. In this case these two nodes should agree that each other are primaries before continuing. This just adds another layer of validation so that end users get the error sooner that they may have copied the wrong node ID.\r\n\r\nFollow up from https://github.com/redis/redis/pull/3450#issuecomment-892884390.\r\n\r\nNote the other PR also had a FIX for SETSLOT <slot> node <nodeid> which you may call by default across the entire cluster to force acknowledging ownership of a slot, and someone might not know the state. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 898,
        "deletions": 182,
        "changed_files": 18,
        "created_at": "2021-08-11T11:16:11Z",
        "closed_at": "2021-11-03T18:47:18Z",
        "merged_at": "2021-11-03T18:47:18Z",
        "body": "Redis lists are stored in quicklist, which is currently a linked list of ziplists.\r\nZiplists are limited to storing elements no larger than 4GB, so when bigger items are added they're getting truncated.\r\nThis PR changes quicklists so that they're capable of storing large items in quicklist nodes that are plain string buffers rather than ziplist.\r\n\r\nAs part of the PR there were few other changes in redis: \r\n1. new DEBUG sub-commands: \r\n   - QUICKLIST-PACKED-THRESHOLD - set the threshold of for the node type to be plan or ziplist. default (1GB)\r\n   - QUICKLIST <key> - Shows low level info about the quicklist encoding of <key>\r\n2. rdb format change:\r\n   - A new type was added - RDB_TYPE_LIST_QUICKLIST_2 . \r\n   - container type (packed / plain) was added to the beginning of the rdb object (before the actual node list).\r\n3. testing:\r\n   - Tests that requires over 100MB will be by default skipped. a new flag was added to 'runtest' to run the large memory tests (not used by default)",
        "comments": 20
    },
    {
        "merged": true,
        "additions": 191,
        "deletions": 185,
        "changed_files": 9,
        "created_at": "2021-08-11T08:06:57Z",
        "closed_at": "2021-08-31T06:25:36Z",
        "merged_at": "2021-08-31T06:25:36Z",
        "body": "This PR replaces the radix tree used for the mapping from cluster slot to keys.\r\n\r\nIn cluster mode, there is a mapping from cluster slot to the keys in the slot. Until now, this was stored in a radix tree, duplicating the keys. This PR replaces the parallel structure with a new structure which has the following design:\r\n\r\n* In each dictEntry (hashtable bucket) in the main DB dict, memory for two extra pointers is allocated.\r\n* The entries with keys belonging to the same cluster slot form a linked list using these pointers.\r\n* For each of the 16384 cluster slots, there is a pointer to the beginning of each of these lists.\r\n\r\nThis new structure saves up to 20% memory and reduces the latency for adding and deleting keys by up to 50% (for keys of length 16 and very small values). This is at the cost of a latency regression of 5-10% for accessing and updating existing keys. For details, see the benchmark results posted in comments below in this PR.\r\n\r\nAnother side effect of this change is that now this data structure gets implicitly defragged when defrag.c reallocates the dictEntries, so this solves a problem in which earlier versions may have been unable to resolve fragmentation issues when redis was used in cluster mode. \r\n\r\nThis work is based on #8210 which prepares for allocating extra memory in the dict entries.\r\n\r\nAdditionally, the slot-to-keys API is moved to cluster.c and cluster.h (from server.h and db.c).",
        "comments": 54
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-08-10T10:20:48Z",
        "closed_at": "2021-08-10T11:28:25Z",
        "merged_at": "2021-08-10T11:28:25Z",
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2021-08-10T03:25:42Z",
        "closed_at": "2021-08-10T08:32:27Z",
        "merged_at": "2021-08-10T08:32:27Z",
        "body": "in #8974 @ShooterIT introduced `madvise(MADV_DONTNEED)` to help release memory to reduce COW.\r\nBut the requires header file `<sys/mman.h>`  is only included for linux system. \r\n\r\nSo when I tried to make with jemalloc in my APPLE machine, I got compilation errors like \r\n```\r\nzmalloc.c:363:9: error: implicit declaration of function 'madvise' is invalid in C99\r\n      [-Werror,-Wimplicit-function-declaration]\r\n        madvise((void *)aligned_ptr, real_size&~page_size_mask, MADV_DONTNEED);\r\n        ^\r\nzmalloc.c:363:65: error: use of undeclared identifier 'MADV_DONTNEED'\r\n        madvise((void *)aligned_ptr, real_size&~page_size_mask, MADV_DONTNEED);\r\n```\r\n\r\nI added a condition to avoid this error. So this would only work for linux system. \r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 63,
        "deletions": 16,
        "changed_files": 5,
        "created_at": "2021-08-09T11:53:34Z",
        "closed_at": "2021-08-09T14:13:47Z",
        "merged_at": "2021-08-09T14:13:47Z",
        "body": "This pr mainly fixes empty keys due to RDB loading and restore command, which was omitted in #9297.\r\n\r\n1) When loading quicklsit, if all the ziplists in the quicklist are empty, NULL will be returned.\r\n    If only some of the ziplists are empty, then we will skip the empty ziplists silently.\r\n2) When loading hash zipmap, if zipmap is empty, NULL will be returned.\r\n3) When loading hash ziplist, if ziplist is empty, NULL will be returned.\r\n4) Add RDB loading test with sanitize.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-08-09T00:47:44Z",
        "closed_at": "2021-08-10T12:03:50Z",
        "merged_at": "2021-08-10T12:03:49Z",
        "body": "Copy from https://github.com/redis/redis/pull/3589 and do some improvement.\r\nWe can use `Ctrl+C` to exit monitor/pubsub modeand come back to redis-cli.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 13,
        "changed_files": 4,
        "created_at": "2021-08-08T14:11:40Z",
        "closed_at": "2021-08-10T07:19:21Z",
        "merged_at": "2021-08-10T07:19:21Z",
        "body": "Following the comments on #8659, this PR fix some formating and naming issues.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-08-08T11:54:34Z",
        "closed_at": "2021-08-08T15:30:17Z",
        "merged_at": "2021-08-08T15:30:17Z",
        "body": "Fixes #9342 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-08-08T04:07:51Z",
        "closed_at": "2021-08-09T00:34:11Z",
        "merged_at": "2021-08-09T00:34:11Z",
        "body": "Currently if the replica is configured with a smaller `client-query-buffer-limit` value than the master, the master will be unable to replicate commands longer than this limit to the replica, which makes the replica disconnect from the master. When the replica tries to re-connect, if the master's replication backlog buffer is able to accomodate the command, the master will accept the PSYNC from the replica and send the command down to replica again, but the replica will be unable to apply the command again and disconnect - and get into a bad re-sync loop. \r\n\r\nThis commit exempts the replication client on the replica from this limit to fix this replication problem. ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 50,
        "changed_files": 2,
        "created_at": "2021-08-08T02:40:27Z",
        "closed_at": "2021-08-09T08:03:59Z",
        "merged_at": "2021-08-09T08:03:59Z",
        "body": "AOF fake client creation (createAOFClient) was doing similar work as createClient, with some minor differences, most of which unintended, this was dangerous and meant that many changes to createClient should have always been reflected to aof.c\r\nThis cleanup changes createAOFClient to call createClient with NULL, like we do in module.c and elsewhere.\r\n\r\n##### Original description:\r\nI noticed that when fake AOF client is created in `createAOFClient()`, not all the internal attributes in the client struct are properly initialized as compared to `createClient()` in `networking.c`. This can be potentially dangerous in terms of client usage later on when un-initialized variables get accessed. This commit fixes this issue. \r\n\r\nI haven't checked if all the fields in `client` struct are properly initialized in the regular `createClient()` routine, but I have a feeling it is better maintained and less likely to have missed out on some fields. I am open to also making this similar change in that routine as I think it can help simplify the code by not needing to explicitly setting a lot of fields to NULL or 0. ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 102,
        "deletions": 44,
        "changed_files": 2,
        "created_at": "2021-08-07T00:19:18Z",
        "closed_at": "2021-09-09T14:40:33Z",
        "merged_at": "2021-09-09T14:40:33Z",
        "body": "When doing an ACL load, if multiple users are provided multiple times the engine simply takes the last line and throws away the earlier lines. This isn't expected behavior, so we'll now throw an error to make that expectation clear. Take a special look at the handling of the default user, since it is a special case since it can be \"duplicated\" exactly once. \r\n\r\nThe previous behavior did a lot of touching of the global reference to the default user, the new code doesn't mangle the global pointer anymore. There was more refactoring I wanted to do, but didn't think it strictly support this PR so omitted for now.\r\n\r\nWe now throw an error before parsing the SetUser options as well, so I removed some redundant validation that was done. Previously, all the ACLs were first applied to a \"fake user\" and then applied to the real user. This isn't necessary since we create a backup of all the existing users which we fall back to on error.\r\n\r\nResolves: https://github.com/redis/redis/issues/9232\r\n\r\nNote that this could be considered a backward breaking change if a user has a bad ACL to begin with.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-08-06T09:56:38Z",
        "closed_at": "2021-08-24T06:48:49Z",
        "merged_at": null,
        "body": "1. Optimize the propagation of useless script flush commands\r\n2. Use forceCommandPropagation() to propagate (more accurate), like the script load command, not dirty++",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 9,
        "changed_files": 5,
        "created_at": "2021-08-06T09:07:44Z",
        "closed_at": "2021-08-07T02:27:24Z",
        "merged_at": "2021-08-07T02:27:24Z",
        "body": "The newest Redis provides a very comprehensive of error statistics. But some errors counting  is still missed.\r\nThis PR helps count errors for some behaviors which  add error replies with shared object. \r\n\r\nBefore: \r\n(In this case, the client got the replied error message as no-key-error, but the errorstats does not count.)\r\n```\r\n\u279c  ~ redis-cli\r\n127.0.0.1:6379> info errorstats\r\n# Errorstats\r\n127.0.0.1:6379> debug ziplist a\r\n(error) ERR no such key\r\n127.0.0.1:6379> info errorstats\r\n# Errorstats\r\n```\r\n\r\nAfter\r\n```\r\n\u279c  ~ redis-cli\r\n127.0.0.1:6379> info errorstats\r\n# Errorstats\r\n127.0.0.1:6379> debug ziplist a\r\n(error) ERR no such key\r\n127.0.0.1:6379> info errorstats\r\n# Errorstats\r\nerrorstat_ERR:count=1\r\n```\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 185,
        "deletions": 10,
        "changed_files": 7,
        "created_at": "2021-08-06T04:21:04Z",
        "closed_at": "2021-10-25T10:08:34Z",
        "merged_at": "2021-10-25T10:08:34Z",
        "body": "Add timestamp annotation in AOF, one part of #9325.\r\n\r\nEnabled with the new `aof-timestamp-enabled` config option.\r\n\r\nTimestamp annotation format is \"#TS:${timestamp}\\r\\n\".\"\r\nTS\" is short of timestamp and this method could save extra bytes in AOF.\r\n\r\nWe can use timestamp annotation for some special functions. \r\n- know the executing time of commands\r\n- restore data to a specific point-in-time (by using redis-check-aof to truncate the file)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 290,
        "deletions": 45,
        "changed_files": 3,
        "created_at": "2021-08-06T01:27:54Z",
        "closed_at": "2021-09-12T08:31:22Z",
        "merged_at": "2021-09-12T08:31:22Z",
        "body": "Make bitpos/bitcount support bit index:\r\n\r\n```\r\nBITPOS key bit [start [end [BIT|BYTE]]]\r\nBITCOUNT key [start end [BIT|BYTE]]\r\n```\r\n\r\nThe default behavior is `BYTE`, so these commands are still compatible with old.\r\n\r\nFix https://github.com/redis/redis/issues/9151.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 754,
        "deletions": 303,
        "changed_files": 20,
        "created_at": "2021-08-05T15:32:54Z",
        "closed_at": "2021-11-04T08:46:50Z",
        "merged_at": "2021-11-04T08:46:50Z",
        "body": "For diskless replication in swapdb mode, considering we already spend replica memory having a backup of current db to restore in case of failure, we can have the following benefits by instead swapping database only in case we succeeded in transferring db from master:\r\n\r\n- Avoid `LOADING` response during failed and successful synchronization for cases where the replica is already up and running with data.\r\n- Faster total time of diskless replication, because now we're moving from Transfer + Flush + Load time to Transfer + Load only. Flushing the tempDb is done asynchronously after swapping.\r\n- This could be implemented also for disk replication with similar benefits if consumers are willing to spend the extra memory usage.\r\n\r\nGeneral notes:\r\n- The concept of `backupDb` becomes `tempDb` for clarity.\r\n- Async loading mode will only kick in if the replica is syncing from a master that has the same repl-id the one it had before. i.e. the data it's getting belongs to a different time of the same timeline. \r\n- New property in INFO: `async_loading` to differentiate from the blocking loading\r\n- Slot to Key mapping is now a field of `redisDb` as it's more natural to access it from both server.db and the tempDb that is passed around.\r\n- Because this is affecting replicas only, we assume that if they are not readonly and write commands during replication, they are lost after SYNC same way as before, but we're still denying CONFIG SET here anyways to avoid complications.\r\n\r\nConsiderations for review:\r\n- We have many cases where server.loading flag is used and even though I tried my best, there may be cases where async_loading should be checked as well and cases where it shouldn't (would require very good understanding of whole code)\r\n- Several places that had different behavior depending on the loading flag where actually meant to just handle commands coming from the AOF client differently than ones coming from real clients, changed to check CLIENT_ID_AOF instead.\r\n\r\n**Additional for Release Notes**\r\n- Bugfix - server.dirty was not incremented for any kind of diskless replication, as effect it wouldn't contribute on triggering next database SAVE\r\n- New flag for RM_GetContextFlags module API: REDISMODULE_CTX_FLAGS_ASYNC_LOADING\r\n- Deprecated RedisModuleEvent_ReplBackup. Starting from Redis 7.0, we don't fire this event.\r\nInstead, we have the new RedisModuleEvent_ReplAsyncLoad holding 3 sub-events: STARTED, ABORTED and COMPLETED.\r\n- New module flag REDISMODULE_OPTIONS_HANDLE_REPL_ASYNC_LOAD for RedisModule_SetModuleOptions to allow modules to declare they support the diskless replication with async loading (when absent, we fall back to disk-based loading).",
        "comments": 29
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 40,
        "changed_files": 4,
        "created_at": "2021-08-05T14:36:03Z",
        "closed_at": "2021-08-06T17:50:34Z",
        "merged_at": "2021-08-06T17:50:34Z",
        "body": "Also update qbuf tests to verify both idle and peak based resizing logic.\r\nAnd delete unused function: getClientsMaxBuffers\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 169,
        "deletions": 12,
        "changed_files": 9,
        "created_at": "2021-08-05T12:26:12Z",
        "closed_at": "2021-08-05T19:56:14Z",
        "merged_at": "2021-08-05T19:56:14Z",
        "body": "Recently we found two issues in the fuzzer tester: #9302 #9285\r\nAfter fixing them, more problems surfaced and this PR (as well as #9297) aims to fix them.\r\n\r\nHere's a list of the fixes\r\n- Prevent an overflow when allocating a dict hashtable\r\n- Prevent OOM when attempting to allocate a huge string\r\n- Prevent a few invalid accesses in listpack\r\n- Improve sanitization of listpack first entry\r\n- Validate integrity of stream consumer groups PEL\r\n- Validate integrity of stream listpack entry IDs\r\n- Validate ziplist tail followed by extra data which start with 0xff\r\n\r\njoint effort with @sundb",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1567,
        "deletions": 860,
        "changed_files": 31,
        "created_at": "2021-08-05T11:50:02Z",
        "closed_at": "2022-08-24T05:35:47Z",
        "merged_at": "2022-08-24T05:35:47Z",
        "body": "There are many commits in this PR, the detailed changes is described in each commit message.\r\n\r\n### Main changes in this PR\r\n\r\n* Fully abstract connection type, and hide connection type specified methods. Ex, currently TLS class looks like:\r\n```\r\nstatic ConnectionType CT_TLS = {\r\n    /* connection type */\r\n    .get_type = connTLSGetType,\r\n\r\n    /* connection type initialize & finalize & configure */\r\n    .init = tlsInit,\r\n    .cleanup = tlsCleanup,\r\n    .configure = tlsConfigure,\r\n\r\n    /* ae & accept & listen & error & address handler */\r\n    .ae_handler = tlsEventHandler,\r\n    .accept_handler = tlsAcceptHandler,\r\n    .addr = connTLSAddr,\r\n    .listen = connTLSListen,\r\n\r\n    /* create/close connection */\r\n    .conn_create = connCreateTLS,\r\n    .conn_create_accepted = connCreateAcceptedTLS,\r\n    .close = connTLSClose,\r\n\r\n    /* connect & accept */\r\n    .connect = connTLSConnect,\r\n    .blocking_connect = connTLSBlockingConnect,\r\n    .accept = connTLSAccept,\r\n\r\n    /* IO */\r\n    .read = connTLSRead,\r\n    .write = connTLSWrite,\r\n    .writev = connTLSWritev,\r\n    .set_write_handler = connTLSSetWriteHandler,\r\n    .set_read_handler = connTLSSetReadHandler,\r\n    .get_last_error = connTLSGetLastError,\r\n    .sync_write = connTLSSyncWrite,\r\n    .sync_read = connTLSSyncRead,\r\n    .sync_readline = connTLSSyncReadLine,\r\n\r\n    /* pending data */\r\n    .has_pending_data = tlsHasPendingData,\r\n    .process_pending_data = tlsProcessPendingData,\r\n\r\n    /* TLS specified methods */\r\n    .get_peer_cert = connTLSGetPeerCert,\r\n};\r\n\r\nint RedisRegisterConnectionTypeTLS()\r\n{\r\n    return connTypeRegister(&CT_TLS);\r\n}\r\n```\r\n\r\n* Also abstract Unix socket class. Currently, the connection framework becomes like:\r\n```\r\n                       uplayer\r\n                          |\r\n                   connection layer\r\n                     /    |     \\\r\n                   TCP   Unix   TLS\r\n    \r\n```\r\n\r\n* It's possible to build TLS as a shared library (`make BUILD_TLS=module`). Loading the shared library(redis-tls.so) into Redis by Redis module subsystem, and Redis starts to listen TLS port. Ex:\r\n```\r\n    ./src/redis-server --tls-port 6379 --port 0 \\\r\n        --tls-cert-file ./tests/tls/redis.crt \\\r\n        --tls-key-file ./tests/tls/redis.key \\\r\n        --tls-ca-cert-file ./tests/tls/ca.crt \\\r\n        --loadmodule src/redis-tls.so\r\n```\r\n\r\n* It's possible to support more connection type, ex RDMA:\r\n    https://github.com/redis/redis/pull/9161\r\n\r\n### Interface changes\r\n* RM_GetContextFlags supports a new flag: REDISMODULE_CTX_FLAGS_SERVER_STARTUP\r\n* INFO SERVER includes a list of listeners:\r\n```\r\nlistener0:name=tcp,bind=127.0.0.1,port=6380\r\nlistener1:name=unix,bind=/run/redis.sock\r\nlistener2:name=tls,bind=127.0.0.1,port=6379\r\n```\r\n\r\n### Other notes\r\n\r\n* Fix wrong signature of RedisModuleDefragFunc, this could break\r\n  compilation of a module, but not the ABI\r\n* Some reordering of initialization order in server.c:\r\n  * Move initialization of listeners to be after loading the modules\r\n  * Config TLS after initialization of listeners\r\n  * Init cluster after initialization of listeners\r\n* Sentinel does not support the TLS module or any connection module since it uses hiredis for outbound connections, so when TLS is built as a module, sentinel lacks TLS support.\r\n",
        "comments": 64
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 24,
        "changed_files": 3,
        "created_at": "2021-08-04T16:45:52Z",
        "closed_at": "2021-08-05T08:09:24Z",
        "merged_at": "2021-08-05T08:09:24Z",
        "body": "Fix that there is no sample latency after the key expires via expireIfNeeded().\r\nSome refactoring for shared code.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2021-08-04T16:27:51Z",
        "closed_at": "2021-08-05T08:26:10Z",
        "merged_at": "2021-08-05T08:26:10Z",
        "body": "The psync2 test has failed several times recently.\r\nIn #9159 we only solved half of the problem.\r\ni.e. reordering of the replica that's already connected to\r\nthe newly promoted master.\r\n\r\nConsider this scenario:\r\n0 slaveof 2\r\n1 slaveof 2\r\n3 slaveof 2\r\n4 slaveof 1\r\n0 slaveof no one, became a new master got a new replid\r\n2 slaveof 0, partial resync and got the new replid\r\n3 reconnect 2, inherit the new replid\r\n3 slaveof 4, use the new replid and got a full resync\r\n\r\nAnd another scenario:\r\n1 slaveof 3\r\n2 slaveof 4\r\n3 slaveof 0\r\n4 slaveof 0\r\n4 slaveof no one, became a new master got a new replid\r\n2 reconnect 4, inherit the new replid\r\n2 slaveof 1, use the new replid and got a full resync\r\n\r\nSo maybe we should reattach replicas in the right order.\r\ni.e. In the above example, if it would have reattached 1, 3 and 0 to\r\nthe new chain formed by 4 before trying to attach 2 to 1, it would succeed.\r\n\r\nThis commit break the SLAVEOF loop into two loops. (ideas from oran)\r\n\r\nFirst loop that uses random to decide who replicates from who.\r\nSecond loop that does the actual SLAVEOF command.\r\nIn the second loop, we make sure to execute it in the right order,\r\nand after each SLAVEOF, wait for it to be connected before we proceed.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-08-04T10:25:39Z",
        "closed_at": "2021-08-05T05:20:30Z",
        "merged_at": "2021-08-05T05:20:30Z",
        "body": "Fixes test case on slow machines. See https://github.com/redis/redis/pull/8930#issuecomment-892374015.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 280,
        "deletions": 247,
        "changed_files": 6,
        "created_at": "2021-08-03T23:35:40Z",
        "closed_at": "2021-09-14T16:45:07Z",
        "merged_at": "2021-09-14T16:45:07Z",
        "body": "- Add `-u <uri>` command line option to support `redis://` URI scheme.\r\n- included server connection information object (`struct cliConnInfo`), used to describe an ip:port pair, db num user input, and user:pass to avoid a large number of function arguments.\r\n- Using sds on connection info strings for redis-benchmark/redis-cli\r\n\r\n\r\nTo test solely redis-benchmark:\r\n```\r\ntclsh tests/test_helper.tcl --single integration/redis-benchmark\r\n```\r\n\r\nrevamp of #4507 ( cc @itamarhaber )",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 78,
        "deletions": 54,
        "changed_files": 3,
        "created_at": "2021-08-03T20:52:52Z",
        "closed_at": "2021-08-23T18:00:40Z",
        "merged_at": "2021-08-23T18:00:40Z",
        "body": "This PR is for https://github.com/redis/redis/issues/9071.\r\n\r\nThis aims to solve the issue in CONFIG SET maxmemory can only set maxmemory to up to 9223372036854775807 (2^63) while the maxmemory should be ULLONG. Added a memtoull function to convert a string representing an amount of memory into the number of bytes (similar to memtoll but for ull). Also added ull2string to convert a ULLong to string (Similar to ll2string).",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-08-03T16:00:18Z",
        "closed_at": "2021-08-04T08:19:48Z",
        "merged_at": "2021-08-04T08:19:48Z",
        "body": "Some background:\r\nThis fixes a problem that used to be dead code till now,\r\nbut became alive (only in the unit tests, not in redis) when #9113 got merged.\r\nThe problem it fixes doesn't actually cause any significant harm,\r\nbut that PR also added a test that fails verification because of that.\r\nThis test was merged with that problem due to human error, we didn't run it on the last modified version before merging.\r\nThe fix in this PR existed in #8641 (closed because it's just dead code) and #4674 (still pending but has other changes in it).\r\n\r\nNow to the actual fix:\r\nOn quicklist insertion, if the insertion offset is -1 or `-(quicklist->count)`, we can insert into the head of the next node rather than the tail of the current node. this is especially important when the current node is full, and adding anything to it will cause it to be split (or be over it's fill limit setting).\r\n\r\nThe bug was that the code attempted to determine that we're adding to the tail of the current node by matching `offset == node->count` when in fact it should have been `offset == node->count-1` (so it never entered that `if`). and also that since we take negative offsets too, we can also match `-1`. (same applies for the head, i.e. `0` and `-count`).\r\n\r\nThe bug will cause the code to attempt inserting into the current node (thinking we have to insert into the middle of the node rather than head or tail), and in case the current node is full it'll have to be split (something that also happens in valid cases).\r\nOn top of that, since it calls _quicklistSplitNode with an edge case, it'll actually split the node in a way that all the entries fall into one split, and 0 into the other, and then still insert the new entry into the first one, causing it to be populated beyond it's intended fill limit.\r\n\r\nThis problem does not create any bug in redis, because the existing code does not iterate from tail to head, and the offset never has a negative value when insert.\r\n\r\nThe other change this PR makes in the test code is just for some coverage, insertion at index 0 is tested a lot, so it's nice to test some negative offsets too.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 477,
        "deletions": 75,
        "changed_files": 11,
        "created_at": "2021-08-03T10:38:26Z",
        "closed_at": "2021-09-23T05:52:56Z",
        "merged_at": "2021-09-23T05:52:56Z",
        "body": "This commit introduced a new flag to the RM_Call:\r\n'C' - Check if the command can be executed according to the ACLs associated with it.\r\n\r\nAlso, three new API's added to check if a command, key, or channel can be executed or accessed\r\nby a user, according to the ACLs associated with it.\r\n- RM_ACLCheckCommandPerm\r\n- RM_ACLCheckKeyPerm\r\n- RM_ACLCheckChannelPerm\r\n\r\nThe user for these API's is a RedisModuleUser object, that for a Module user returned by the RM_CreateModuleUser API, or for a general ACL user can be retrieved by these two new API's:\r\n- RM_GetCurrentUserName - Retrieve the user name of the client connection behind the current context.\r\n- RM_GetModuleUserFromUserName - Get a RedisModuleUser from a user name\r\n\r\nAs a result of getting a RedisModuleUser from name, it can now also access the general ACL users (not just ones created by the module).\r\nThis mean the already existing API RM_SetModuleUserACL(), can be used to change the ACL rules for such users.\r\n",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 88,
        "deletions": 63,
        "changed_files": 2,
        "created_at": "2021-08-03T10:15:10Z",
        "closed_at": "2021-08-29T12:03:05Z",
        "merged_at": "2021-08-29T12:03:05Z",
        "body": "Refresh https://github.com/redis/redis/pull/3465\r\nThis one follow #9313 and goes deeper (validation of config file parsing)\r\n\r\nMove the check/update logic to a new updateClientOutputBufferLimit\r\nfunction. So that it can be used in CONFIG SET and config file parsing.\r\n\r\n--------------------------------------------\r\nAdd some changes before and after\r\n\r\nBefore:\r\n```\r\nIf we set an error value(256fffmb) in redis.conf\r\nclient-output-buffer-limit replica 256fffmb 64mb 60\r\n\r\nWe can start a server and we will get this\r\n127.0.0.1:6379> config get client-output-buffer-limit\r\n1) \"client-output-buffer-limit\"\r\n2) \"normal 0 0 0 slave 0 67108864 60 pubsub 33554432 8388608 60\"\r\n\r\nIf we set an error value(fff60) in redis.conf\r\nclient-output-buffer-limit replica 256mb 64mb fff60\r\n\r\nWe will get this\r\n127.0.0.1:6379> config get client-output-buffer-limit\r\n1) \"client-output-buffer-limit\"\r\n2) \"normal 0 0 0 slave 268435456 67108864 0 pubsub 33554432 8388608 60\"\r\n```\r\n\r\nAfter:\r\n```\r\nIf we set an error value in redis.conf\r\nlike: client-output-buffer-limit areplica 256mb 64mb 60f\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 1835\r\n>>> 'client-output-buffer-limit areplica 256mb 64mb 60f'\r\nInvalid client class specified in buffer limit configuration.\r\n\r\nlike: client-output-buffer-limit replica 256fmb 64mb 60f\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 1835\r\n>>> 'client-output-buffer-limit replica 256fmb 64mb 60f'\r\nError in hard, soft or soft_seconds setting in buffer limit configuration.\r\n\r\nlike: client-output-buffer-limit replica 256mb 64mb 60f\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 1835\r\n>>> 'client-output-buffer-limit replica 256mb 64mb 60f'\r\nError in hard, soft or soft_seconds setting in buffer limit configuration.\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-08-02T03:38:10Z",
        "closed_at": "2021-08-02T05:06:37Z",
        "merged_at": "2021-08-02T05:06:37Z",
        "body": "Fix \"BLPOP\" to \"BRPOP\" in `brpopCommand` comments.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 84,
        "deletions": 43,
        "changed_files": 3,
        "created_at": "2021-08-01T20:58:19Z",
        "closed_at": "2021-08-05T18:59:12Z",
        "merged_at": "2021-08-05T18:59:12Z",
        "body": "Add the ability to mark config variables to be printed in their rewrite syntax on engine crashes. I added the initial couple I think would be useful for debugging, but \r\n\r\nSort of address https://github.com/redis/redis/pull/6734, which was opened by @itamarhaber, but I changed the wording to describe the intent of the flag (for debugging information) instead of be instructive in what the config does.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 19,
        "changed_files": 1,
        "created_at": "2021-08-01T14:42:27Z",
        "closed_at": "2022-02-06T13:45:58Z",
        "merged_at": null,
        "body": "This implements a rudimentary improvement (I hope!) to the handling of optional command arguments in the CLI's hints (issue #8084 ). It treats the entire optional argument block as a single word, and removes it from the hint as a unit. The current behavior simply stops hinting when an optional argument is reached.\r\n\r\nThis is not really \"the right thing to do\". Optional arguments often include multiple words. This doesn't handle repeated args or nested optional args. The only question is whether this is incrementally better than what we already have.\r\n\r\nSolving the problem correctly is much more complicated.",
        "comments": 30
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-08-01T12:20:04Z",
        "closed_at": "2021-08-05T19:57:05Z",
        "merged_at": "2021-08-05T19:57:05Z",
        "body": "The execution of the RPOPLPUSH command by the fuzzer created junk keys,\r\nthat were later being selected by RANDOMKEY and modified.\r\nThis also meant that lists were statistically tested more than other\r\nfiles.\r\n\r\nFix the fuzzer not to pass junk key names to RPOPLPUSH, and add a check\r\nthat detects that new keys are not added by the fuzzer to detect future\r\nsimilar issues.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-08-01T10:52:58Z",
        "closed_at": "2021-08-01T12:07:27Z",
        "merged_at": "2021-08-01T12:07:27Z",
        "body": "In some cases large replies on slow systems may only be partially read\r\nby the test suite, resulting with parsing errors.\r\n\r\nThis fix is still timing sensitive but should greatly reduce the chances\r\nof this happening.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2021-07-30T15:13:45Z",
        "closed_at": "2021-08-09T06:40:29Z",
        "merged_at": "2021-08-09T06:40:29Z",
        "body": "This is in case we want to go with the solution of creating a SORT_RO version of SORT so we can run it in read-only replicas.\r\nFor issue: https://github.com/redis/redis/issues/643\r\n\r\nVariable name noStore follows style of existing code, but I'd be happy to name it readOnly as well.\r\n\r\nPlease note I'm not an active C developer, but I think this is straightforward.\r\nTo add tests if this is an accepted solution.",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 127,
        "deletions": 18,
        "changed_files": 6,
        "created_at": "2021-07-30T12:05:05Z",
        "closed_at": "2021-08-05T19:42:20Z",
        "merged_at": "2021-08-05T19:42:20Z",
        "body": "When we load rdb or restore command, if we encounter a length of 0, it will result in the creation of an empty key.\r\nThis could either be a corrupt payload, or a result of a bug (see #8453 )\r\n\r\nThis pr mainly fixes the following:\r\n1) When restore command will return `Bad data format` error.\r\n2) When loading RDB, we will silently discard the key.",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 263,
        "deletions": 44,
        "changed_files": 2,
        "created_at": "2021-07-29T19:53:52Z",
        "closed_at": "2021-08-05T08:12:56Z",
        "merged_at": "2021-08-05T08:12:56Z",
        "body": "For test purpose, we could change some parameters by SENTINEL DEBUG command\r\n It is mentioned in the following 2 PR:\r\n#8891 and #8710\r\n\r\nThe command format has 2 options:\r\n1. SENTINEL DEBUG: display our current configurable parameters value\r\n![Capture](https://user-images.githubusercontent.com/51993843/128195464-78c1254a-1682-4fe7-aea9-bde7ea65a3d8.JPG)\r\n\r\n2. SENTINEL DEBUG [<option> <value>...]: update our current configurable parameters values (one or more)\r\nfor example: sentinel debug info-period 6000\r\n\r\nThese configurable parameters as following: \r\nSENTINEL_INFO_PERIOD \r\nSENTINEL_PING_PERIOD \r\nSENTINEL_ASK_PERIOD \r\nSENTINEL_PUBLISH_PERIOD \r\nSENTINEL_DEFAULT_DOWN_AFTER \r\nSENTINEL_TILT_TRIGGER \r\nSENTINEL_TILT_PERIOD\r\nSENTINEL_SLAVE_RECONF_TIMEOUT \r\nSENTINEL_MIN_LINK_RECONNECT_PERIOD \r\nSENTINEL_ELECTION_TIMEOUT \r\nSENTINEL_SCRIPT_MAX_RUNTIME\r\nSENTINEL_SCRIPT_RETRY_DELAY\r\nSENTINEL_DEFAULT_FAILOVER_TIMEOUT ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-29T15:12:10Z",
        "closed_at": "2021-07-29T16:45:30Z",
        "merged_at": "2021-07-29T16:45:30Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 122,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-07-29T13:19:46Z",
        "closed_at": "2021-10-07T05:22:28Z",
        "merged_at": "2021-10-07T05:22:27Z",
        "body": "Hide empty replicas - Cluster bus delay broadcast new replica address until replica is in sync with primary \u00b7 Issue #8208 \u00b7 redis/redis (https://github.com/redis/redis/issues/8208): One possible solution for this without changing cluster bus is to hide empty replicas from CLUSTER SLOTS command. Empty means having a reported replication offset of zero. This relies on the replication offset reported by nodes in cluster PING/PONG messages.\r\n\r\nHiding empty replicas from CLUSTER SLOTS (while still showing them in CLUSTER NODES) is similar to the current hiding of failed nodes. From a data-path POV, an empty replica is not very useful.",
        "comments": 24
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-29T08:00:48Z",
        "closed_at": "2021-10-11T13:02:06Z",
        "merged_at": null,
        "body": "Prevent clients from being blocked forever in cluster when they block with their own module command and the hash slot is migrated to another master at the same time. \r\n\r\nAlso send a redirection error to the blocked clients.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2021-07-28T20:57:57Z",
        "closed_at": "2021-07-30T00:29:24Z",
        "merged_at": "2021-07-30T00:29:24Z",
        "body": "Redis 6.0 contained a bug when the master uses disk-based replication (repl-diskless-sync is no) and the replica is disk-less (repl-diskless-load is set to non-default value).\r\nThe bug could cause the rdb loading code in the replica to buffer too much data from the socket and fail the replication.\r\n\r\nDue to https://github.com/redis/redis/commit/da840e9851bab8d1674e245a812b2105be111208 the error condition does not depend on the while loop where we read from socket. This change cleans up the code and extracts the condition outside the loop. \r\nThe change adds errno to \"Failed trying to load the MASTER synchronization DB\" error message in readSyncBulkPayload() to make debugging of the similar problems easier in the future.",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2021-07-28T13:54:30Z",
        "closed_at": "2021-07-29T09:11:29Z",
        "merged_at": "2021-07-29T09:11:29Z",
        "body": "The `lru_clock` and `lru` bits in `robj` save the least significant 24 bits of the unixtime (seconds since 1/1/1970), and wrap around every 194 days.\r\nThe `objectSetLRUOrLFU` function, which is used in RESTORE with IDLETIME argument, and also in replica or master loading an RDB that contains LRU, and by a module API had a bug that's triggered when that happens.\r\n\r\nThe scenario was that the idle time that came from the user, let's say RESTORE command is about 1000 seconds (e.g. in the `RESTORE can set LRU` test we have), and the current `lru_clock` just wrapped around and is less than 1000 (i.e. a period of 1000 seconds once in some 6 months), the expression in that function would produce a negative value and the code (and comment) specified that the best way to solve that is push the idle time backwards into the past by 3 months. i.e. an idle time of 3 months instead of 1000 seconds.\r\n\r\ninstead, the right thing to do is to unwrap it, and put it near LRU_CLOCK_MAX. since now `lru_clock` is smaller than `obj->lru` it will be unwrapped again by `estimateObjectIdleTime`.\r\n\r\nbug was introduced by 052e03495f, but the code before it also seemed wrong.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-07-26T09:41:13Z",
        "closed_at": "2021-07-29T05:58:51Z",
        "merged_at": null,
        "body": "fix #9275 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 105,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-07-23T17:00:52Z",
        "closed_at": "2021-08-01T16:32:25Z",
        "merged_at": "2021-08-01T16:32:25Z",
        "body": "With an empty src key, we need to deal with two situations:\r\n1. non-STORE: We should return emptyarray.\r\n2. STORE: Try to delete the store key and return 0.\r\n\r\nThis applies to both GEOSEARCHSTORE (new to v6.2), and\r\nalso GEORADIUS STORE (which was broken since forever)\r\n\r\nThis pr try to fix #9261. i.e. both STORE variants would have behaved like the non-STORE variants when the source key was missing, returning an empty array and not deleting the destination key, instead of returning 0, and deleting the destination key.\r\n\r\nAlso add more tests for some commands.\r\n- GEORADIUS: wrong type src key\u3001non existing src key\u3001empty search\u3001store with non existing src key\u3001store with empty search\r\n- GEORADIUSBYMEMBER: wrong type src key\u3001non existing src key\u3001non existing member\u3001store with non existing src key\r\n- GEOSEARCH: wrong type src key\u3001non existing src key\u3001empty search\u3001frommember with non existing member\r\n- GEOSEARCHSTORE: wrong type key\u3001non existing src key\u3001fromlonlat with empty search\u3001frommember with non existing member\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1532,
        "deletions": 18,
        "changed_files": 23,
        "created_at": "2021-07-23T07:45:32Z",
        "closed_at": "2022-05-05T02:13:42Z",
        "merged_at": null,
        "body": "In the production environment, RDMA gets popular and common for a\r\nnetworking card. So support RDMA as transport layer protocol to\r\nimprove performance and cost reduction.\r\n\r\nNote that this feature is ONLY implemented/tested on Linux.\r\n\r\nActually, this is the v2 implementation. The v1 uses low level IB\r\nverbs API directly, the code and discuss ses PR:\r\n    https://github.com/redis/redis/pull/9161\r\n\r\nInstead of low level API, the v2 use rsocket which is implemented\r\nby rdma-core to simplify the work in Redis.\r\n\r\nThe test result is quite exciting:\r\nCPU: Intel(R) Xeon(R) Platinum 8260.\r\nNIC: Mellanox ConnectX-5.\r\nConfig of redis: appendonly no, port 6379, rdma-port 6379, appendonly no,\r\n                 server_cpulist 12, bgsave_cpulist 16.\r\nFor RDMA: ./redis-benchmark -h HOST -c 30 -n 10000000 -r 1000000000 \\\r\n          --threads 8 -d 512 -t ping,set,get,lrange_100 --rdma\r\nFor TCP: ./redis-benchmark -h HOST -c 30 -n 10000000 -r 1000000000 \\\r\n          --threads 8 -d 512 -t ping,set,get,lrange_100\r\n\r\n====== PING_INLINE ======\r\n    TCP: QPS: 159017   AVG LAT: 0.183\r\nv1 RDMA: QPS: 523944   AVG LAT: 0.054\r\nv2 RDMA: QPS: 492683   AVG LAT: 0.052\r\n\r\n====== PING_MBULK ======\r\n    TCP: QPS: 162256   AVG LAT: 0.179\r\nv1 RDMA: QPS: 509839   AVG LAT: 0.056\r\nv2 RDMA: QPS: 532226   AVG LAT: 0.048\r\n\r\n====== SET ======\r\n    TCP: QPS: 154700   AVG LAT: 0.187\r\nv1 RDMA: QPS: 492368   AVG LAT: 0.058\r\nv2 RDMA: QPS: 295534   AVG LAT: 0.095\r\n\r\n====== GET ======\r\n    TCP: QPS: 159022   AVG LAT: 0.182\r\nv1 RDMA: QPS: 525099   AVG LAT: 0.054\r\nv1 RDMA: QPS: 411488   AVG LAT: 0.065\r\n\r\n====== LPUSH (needed to benchmark LRANGE) ======\r\n    TCP: QPS: 142537   AVG LAT: 0.207\r\nv1 RDMA: QPS: 395038   AVG LAT: 0.073\r\nv2 RDMA: QPS: 353232   AVG LAT: 0.079\r\n\r\n====== LRANGE_100 (first 100 elements) ======\r\n    TCP: QPS:  36171   AVG LAT: 0.657\r\nv1 RDMA: QPS:  55266   AVG LAT: 0.412\r\nv2 RDMA: QPS:  52228   AVG LAT: 0.468\r\n\r\nSigned-off-by: zhenwei pi <pizhenwei@bytedance.com>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 66,
        "deletions": 19,
        "changed_files": 6,
        "created_at": "2021-07-21T15:11:39Z",
        "closed_at": "2021-07-21T18:07:15Z",
        "merged_at": "2021-07-21T18:07:15Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues that affect\r\nauthenticated client connections on 32-bit versions. MODERATE otherwise.\r\n\r\nFix integer overflow in BITFIELD on 32-bit versions (CVE-2021-32761).\r\nAn integer overflow bug in Redis version 2.2 or newer can be exploited using the\r\nBITFIELD command to corrupt the heap and potentially result with remote code\r\nexecution.\r\n\r\nBug fixes:\r\n* Fix overflows on 32-bit versions in GETBIT, SETBIT, BITCOUNT, BITPOS, and BITFIELD (#9191)\r\n* Fix ziplist length updates on big-endian platforms (#2080)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 842,
        "deletions": 152,
        "changed_files": 26,
        "created_at": "2021-07-21T15:03:36Z",
        "closed_at": "2021-07-21T18:07:03Z",
        "merged_at": "2021-07-21T18:07:03Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues that affect\r\nauthenticated client connections on 32-bit versions. MODERATE otherwise.\r\n\r\nFix integer overflow in BITFIELD on 32-bit versions (CVE-2021-32761).\r\nAn integer overflow bug in Redis version 2.2 or newer can be exploited using the\r\nBITFIELD command to corrupt the heap and potentially result with remote code\r\nexecution.\r\n\r\nBug fixes that involve behavior changes:\r\n* Change reply type for ZPOPMAX/MIN with count in RESP3 to nested array (#8981).\r\n  Was using a flat array like in RESP2 instead of a nested array like ZRANGE does.\r\n\r\nBug fixes:\r\n* Fail EXEC command in case a watched key is expired (#9194)\r\n* Fix SMOVE not to invalidate dest key (WATCH and tracking) when member already exists (#9244)\r\n* Fix SINTERSTORE not to delete dest key when getting a wrong type error (#9032)\r\n* Fix overflows on 32-bit versions in GETBIT, SETBIT, BITCOUNT, BITPOS, and BITFIELD (#9191)\r\n* Set TCP keepalive on inbound cluster bus connections (#9230)\r\n* Fix ziplist length updates on big-endian platforms (#2080)\r\n* Fix diskless replica loading to recover from RDB short read on module AUX data (#9199)\r\n* Fix race in client side tracking (#9116)\r\n* If diskless repl child is killed, make sure to reap the child pid (#7742)\r\n* Add a timeout mechanism for replicas stuck in fullsync (#8762)\r\n\r\nCLI tools:\r\n* redis-cli cluster import support source and target that require auth (#7994)\r\n* redis-cli cluster import command may issue wrong MIGRATE command, sending COPY instead of REPLACE (#8945)\r\n* redis-cli support for RESP3 set type in CSV and RAW output (#7338)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1099,
        "deletions": 227,
        "changed_files": 53,
        "created_at": "2021-07-21T13:38:14Z",
        "closed_at": "2021-07-21T18:06:49Z",
        "merged_at": "2021-07-21T18:06:49Z",
        "body": "Upgrade urgency: SECURITY, contains fixes to security issues that affect\r\nauthenticated client connections on 32-bit versions. MODERATE otherwise.\r\n\r\nFix integer overflow in BITFIELD on 32-bit versions (CVE-2021-32761).\r\nAn integer overflow bug in Redis version 2.2 or newer can be exploited using the\r\nBITFIELD command to corrupt the heap and potentially result with remote code\r\nexecution.\r\n\r\nBug fixes that involve behavior changes:\r\n* Change reply type for ZPOPMAX/MIN with count in RESP3 to nested array (#8981).\r\n  Was using a flat array like in RESP2 instead of a nested array like ZRANGE does.\r\n* Fix reply type for HRANDFIELD and ZRANDMEMBER when key is missing (#9178).\r\n  Was using a null array instead of an empty array.\r\n* Fix reply type for ZRANGESTORE when source key is missing (#9089).\r\n  Was using an empty array like ZRANGE instead of 0 (used in the STORE variant).\r\n\r\nBug fixes that are only applicable to previous releases of Redis 6.2:\r\n* ZRANDMEMBER WITHSCORES with negative COUNT may return bad score (#9162)\r\n* Fix crash after CLIENT UNPAUSE when threaded I/O config is enabled (#9041)\r\n* Fix XTRIM or XADD with LIMIT may delete more entries than the limit (#9048)\r\n* Fix build issue with OpenSSL 1.1.0 (#9233)\r\n\r\nOther bug fixes:\r\n* Fail EXEC command in case a watched key is expired (#9194)\r\n* Fix SMOVE not to invalidate dest key (WATCH and tracking) when member already exists (#9244)\r\n* Fix SINTERSTORE not to delete dest key when getting a wrong type error (#9032)\r\n* Fix overflows on 32-bit versions in GETBIT, SETBIT, BITCOUNT, BITPOS, and BITFIELD (#9191)\r\n* Improve MEMORY USAGE on stream keys (#9164)\r\n* Set TCP keepalive on inbound cluster bus connections (#9230)\r\n* Fix diskless replica loading to recover from RDB short read on module AUX data (#9199)\r\n* Fix race in client side tracking (#9116)\r\n* Fix ziplist length updates on big-endian platforms (#2080)\r\n\r\n\r\nCLI tools:\r\n* redis-cli cluster import command may issue wrong MIGRATE command, sending COPY instead of REPLACE (#8945)\r\n* redis-cli --rdb fixes when using \"-\" to write to stdout (#9136, #9135)\r\n* redis-cli support for RESP3 set type in CSV and RAW output (#7338)\r\n\r\nModules:\r\n* Module API for getting current command name (#8792)\r\n* Fix RM_StringTruncate when newlen is 0 (#3718)\r\n* Fix CLIENT UNBLOCK crashing modules without timeout callback (#9167)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 117,
        "deletions": 76,
        "changed_files": 5,
        "created_at": "2021-07-21T11:08:31Z",
        "closed_at": "2021-08-02T05:31:33Z",
        "merged_at": "2021-08-02T05:31:33Z",
        "body": "Fixes:\r\n- When a consumer is created as a side effect, redis didn't issue a keyspace notification,\r\n  nor incremented the server.dirty (affects periodic snapshots).\r\n  this was a bug in XREADGROUP, XCLAIM, and XAUTOCLAIM.\r\n- When attempting to delete a non-existent consumer, don't issue a keyspace notification\r\n  and don't increment server.dirty\r\n  this was a bug in XGROUP DELCONSUMER\r\n\r\nOther changes:\r\n- Changed streamLookupConsumer() to always only do lookup consumer (never do implicit creation),\r\n  Its last seen time is updated unless the SLC_NO_REFRESH flag is specified.\r\n- Added streamCreateConsumer() to create a new consumer. When the creation is successful,\r\n  it will notify and dirty++ unless the SCC_NO_NOTIFY or SCC_NO_DIRTIFY flags is specified.\r\n- Changed streamDelConsumer() to always only do delete consumer.\r\n- Added keyspace notifications tests about stream events.\r\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 324,
        "deletions": 83,
        "changed_files": 15,
        "created_at": "2021-07-20T13:37:28Z",
        "closed_at": "2021-08-04T14:25:20Z",
        "merged_at": null,
        "body": "For test purpose, adding configurable variable for delayed time in Sentinel Test file\r\nIt is mentioned in the following 2 PR:\r\nhttps://github.com/redis/redis/pull/8891 and https://github.com/redis/redis/pull/8710",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 61,
        "deletions": 41,
        "changed_files": 1,
        "created_at": "2021-07-19T22:06:38Z",
        "closed_at": "2021-07-30T00:31:51Z",
        "merged_at": "2021-07-30T00:31:51Z",
        "body": "Currently in CME Redis when writing to the cluster bus send buffer If there is not enough space Redis grows the send buffer. After Redis sends the message on the cluster bus it never frees this memory normally. This can consume an arbitrary amount of memory if someone is sending a lot of message on the cluster. An example that can cause this is pubsub messages. \r\n\r\nMy solution is to free the memory in cluster cron job similar to how the client query buffer works https://github.com/redis/redis/blob/unstable/src/server.c#L1672. Another option is to free the memory in the send handler https://github.com/redis/redis/blob/unstable/src/cluster.c#L2296.\r\n\r\nTesting:\r\n\r\nCreate a two node cluster. On Node 1 ran while :; do     ./redis-cli -p port PUBLISH channel msg; done. Then I paused node 2 for 10 seconds with sudo kill -STOP PID and sudo kill -CONT PID. Then I checked the used_memory_dataset.\r\n\r\nBefore commit:\r\n\r\nused_memory_dataset:4321168\r\n\r\nAfter commit:\r\n\r\nused_memory_dataset:133576",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 39,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2021-07-18T09:32:08Z",
        "closed_at": "2021-08-01T08:29:32Z",
        "merged_at": "2021-08-01T08:29:32Z",
        "body": "",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-18T07:29:16Z",
        "closed_at": "2021-07-22T10:40:00Z",
        "merged_at": "2021-07-22T10:40:00Z",
        "body": "This fixes an issue with zslGetRank which will happen only if the\r\nskiplist data stracture is added two entries with the same element name,\r\nthis can't happen in redis zsets (we use dict), but in theory this is a\r\nbug in the underlaying skiplist code.\r\n\r\nFixes #3081 and #4032",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-17T07:35:24Z",
        "closed_at": "2021-07-17T18:03:28Z",
        "merged_at": "2021-07-17T18:03:28Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-07-16T15:06:18Z",
        "closed_at": "2021-07-17T06:54:06Z",
        "merged_at": "2021-07-17T06:54:06Z",
        "body": "In case dest key already contains the member, the dest key isn't modified, so the command shouldn't invalidate watch.\r\nFixes #9242",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-16T04:18:15Z",
        "closed_at": "2021-07-19T08:10:25Z",
        "merged_at": "2021-07-19T08:10:25Z",
        "body": "genModulesInfoStringRenderModulesList lack `|`.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 50,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2021-07-15T20:47:53Z",
        "closed_at": "2021-07-29T09:32:29Z",
        "merged_at": "2021-07-29T09:32:29Z",
        "body": "There is a bug that is mentioned in issue 8786 https://github.com/redis/redis/issues/8786. \r\n\r\nThe issue is that when a sentinel with the same address and IP is turned on with a different runid, its port is set to 0 but it is still present in the dictionary master->sentinels which contain all the sentinels for a master. This causes a problem when we do INFO SENTINEL because it takes the size of the dictionary of sentinels. This might also cause a problem for failover if enough sentinels have their port set to 0 since the number of voters in failover is also determined by the size of the dictionary of sentinels.\r\n\r\nWe suggest that we should remove the sentinels with the port set to zero from the dictionary of sentinels.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-07-15T11:08:02Z",
        "closed_at": "2022-03-09T05:55:07Z",
        "merged_at": null,
        "body": "Fix https://github.com/redis/redis/issues/9223\r\n\r\nIf source node lost last slot and target node receives CLUSTER SETSLOT\r\nbefore source node, source node will be replica of target node. Then\r\nsource node will report error when CLUSTER SETSLOT is received. So we\r\nneed to send CLUSTER SETSLOT to source node before target node.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 115,
        "deletions": 34,
        "changed_files": 5,
        "created_at": "2021-07-14T15:01:04Z",
        "closed_at": "2021-07-14T16:14:31Z",
        "merged_at": "2021-07-14T16:14:31Z",
        "body": "- promote the code in DEBUG PROTOCOL to addReplyBigNum\r\n- DEBUG PROTOCOL ATTRIB skips the attribute when client is RESP2\r\n- networking.c addReply for push and attributes generate assertion when\r\n  called on a RESP2 client, anything else would produce a broken\r\n  protocol that clients can't handle.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-07-14T11:41:26Z",
        "closed_at": "2022-02-21T09:31:32Z",
        "merged_at": null,
        "body": "After a discussion in #9068 and #9194, we reached an agreement\r\nthat we should handle all scenarios about expire, not only the real\r\nexpired deletion but also the logic expired time.\r\n\r\nThis PR aims to fix the issue below:\r\nTime 1: we have a key \"foo\" is expired but not deleted.\r\nTime 2: we WATCH the key \"foo\".\r\nTime 3: execute EXEC and we find \"foo\" is expired and discard the\r\ntransaction, but it's not right, cause key \"foo\" is expired before\r\nWATCH.\r\n\r\nTo fix it, the WATCH command now calls expireIfNeeded() to delete\r\nthe expired keys, and considering stale data would not be deleted if\r\nclients are paused, WATCH command is now with may-replicate flag.\r\n\r\nNotes: expireIfNeeded() cannot work in replicas, but WATCH expired keys\r\nin replicas is rare case, we can fix it in future.\r\n\r\n@redis/core-team @zuiderkwast please check.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-07-14T07:48:13Z",
        "closed_at": "2021-07-14T09:32:14Z",
        "merged_at": "2021-07-14T09:32:14Z",
        "body": "Found this while building on Debian Stretch and confirmed the problem is solved. I believe OpenSSL 1.1.0 is less popular, older systems use <1.1.0 and newer systems tend to use 1.1.1 for TLSv3 support.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-07-13T09:23:13Z",
        "closed_at": "2021-07-16T03:40:26Z",
        "merged_at": "2021-07-16T03:40:26Z",
        "body": "Addresses the issue where an inbound link is disconnected but is never detected since the packet was dropped. This can cause a memory leak as the connection will not be detected through any other mechanism. \r\n\r\nFor example, A and B are two nodes in a cluster.\r\n\r\nWhen A is partitioned away, B will free the \"struct clusterLink\" which is used for send ping to A. But the corresponding \"struct clusterLink\" in A won't be freed.\r\n\r\nWhen partition heals, B uses a new clusterLink to send ping to A and A will create another new corresponding clusterLink. The older \"struct clusterLink\" above is still in the memory.\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 290,
        "deletions": 295,
        "changed_files": 23,
        "created_at": "2021-07-13T08:42:36Z",
        "closed_at": "2021-08-05T05:25:58Z",
        "merged_at": "2021-08-05T05:25:58Z",
        "body": "reduce dict struct memory overhead\r\non 64bit dict size goes down from jemalloc's 96 byte bin to its 56 byte bin.\r\n\r\nsummary of changes:\r\n- Remove `privdata` from callbacks and dict creation. (this affects many files, see \"Interface change\" below).\r\n- Meld `dictht` struct into the `dict` struct to eliminate struct padding. (this affects just dict.c and defrag.c)\r\n- Eliminate the `sizemask` field, can be calculated from size when needed.\r\n- Convert the `size` field into `size_exp` (exponent), utilizes one byte instead of 8.\r\n\r\nInterface change: pass dict pointer to dict type call back functions.\r\nThis is instead of passing the removed privdata field. In the future if\r\nwe'd like to have private data in the callbacks we can extract it from\r\nthe dict type. We can extend dictType to include a custom dict struct\r\nallocator and use it to allocate more data at the end of the dict\r\nstruct. This data can then be used to store private data later acccessed\r\nby the callbacks.",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-11T20:28:27Z",
        "closed_at": "2021-07-14T07:46:33Z",
        "merged_at": "2021-07-14T07:46:33Z",
        "body": "This change ensures that if the connTLSWrite fails to write when the socket is closed,\r\nit returns -1. This would ensure that the client is closed in writeClient function. This also makes the function more compliant with the POSIX write function.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-10T04:54:52Z",
        "closed_at": "2021-07-10T15:04:55Z",
        "merged_at": "2021-07-10T15:04:55Z",
        "body": "The explanation of sha256.h is `Defines the API for the corresponding SHA1 implementation.`. But actually this file `Defines the API for the corresponding SHA256 implementation.`\r\n\r\nSo I fixed this.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-07-09T16:05:15Z",
        "closed_at": "2021-07-11T08:26:31Z",
        "merged_at": "2021-07-11T08:26:31Z",
        "body": "Related to issue: https://github.com/redis/redis/issues/7170\r\n\r\nThe if judgement `nextdiff == -4 && reqlen < 4` in __ziplistInsert.\r\nIt's strange, but it's useful. There will be problems during chain update.\r\n\r\nThis commit attempts to construct a situation that will cause an exception in the past.\r\nAnd adding tests to cover this situation.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 107,
        "deletions": 28,
        "changed_files": 4,
        "created_at": "2021-07-09T13:06:18Z",
        "closed_at": "2021-11-30T17:56:40Z",
        "merged_at": "2021-11-30T17:56:40Z",
        "body": "Resolves #7835 \r\n\r\nDemo:\r\n\r\n```\r\n127.0.0.1:6379> XADD x 1-0 foo bar\r\n\"1-0\"\r\n127.0.0.1:6379> XADD x 1-* foo bar\r\n\"1-1\"\r\n127.0.0.1:6379> XADD x 1-* foo bar\r\n\"1-2\"\r\n127.0.0.1:6379> XADD x * foo bar\r\n\"1625835471208-0\"\r\n127.0.0.1:6379> XADD x 1625835471208-* foo bar\r\n\"1625835471208-1\"\r\n127.0.0.1:6379> XADD x 1625835471208-18446744073709551615 foo bar\r\n\"1625835471208-18446744073709551615\"\r\n127.0.0.1:6379> XADD x 1625835471208-* foo bar\r\n(error) ERR The ID specified in XADD is equal or smaller than the target stream top item\r\n```\r\n\r\nTODO:\r\n\r\n- [x] Address the `0-*` use case (i.e. resolve new test failing)\r\n- [x] Documentation: https://github.com/redis/redis-doc/pull/1695",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2021-07-09T04:44:56Z",
        "closed_at": "2021-07-09T18:45:59Z",
        "merged_at": "2021-07-09T18:45:59Z",
        "body": "redis-check-aof/redis-check-rdb.\r\n\r\nRelated to https://github.com/redis/redis/pull/9176. Before this commit, redis-server starts as\r\nredis-check-aof/redis-check-rdb if the directory it is started from\r\ncontains the string redis-check-aof/redis-check-rdb. We check the\r\nexecutable name instead of directory.\r\n\r\nThis is not so important. Just for correctness and consistency.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-08T17:05:45Z",
        "closed_at": "2021-07-11T06:54:07Z",
        "merged_at": "2021-07-11T06:54:07Z",
        "body": "This attempts to catch any non-standard configuration where the test may\r\nfail and produce a false positive.\r\n\r\nFixes #9185 ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-07-07T13:43:13Z",
        "closed_at": "2021-07-13T15:16:06Z",
        "merged_at": "2021-07-13T15:16:06Z",
        "body": "Add missing comma in memory/xgroup command help message.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-07-07T03:16:24Z",
        "closed_at": "2021-09-03T22:52:40Z",
        "merged_at": "2021-09-03T22:52:39Z",
        "body": "Someone mentioned on https://github.com/redis/redis/pull/4721 that the guarantees for pub-sub pattern matching changed in v6.2, previously it was the total number of pattern subscriptions but it became the total number patterns in https://github.com/redis/redis/pull/8472. This change is just codifying that change and we will explicitly mention it in the release notes.",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2021-07-06T18:38:40Z",
        "closed_at": "2021-07-20T18:48:44Z",
        "merged_at": "2021-07-20T18:48:44Z",
        "body": "- SELECT and WAIT don't read or write from the keyspace (unlike DEL, EXISTS, EXPIRE, DBSIZE, KEYS, etc).\r\nthey're more similar to AUTH and HELLO (and maybe PING and COMMAND).\r\nthey only affect the current connection, not the server state, so they should be `@connection`, not `@keyspace`\r\n\r\n- ROLE, like LASTSAVE is `@admin` (and `@dangerous` like INFO) \r\n\r\n- ASKING, READONLY, READWRITE are `@connection` too (not `@keyspace`)\r\n\r\n- Additionally, i'm now documenting the exact meaning of each ACL category so it's clearer which commands belong where.",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-07-06T09:21:15Z",
        "closed_at": "2022-08-02T15:06:48Z",
        "merged_at": null,
        "body": null,
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1843,
        "deletions": 290,
        "changed_files": 14,
        "created_at": "2021-07-06T06:54:05Z",
        "closed_at": "2021-08-04T13:28:07Z",
        "merged_at": "2021-08-04T13:28:07Z",
        "body": "## Current state\r\n1. Lua has its own parser that handles parsing `reds.call` replies and translates them to Lua objects that can be used by the user Lua code. The parser partially handles resp3 (missing big number, verbatim, attribute, ...)\r\n2. Modules have their own parser that handles parsing `RM_Call` replies and translates them to RedisModuleCallReply objects. The parser does not support resp3.\r\n\r\nIn addition, in the future, we want to add Redis Function (https://github.com/redis/redis/issues/8693) that will probably support more languages. At some point maintaining so many parsers will stop scaling (bug fixes and protocol changes will need to be applied on all of them). We will probably end up with different parsers that support different parts of the resp protocol (like we already have today with Lua and modules)\r\n\r\n## PR Changes\r\nThis PR attempt to unified the reply parsing of Lua and modules (and in the future Redis Function) by introducing a new parser unit (`resp_parser.c`). The new parser handles parsing the reply and calls different callbacks to allow the users (another unit that uses the parser, i.e, Lua, modules, or Redis Function) to analyze the reply.\r\n\r\n### Lua API Additions\r\nThe code that handles reply parsing on `scripting.c` was removed. Instead, it uses the resp_parser to parse and create a Lua object out of the reply. As mentioned above the Lua parser did not handle parsing big numbers, verbatim, and attribute. The new parser can handle those and so Lua also gets it for free. Those are translated to Lua objects in the following way:\r\n1. Big Number - Lua table `{'big_number':'<str representation for big number>'}`\r\n2. Verbatim - Lua table `{'verbatim_string':{'format':'<verbatim format>', 'string':'<verbatim string value>'}}`\r\n3. Attribute - currently ignored and not expose to the Lua parser, another issue will be open to decide how to expose it.\r\n\r\nTests were added to check resp3 reply parsing on Lua\r\n\r\n### Modules API Additions\r\nThe reply parsing code on `module.c` was also removed and the new resp_parser is used instead. In addition, the RedisModuleCallReply was also extracted to a separate unit located on `call_reply.c` (in the future, this unit will also be used by Redis Function). A nice side effect of unified parsing is that modules now also support resp3. Resp3 can be enabled by giving `3` as a parameter to the fmt argument of `RM_Call`. It is also possible to give `0`, which will indicate an auto mode. i.e, Redis will automatically chose the reply protocol base on the current client set on the RedisModuleCtx (this mode will mostly be used when the module want to pass the reply to the client as is). In addition, the following RedisModuleAPI were added to allow analyzing resp3 replies:\r\n* New RedisModuleCallReply types:\r\n   * `REDISMODULE_REPLY_MAP`\r\n   * `REDISMODULE_REPLY_SET`\r\n   * `REDISMODULE_REPLY_BOOL`\r\n   * `REDISMODULE_REPLY_DOUBLE`\r\n   * `REDISMODULE_REPLY_BIG_NUMBER`\r\n   * `REDISMODULE_REPLY_VERBATIM_STRING`\r\n   * `REDISMODULE_REPLY_ATTRIBUTE`\r\n\r\n* New RedisModuleAPI:\r\n   * `RedisModule_CallReplyDouble` - getting double value from resp3 double reply\r\n   * `RedisModule_CallReplyBool` - getting boolean value from resp3 boolean reply\r\n   * `RedisModule_CallReplyBigNumber` - getting big number value from resp3 big number reply\r\n   * `RedisModule_CallReplyVerbatim` - getting format and value from resp3 verbatim reply\r\n   * `RedisModule_CallReplySetElement` - getting element from resp3 set reply\r\n   * `RedisModule_CallReplyMapElement` - getting key and value from resp3 map reply\r\n   * `RedisModule_CallReplyAttribute` - getting a reply attribute\r\n   * `RedisModule_CallReplyAttributeElement` - getting key and value from resp3 attribute reply\r\n   \r\n* New context flags:\r\n   * `REDISMODULE_CTX_FLAGS_RESP3` - indicate that the client is using resp3\r\n\r\nTests were added to check the new RedisModuleAPI\r\n\r\n### Modules API Changes\r\n* RM_ReplyWithCallReply might return REDISMODULE_ERR if the given CallReply is in resp3 but the client expects resp2. This is not a breaking change because in order to get a resp3 CallReply one needs to specifically specify `3` as a parameter to the fmt argument of `RM_Call` (as mentioned above).\r\n\r\nTests were added to check this change\r\n\r\n### More small Additions\r\n* Added `debug set-disable-deny-scripts` that allows to turn on and off the commands no-script flag protection. This is used by the Lua resp3 tests so it will be possible to run `debug protocol` and check the resp3 parsing code.\r\n\r\nTODO:\r\n- [x] Parse resp3 attribute and add it to Lua and modules\r\n- [ ] Document Lua big number and verbatim reply format\r\n\r\nLeft for future PR's:\r\n- Parse push notifications - need to be decided if it's relevant for module/Lua, for now, we are not implementing it",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-06T06:12:35Z",
        "closed_at": "2021-07-06T07:46:10Z",
        "merged_at": "2021-07-06T07:46:10Z",
        "body": "So that we can avoid commands that are obviously wrong.\r\n\r\nAlso unified with loadServerConfigFromString\r\nbecause we also checked the range.\r\n\r\nMinor cleanup...\r\n\r\nBefore:\r\n```\r\n127.0.0.1:6379> replicaof 127.0.0.1 65536\r\nOK\r\n127.0.0.1:6379> replicaof 127.0.0.1 -1111\r\nOK\r\n```\r\n\r\nAfter:\r\n```\r\n127.0.0.1:6379> replicaof 127.0.0.1 -1\r\n(error) ERR Invalid master port\r\n127.0.0.1:6379> replicaof 127.0.0.1 -111\r\n(error) ERR Invalid master port\r\n127.0.0.1:6379> replicaof 127.0.0.1 65536\r\n(error) ERR Invalid master port\r\n```\r\n\r\nloadServerConfigFromString:\r\n```c\r\n        } else if ((!strcasecmp(argv[0],\"slaveof\") ||\r\n                    !strcasecmp(argv[0],\"replicaof\")) && argc == 3) {\r\n            slaveof_linenum = linenum;\r\n            sdsfree(server.masterhost);\r\n            if (!strcasecmp(argv[1], \"no\") && !strcasecmp(argv[2], \"one\")) {\r\n                server.masterhost = NULL;\r\n                continue;\r\n            }\r\n            server.masterhost = sdsnew(argv[1]);\r\n            char *ptr;\r\n            server.masterport = strtol(argv[2], &ptr, 10);\r\n            if (server.masterport < 0 || server.masterport > 65535 || *ptr != '\\0') {\r\n                err = \"Invalid master port\"; goto loaderr;\r\n            }\r\n            server.repl_state = REPL_STATE_CONNECT;\r\n```\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2021-07-06T02:28:08Z",
        "closed_at": "2021-07-06T07:01:37Z",
        "merged_at": "2021-07-06T07:01:37Z",
        "body": "touch keys only if the key is in one of the dbs,\r\nin case rewind the list when the key doesn't exist.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-07-05T20:58:19Z",
        "closed_at": "2021-07-06T05:21:17Z",
        "merged_at": "2021-07-06T05:21:17Z",
        "body": "Currently a replica is able to recover from a short read (when diskless loading is enabled) and avoid crashing/exiting, replying to the master and then the rdb could be sent again by the master for another load attempt by the replica.\r\nThere were a few scenarios that were not behaving similarly, such as when there is no end-of-file marker, or when module aux data failed to load, which should be allowed to occur due to a short read.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-07-05T13:57:47Z",
        "closed_at": "2021-07-05T15:09:49Z",
        "merged_at": "2021-07-05T15:09:49Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 612,
        "deletions": 66,
        "changed_files": 2,
        "created_at": "2021-07-04T22:52:27Z",
        "closed_at": "2022-01-20T06:37:57Z",
        "merged_at": null,
        "body": "# ACL Role\r\n\r\nACL role is a resource which will have the same set of rules (syntactically and semantically) which are valid for a user but won't contain passwords in it. ACL role can be associated to multiple users. If a ACL role is associated to a user, a command execution under a user's context will be validated against the user's permission as well as role(s) permission, if either of it satisfies the command is valid to be ran.\r\n\r\n## Current Pain\r\n\r\n* **Global Rules** - Currently a client could create a user and associate set of acl rules to it. However, the rule context is global ([the commands and keys supplied are treated as global](https://github.com/redis/redis/issues/7291#issue-621774067)). This causes clients to create multiple users each restricted to set of keys and commands which is cumbersome. \r\n\r\n* **Lack of Reusability** - If multiple users need to have same set of rules, currently each user needs to be created with the same set of rules and managed independently, this is a painful operation to manage lot's of user. Rather common permission to a single resource and the same resource could be associated and reused.\r\n\r\n## Evaluation Logic\r\n\r\n* Rule of user and each role is evaluated in isolation. If any one of the rule is valid, the command execution is allowed.\r\n* Ordering of roles doesn't matter i.e. sequence independent.\r\n\r\n## Flat Hierarchy\r\n\r\n* User can contain multiple roles associated to them.\r\n* Roles can't associate with another role to avoid complex recursive dependency.\r\n\r\n## API(s)\r\n\r\n### ACL SETROLE\r\n```\r\nACL SETROLE rolename [rule [rule ...]]\r\n\r\nResponse: OK (On successful creation).\r\n```\r\n\r\n### ACL GETROLE\r\n```\r\nACL GETROLE rolename\r\n\r\nResponse: Map of key (flags, commands, keys, channels) values (list)\r\n```\r\n\r\n### ACL DELROLE\r\n```\r\nACL DELROLE rolename [rolename ...]\r\n```\r\n\r\n### ACL ROLELIST\r\n```\r\nACL ROLELIST\r\n\r\nResponse: List of roles with all information in the file save format.\r\n```\r\n\r\n### ACL ROLES\r\n```\r\nACL ROLES\r\nResponse: List of role names\r\n```\r\n\r\n### ACL SETUSER\r\n```\r\nACL SETUSER username [rule [rule ...]]\r\n\r\nNew rules\r\n\r\n* $+<rolename> - associate a role to the user.\r\n* $-<rolename> - disassociate a role from the user.\r\n```\r\n\r\n\r\n## Example usage of all the API(s)\r\n\r\n\r\n### Create a role\r\n```\r\n127.0.0.1:6379> acl setrole orders +@all ~orders*\r\nOK\r\n127.0.0.1:6379> acl setrole sales +@all ~sales*\r\nOK\r\n```\r\n\r\n### Associate roles with user.\r\n```\r\n127.0.0.1:6379> acl setuser test on nopass $+orders $+sales\r\nOK\r\n```\r\n### Describe user with roles information.\r\n```\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* &* +@all\"\r\n2) \"user test on nopass &* -@all $+orders $+sales\"\r\n```\r\n\r\n### Permission denied if role permission also doesn't satisfy.\r\n```\r\n127.0.0.1:6379> auth test a\r\nOK\r\n127.0.0.1:6379> set orders1 1000\r\nOK\r\n(9.94s)\r\n127.0.0.1:6379> set sales1 1000\r\nOK\r\n127.0.0.1:6379> set s 10\r\n(error) NOPERM this user has no permissions to run the 'set' command or its subcommand\r\n```\r\n\r\n### Remove role from user.\r\n```\r\n127.0.0.1:6379> acl setuser test $-sales\r\nOK\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* &* +@all\"\r\n2) \"user test on nopass &* -@all $+orders\"\r\n```\r\n\r\n### List all roles and rolenames.\r\n```\r\n127.0.0.1:6379> acl rolelist\r\n1) \"role orders ~orders* &* +@all\"\r\n2) \"role sales ~sales* &* +@all\"\r\n\r\n127.0.0.1:6379> acl roles\r\n1) \"orders\"\r\n2) \"sales\"\r\n```\r\n### Get all information for a role.\r\n```\r\n127.0.0.1:6379> acl getrole orders\r\n1) \"flags\"\r\n2) 1) \"allchannels\"\r\n   2) \"allcommands\"\r\n3) \"commands\"\r\n4) \"+@all\"\r\n5) \"keys\"\r\n6) 1) \"orders*\"\r\n7) \"channels\"\r\n8) 1) \"*\"\r\n```\r\n\r\n### Delete role (associated with/without user).\r\n```\r\n127.0.0.1:6379> acl delrole orders\r\n(error) ERR Role is associated to users. Disassociate all users first to delete a role.\r\n127.0.0.1:6379> acl setuser test $-orders\r\nOK\r\n127.0.0.1:6379> acl delrole orders\r\n(integer) 1\r\n127.0.0.1:6379> acl rolelist\r\n1) \"role sales ~sales* &* +@all\"\r\n```\r\n\r\n- [x] Implement basic API(s)\r\n- [x] Implement ACL LOAD/SAVE\r\n- [ ] Add tests\r\n- [ ] Add roles to ACL HELP\r\n- [ ] Disable allchannels by default for roles.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2021-07-04T12:45:48Z",
        "closed_at": "2021-07-11T10:17:24Z",
        "merged_at": "2021-07-11T10:17:23Z",
        "body": "There are two issues fixed in this Pr: \r\n1) fixing issue #9068, we want to fail the EXEC command in case there is a watched key that's logically expired but not yet deleted by active expire or lazy expire.\r\n\r\n2) we saw that currently cache time is update in every call, this time is being also being use for the isKeyExpired comparison, we in case of a nested call, we want to update the cache time only in the first call (execCommand)",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-07-04T11:57:57Z",
        "closed_at": "2021-07-04T16:43:58Z",
        "merged_at": "2021-07-04T16:43:58Z",
        "body": "This makes it possible to distinguish between null response and an empty\r\narray (currently the tests infra translates both to an empty string/list)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-04T08:27:47Z",
        "closed_at": "2021-07-04T11:21:53Z",
        "merged_at": "2021-07-04T11:21:53Z",
        "body": "fixes test issue introduced in #9167\r\n1. invalid reads due to accessing non-retained string (passed as unblock context).\r\n2. leaking module blocked client context, see #6922 for info.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 20,
        "changed_files": 4,
        "created_at": "2021-07-04T03:02:40Z",
        "closed_at": "2021-07-21T13:25:20Z",
        "merged_at": "2021-07-21T13:25:20Z",
        "body": "GETBIT, SETBIT may access wrong address because of wrap.\r\nBITCOUNT and BITPOS may return wrapped results.\r\nBITFIELD may access the wrong address but also allocate insufficient memory and segfault (see CVE-2021-32761).\r\n\r\nThis commit uses `uint64_t` or `long long` instead of `size_t`.\r\nrelated https://github.com/redis/redis/pull/8096\r\n\r\nAt 32bit platform:\r\n> setbit bit 4294967295 1\r\n(integer) 0\r\n> config set proto-max-bulk-len 536870913\r\nOK\r\n> append bit \"\\xFF\"\r\n(integer) 536870913\r\n> getbit bit 4294967296\r\n(integer) 0\r\n\r\nWhen the bit index is larger than 4294967295, size_t can't hold bit index. In the past,  `proto-max-bulk-len` is limit to 536870912, so there is no problem.\r\n\r\nAfter this commit, bit position is stored in `uint64_t` or `long long`. So when `proto-max-bulk-len > 536870912`, 32bit platforms can still be correct.\r\n\r\nFor 64bit platform, this problem still exists. The major reason is bit pos 8 times of byte pos. When proto-max-bulk-len is very larger, bit pos may overflow.\r\nBut at 64bit platform, we don't have so long string. So this bug may never happen.\r\n\r\nAdditionally this commit add a test cost `512MB` memory which is tag as `large-memory`. Make freebsd ci and valgrind ci ignore this test.",
        "comments": 20
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2021-07-02T22:03:49Z",
        "closed_at": "2021-07-03T15:51:53Z",
        "merged_at": "2021-07-03T15:51:53Z",
        "body": "incrDec was manually formatting the integer response. This uses the standard helper function. ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2021-07-02T11:56:29Z",
        "closed_at": "2021-07-11T06:54:07Z",
        "merged_at": null,
        "body": "At some machine, firewall is configured accept any packets from\r\n127.0.0.1 but reject any packets from other 127.0.0.*. So\r\nbind-source-addr test using source addr 127.0.0.2 will fail.\r\nSo in the test, we start server bind 127.0.0.2 and bind source addr\r\n127.0.0.1 .\r\n\r\nAlso fix in test start_server, when provide bind config, server_is_up may take\r\nwrong host.\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-07-02T09:43:22Z",
        "closed_at": "2021-07-15T23:09:05Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 47,
        "deletions": 4,
        "changed_files": 5,
        "created_at": "2021-06-30T13:34:45Z",
        "closed_at": "2021-07-05T07:41:57Z",
        "merged_at": "2021-07-05T07:41:57Z",
        "body": "we treat non-existing key as an empty hash or zset\r\n\r\nBefore:\r\n```\r\n127.0.0.1:6379> hrandfield a 1\r\n(nil)\r\n127.0.0.1:6379> zrandmember a 1\r\n(nil)\r\n```\r\n\r\nAfter:\r\n```\r\n127.0.0.1:6379> hrandfield a 1\r\n(empty array)\r\n127.0.0.1:6379> zrandmember a 1\r\n(empty array)\r\n```\r\n\r\nalso can check https://redis.io/commands/hrandfield\r\n```\r\nExactly count fields, or an empty array if the hash is empty (non-existing key), are always returned.\r\n```\r\n\r\nAlso add test to check emptyarray.",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2021-06-30T12:17:51Z",
        "closed_at": "2021-07-05T07:34:21Z",
        "merged_at": "2021-07-05T07:34:20Z",
        "body": "Copy from #1339, by @RajivKurian:\r\nUsing accept4 on linux lets us skip the two calls to fcntl() to set a socket as non-blocking. This saves two system calls on linux. A corresponding issue was filed at #1332.\r\n\r\nA few other reasons to justify this change (as pointed by @moreaki):\r\n\r\nhttp://article.gmane.org/gmane.linux.kernel/758201\r\nhttp://lwn.net/Articles/281965/\r\nhttp://udrepper.livejournal.com/20407.html\r\nhttp://danwalsh.livejournal.com/53603.html\r\n\r\nAccept4 was in Linux 2.6.28 (Dec 2008) and support in GLIBC is available starting with version 2.10 (May 2009). We could also add support for older GLIBC versions if the maintainers think it would be useful.\r\n\r\nChanges:\r\n* Add the SOCK_CLOEXEC option to the accept4() call\r\n  This  ensure that a fork/exec call does not leak a file descriptor.\r\n* Move anetCloexec and connNonBlock info anetGenericAccept\r\n* Moving connNonBlock from accept handlers to anetGenericAccept\r\n\r\nMoving connNonBlock from createClient, is safe because createClient is\r\nused in the following ways:\r\n1. without a connection (fake client)\r\n2. on an accepted connection (see above)\r\n3. creating the master client by using connConnect (see below)\r\n\r\nThe third case, can either use anetTcpNonBlockConnect, or connTLSConnect\r\nwhich is by default non-blocking.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-06-30T11:59:24Z",
        "closed_at": "2021-07-01T05:05:18Z",
        "merged_at": "2021-07-01T05:05:18Z",
        "body": "Before this commit, redis-server starts in sentinel mode if the first startup\r\nargument has the string redis-sentinel, so redis also starts in sentinel mode\r\nif the directory it was started from contains the string redis-sentinel.\r\nNow we check the executable name instead of directory.\r\n\r\nSome examples:\r\n1. Execute ./redis-sentinel/redis/src/redis-sentinel, starts in sentinel mode.\r\n2. Execute ./redis-sentinel/redis/src/redis-server, starts in server mode,\r\n   but before, redis will start in sentinel mode.\r\n3. Execute ./redis-sentinel/redis/src/redis-server --sentinel, of course, like\r\n   before, starts in sentinel mode.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-06-30T09:22:50Z",
        "closed_at": "2021-07-01T05:19:05Z",
        "merged_at": "2021-07-01T05:19:05Z",
        "body": "For the sdscatfmt function in sds.c, when the parameter fmt ended up with '%',\r\nthe behavior is undefined. This commit fix this bug.\r\n\r\nlucky we don't have any format string that ends with % and we don't take user input\r\nto use it as format string, so this doesn't fix any real bug....\r\n\r\nThis replaces #2698 which had indentation issues that i was unable to fix",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 41,
        "changed_files": 7,
        "created_at": "2021-06-29T16:04:43Z",
        "closed_at": "2021-08-17T06:54:28Z",
        "merged_at": null,
        "body": "After handling a bigarg there's no point performing a query buff pre allocation. Lets leave the qbuf allocation logic to one place only: `readQueryFromClient()`. We might want to keep the code as is, but in so lets just alloc an empty sds after handling the bigarg.\r\n\r\n- [ ] Still need to fix querybuf shrink test accordingly\r\n\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-29T12:12:47Z",
        "closed_at": "2021-06-29T13:48:52Z",
        "merged_at": "2021-06-29T13:48:52Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-29T11:57:24Z",
        "closed_at": "2022-02-06T05:53:44Z",
        "merged_at": null,
        "body": "When level has reached ZSKIPLIST_MAXLEVEL in zslRandomLevel ,the increasing should stop and return ZSKIPLIST_MAXLEVEL.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 119,
        "deletions": 3,
        "changed_files": 5,
        "created_at": "2021-06-29T11:05:05Z",
        "closed_at": "2021-07-01T14:11:28Z",
        "merged_at": "2021-07-01T14:11:28Z",
        "body": "Modules that use background threads with thread safe contexts are likely\r\nto use `RM_BlockClient()` without a timeout function, because they do not\r\nset up a timeout.\r\n\r\nBefore this commit, `CLIENT UNBLOCK` would result with a crash as the\r\n`NULL` timeout callback is called. Beyond just crashing, this is also\r\nlogically wrong as it may throw the module into an unexpected client\r\nstate.\r\n\r\nThis commits makes `CLIENT UNBLOCK` on such clients behave the same as\r\nany other client that is not in a blocked state and therefore cannot be\r\nunblocked.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 934,
        "deletions": 296,
        "changed_files": 16,
        "created_at": "2021-06-29T08:21:13Z",
        "closed_at": "2021-10-25T06:24:32Z",
        "merged_at": "2021-10-25T06:24:32Z",
        "body": "## Background\r\nFor redis master, one replica uses one copy of replication buffer, that is a big waste of memory, more replicas more waste, and allocate/free memory for every reply list also cost much. If we set client-output-buffer-limit small and write traffic is heavy, master may disconnect with replicas and can't finish synchronization with replica. If we set  client-output-buffer-limit  big, master may be OOM when there are many replicas that separately keep much memory. Because replication buffers of different replica client are the same, one simple idea is that all replicas only use one replication buffer, that will effectively save memory.\r\n\r\nSince replication backlog content is the same as replicas' output buffer, now we can discard replication backlog memory and use global shared replication buffer to implement replication backlog mechanism.\r\n\r\n## Implementation\r\nI create one global \"replication buffer\" which contains content of replication stream. The structure of \"replication buffer\" is similar to the reply list that exists in every client. But the node of list is `replBufBlock`, which has `id, repl_offset, refcount` fields.\r\n```c\r\n/* Replication buffer blocks is the list of replBufBlock.\r\n *\r\n * +--------------+       +--------------+       +--------------+\r\n * | refcount = 1 |  ...  | refcount = 0 |  ...  | refcount = 2 |\r\n * +--------------+       +--------------+       +--------------+\r\n *      |                                            /       \\\r\n *      |                                           /         \\\r\n *      |                                          /           \\\r\n *  Repl Backlog                               Replia_A      Replia_B\r\n * \r\n * Each replica or replication backlog increments only the refcount of the\r\n * 'ref_repl_buf_node' which it points to. So when replica walks to the next\r\n * node, it should first increase the next node's refcount, and when we trim\r\n * the replication buffer nodes, we remove node always from the head node which\r\n * refcount is 0. If the refcount of the head node is not 0, we must stop\r\n * trimming and never iterate the next node. */\r\n\r\n/* Similar with 'clientReplyBlock', it is used for shared buffers between\r\n * all replica clients and replication backlog. */\r\ntypedef struct replBufBlock {\r\n    int refcount;           /* Number of replicas or repl backlog using. */\r\n    long long id;           /* The unique incremental number. */\r\n    long long repl_offset;  /* Start replication offset of the block. */\r\n    size_t size, used;\r\n    char buf[];\r\n} replBufBlock;\r\n```\r\nSo now when we feed replication stream into replication backlog and all replicas, we only need to feed stream into replication buffer `feedReplicationBuffer`. In this function, we set some fields of replication backlog and replicas to references of the global replication buffer blocks. And we also need to check replicas' output buffer limit to free if exceeding `client-output-buffer-limit`, and trim replication backlog if exceeding `repl-backlog-size`.\r\n\r\nWhen sending reply to replicas, we also need to iterate replication buffer blocks and send its content, when totally sending one block for replica, we decrease current node count and increase the next current node count, and then free the block which reference is 0 from the head of replication buffer blocks.\r\n\r\nSince now we use linked list to manage replication backlog, it may cost much time for iterating all linked list nodes to find corresponding replication buffer node. So we create a rax tree to store some nodes  for index, but to avoid rax tree occupying too much memory, i record one per 64 nodes for index.\r\n\r\nCurrently, to make partial resynchronization as possible as much, we always let replication backlog as the last reference of replication buffer blocks, backlog size may exceeds our setting if slow replicas that reference vast replication buffer blocks, and this method doesn't increase memory usage since they share replication buffer. To avoid freezing server for freeing unreferenced replication buffer blocks when we need to trim backlog for exceeding backlog size setting, we trim backlog incrementally (free 64 blocks per call now), and make it faster in `beforeSleep`(free 640 blocks).\r\n\r\n### Other changes\r\n- `mem_total_replication_buffers`: we add this field in INFO command, it means the total memory of replication buffers used.\r\n- `mem_clients_slaves`:  now even replica is slow to replicate, and its output buffer memory is not 0, but it still may be 0, since replication backlog and replicas share one global replication buffer, only if replication buffer memory is more than the repl backlog setting size, we consider the excess as replicas' memory. Otherwise, we think replication buffer memory is the consumption of repl backlog.\r\n- Key eviction\r\n    Since all replicas and replication backlog share global replication buffer, we think only the part of exceeding backlog size the extra separate consumption of replicas.\r\n    Because we trim backlog incrementally in the background, backlog size may exceeds our setting if slow replicas that reference vast replication buffer blocks disconnect. To avoid massive eviction loop, we don't count the delayed freed replication backlog into used memory even if there are no replicas, i.e. we also regard this memory as replicas's memory.\r\n- `client-output-buffer-limit` check for replica clients\r\n     It doesn't make sense to set the replica clients output buffer limit lower than the repl-backlog-size config (partial sync will succeed and then replica will get disconnected). Such a configuration is ignored (the size of repl-backlog-size will be used). This doesn't have memory consumption implications since the replica client will share the backlog buffers memory.\r\n- Drop replication backlog after loading data if needed\r\n    We always create replication backlog if server is a master, we need it because we put DELs in it when loading expired keys in RDB, but if RDB doesn't have replication info or there is no rdb, it is not possible to support partial resynchronization, to avoid extra memory of replication backlog, we drop it.\r\n- Multi IO threads\r\n   Since all replicas and replication backlog use global replication buffer,  if I/O threads are enabled, to guarantee data accessing thread safe, we must let main thread handle sending the output buffer to all replicas. But before, other IO threads could handle sending output buffer of all replicas.\r\n\r\n## Other optimizations\r\nThis solution resolve some other problem:\r\n- When replicas disconnect with master since of out of output buffer limit, releasing the output buffer of replicas may freeze server if we set big `client-output-buffer-limit` for replicas, but now, it doesn't cause freezing.\r\n- This implementation may mitigate reply list copy cost time(also freezes server) when one replication has huge reply buffer and another replica can copy buffer for full synchronization. now, we just copy reference info, it is very light.\r\n- If we set replication backlog size big, it also may cost much time to copy replication backlog into replica's output buffer. But this commit eliminates this problem.\r\n- Resizing replication backlog size doesn't empty current replication backlog content.",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-06-29T03:02:33Z",
        "closed_at": "2021-06-29T11:37:02Z",
        "merged_at": "2021-06-29T11:37:02Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-28T15:07:53Z",
        "closed_at": "2021-06-29T11:34:19Z",
        "merged_at": "2021-06-29T11:34:19Z",
        "body": "Affects MEMORY USAGE",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2021-06-28T11:57:57Z",
        "closed_at": "2021-06-29T07:14:29Z",
        "merged_at": "2021-06-29T07:14:28Z",
        "body": "ZRANDMEMBER WITHSCORES returns a bad score when used with negative count (or count of 1), and non-ziplist encoded zset.\r\nFix issue: https://github.com/redis/redis/issues/9160\r\n\r\n- [x] todo: need test code",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2756,
        "deletions": 19,
        "changed_files": 20,
        "created_at": "2021-06-28T11:47:07Z",
        "closed_at": "2021-07-23T06:37:08Z",
        "merged_at": null,
        "body": "In the production environment, RDMA gets popular and common for a\r\nnetworking card. So support RDMA as transport layer protocol to\r\nimprove performance and cost reduction.\r\n\r\nNote that this feature is ONLY implemented/tested on Linux.\r\n\r\nSeveral steps of the full job:\r\n1, support RDMA for the connection from client & server. This is the\r\n   most import part, and luckly it's implemented in this patch.\r\n   Add a new config \"rdma-port\" for the server side to listen on\r\n   a RDMA port. Both redis-cli and redis-benchmark work fine with a\r\n   new argument '--rdma'. \"REPLICAOF\" command launches a RDMA client\r\n   if \"rdma-replication\" is enabled, and it works fine.\r\n\r\n2, To support RDMA cluster mode.\r\n\r\n3, To implement async read/write for client side. Because RDMA does\r\n   NOT support POLLOUT event, it's a little difficult to implement\r\n   the async IO mechanism for hiredis.\r\n\r\nThe test result is quite exciting:\r\nCPU: Intel(R) Xeon(R) Platinum 8260.\r\nNIC: Mellanox ConnectX-5.\r\nConfig of redis: appendonly no, port 6379, rdma-port 6379, appendonly no,\r\n                 server_cpulist 12, bgsave_cpulist 16.\r\nFor RDMA: ./redis-benchmark -h HOST -c 30 -n 10000000 -r 1000000000 \\\r\n          --threads 8 -d 512 -t ping,set,get,lrange_100 --rdma\r\nFor TCP: ./redis-benchmark -h HOST -c 30 -n 10000000 -r 1000000000 \\\r\n          --threads 8 -d 512 -t ping,set,get,lrange_100\r\n\r\n====== PING_INLINE ======\r\n TCP: QPS: 159017   AVG LAT: 0.183\r\nRDMA: QPS: 523944   AVG LAT: 0.054\r\n\r\n====== PING_MBULK ======\r\n TCP: QPS: 162256   AVG LAT: 0.179\r\nRDMA: QPS: 509839   AVG LAT: 0.056\r\n\r\n====== SET ======\r\n TCP: QPS: 154700   AVG LAT: 0.187\r\nRDMA: QPS: 492368   AVG LAT: 0.058\r\n\r\n====== GET ======\r\n TCP: QPS: 159022   AVG LAT: 0.182\r\nRDMA: QPS: 525099   AVG LAT: 0.054\r\n\r\n====== LPUSH (needed to benchmark LRANGE) ======\r\n TCP: QPS: 142537   AVG LAT: 0.207\r\nRDMA: QPS: 395038   AVG LAT: 0.073\r\n\r\n====== LRANGE_100 (first 100 elements) ======\r\n TCP: QPS:  36171   AVG LAT: 0.657\r\nRDMA: QPS:  55266   AVG LAT: 0.412\r\n\r\nSigned-off-by: zhenwei pi <pizhenwei@bytedance.com>",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-06-28T09:11:41Z",
        "closed_at": "2021-06-30T06:18:10Z",
        "merged_at": "2021-06-30T06:18:10Z",
        "body": "*** [err]: PSYNC2: total sum of full synchronizations is exactly 4 intests/integration/psync2.tcl\r\nExpected 5 == 4 (context: type eval line 8 cmd {assert {$sum == 4}} proc::test)\r\n\r\nSometime the test got an unexpected full sync since a replica switch to master,\r\nbefore the new master change propagated the new replid to all replicas,\r\na replica attempted to sync with it using a wrong replid and triggered a full resync.\r\n\r\nConsider this scenario:\r\n1 slaveof 4 full resync\r\n0 slaveof 4 full resync\r\n2 slaveof 0 full resync\r\n3 slaveof 1 full resync\r\n\r\n1 slaveof no one, replid changed\r\n3 reconnect 1, did a partial resyn and got the new replid\r\n\r\nBefore 2 inherits the new replid.\r\n3 slaveof 2\r\n3 try to do a partial resyn with 2.\r\nBut their replication ids are inconsistent, so a full resync happens.\r\n\r\nCI: https://github.com/redis/redis/runs/2779092193?check_suite_focus=true",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 55,
        "changed_files": 2,
        "created_at": "2021-06-28T09:08:59Z",
        "closed_at": "2021-06-30T10:32:51Z",
        "merged_at": "2021-06-30T10:32:51Z",
        "body": "looks like it was forgotten by mistake in #8170",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 8,
        "changed_files": 6,
        "created_at": "2021-06-26T16:39:02Z",
        "closed_at": "2021-10-03T05:19:33Z",
        "merged_at": "2021-10-03T05:19:33Z",
        "body": "  minor code cleanup\r\n\r\n  1. Remove forward declarations from header files to functions that do not exist:\r\n  hmsetCommand and rdbSaveTime.\r\n\r\n  2. Minor phrasing fixes in #9519\r\n\r\n  3. Add missing sdsfree(title) and fix typo in redis-benchmark.\r\n\r\n  4. Modify some error comments in some zset commands.\r\n\r\n  5. Fix copy-paste bug comment in syncWithMaster about `ip-address`.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-25T15:49:00Z",
        "closed_at": "2021-07-15T23:01:11Z",
        "merged_at": "2021-07-15T23:01:11Z",
        "body": "fix the word timeout spell error",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-25T14:34:15Z",
        "closed_at": "2021-06-29T09:55:47Z",
        "merged_at": null,
        "body": "In old way, we use ++next_db in performEvictions,\r\nso the db search will start with db1 at the first time,\r\nand back to db0 at the end.\r\n\r\nThis commit will make sure we can start from db0.\r\nIt\u2019s not actually a bug, it might be more uniform.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-06-25T07:42:48Z",
        "closed_at": "2021-06-30T13:55:09Z",
        "merged_at": "2021-06-30T13:55:09Z",
        "body": "This seems to be an unimportant bug that was accidentally generated. If the user does not specify limit in streamParseAddOrTrimArgsOrReply, the initial value of `args->limit` is `100 * server.stream_node_max_entries`, which may lead to out of bounds, and then the default function of limit in xadd becomes invalid (this failure occurs in streamTrim).\r\n\r\nAdditionally, provide sane default for `args->limit` in case `stream_node_max_entries` is set to 0.\r\n\r\nOf course, such a large value is generally not set, but this is a value that allows users to set and does cause problems.\r\n\r\nAnd the comment looks wrong.\r\n\r\nMaybe we need some notes to explain why the boundary is set to `LLONG_MAX/100` in the configuration initialization\r\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2021-06-25T05:26:43Z",
        "closed_at": "2021-08-25T11:58:35Z",
        "merged_at": "2021-08-25T11:58:35Z",
        "body": "In multipe threads mode, every thread output throughput info. This\r\nmay cause some problems:\r\n- Bug in https://github.com/redis/redis/pull/8615;\r\n- The show throughput is called too frequently;\r\n- showThroughput which updates shared variable lacks synchronization\r\nmechanism.\r\n\r\nThis commit also reverts changes in #8615 and changes time event\r\ninterval to macro.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-06-25T00:20:22Z",
        "closed_at": "2021-06-26T02:04:04Z",
        "merged_at": "2021-06-26T02:04:04Z",
        "body": "In the previous version, the link number to \"Redis Cluster specification\" is conflicted with \"SECURITY.md\" defined in line 231, causing github markdown viewer to mis-redirect the link to https://github.com/redis/redis/blob/unstable/SECURITY.md.\r\nThis commit fix the link confict issue.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 16,
        "changed_files": 2,
        "created_at": "2021-06-24T12:51:54Z",
        "closed_at": "2021-06-27T08:34:48Z",
        "merged_at": "2021-06-27T08:34:48Z",
        "body": "Some information abut the new protected-mode behavior that slipped from #9034.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 7,
        "changed_files": 8,
        "created_at": "2021-06-24T12:38:49Z",
        "closed_at": "2021-06-24T16:48:18Z",
        "merged_at": "2021-06-24T16:48:18Z",
        "body": "In the past, the first bind address that was explicitly specified was\r\nalso used to bind outgoing connections. This could result with some\r\nproblems. For example: on some systems using `bind 127.0.0.1` would\r\nresult with outgoing connections also binding to `127.0.0.1` and failing\r\nto connect to remote addresses.\r\n\r\nWith the recent change to the way `bind` is handled (#9034), this presented\r\nother issues:\r\n\r\n* The default first bind address is '*' which is not a valid address.\r\n* We make no distinction between user-supplied config that is identical\r\nto the default, and the default config.\r\n\r\nThis commit addresses both these issues by introducing an explicit\r\nconfiguration parameter to control the bind address on outgoing\r\nconnections.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-06-24T06:50:09Z",
        "closed_at": "2021-06-24T09:44:13Z",
        "merged_at": "2021-06-24T09:44:13Z",
        "body": "it failed because on 32bit builds, the maxmemory config is enabled implicitly (by redis).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-06-24T04:55:50Z",
        "closed_at": "2021-06-24T15:04:20Z",
        "merged_at": "2021-06-24T15:04:19Z",
        "body": "The `showThroughput` output may display wrong:\r\n```\r\nPING_INLINE: rps=78000.0 (overall: 75001.1) avg_msec=0.845 (overall: 0.649)))\r\n```\r\n\r\nOther info or error output also have this problem. But fix will change a lot of lines.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-06-24T03:50:53Z",
        "closed_at": "2021-06-24T12:08:26Z",
        "merged_at": "2021-06-24T12:08:26Z",
        "body": "In this judgment, we have used `raxSeek` to move raxIterator to the end, and the use of `raxNext` here does not actually make any sense. \r\nIt only uses the judgment of `ri.flags` to detect whether the `raxSeek` is successful. \r\n\r\n`raxSeek` fails when the flag contains `RAX_ITER_EOF`. So we can replace `raxNext` as the judgment of flags. ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2021-06-23T14:55:31Z",
        "closed_at": "2021-07-07T05:26:26Z",
        "merged_at": "2021-07-07T05:26:26Z",
        "body": "1. redis-cli can output --rdb data to stdout\r\n   but redis-cli also write some messages to stdout which will mess up the rdb.\r\n\r\n2. Make redis-cli flush stdout when printing a reply\r\n  This was needed in order to fix a hung in redis-cli test that uses\r\n  --replica.\r\n   Note that printf does flush when there's a newline, but fwrite does not.\r\n\r\n3. fix the redis-cli --replica test which used to pass previously\r\n   because it didn't really care what it read, and because redis-cli\r\n   used printf to print these other things to stdout.\r\n\r\n4. improve redis-cli --replica test to run with both diskless and disk-based.",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-06-23T12:29:54Z",
        "closed_at": "2021-06-30T13:49:54Z",
        "merged_at": "2021-06-30T13:49:54Z",
        "body": "In 6.2 I got errors. I used redis-cli --rdb /dev/stdout or even redis-cli --rdb -, but now process fails, because it can't make fsync to stdout.\r\nI've found \"-\" arg for stdout, so I've improved this code and now it will skip ftruncate and fsync for stdout file\r\n\r\n\r\nexample of failed redis-cli\r\n\r\n```redis-cli --rdb - > /dev/null\r\nSYNC sent to master, writing bytes of bulk transfer until EOF marker to '-'\r\nftruncate failed: Invalid argument.\r\nTransfer finished with success after 178 bytes\r\nFail to fsync '-': Invalid argument\r\n```\r\n\r\n```\r\nredis-cli --rdb /dev/stdout > /dev/null\r\nSYNC sent to master, writing bytes of bulk transfer until EOF marker to '/dev/stdout'\r\nftruncate failed: Invalid argument.\r\nTransfer finished with success after 178 bytes\r\nFail to fsync '/dev/stdout': Invalid argument\r\n```\r\n\r\nand exit code of this == 1",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 177,
        "deletions": 76,
        "changed_files": 1,
        "created_at": "2021-06-23T12:01:49Z",
        "closed_at": "2021-06-23T19:13:24Z",
        "merged_at": "2021-06-23T19:13:24Z",
        "body": "daily was broken by #9119 (https://github.com/redis/redis/actions/runs/962525900), seems that for cron scheduled tasks, these `if`s aren't evaluated to false.\r\nBut also it turns out that workflow_dispatch is only able to run CI on branches in the main repo (not on PRs).\r\nthis is an attempt to overcome that by being able to checkout from any repo we want.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-23T03:41:47Z",
        "closed_at": "2021-06-27T10:51:37Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 93,
        "deletions": 81,
        "changed_files": 4,
        "created_at": "2021-06-22T21:56:44Z",
        "closed_at": "2021-08-03T09:21:30Z",
        "merged_at": "2021-08-03T09:21:30Z",
        "body": "The following PR enables the -x option (Read last argument from STDIN) on redis-benchmark. \r\nTo be able to use the code from redis-cli some helper methods were moved to cli_common.(h|c)\r\n\r\nExample usage:\r\n```\r\nmake redis-benchmark\r\nwget https://raw.githubusercontent.com/RedisJSON/RedisJSON/1.0/test/files/pass-100.json\r\ncat pass-100.json | jq -c . | ./src/redis-benchmark -n 10000000 --threads 2 -r 1000 -x set __rand_int__\r\n```\r\n\r\nSample output:\r\n```\r\n cat pass-100.json | jq -c . | ./src/redis-benchmark -n 5000000 --threads 2 -r 1000 -x set __rand_int__ \r\n====== set __rand_int__ ======                                                       \r\n  5000000 requests completed in 29.33 seconds\r\n  50 parallel clients\r\n  269 bytes payload\r\n  keep alive: 1\r\n  host configuration \"save\": \r\n  host configuration \"appendonly\": no\r\n  multi-thread: yes\r\n  threads: 2\r\n\r\nLatency by percentile distribution:\r\n0.000% <= 0.047 milliseconds (cumulative count 23)\r\n50.000% <= 0.263 milliseconds (cumulative count 2537298)\r\n75.000% <= 0.295 milliseconds (cumulative count 3814015)\r\n87.500% <= 0.351 milliseconds (cumulative count 4402008)\r\n93.750% <= 0.415 milliseconds (cumulative count 4695634)\r\n96.875% <= 0.455 milliseconds (cumulative count 4844416)\r\n98.438% <= 0.511 milliseconds (cumulative count 4930444)\r\n99.219% <= 0.551 milliseconds (cumulative count 4964584)\r\n99.609% <= 0.623 milliseconds (cumulative count 4980511)\r\n99.805% <= 0.799 milliseconds (cumulative count 4990673)\r\n99.902% <= 0.871 milliseconds (cumulative count 4995321)\r\n99.951% <= 0.991 milliseconds (cumulative count 4997620)\r\n99.976% <= 1.247 milliseconds (cumulative count 4998810)\r\n99.988% <= 1.447 milliseconds (cumulative count 4999391)\r\n99.994% <= 1.823 milliseconds (cumulative count 4999695)\r\n99.997% <= 2.623 milliseconds (cumulative count 4999848)\r\n99.998% <= 3.263 milliseconds (cumulative count 4999924)\r\n99.999% <= 4.463 milliseconds (cumulative count 4999962)\r\n100.000% <= 16.911 milliseconds (cumulative count 4999982)\r\n100.000% <= 17.023 milliseconds (cumulative count 4999991)\r\n100.000% <= 17.103 milliseconds (cumulative count 4999996)\r\n100.000% <= 17.151 milliseconds (cumulative count 4999998)\r\n100.000% <= 17.167 milliseconds (cumulative count 4999999)\r\n100.000% <= 17.183 milliseconds (cumulative count 5000000)\r\n100.000% <= 17.183 milliseconds (cumulative count 5000000)\r\n\r\nCumulative distribution of latencies:\r\n0.365% <= 0.103 milliseconds (cumulative count 18274)\r\n29.656% <= 0.207 milliseconds (cumulative count 1482814)\r\n78.315% <= 0.303 milliseconds (cumulative count 3915769)\r\n93.304% <= 0.407 milliseconds (cumulative count 4665204)\r\n98.296% <= 0.503 milliseconds (cumulative count 4914798)\r\n99.570% <= 0.607 milliseconds (cumulative count 4978483)\r\n99.723% <= 0.703 milliseconds (cumulative count 4986157)\r\n99.824% <= 0.807 milliseconds (cumulative count 4991197)\r\n99.927% <= 0.903 milliseconds (cumulative count 4996336)\r\n99.955% <= 1.007 milliseconds (cumulative count 4997765)\r\n99.965% <= 1.103 milliseconds (cumulative count 4998250)\r\n99.972% <= 1.207 milliseconds (cumulative count 4998596)\r\n99.981% <= 1.303 milliseconds (cumulative count 4999040)\r\n99.986% <= 1.407 milliseconds (cumulative count 4999313)\r\n99.990% <= 1.503 milliseconds (cumulative count 4999477)\r\n99.991% <= 1.607 milliseconds (cumulative count 4999557)\r\n99.993% <= 1.703 milliseconds (cumulative count 4999627)\r\n99.994% <= 1.807 milliseconds (cumulative count 4999687)\r\n99.994% <= 1.903 milliseconds (cumulative count 4999720)\r\n99.995% <= 2.007 milliseconds (cumulative count 4999728)\r\n99.995% <= 2.103 milliseconds (cumulative count 4999745)\r\n99.998% <= 3.103 milliseconds (cumulative count 4999893)\r\n99.999% <= 4.103 milliseconds (cumulative count 4999956)\r\n99.999% <= 5.103 milliseconds (cumulative count 4999975)\r\n100.000% <= 17.103 milliseconds (cumulative count 4999996)\r\n100.000% <= 18.111 milliseconds (cumulative count 5000000)\r\n\r\nSummary:\r\n  throughput summary: 170450.67 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n        0.264     0.040     0.263     0.431     0.535    17.183\r\n```\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 789,
        "deletions": 121,
        "changed_files": 17,
        "created_at": "2021-06-22T10:03:14Z",
        "closed_at": "2022-02-23T20:34:58Z",
        "merged_at": "2022-02-23T20:34:58Z",
        "body": "Adds the ability to track the lag of a consumer group (CG), that is, the number of entries yet-to-be-delivered from the stream.\r\n\r\nThe proposed constant-time solution is in the spirit of \"best-effort.\"\r\n\r\nPartially addresses #8737.\r\n\r\n## Description of approach\r\n\r\nWe add a new \"entries_added\" property to the stream. This starts at 0 for a new stream and is incremented by 1 with every `XADD`.  It is essentially an all-time counter of the entries added to the stream.\r\n\r\nGiven the stream's length and this counter value, we can trivially find the logical \"entries_added\" counter of the first ID if and only if the stream is contiguous. A fragmented stream contains one or more tombstones generated by `XDEL`s. The new \"xdel_max_id\" stream property tracks the latest tombstone.\r\n\r\nThe CG also tracks its last delivered ID's as an \"entries_read\" counter and increments it independently when delivering new messages, unless the this read counter is invalid (-1 means invalid offset). When the CG's counter is available, the reported lag is the difference between added and read counters.\r\n\r\nLastly, this also adds a \"first_id\" field to the stream structure in order to make looking it up cheaper in most cases.\r\n\r\n## Limitations\r\n\r\nThere are two cases in which the mechanism isn't able to track the lag. In these cases, `XINFO` replies with `null` in the \"lag\" field.\r\n\r\nThe first case is when a CG is created with an arbitrary last delivered ID, that isn't \"0-0\", nor the first or the last entries of the stream. In this case, it is impossible to obtain a valid read counter (short of an O(N) operation). The second case is when there are one or more tombstones fragmenting the stream's entries range.\r\n\r\nIn both cases, given enough time and assuming that the consumers are active (reading and lacking) and advancing, the CG should be able to catch up with the tip of the stream and report zero lag. Once that's achieved, lag tracking would resume as normal (until the next tombstone is set).\r\n\r\n## API changes\r\n\r\n* `XGROUP CREATE` added with the optional named argument `[ENTRIESREAD entries-read]` for explicitly specifying the new CG's counter.\r\n* `XGROUP SETID` added with an optional positional argument `[ENTRIESREAD entries-read]`for specifying the CG's counter.\r\n* `XINFO` reports the maximal tombstone ID, the recorded first entry ID, and total number of entries added to the stream.\r\n* `XINFO` reports the current lag and logical read counter of CGs.\r\n* `XSETID` is an internal command that's used in replication/aof. It has been added with the optional positional arguments `[ENTRIESADDED entries-added] [MAXDELETEDID max-deleted-entry-id]` for propagating the CG's offset and maximal tombstone ID of the stream.\r\n\r\n## The generic unsolved problem\r\n\r\nThe current stream implementation doesn't provide an efficient way to obtain the approximate/exact size of a range of entries. While it could've been nice to have that ability (#5813) in general, let alone specifically in the context of CGs, the risk and complexities involved in such implementation are in all likelihood prohibitive.\r\n\r\n## A refactoring note\r\n\r\nThe `streamGetEdgeID` has been refactored to accommodate both the existing seek of any entry as well as seeking non-deleted entries (the addition of the `skip_tombstones` argument). Furthermore, this refactoring also migrated the seek logic to use the `streamIterator` (rather than `raxIterator`) that was, in turn, extended with the `skip_tombstones` Boolean struct field to control the emission of these.\r\n\r\n## TODO\r\n\r\n- [ ] (reserved for future use)\r\n- [x] Rationalize about `uint64_t` overflow WRT `offset++`\r\n- [x] Rationalize about `uint64_t` to `long long` (replies) castings\r\n- [ ] Consider adding scan-like `XGROUP CREATE|SETID .... [COMPUTE-OFFSET cursor]` for arbitrary IDs at the cost of O(N)\r\n- [ ] Consider adding an `XGROUPLAG <key> [group ...]` command for reduced verbosity compared to `XINFO STREAM ... FULL` and `XINFO GROUPS`\r\n- [x] Handle RDB persistence\r\n- [x] Test migration from older persistence (rdb_ver < 10)\r\n- [x] See if the Module API needs attention: none at this point.\r\n- More tests:\r\n  - [ ] (reserved for future use)\r\n  - [x] The 'first_id' stream field isn't exposed so it isn't covered by tests - fixed.\r\n  - [x] AOF rewrite\r\n  - [x] `XGROUP SETID` extended form\r\n- [x] Documentation: https://github.com/redis/redis-doc/pull/1694",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-06-22T10:00:48Z",
        "closed_at": "2021-07-04T11:44:35Z",
        "merged_at": null,
        "body": "raxIteratorAddChars and raxStackPush are functions that are called very frequently. The use of malloc_size can reduce the probability of memory allocation without any effort.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 33,
        "changed_files": 4,
        "created_at": "2021-06-22T09:49:25Z",
        "closed_at": "2021-06-22T14:22:40Z",
        "merged_at": "2021-06-22T14:22:40Z",
        "body": "- Introduce a new sdssubstr api as a building block for sdsrange.\r\n  The API of sdsrange is many times hard to work with and also has\r\n  corner case that cause bugs. sdsrange is easy to work with and also\r\n  simplifies the implementation of sdsrange.\r\n- Revert the fix to RM_StringTruncate and just use sdssubstr instead of\r\n  sdsrange.\r\n- Solve valgrind warnings from the new tests introduced by the previous\r\n  PR.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2021-06-22T05:23:55Z",
        "closed_at": "2021-06-30T11:56:45Z",
        "merged_at": "2021-06-30T11:56:45Z",
        "body": "Fix https://github.com/redis/redis/issues/5485.\r\nIf user executes `redis-cli --help`, the content should be output to `stdout` and exits with `0`.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2021-06-21T13:14:39Z",
        "closed_at": "2021-06-22T08:10:12Z",
        "merged_at": "2021-06-22T08:10:11Z",
        "body": "# replication-3.tcl\r\nhad a test timeout failure with valgrind on daily CI:\r\n```\r\n*** [err]: SLAVE can reload \"lua\" AUX RDB fields of duplicated scripts in tests/integration/replication-3.tcl\r\nReplication not started.\r\n```\r\nreplication took more than 70 seconds.\r\nhttps://github.com/redis/redis/runs/2854037905?check_suite_focus=true\r\n\r\non my machine it takes only about 30, but i can see how 50 seconds isn't enough.\r\n\r\n# replication.tcl\r\nloading was over too quickly in freebsd daily CI:\r\n```\r\n*** [err]: slave fails full sync and diskless load swapdb recovers it in tests/integration/replication.tcl\r\nExpected '0' to be equal to '1' (context: type eval line 44 cmd {assert_equal [s -1 loading] 1} proc ::start_server)\r\n```\r\n\r\n# rdb.tcl\r\nloading was over too quickly.\r\nincrease the time loading takes, and decrease the amount of work we try to achieve in that time.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 165,
        "deletions": 86,
        "changed_files": 1,
        "created_at": "2021-06-21T09:23:54Z",
        "closed_at": "2021-06-22T14:23:59Z",
        "merged_at": "2021-06-22T14:23:59Z",
        "body": "https://docs.github.com/en/actions/managing-workflow-runs/manually-running-a-workflow\r\n\r\nit looks something like this (earlier version)\r\n![image](https://user-images.githubusercontent.com/7045099/122762476-05e17d80-d2a6-11eb-8b26-e79646f7bc39.png)\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1336,
        "deletions": 365,
        "changed_files": 58,
        "created_at": "2021-06-20T18:59:06Z",
        "closed_at": "2021-07-11T20:00:13Z",
        "merged_at": null,
        "body": "This change also ensure that if the connTLSWrite fails to write when the socket is closed,\r\nit returns -1. This would ensure that the client is closed in writeClient function. This also makes the function more compliant with the POSIX write function.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-06-20T16:07:59Z",
        "closed_at": "2021-06-22T05:53:28Z",
        "merged_at": "2021-06-22T05:53:28Z",
        "body": "Saw by chance. It has been checked that only these two places need to be modified...\r\nMinro cleanup",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-06-20T15:30:21Z",
        "closed_at": "2021-06-22T04:36:00Z",
        "merged_at": "2021-06-22T04:36:00Z",
        "body": "The `Tracking gets notification of expired keys` test in tracking.tcl\r\nused to hung in valgrind CI quite a lot.\r\n\r\nIt turns out the reason is that with valgrind and a busy machine, the\r\nserver cron active expire cycle could easily run in the same event loop\r\nas the command that created `mykey`, so that when they key got expired,\r\nthere were two change events to broadcast, one that set the key and one\r\nthat expired it, but since we used raxTryInsert, the client that was\r\nassociated with the \"last\" change was the one that created the key, so\r\nthe NOLOOP filtered that event.\r\n\r\nThis commit adds a test that reproduces the problem by using lazy expire\r\nin a multi-exec which makes sure the key expires in the same event loop\r\nas the one that added it.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 24,
        "changed_files": 1,
        "created_at": "2021-06-20T13:48:21Z",
        "closed_at": "2021-06-30T05:21:19Z",
        "merged_at": "2021-06-30T05:21:19Z",
        "body": "In the original version, the operation of traversing the stack only seems to reconstruct the key that does not contain the current node.  \r\n\r\nBut in fact We have got the matched length and splitpos in the key in the raxlowwalk, so I think we can simplify the logic of this part.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2021-06-20T06:17:44Z",
        "closed_at": "2021-08-02T05:30:25Z",
        "merged_at": "2021-08-02T05:30:25Z",
        "body": "In _quicklistInsert when `at_head` / `at_tail` is true, but `prev` / `next` is NULL,\r\nthe code was reaching the last if-else block at the bottom of the function,\r\nand would have unnecessarily executed _quicklistSplitNode, instead of just creating a new node.\r\nThis was because the penultimate if-else was checking `node->next && full_next`.\r\nbut in fact it was unnecessary to check if `node->next` exists, if we're gonna create one anyway,\r\nwe only care that it's not full, or doesn't exist, so the condition could have been changed to `!node->next || full_next`.\r\n\r\nInstead, this PR makes a small refactory to negate `full_next` to a more meaningful variable\r\n`avail_next` that indicates that the next node is available for pushing additional elements or\r\nnot (this would be true only if it exists and it is non-full)",
        "comments": 18
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-06-20T05:18:15Z",
        "closed_at": "2021-06-20T08:59:11Z",
        "merged_at": null,
        "body": "This seems to be an invalid copy. Although only the defragraxnode is set, But I think it's a good way to delete this statement.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-06-19T14:04:25Z",
        "closed_at": "2021-06-20T05:04:12Z",
        "merged_at": "2021-06-20T05:04:12Z",
        "body": "So that we can easily see the lines of the config.\r\nAlso unified with other error handling.\r\n\r\nBefore:\r\n```\r\n11634:C 19 Jun 2021 22:00:34.824 # Can't chdir to '/ppp': No such file or directory\r\n```\r\n\r\nAfter:\r\n```\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 394\r\n>>> 'dir /ppp'\r\nCan't chdir to '/ppp': No such file or directory\r\n```\r\n\r\nAlso a minor cleanup :)",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-06-19T13:34:14Z",
        "closed_at": "2021-06-20T03:41:48Z",
        "merged_at": null,
        "body": "Since we can not start a redis server on port 0.\r\nSee https://github.com/redis/redis/blob/unstable/src/server.c#L3192\r\n\r\nAlso masterport is an int. It seems better to use atoi.\r\nIt's more intuitive and reduces the code. (Minor cleanup)\r\n\r\nref link: https://github.com/redis/redis/pull/7842",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-06-19T12:46:23Z",
        "closed_at": "2021-06-20T04:53:03Z",
        "merged_at": null,
        "body": "If we set changes to 0, bgsave will be performed every few seconds.\r\nBecause changes is actually 0, this makes no sense.\r\n(Although no one would do it... Minor cleanup)\r\n\r\nBefore:\r\n```\r\n27478:M 19 Jun 2021 20:45:36.097 * 0 changes in 1 seconds. Saving...\r\n27478:M 19 Jun 2021 20:45:36.097 * Background saving started by pid 27489\r\n27489:C 19 Jun 2021 20:45:36.107 * DB saved on disk\r\n27489:C 19 Jun 2021 20:45:36.108 * RDB: 0 MB of memory used by copy-on-write\r\n27478:M 19 Jun 2021 20:45:36.197 * Background saving terminated with success\r\n\r\n27478:M 19 Jun 2021 20:45:38.001 * 0 changes in 1 seconds. Saving...\r\n27478:M 19 Jun 2021 20:45:38.001 * Background saving started by pid 27492\r\n27492:C 19 Jun 2021 20:45:38.009 * DB saved on disk\r\n27492:C 19 Jun 2021 20:45:38.009 * RDB: 0 MB of memory used by copy-on-write\r\n27478:M 19 Jun 2021 20:45:38.101 * Background saving terminated with success\r\n```\r\n\r\n\r\nAfter:\r\n```\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 391\r\n>>> 'save 1 0'\r\nInvalid save parameters\r\n```",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-06-17T16:31:46Z",
        "closed_at": "2021-06-17T18:45:21Z",
        "merged_at": "2021-06-17T18:45:21Z",
        "body": "1) Missing `RedisModule_DictDel`\r\nhttps://github.com/redis/redis/blob/ada60d8003765eeb0d171904f17d731de7f94b50/tests/modules/datatype2.c#L301\r\nIt free the memory, but does not remove it from the db, this will cause the r flushdb in the test to repeatedly free the same memory address.\r\nIt will only crash under glic malloc, jemalloc will have no problem releasing the same memory address repeatedly.\r\n\r\n2) Missing RedisModule_AutoMemory\r\nThis will cause the return of `RedisModule_DictNext `in `MemPoolFreeDb `to not be released, causes memory leaks.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-17T15:16:28Z",
        "closed_at": "2021-06-17T20:37:38Z",
        "merged_at": "2021-06-17T20:37:38Z",
        "body": "Use exitFromChild to exit from child process in linuxMadvFreeForkBugCheck. This ensures that _exit is called instead of exit when exiting from the child process. This also makes the code consistent to other places in code where we exit from child process. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-17T15:02:25Z",
        "closed_at": "2021-06-17T18:49:16Z",
        "merged_at": "2021-06-17T18:49:16Z",
        "body": "@oranagra @yoav-steinberg \r\nDue to the change in #9003, a long-standing bug was raised under ```valgrind```.\r\nThis bug can cause the master-slave sync to take a very long time, causing the ```pendingquerybuf.tcl``` test to fail.\r\nThis problem does not only occur in master-slave sync, it is triggered when the big arg is greater than 32k.\r\nstep:\r\n```sh\r\ndd if=/dev/zero of=bigfile bs=1M count=32\r\n./src/redis-cli -x hset a a < bigfile\r\n```\r\n\r\n1) Make room for querybuf in processMultibulkBuffer, now the alloc of querybuf will be more than 32k.\r\nhttps://github.com/redis/redis/blob/b586d5b567f78b27569f4ef11e5839f22d82e865/src/networking.c#L2159\r\n2) If this happens to trigger the ```clientsCronResizeQueryBuffer```,  querybuf will be resized to 0.\r\n3) Finally, in readQueryFromClient, we expand the querybuf non-greedily, from 0 to 32k.\r\n    Old code, make room for querybuf is greedy, so it only needs 11 times to expand to 32M(16k*(2^11)), but now we need 2048(32*1024/16) times to reach it, due to the slow allocation under valgrind tha exposed the problem.\r\n\r\nThe fix for the excessive shrinking of the query buf to 0, will be handled in #5013 (that other change on it's own can fix failing test too), but the fix in this PR will also fix the failing test.\r\n\r\nThe fix in this PR will make the reading in `readQueryFromClient` more aggressive when working on a big arg (so that it is in par with the same code in `processMultibulkBuffer` (i.e. the two calls to `sdsMakeRoomForNonGreedy` should both use the bulk size).\r\nIn the current code (before this fix) the one in readQueryFromClient always has `readlen = PROTO_IOBUF_LEN`",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-06-17T13:03:08Z",
        "closed_at": "2021-07-04T11:43:50Z",
        "merged_at": null,
        "body": "Although `(h->iskey)` such a judgment condition in conjunction with `raxGetData and raxSetData` can produce the same effect as `(h->iskey && !h->isnull)`, bu second type is mostly used in the code Judgment conditions, so modify two exceptions.\r\n\r\nAs for modifying `isnull = 1`, the reason is that it is really confusing to use `iskey = 0 && isnull = 0` during initialization. The reason for the correct result is that raxSetData() modifies isnull to the correct value. \r\n\r\nThe last is to modify a symbol, obviously `i` will not be greater than `len` under any circumstances.\r\n\r\nThese changes will make the code easier to understand.\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2021-06-17T12:52:22Z",
        "closed_at": "2021-06-20T06:46:27Z",
        "merged_at": "2021-06-20T06:46:27Z",
        "body": "cleanups:\r\n1: Re-introduce debug leak subcommand in help text.\r\nMistankenly deleted in https://github.com/redis/redis/pull/5531\r\n\r\n2: Formatted the text.\r\nSome text lacks commas resulting in no line breaks.\r\n\r\n3: Supplementary debug restart command descriptions of delay arg.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-17T11:09:08Z",
        "closed_at": "2021-07-05T07:55:55Z",
        "merged_at": "2021-07-05T07:55:55Z",
        "body": "Currently multi state and db num are not being reset after reset command:\r\nBefore the change:\r\n ```\r\n$./src/redis-cli\r\n127.0.0.1:6379> select 2\r\nOK\r\n127.0.0.1:6379[2]> multi\r\nOK\r\n127.0.0.1:6379[2](TX)> get x\r\nQUEUED\r\n127.0.0.1:6379[2](TX)> reset\r\nRESET\r\n127.0.0.1:6379[2](TX)> exec\r\n(error) ERR EXEC without MULTI\r\n127.0.0.1:6379[2]> \r\n```\r\nAfter:\r\n```\r\n$./src/redis-cli\r\n127.0.0.1:6379> select 2\r\nOK\r\n127.0.0.1:6379[2]> multi\r\nOK\r\n127.0.0.1:6379[2](TX)> get x\r\nQUEUED\r\n127.0.0.1:6379[2](TX)> reset\r\nRESET\r\n127.0.0.1:6379> \r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2021-06-17T08:23:22Z",
        "closed_at": "2021-06-17T10:30:37Z",
        "merged_at": "2021-06-17T10:30:37Z",
        "body": "Hi guys, \r\n\r\nI found that when I use MEMORY USAGE command for a key, the result is not same compared to the result of INFO MEMORY.\r\n\r\nFor example,  I run `set a b` in an empty redis(with **jemalloc allocator**) and check `used_memory` with INFO MEMORY. The used memory increment is `88` bytes:\r\n- 32 for creating a dictEntry table in dictht;\r\n- 24 for a dictEntry;\r\n- 8 for key sds;\r\n- 24 for the value obj (embstr) ;\r\n\r\nWhen I run `MEMORY USAGE a`, I'd like to see a result like `56`(while the left `32`  is actually the db hashtable consumption), but actually get `51`, which is computed by `objectComputeSize`.\r\n\r\nSo I use `zmalloc_size` to get the the allocated size of ziplist, intset and EMBSTR string. And other sds size is already fixed using sdsZmallocSize. For stream, I am not familiar with the structure now. So there is no modificaiton.\r\n\r\nI saw the related problem discussed in #6263 and #6198.\r\n\r\nThis PR improve MEMORY USAGE command to include internal fragmentation overheads of:\r\n1. EMBSTR encoded strings\r\n2. ziplist encoded zsets and hashes\r\n3. List type nodes",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-06-17T07:13:19Z",
        "closed_at": "2021-06-21T12:24:39Z",
        "merged_at": null,
        "body": "i build Redis on suse 11 sp4 and got error \uff1a\r\n```\r\nnetworking.o: In function `IOThreadMain':\r\n/data/redis/src/networking.c:3058: undefined reference to `pthread_setname_np'\r\nbio.o: In function `bioProcessBackgroundJobs':\r\n/data/redis/src/bio.c:165: undefined reference to `pthread_setname_np'\r\n/data/redis/src/bio.c:159: undefined reference to `pthread_setname_np'\r\n/data/redis/src/bio.c:162: undefined reference to `pthread_setname_np'\r\ncollect2: error: ld returned 1 exit status\r\nmake[1]: *** [redis-server] Error 1\r\nmake[1]: Leaving directory `/data/redis/src'\r\nmake: *** [all] Error 2\r\n```\r\nsystem info\uff1a\r\n\r\n```\r\n70461d7ac821:/data/redis # uname -a\r\nLinux 70461d7ac821 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n70461d7ac821:/data/redis # cat /etc/issue\r\n\r\nWelcome to SUSE Linux Enterprise Server 11 SP4  (x86_64) - Kernel \\r (\\l).\r\n```\r\n \r\nand i found `pthread_setname_np`  does not exist in `libpthread-2.11.3.so`\r\n```\r\n70461d7ac821:/lib64 # ll | grep thread\r\n-rwxr-xr-x 1 root root  135764 Apr 24  2015 libpthread-2.11.3.so\r\nlrwxrwxrwx 1 root root      20 Dec 29  2017 libpthread.so.0 -> libpthread-2.11.3.so\r\n-rwxr-xr-x 1 root root   36714 Apr 24  2015 libthread_db-1.0.so\r\nlrwxrwxrwx 1 root root      19 Dec 29  2017 libthread_db.so.1 -> libthread_db-1.0.so\r\n70461d7ac821:/lib64 # strings libpthread-2.11.3.so | grep pthread_setname_np\r\n70461d7ac821:/lib64 # \r\n```\r\n\r\nis libpthread too old \uff1f\r\n\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-16T10:37:50Z",
        "closed_at": "2021-06-16T19:01:37Z",
        "merged_at": "2021-06-16T19:01:37Z",
        "body": "Fix test failure which introduced by #9003.\r\nThe following case will occur when querybuf expansion will allocate memory equal to (16*1024)k.\r\n1) make use ```CFLAGS=-DNO_MALLOC_USABLE_SIZE```.\r\n2) ```malloc``` will not allocate more under ```alpine```.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-06-15T21:51:11Z",
        "closed_at": "2021-06-29T13:38:10Z",
        "merged_at": "2021-06-29T13:38:10Z",
        "body": "Using `ZRANGESTORE` and setting the `src` to an empty key will return an empty array and leave the `dest` key as-is:\r\n```\r\nZADD dest 0 member\r\n(integer) 1\r\nZRANGESTORE dest src 0 -1\r\n(empty array)\r\nZCARD dest\r\n(integer) 1\r\n```\r\n\r\nthe expected result is:\r\n```\r\nZADD dest 0 member\r\n(integer) 1\r\nZRANGESTORE dest src 0 -1\r\n(integer) 0\r\nZCARD dest\r\n(integer) 0\r\n```",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-15T04:54:55Z",
        "closed_at": "2021-06-15T11:36:05Z",
        "merged_at": "2021-06-15T11:36:04Z",
        "body": "Do not queue command in an already aborted MULTI state.\r\nWe can detect an error (watched key).\r\nSo in queueMultiCommand, we also can return early.\r\nLike we deal with `CLIENT_DIRTY_EXEC`.\r\n\r\n\r\n**For example**:\r\n```\r\n127.0.0.1:6379> watch a\r\nOK\r\n127.0.0.1:6379> get a\r\n\"1\"\r\n127.0.0.1:6379> get a           # client2 modify the key\r\n\"2\"\r\n127.0.0.1:6379> multi\r\nOK\r\n127.0.0.1:6379(TX)> get a    # we can not queue the command (save memory or cpu)\r\nQUEUED\r\n127.0.0.1:6379(TX)> exec\r\n(nil)\r\n```\r\n\r\nCLIENT_DIRTY_EXEC ref: https://github.com/redis/redis/pull/7370\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 15,
        "changed_files": 2,
        "created_at": "2021-06-14T08:56:43Z",
        "closed_at": "2021-06-27T04:00:01Z",
        "merged_at": null,
        "body": "In old way, we traverse the whole dict and reset the event.\r\nBecause the dict is small now, the cost is low.\r\nAt the same time the old code is a bit complicated.\r\nSimplifies the code and improves readability.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 52,
        "changed_files": 9,
        "created_at": "2021-06-13T14:14:51Z",
        "closed_at": "2021-06-16T03:35:13Z",
        "merged_at": "2021-06-16T03:35:13Z",
        "body": "\u2026ations into cluster.h\r\n\r\n(no logic change -- just moving declarations/definitions around)\r\n\r\nThis initial effort leaves two items out of scope - the configuration parsing into the server\r\nstruct and the internals exposed by the clusterNode struct.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-06-12T14:55:37Z",
        "closed_at": "2021-06-14T08:42:45Z",
        "merged_at": null,
        "body": "When a db finishes defrag and the time is not finished, it will pass the judgment of \"if (!cursor)\" to perform some initialization operations and the finishing work after defrag all db, among which (cursor=0) is not meaningful. So I think these codes should be deleted.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-06-12T14:38:11Z",
        "closed_at": "2021-06-14T08:48:15Z",
        "merged_at": "2021-06-14T08:48:15Z",
        "body": "The statement cpu_pct = LIMIT ensures that the minimum cpu_pct is server.active_defrag_cycle_min. And it is meaningless for server.active_defrag_cycle_min to be zero. In other words, as long as the (!server.active_defrag_running) condition is met, (cpu_pct> server.active_defrag_running) must be met, so we can delete the first condition.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-06-12T14:28:52Z",
        "closed_at": "2021-06-15T07:01:12Z",
        "merged_at": "2021-06-15T07:01:11Z",
        "body": "Small cleanup and consistency.\r\n\r\nlike https://github.com/redis/redis/pull/7476",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-06-12T14:13:29Z",
        "closed_at": "2021-06-14T07:55:18Z",
        "merged_at": "2021-06-14T07:55:17Z",
        "body": "This seems to be a very obvious oversight. The OBJ_ENCODING_SKIPLIST was judged when defrag OBJ_LIST. But OBJ_ENCODING_SKIPLIST is no longer the encoding method of List. So I think this code should be deleted.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-06-10T21:54:25Z",
        "closed_at": "2021-06-13T05:42:21Z",
        "merged_at": "2021-06-13T05:42:21Z",
        "body": "Fix flaky test case reported here: https://github.com/redis/redis/pull/8474/#issuecomment-855346434\r\n\r\nThe root cause is that one test (`5 keys in, 5 keys out`) is leaking a volatile key that can expire while another later test(`All TTL in commands are propagated as absolute timestamp in replication stream`) is running. Such leaked expiration injects an unexpected `DEL` command into the replication command during the later test, causing it to fail.\r\n\r\nThe fixes are two fold:\r\n1. Plug the leak in the first test.\r\n2. Add FLUSHALL to the later test, to avoid future interference from other tests.\r\n\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-10T12:19:30Z",
        "closed_at": "2021-07-25T06:58:21Z",
        "merged_at": "2021-07-25T06:58:21Z",
        "body": "This will exit abnormally when the exec search fails, so you need to catch the exception here to prevent the test from exiting abnormally, and return a valid value to the caller.\r\n\r\nIf we have another tcl statement as follows: exec grep \"hello world\" < hello.txt\r\nbut there is no \"hello world\" in the hello.txt file\r\n\r\nWe will get the following output\uff1a\r\n`\r\nchild process exited abnormally\r\n    while executing\r\n\"exec grep \"hello\" < hello.txt\"\r\n`",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-10T12:04:49Z",
        "closed_at": "2021-06-10T23:33:16Z",
        "merged_at": "2021-06-10T23:33:16Z",
        "body": "Function listTypeTryConversion is so old that there is no function definition, so I think it\u2019s a good idea to delete it",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-06-10T03:04:06Z",
        "closed_at": "2021-06-13T06:31:20Z",
        "merged_at": "2021-06-13T06:31:20Z",
        "body": "Minor code cleanup.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-09T12:01:36Z",
        "closed_at": "2021-06-14T03:35:24Z",
        "merged_at": "2021-06-14T03:35:23Z",
        "body": "When `msetnx` with wrong numer of arguments,\r\nredis just gives  error msg `\"wrong number of arguments for MSET\"`.\r\n\r\nthe error msg should point to request command `msetnx`.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 172,
        "changed_files": 6,
        "created_at": "2021-06-08T14:40:11Z",
        "closed_at": "2021-06-16T06:47:25Z",
        "merged_at": "2021-06-16T06:47:25Z",
        "body": "Gopher support was added mainly because it was simple (trivial to add).\r\nBut apparently even something that was trivial at the time, does cause complications\r\ndown the line when adding more features.\r\nWe recently ran into a few issues with io-threads conflicting with the gopher support.\r\nWe had to either complicate the code further in order to solve them, or drop gopher.\r\nAFAIK it's completely unused, so we wanna chuck it, rather than keep supporting it.\r\n\r\nSee #8989",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-08T03:36:04Z",
        "closed_at": "2021-06-08T09:39:28Z",
        "merged_at": "2021-06-08T09:39:28Z",
        "body": "before:\r\n```\r\n73601:M 08 Jun 2021 11:34:55.327 - Client closed connection\r\n```\r\n\r\nafter (we can get client ip:port which is useful when debugging):\r\n```\r\n72808:M 08 Jun 2021 11:31:31.851 - Accepted 127.0.0.1:56263\r\n72808:M 08 Jun 2021 11:31:33.444 - Client closed connection id=4 addr=127.0.0.1:56263 laddr=127.0.0.1:6379 fd=8 name= age=2 idle=2 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=33274 argv-mem=0 obl=0 oll=0 omem=0 tot-mem=50704 events=r cmd=command user=default redir=-1\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2021-06-07T17:07:45Z",
        "closed_at": "2023-06-20T11:33:17Z",
        "merged_at": "2023-06-20T11:33:17Z",
        "body": "Now we will check the offset in zrangeGenericCommand.\r\nWith a negative offset, we will throw an error and return.\r\n\r\nThis also resolve the issue of zeroing the destination key\r\nin case of the \"store\" variant when we input a negative offset.\r\n```\r\n127.0.0.1:6379> set key value\r\nOK\r\n127.0.0.1:6379> zrangestore key myzset 0 10 byscore limit -1 10\r\n(integer) 0\r\n127.0.0.1:6379> exists key\r\n(integer) 0\r\n```\r\n\r\nThis change affects the following commands:\r\n- ZRANGE / ZRANGESTORE / ZRANGEBYLEX / ZRANGEBYSCORE\r\n- ZREVRANGE / ZREVRANGEBYSCORE / ZREVRANGEBYLEX\r\n\r\nNote: This PR was reverted in #12377, and we would re-introduced\r\nit in Redis 8.0 due to the breaking change",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-06-04T13:53:16Z",
        "closed_at": "2021-06-07T11:43:36Z",
        "merged_at": "2021-06-07T11:43:36Z",
        "body": "The decision to stop trimming due to LIMIT in XADD and XTRIM was after the limit was reached.\r\ni.e. the code was deleting **at least** that count of records (from the LIMIT argument's perspective, not the MAXLEN),\r\ninstead of **up to** that count of records.\r\nsee #9046",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-04T11:57:35Z",
        "closed_at": "2021-06-06T12:09:06Z",
        "merged_at": "2021-06-06T12:09:06Z",
        "body": "`xtrimCommand` call `streamParseAddOrTrimArgsOrReply ` should use `xadd==0`.\r\nWrong behavior:\r\n```\r\n127.0.0.1:6383> xtrim key maxlen ~ 3 nomkstream *\r\n(integer) 0\r\n```\r\n\r\nWhen the syntax is valid, it does not cause any bugs because the params of XADD is superset of XTRIM.\r\nJust XTRIM will not respond with error on invalid syntax. The syntax of XADD will also be accpeted by XTRIM.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-03T07:20:14Z",
        "closed_at": "2021-06-03T17:34:55Z",
        "merged_at": "2021-06-03T17:34:55Z",
        "body": "we recently seen this in [CI](https://github.com/redis/redis/runs/2723627060?check_suite_focus=true):\r\n```\r\n*** [err]: Interactive CLI: INFO response should be printed raw in tests/integration/redis-cli.tcl\r\nExpected 0 (context: type eval line 4 cmd {assert [regexp {^$|^#|^[^#:]+:} $line]} proc ::test)\r\n```\r\nif / when it'll happen again, we'll get more details.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-03T04:59:03Z",
        "closed_at": "2021-06-15T06:21:52Z",
        "merged_at": "2021-06-15T06:21:52Z",
        "body": "Fix coredump after `Client Unpause` command when threaded I/O is also enabled for the reading + parsing side and  clients are sending pipelining requests.\r\n\r\nenable threaded I/O in redis.conf:\r\n```\r\nio-threads 4\r\nio-threads-do-reads yes\r\n```\r\nstart redis-benchmark sending pipelining requests:\r\n```\r\nredis-benchmark -t set -P 1000 -c 10 -r 100000000 -n 100000000\r\n```\r\nsend `client pause` and `client unpause`:\r\n```\r\n127.0.0.1:6379> client pause 100000 write\r\nOK\r\n127.0.0.1:6379> client unpause\r\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\r\n```\r\nBacktrace:\r\n```\r\n0   redis-server                        0x000000010e9e7b79 unblockClient + 329\r\n1   redis-server                        0x000000010e974e23 clientCommand + 3459\r\n2   redis-server                        0x000000010e95e2eb call + 267\r\n3   redis-server                        0x000000010e95f27e processCommand + 2398\r\n4   redis-server                        0x000000010e976e39 processInputBuffer + 393\r\n5   redis-server                        0x000000010ea1be48 connSocketEventHandler + 280\r\n6   redis-server                        0x000000010e954e24 aeProcessEvents + 788\r\n7   redis-server                        0x000000010e95507d aeMain + 29\r\n8   redis-server                        0x000000010e96379d main + 1885\r\n9   libdyld.dylib                       0x00007fff599af015 start + 1\r\n```\r\n\r\nSince a same client was pushed into block_list repeatedly,  when we send `Client unpause`, unblockClient will call the btype-specific function to cleanup the state repeatedly which result in coredump at `serverPanic(\"Unknown btype in unblockClient().\")`\r\n\r\n\r\n\r\n\r\n\r\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-02T20:32:19Z",
        "closed_at": "2021-06-03T16:18:28Z",
        "merged_at": "2021-06-03T16:18:28Z",
        "body": "the moduleUnload function is not up-to-date. this commit adds the additional error in comment",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-06-02T20:28:12Z",
        "closed_at": "2021-06-04T15:33:07Z",
        "merged_at": "2021-06-04T15:33:07Z",
        "body": "before this commit, sentinel set command will accept args like `sentinel set <master-name>`, which is not a valid option since sentinel set will need at least one valid config. but sentinel will still accept it. eg.\r\n`\r\n127.0.0.1:26379> sentinel set foo\r\n\r\nOK\r\n`\r\nIn this commit, we can do this invalid arg check for sentinel set command.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 138,
        "deletions": 16,
        "changed_files": 8,
        "created_at": "2021-06-02T09:40:13Z",
        "closed_at": "2021-06-22T09:50:18Z",
        "merged_at": "2021-06-22T09:50:18Z",
        "body": "TL;DR This commit involves one new feature and two breaking changes:\r\n\r\n* Feature: specifying an empty `bind` address prevents Redis from listening on a TCP port. Previously, such configuration was not accepted so this is not a breaking change.\r\n* Breaking change: `CONFIG GET bind` will always return an explicit configuration value. In the past, if the default settings were used (listening on all IPv4 and IPv6 addresses), the value was empty.\r\n* Breaking change: Modifying the `bind` parameter to a non-default value will no longer implicitly disabled `protected-mode`.\r\n\r\nThe `protected-mode` behavior is changed because it is no longer possible to differentiate between the default bind settings and user-provided settings that have an identical value (`* -::*`).\r\n\r\nBefore this commit, the `bind` configuration parameter was anomalous:\r\n\r\n* If not specified, it would be treated as `* -::*`: The server must succeed\r\n  binding on `0.0.0.0`, and will also attempt `::*` but can silently fail.\r\n* If not specified, `CONFIG GET bind` would return an empty value.\r\n* Specifying `bind \"\"` in the configuration file would fail, because the empty\r\n  string could not be mapped to an address family.\r\n* Specifying `CONFIG SET bind \"\"` would fail with an erroneous \"Too many bind\r\n  addresses specified\" error.\r\n\r\nThis commits fixes these issues and introduces a more consistent behavior which\r\nis also backwards compatible:\r\n\r\n* The default value is maintained if not explicitly specified in the\r\n  configuration file.\r\n* Using `CONFIG GET bind` will always return with the actual value applied. If\r\n  no value was specified in the configuration file, the default `* -::*` will be\r\n  returned.\r\n* Explicitly specifying `CONFIG SET bind \"\"` or `bind \"\"` in the configuration\r\n  file will result with no binding on a TCP IPv4/IPv6 address. This\r\n  configuration is illegal for cluster mode, where a cluster bus is required.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 1662,
        "deletions": 1372,
        "changed_files": 66,
        "created_at": "2021-06-02T09:37:57Z",
        "closed_at": "2021-06-09T12:13:25Z",
        "merged_at": "2021-06-09T12:13:25Z",
        "body": "This commit revives the improves the ability to run the test suite against\r\nexternal servers, instead of launching and managing `redis-server` processes as\r\npart of the test fixture.\r\n\r\nThis capability existed in the past, using the `--host` and `--port` options.\r\nHowever, it was quite limited and mostly useful when running a specific tests.\r\nAttempting to run larger chunks of the test suite experienced many issues:\r\n\r\n* Many tests depend on being able to start and control `redis-server` themselves,\r\nand there's no clear distinction between external server compatible and other\r\ntests.\r\n* Cluster mode is not supported (resulting with `CROSSSLOT` errors).\r\n\r\nThis PR cleans up many things and makes it possible to run the entire test suite\r\nagainst an external server. It also provides more fine grained controls to\r\nhandle cases where the external server supports a subset of the Redis commands,\r\nlimited number of databases, cluster mode, etc.\r\n\r\nThe tests directory now contains a `README.md` file that describes how this\r\nworks.\r\n\r\nThis commit also includes additional cleanups and fixes:\r\n\r\n* Tests can now be tagged.\r\n* Tag-based selection is now unified across `start_server`, `tags` and `test`.\r\n* More information is provided about skipped or ignored tests.\r\n* Repeated patterns in tests have been extracted to common procedures, both at a\r\n  global level and on a per-test file basis.\r\n* Cleaned up some cases where test setup was based on a previous test executing\r\n  (a major anti-pattern that repeats itself in many places).\r\n* Cleaned up some cases where test teardown was not part of a test (in the\r\n  future we should have dedicated teardown code that executes even when tests\r\n  fail).\r\n* Fixed some tests that were flaky running on external servers.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 210,
        "deletions": 17,
        "changed_files": 2,
        "created_at": "2021-06-02T07:02:22Z",
        "closed_at": "2021-06-13T07:53:46Z",
        "merged_at": "2021-06-13T07:53:46Z",
        "body": "In `sunion` command, if we type any keys wrong(like type error), there will be an error\r\n```\r\n127.0.0.1:6381> flushdb\r\nOK\r\n127.0.0.1:6381> sadd c 1 2 3 \r\n(integer) 3\r\n127.0.0.1:6381> smembers c\r\n1) \"1\"\r\n2) \"2\"\r\n3) \"3\"\r\n127.0.0.1:6381> sunionstore c a b\r\n(integer) 0\r\n127.0.0.1:6381> smembers c\r\n(empty array)\r\n127.0.0.1:6381> \r\n127.0.0.1:6381> sadd c 1 2 3 \r\n(integer) 3\r\n127.0.0.1:6381> set a 123  // `a` is a string\r\nOK\r\n127.0.0.1:6381> sunion a b\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n127.0.0.1:6381> sunion b a\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n127.0.0.1:6381> sunionstore c a b\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n127.0.0.1:6381> sunionstore c b a\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n```\r\n\r\nIn sinter command, it will be different. In sinter command, i think the result will be fine(although unlike sunion)\r\nBut in sinterstore command, the different will cause totally different about the `dstkey`.\r\n```\r\n127.0.0.1:6381> flushdb\r\nOK\r\n127.0.0.1:6381> sinter a b\r\n(empty array)\r\n127.0.0.1:6381> sinter b a\r\n(empty array)\r\n127.0.0.1:6381> set a 123\r\nOK\r\n127.0.0.1:6381> sinter a b\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n127.0.0.1:6381> sinter b a\r\n(empty array)\r\n127.0.0.1:6381> sadd c 1 2 3\r\n(integer) 3\r\n// in here, there will be an type error, and `c(dstkey)` is exist\r\n127.0.0.1:6381> sinterstore c a b\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n127.0.0.1:6381> exists c\r\n(integer) 1\r\n// in here, we will delete the c(dstkey), although `a` is a string\r\n127.0.0.1:6381> sinterstore c b a\r\n(integer) 0\r\n127.0.0.1:6381> exists c\r\n(integer) 0\r\n```\r\n\r\nSo i update `sinterGenericCommand`, and make them behave uniformly. After output:\r\n```\r\n127.0.0.1:6379> flushdb\r\nOK\r\n127.0.0.1:6379> sinter a b\r\n(empty array)\r\n127.0.0.1:6379> sinter b a\r\n(empty array)\r\n127.0.0.1:6379> set a 123\r\nOK\r\n127.0.0.1:6379> sinter a b\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n127.0.0.1:6379> sinter b a\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n127.0.0.1:6379> \r\n127.0.0.1:6379> sadd c 1 2 3\r\n(integer) 3\r\n127.0.0.1:6379> sinterstore c a b\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n127.0.0.1:6379> exists c\r\n(integer) 1\r\n127.0.0.1:6379> sinterstore c b a\r\n(error) WRONGTYPE Operation against a key holding the wrong kind of value\r\n127.0.0.1:6379> exists c\r\n(integer) 1\r\n```\r\n\r\nAlso add more tests for some commands.\r\nMainly focus on \r\n- `wrong type error`: \r\n\texpand test case (base on sinter bug) in non-store variant\r\n\tadd tests for store variant (although it exists in non-store variant, i think it would be better to have same tests)\r\n- the dstkey result when we meet `non-exist key (empty set)` in *store\r\n\r\nsdiff:\r\n- improve test case about wrong type error (the one we found in sinter, although it is safe in sdiff)\r\n- add test about using non-exist key (treat it like an empty set)\r\nsdiffstore:\r\n- according to sdiff test case, also add some tests about `wrong type error` and `non-exist key`\r\n- the different is that in sdiffstore, we will consider the `dstkey` result\r\n\r\nsunion/sunionstore add more tests (same as above)\r\n\r\nsinter/sinterstore also same as above ...",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2021-06-02T04:19:50Z",
        "closed_at": "2021-07-26T07:07:21Z",
        "merged_at": "2021-07-26T07:07:20Z",
        "body": "Add two INFO metrics:\r\n```\r\ntotal_eviction_exceeded_time:69734\r\ncurrent_eviction_exceeded_time:10230\r\n```\r\n`current_eviction_exceeded_time` if greater than 0, means how much time current used memory is greater than `maxmemory`. And we are still over the maxmemory. If used memory is below `maxmemory`, this metric is reset to 0.\r\n`total_eviction_exceeded_time` means total time used memory is greater than `maxmemory` since server startup. \r\nThe units of these two metrics are ms.",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-06-01T12:53:48Z",
        "closed_at": "2021-06-08T12:01:29Z",
        "merged_at": null,
        "body": "see https://github.com/redis/redis/issues/9025#issue-908214581 ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-06-01T09:04:43Z",
        "closed_at": "2021-06-01T11:37:47Z",
        "merged_at": null,
        "body": "might be a bug when set null character at the end of a sds in `sdsrange`.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 4,
        "changed_files": 5,
        "created_at": "2021-06-01T06:19:46Z",
        "closed_at": "2021-06-01T14:03:44Z",
        "merged_at": "2021-06-01T14:03:44Z",
        "body": "Upgrade urgency: SECURITY, Contains fixes to security issues that affect\r\nauthenticated client connections. MODERATE otherwise.\r\n\r\nFix integer overflow in STRALGO LCS (CVE-2021-32625)\r\nAn integer overflow bug in Redis version 6.0 or newer can be exploited using the\r\nSTRALGO LCS command to corrupt the heap and potentially result with remote code\r\nexecution. This is a result of an incomplete fix by CVE-2021-29477.\r\n\r\nOther bug fixes:\r\n* Fix crash in UNLINK on a stream key with deleted consumer groups (#8932)\r\n* SINTERSTORE: Add missing keyspace del event when none of the sources exist (#8949)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 304,
        "deletions": 127,
        "changed_files": 23,
        "created_at": "2021-06-01T06:14:46Z",
        "closed_at": "2021-06-01T14:03:36Z",
        "merged_at": "2021-06-01T14:03:36Z",
        "body": "Upgrade urgency: SECURITY, Contains fixes to security issues that affect\r\nauthenticated client connections. MODERATE otherwise.\r\n\r\nFix integer overflow in STRALGO LCS (CVE-2021-32625)\r\nAn integer overflow bug in Redis version 6.0 or newer can be exploited using the\r\nSTRALGO LCS command to corrupt the heap and potentially result with remote code\r\nexecution. This is a result of an incomplete fix by CVE-2021-29477.\r\n\r\nBug fixes that are only applicable to previous releases of Redis 6.2:\r\n* Fix crash after a diskless replication fork child is terminated (#8991)\r\n* Fix redis-benchmark crash on unsupported configs (#8916)\r\n\r\nOther bug fixes:\r\n* Fix crash in UNLINK on a stream key with deleted consumer groups (#8932)\r\n* SINTERSTORE: Add missing keyspace del event when none of the sources exist (#8949)\r\n* Sentinel: Fix CONFIG SET of empty string sentinel-user/sentinel-pass configs (#8958)\r\n* Enforce client output buffer soft limit when no traffic (#8833)\r\n\r\nImprovements:\r\n* Hide AUTH passwords in MIGRATE command from slowlog (#8859)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-06-01T05:35:40Z",
        "closed_at": "2021-06-14T13:46:45Z",
        "merged_at": "2021-06-14T13:46:45Z",
        "body": "New:\r\nBased on the results of the comments, this commit introduced `slowlog get -1` to get all slow logs.\r\n```\r\n127.0.0.1:6379> slowlog len\r\n(integer) 0\r\n127.0.0.1:6379> slowlog get -1\r\n(empty array)\r\n127.0.0.1:6379> slowlog get -2\r\n(error) ERR count should be greater than or equal to -1\r\n127.0.0.1:6379> config set slowlog-log-slower-than 0\r\nOK\r\n127.0.0.1:6379> set a 1\r\nOK\r\n127.0.0.1:6379> slowlog get -1\r\n1) 1) (integer) 1\r\n   2) (integer) 1622739251\r\n   3) (integer) 12\r\n   4) 1) \"set\"\r\n      2) \"a\"\r\n      3) \"1\"\r\n   5) \"127.0.0.1:55892\"\r\n   6) \"\"\r\n2) 1) (integer) 0\r\n   2) (integer) 1622739247\r\n   3) (integer) 6\r\n   4) 1) \"config\"\r\n      2) \"set\"\r\n      3) \"slowlog-log-slower-than\"\r\n      4) \"0\"\r\n   5) \"127.0.0.1:55892\"\r\n   6) \"\"\r\n127.0.0.1:6379> slowlog help\r\n 1) SLOWLOG <subcommand> [<arg> [value] [opt] ...]. Subcommands are:\r\n 2) GET [<count>]\r\n 3)     Return top <count> entries from the slowlog (default: 10).\r\n 4)     It is also allowed to pass in -1 to get all slow logs.\r\n 5)     Entries are made of:\r\n 6)     id, timestamp, time in microseconds, arguments array, client IP and port,\r\n 7)     client name\r\n 8) LEN\r\n 9)     Return the length of the slowlog.\r\n10) RESET\r\n11)     Reset the slowlog.\r\n12) HELP\r\n13)     Prints this help.\r\n127.0.0.1:6379>\r\n```\r\n@oranagra Take a look for me when you have time. Check the code style and the help txt. Make sure i do it right. Thanks!\r\nNeed to be updated on the redis-doc?\r\n\r\n\r\n--------- old comment ----------\r\nBefore: (When typing a negative count, it will traverse the whole slowlog list. When the slowlog list is huge. May cause serious consequences if the user type a wrong count.)\r\n```\r\n127.0.0.1:6379> slowlog reset\r\nOK\r\n127.0.0.1:6379> slowlog get -1\r\n1) 1) (integer) 13\r\n   2) (integer) 1622525509\r\n   3) (integer) 25\r\n   4) 1) \"slowlog\"\r\n      2) \"reset\"\r\n   5) \"127.0.0.1:49392\"\r\n   6) \"\"\r\n127.0.0.1:6379>\r\n127.0.0.1:6379> slowlog get 0\r\n(empty array)\r\n127.0.0.1:6379>\r\n```\r\n\r\nAfter: (So i added a check. I could not help myself...)\r\n```\r\n127.0.0.1:6379> slowlog get -1\r\n(error) ERR value is out of range, must be positive\r\n127.0.0.1:6379> slowlog get 0  (0 is ok)\r\n(empty array)\r\n```\r\n",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-31T17:10:42Z",
        "closed_at": "2021-06-21T21:02:44Z",
        "merged_at": "2021-06-21T21:02:44Z",
        "body": "It seems that readwrite can only be used in cluster mode. So i add a check just like readonly command. Just a cleanup that consistent check\r\n\r\nrewrite doc: https://redis.io/commands/readwrite\r\n\r\n```c\r\n/* The READONLY command is used by clients to enter the read-only mode.\r\n * In this mode slaves will not redirect clients as long as clients access\r\n * with read-only commands to keys that are served by the slave's master. */\r\nvoid readonlyCommand(client *c) {\r\n    if (server.cluster_enabled == 0) {\r\n        addReplyError(c,\"This instance has cluster support disabled\");\r\n        return;\r\n    }\r\n    c->flags |= CLIENT_READONLY;\r\n    addReply(c,shared.ok);\r\n}\r\n\r\n/* The READWRITE command just clears the READONLY command state. */\r\nvoid readwriteCommand(client *c) {\r\n    +if (server.cluster_enabled == 0) {\r\n    +   addReplyError(c,\"This instance has cluster support disabled\");\r\n    +    return;\r\n    +}\r\n    c->flags &= ~CLIENT_READONLY;\r\n    addReply(c,shared.ok);\r\n}\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-31T15:59:21Z",
        "closed_at": "2021-06-08T08:34:38Z",
        "merged_at": "2021-06-08T08:34:38Z",
        "body": "redis-cli is grep friendly for all commands but SUBSCRIBE/PSUBSCRIBE. It is unable to process output from these commands line by line piped to another program because of output buffering. To overcome this situation I propose to flush stdout each time when it is written with reply from these commands the same way as it is already done for all other commands.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 81,
        "deletions": 35,
        "changed_files": 5,
        "created_at": "2021-05-31T12:14:27Z",
        "closed_at": "2021-06-14T07:38:08Z",
        "merged_at": "2021-06-14T07:38:08Z",
        "body": "Today when we load the AOF on startup, the loadAppendOnlyFile checks if\r\nthe file is openning for reading.\r\nThis check is redundent (dead code) as we open the AOF file for writing at initServer,\r\nand the file will always be existing for the loadAppendOnlyFile.\r\n\r\nIn this commit:\r\n- remove all the exit(1) from loadAppendOnlyFile, as it is the caller\r\n  responsibility to decide what to do in case of failure.\r\n- move the opening of the AOF file for writing, to be after we loading it.\r\n- avoid return -ERR in DEBUG LOADAOF, when the AOF is existing but empty",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-31T07:22:12Z",
        "closed_at": "2021-06-01T06:12:46Z",
        "merged_at": "2021-06-01T06:12:46Z",
        "body": "This change also includes improved error handling for large string truncation and insufficient memory.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-05-31T06:06:00Z",
        "closed_at": "2021-06-01T05:01:10Z",
        "merged_at": "2021-06-01T05:01:10Z",
        "body": "The test that was merged yesterday fails with valgrind and freebsd CI\r\nthat are too slow, and 10 seconds apparently passed between the time the\r\ncommand was sent to redis and the time it was actually executed.\r\n\r\n```\r\n*** [err]: All TTL in commands are propagated as absolute timestamp in replication stream in tests/unit/expire.tcl\r\nExpected 'del a' to match 'set foo1 bar PXAT *' (context: type source line 778 file /home/runner/work/redis/redis/tests/test_helper.tcl cmd {assert_match [lindex $patterns $j] [read_from_replication_stream $s]} proc ::assert_replication_stream level 1)\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 17,
        "changed_files": 1,
        "created_at": "2021-05-30T06:23:55Z",
        "closed_at": "2021-06-27T03:54:25Z",
        "merged_at": null,
        "body": "Correct the wrong CONFIG_LATENCY_HISTOGRAM_MAX_VALUE value.\r\n```\r\n#define CONFIG_LATENCY_HISTOGRAM_MAX_VALUE 3000000L          /* <= 30 secs(us precision) */\r\n#define CONFIG_LATENCY_HISTOGRAM_INSTANT_MAX_VALUE 3000000L   /* <= 3 secs(us precision) */\r\n```\r\n\r\nThe origin pr: https://github.com/redis/redis/pull/7600\r\nI am not sure about the origin idea @filipecosta90 PTAL . Or maybe i should just correct the comment?\r\n\r\nAlso format the code. (PS: it won't messy the blame log)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 120,
        "deletions": 15,
        "changed_files": 8,
        "created_at": "2021-05-29T09:25:29Z",
        "closed_at": "2021-06-15T11:46:19Z",
        "merged_at": "2021-06-15T11:46:19Z",
        "body": "The initialize memory of `querybuf` is `PROTO_IOBUF_LEN(1024*16) * 2` (due to sdsMakeRoomFor being greedy), under `jemalloc`, the allocated memory will be 40k.\r\nThis will most likely result in the `querybuf` being resized when call `clientsCronResizeQueryBuffer` unless the client requests it fast enough.\r\n\r\nNote that this bug existed even before #7875, since the condition for resizing includes the sds headers (32k+6).\r\n\r\n## Changes\r\n1. Use non-greedy sdsMakeRoomFor when allocating the initial query buffer (of 16k).\r\n1. Also use non-greedy allocation when working with BIG_ARG (we won't use that extra space anyway)\r\n2. in case we did use a greedy allocation, read as much as we can into the buffer we got (including internal frag), to reduce system calls.\r\n3. introduce a dedicated constant for the shrinking (same value as before)\r\n3. Add test for querybuf.\r\n4. improve a maxmemory test by ignoring the effect of replica query buffers (can accumulate many ACKs on slow env)\r\n5. improve a maxmemory by disabling slowlog (it will cause slight memory growth on slow env).",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-05-28T13:46:51Z",
        "closed_at": "2021-06-27T04:21:40Z",
        "merged_at": null,
        "body": "ref: redis-benchmark: Error/Warning handling updates. https://github.com/redis/redis/pull/8869\r\n\r\nBenchmark should exit when redisConnect* fail. Otherwise it will get stuck in the benchmark and does not do anything. \r\n(PS even the redisConnect recover later ( like restart the server))\r\n\r\nBefore( i think connect error also should exit):\r\n```\r\n[root@binblog redis]# src/redis-benchmark\r\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\r\nWARN: could not fetch server CONFIG\r\n^C\r\n```\r\n\r\nAfter:\r\n```\r\n[root@binblog redis]# src/redis-benchmark\r\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\r\n[root@binblog redis]#\r\n```",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-28T04:22:20Z",
        "closed_at": "2021-06-22T04:37:47Z",
        "merged_at": "2021-06-22T04:37:47Z",
        "body": "In sentinel tilt mode, IMO it will be a good idea to add tilt_mode_since in info command to show how long tilt mode has been lasted, since tilt mode can last for 30 seconds.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1255,
        "deletions": 92,
        "changed_files": 19,
        "created_at": "2021-05-28T01:10:56Z",
        "closed_at": "2021-06-16T06:45:49Z",
        "merged_at": "2021-06-16T06:45:49Z",
        "body": "Hi guys, I\u2019m developing a module recently. There are some resources related to dbid in the module that need to be managed. I need to process these resources when the key is deleted and when doing rdb save and load, so I need to get the current dbid in `rdb_load/rdb_save/unlink` and other callbacks . However, I cannot find a similar api in the current module api. I have tried to embed dbid into my object. This can indeed solve the problem of resource release, but it cannot solve commands such as `rename, move, swapdb`, which may cause key or dbid changes. so I propose this PR.\r\n\r\nFor compatibility reasons, I cannot directly use dbid as a parameter of existing callbacks. So, as we discussed below, I enhanced them and exposed these interfaces with a new version number. \r\n\r\n```\r\n    mem_usage -> mem_usage2;\r\n    free_effort ->  free_effort2;\r\n    unlink -> unlink2;\r\n    copy -> copy2;\r\n```\r\n\r\nI did not enhance the `free` callback because it is too special. First of all, because `free` may be called in bio, we cannot get the `key` and `dbid` information in this context. Secondly, all the meta-information that `free` wants is available in `unlink2` now.",
        "comments": 31
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-27T10:13:17Z",
        "closed_at": "2021-05-30T03:08:34Z",
        "merged_at": "2021-05-30T03:08:34Z",
        "body": "This PR makes redis-cli in pubsub mode grep friendly.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-05-27T08:39:04Z",
        "closed_at": "2021-06-12T14:15:20Z",
        "merged_at": null,
        "body": "This will exit abnormally when the exec search fails, so the exception needs to be caught here to prevent the test from exiting abnormally.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2021-05-27T07:46:42Z",
        "closed_at": "2021-06-11T02:36:13Z",
        "merged_at": null,
        "body": "In `restart_server` and `start_server` of `server.tcl`, we use `spawn_server` to try to start a new instance and print the PID. However, sometimes `spawn_server` would printed and return an invalid PID for reasons like busy port. \r\n\r\nSo I think the PID information might be printed when the server actually started.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-05-27T07:30:37Z",
        "closed_at": "2021-06-06T14:35:30Z",
        "merged_at": "2021-06-06T14:35:30Z",
        "body": "running the \"geo\" unit would have shown that it completed a unit named\r\n\"north\". this was because the variable `$name` was overwritten.\r\nThis commit isn't perfect, but it slightly reduces the chance for\r\nvariable name clash.\r\n\r\n```\r\n$ ./runtest --single unit/geo\r\n.......\r\nTesting unit/geo\r\n.......\r\n[1/1 done]: north (15 seconds)\r\n```",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-26T16:01:39Z",
        "closed_at": "2021-05-27T00:41:20Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-05-26T06:04:53Z",
        "closed_at": "2021-05-26T11:51:53Z",
        "merged_at": "2021-05-26T11:51:53Z",
        "body": "\u2026rSocket\r\n\r\nIn diskless replication, we create a read pipe for the RDB, between the child and the parent.\r\nWhen we close this pipe (fd), the read handler also needs to be removed from the event loop (if it still registered).\r\nOtherwise, next time we will use the same fd, the registration will be fail (panic), because\r\nwe will use EPOLL_CTL_MOD (the fd still register in the event loop), on fd that already removed from epoll_ctl",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 56,
        "deletions": 11,
        "changed_files": 6,
        "created_at": "2021-05-25T10:46:29Z",
        "closed_at": "2021-06-16T06:48:23Z",
        "merged_at": null,
        "body": "This also makes gopher work with io-threads-do-reads enabled as an alternative fix for #7779.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2021-05-24T08:04:35Z",
        "closed_at": "2021-05-24T13:42:33Z",
        "merged_at": null,
        "body": "Fix ci error ```replica buffer don't induce eviction in tests/unit/maxmemory.tcl```\r\n\r\n1) Stop slave before getting memory information.\r\n2) Wait util master received ack from slave after stop slave.\r\n    Since the memory calculation for eviction does not include the slave's querybuf,\r\n    if the ack message from salve is not handled in advance, it may cause the slave's querybuf \r\n    to grow in memory and lead to eviction in subsequent tests.\r\n3) Disable slowlog of master to avoid memory growth.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 23,
        "changed_files": 1,
        "created_at": "2021-05-24T04:45:41Z",
        "closed_at": "2021-06-08T08:25:52Z",
        "merged_at": "2021-06-08T08:25:52Z",
        "body": "This PR include 3 bugs, 1 cleanup and 1 output fix in separate commits.\r\n\r\n* `clusterManagerAddSlots` check `argv_idx` error, `argv_idx` >= 2 is always `true`. \r\n* `node->ip` may be an sds or a c string. Using `%s` to format is always right, `%S` may be wrong.\r\n*  In `clusterManagerFixOpenSlot` `clusterManagerBumpEpoch` call is redundant, because it is already called in `clusterManagerSetSlotOwner`.\r\n* `clusterManagerLoadInfoFromNode` remove unused param `opts`.\r\n* cluster help add more commands in help messages.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-05-23T21:13:58Z",
        "closed_at": "2021-06-16T06:29:58Z",
        "merged_at": "2021-06-16T06:29:57Z",
        "body": "When using RESP3, ZPOPMAX/ZPOPMIN should return nested arrays for consistency\r\nwith other commands (e.g. ZRANGE).\r\n\r\nWe do that only when COUNT argument is present (similarly to how LPOP behaves).\r\nfor reasoning see https://github.com/redis/redis/issues/8824#issuecomment-855427955\r\n\r\nThis is a breaking change only when RESP3 is used, and COUNT argument is present!",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 58,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2021-05-23T07:54:30Z",
        "closed_at": "2021-06-16T18:59:39Z",
        "merged_at": "2021-06-16T18:59:38Z",
        "body": "As described in issue #6351 (as well as https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=988947) this is a very minor patch to loadServerConfig to include individual files or wildcard files.  This will allow us to continue with the debian packaging bug.  Once that's done redis will be able to use an \"include conf.d/*.conf\" statement in the default configuration file which will facilitate customization across upgrades/downgrades.\r\n\r\nThe change itself is trivial:  instead of opening an individual file, the glob call creates a vector of files to open, and each file is opened in turn, and its content is added to the configuration. ",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-23T07:02:30Z",
        "closed_at": "2021-06-06T09:13:01Z",
        "merged_at": "2021-06-06T09:13:01Z",
        "body": "change diff:\r\nThere may be missing `zuiClearIterator` in `zrandmemberWithCountCommand` before return\r\n\r\nAlso the bug that currently does not cause memory leaks.\r\nBecause op->type = OBJ_ZSET, in zuiClearIterator, we will do nothing.\r\n\r\nJust a cleanup that zuiInitIterator and zuiClearIterator should appear in pairs.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-05-22T07:49:23Z",
        "closed_at": "2021-05-27T09:16:58Z",
        "merged_at": null,
        "body": "fix aarch64 build failed with args BUILD_TLS=yes",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2021-05-21T15:48:04Z",
        "closed_at": "2021-05-22T06:39:00Z",
        "merged_at": null,
        "body": "Maybe missing stat_dump_payload_sanitizations++ when we call zipmapValidateIntegrity(deep=1) ?\r\nif not needed or i understand wrong. Feel free to close it\r\n```\r\n                /* Since we don't keep zipmaps anymore, the rdb loading for these\r\n                 * is O(n) anyway, use `deep` validation. */\r\n                server.stat_dump_payload_sanitizations++;  // chang diff\r\n                if (!zipmapValidateIntegrity(encoded, encoded_len, 1)) {\r\n                    rdbReportCorruptRDB(\"Zipmap integrity check failed.\");\r\n```\r\n\r\nAlso fix some typos while reading...",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2021-05-21T12:21:58Z",
        "closed_at": "2021-05-30T08:57:36Z",
        "merged_at": "2021-05-30T08:57:36Z",
        "body": "1. AOF_RW_BUF_BLOCK_SIZE is 10M, if we use [jemalloc](http://jemalloc.net/jemalloc.3.html#size_classes), we will get 12M actually when we alloc aofrwblock, but we only use 10M, there is 20% waste of memory.\r\n\r\n2. If we want to get aof rewrite buffer memory usage, we should also count all other fields(except 'buf') of aofrwblock and the last block's free size. Before, there may be 20% deviation with its real memory usage.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 433,
        "deletions": 10,
        "changed_files": 10,
        "created_at": "2021-05-21T09:16:27Z",
        "closed_at": "2021-08-04T20:01:46Z",
        "merged_at": "2021-08-04T20:01:46Z",
        "body": "## Backgroud\r\nThis idea is from @oranagra , I just try to implement it.\r\nAs we know, after `fork`, one process will copy pages when writing data to these pages(CoW), and another process still keep old pages, they totally cost more memory. For redis, we suffered that redis consumed much memory when the fork child is serializing key/values, even that maybe cause OOM.\r\n\r\nBut actually we find, in redis fork child process, the child process don't need to keep some memory and parent process may write or update that, for example, child process will never access the key-value that is serialized but users may update it in parent process. So we think it may reduce COW if the child process release memory that it is not needed.\r\n\r\n## Implementation\r\nFor releasing key value in child process, we may think we call `decrRefCount` to free memory, but i find the fork child process still use much memory when we don't write any data to redis, and it costs much more time that slows down bgsave. Maybe because memory allocator doesn't really release memory to OS, and it may modify some inner data for this free operation, especially when we free small objects.\r\n\r\nMoreover, CoW is based on  pages, so it is a easy way that we only free the memory bulk that is not less than kernel page size. [madvise(MADV_DONTNEED)](https://man7.org/linux/man-pages/man2/madvise.2.html) can quickly release specified region pages to OS bypassing memory allocator, and allocator still consider that this memory still is used and don't change its inner data.\r\n\r\nThere are some memory we can release in the fork child process:\r\n- **Serialized key-values**\r\n  the fork child process never access serialized key-values, so we try to free them. Because we only can release big bulk memory, and it is time consumed to iterate all items/members/fields/entries of complex data type. So we decide to iterate them and try to release them only when their average size of item/member/field/entry is more than page size of OS.\r\n- **Replication backlog**\r\n  Because replication backlog is a cycle buffer, it will be changed quickly if redis has heavy write traffic, but in fork child process, we don't need to access that.\r\n- **Client buffers**\r\n   If clients have requests during having the fork child process, clients' buffer also be changed frequently. The memory includes client query buffer, output buffer, and client struct used memory.\r\n\r\nTo get child process peak private dirty memory, we need to count peak memory instead of last used memory, because the child process may continue to release memory (since COW used to only grow till now, the last was equivalent to the peak).\r\nAlso we're adding a new `current_cow_peak` info variable (to complement the existing `current_cow_size`)\r\n\r\n## Testing\r\n\r\nOran did more test on powerful NMVE SSD and CPU, and got more metrics https://github.com/redis/redis/pull/8974#issuecomment-877840186\r\nI think we will have less earnings if disk or network(diskless replication) is slow, because, on heavy writing traffic, child process doesn't have enough time to release pages but all keys may be changed and CoW already happened.\r\n\r\nInitial comparison (outdated, see link above):\r\nThis commit may usefully to  mitigate COW for big key values. i use benchmark to continue to write `string` to redis by different value size, let redis dump on disk(HDD). As the following table, we can find the child process use less memory after this commit. Because of child process continue to release memory, the peak of memory shouldn't be too long time, and before, the child process used memory may continue to grow up.\r\n\r\n| Value size                     | No Release    |        Released      |\r\n| ---------------------- | --------- | ------------|\r\n| 4096(total 9907MB)   | 9806MB  |    7336MB |\r\n| **4000(total 7954MB)**  | **7663MB** |   **3809MB**   |\r\n| 8192(total 9838MB)   | 9816MB  |   6620MB   |\r\n| **8100(total 7885MB)**   | **7861MB**  |   **4671MB**    |\r\n| 100k(total 10951MB)   | 10204MB |  8629MB |\r\n| 1024k(total 10052MB)   |  9879MB|  8669MB  |\r\n",
        "comments": 27
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-21T09:08:49Z",
        "closed_at": "2022-11-02T05:42:20Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-21T01:18:56Z",
        "closed_at": "2022-05-09T16:51:43Z",
        "merged_at": null,
        "body": "these days can be module hooks. i.e. in case of FLUSHDB on an empty db, the hooks are only called in the master.\r\n\r\n```c\r\n/* Flushes the whole server data set. */\r\nvoid flushAllDataAndResetRDB(int flags) {\r\n    /* Without that extra dirty++, when db was already empty, FLUSHALL will\r\n     * not be replicated nor put into the AOF. */\r\n    server.dirty++;\r\n}\r\n```",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-20T16:08:35Z",
        "closed_at": "2021-06-28T12:57:41Z",
        "merged_at": null,
        "body": "comparision -> comparison",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-20T08:27:59Z",
        "closed_at": "2021-05-20T12:04:17Z",
        "merged_at": "2021-05-20T12:04:17Z",
        "body": "In #5453 we use sds allocated size to get aof buf occupation size, but actually we need to use zmalloc size of it. I find @oranagra have fixed another one in #7864",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2021-05-20T08:20:47Z",
        "closed_at": "2021-05-30T14:56:04Z",
        "merged_at": "2021-05-30T14:56:04Z",
        "body": "Without this fix, FLUSHALL ASYNC would have freed these in a background thread,\r\neven if they didn't contain many elements (unlike how it works with other structures), which could be inefficient.\r\nsee some considerations https://github.com/redis/redis/pull/8258#discussion_r635817455",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 45,
        "deletions": 32,
        "changed_files": 5,
        "created_at": "2021-05-20T08:09:20Z",
        "closed_at": "2021-06-08T10:40:12Z",
        "merged_at": "2021-06-08T10:40:12Z",
        "body": "When we allocate a client struct with 16k reply buffer, the allocator we may give us 20K,\r\nThis commit makes use of that extra space.\r\nAdditionally, it tries to store whatever it can from the reply into the static 'buf' before\r\nallocating a new node for the reply list.\r\n\r\nSuggested by @oranagra in https://github.com/redis/redis/pull/8966#pullrequestreview-662844775\r\n\r\nSimilar changes (can all be mentioned in the release notes together): #8975, #8966",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 53,
        "changed_files": 10,
        "created_at": "2021-05-19T08:18:18Z",
        "closed_at": "2021-05-20T12:29:43Z",
        "merged_at": "2021-05-20T12:29:43Z",
        "body": "When test stop 'load handler' by killing the process that generating the load,\r\nsome commands that already in the input buffer, still might be processed by the server.\r\nThis may cause some instability in tests, that count on that no more commands\r\nprocessed after we stop the `load handler'\r\n\r\nIn this commit, new proc 'wait_load_handlers_disconnected' added, to verify that no more\r\ncammands from any 'load handler' prossesed, by checking that the clients who\r\ngenreate the load is disconnceted.\r\n\r\nAlso, replacing check of dbsize with wait_for_ofs_sync before comparing debug digest, as\r\nit would fail in case the last key the workload wrote was an overridden key (not a new one).\r\n\r\nAffected tests\r\nRace fix:\r\n- failover command to specific replica works\r\n- Connect multiple replicas at the same time (issue #141), master diskless=$mdl, replica diskless=$sdl\r\n- AOF rewrite during write load: RDB preamble=$rdbpre\r\n\r\nCleanup and speedup:\r\n- Test replication with blocking lists and sorted sets operations\r\n- Test replication with parallel clients writing in different DBs\r\n- Test replication partial resync: $descr (diskless: $mdl, $sdl, reconnect: $reconnect\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 8,
        "changed_files": 3,
        "created_at": "2021-05-19T07:16:12Z",
        "closed_at": "2021-05-20T05:23:53Z",
        "merged_at": "2021-05-20T05:23:53Z",
        "body": "According[ jemalloc size classes](http://jemalloc.net/jemalloc.3.html#size_classes), we may allocate much more memory than our setting of  `repl_backlog_size`, but we don't make full use of it. For example, we actually have 112MB for replication backlog if we set `repl_backlog_size` to 100MB, but we only use 100MB, there is 12% waste fo memory.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-19T05:27:47Z",
        "closed_at": "2021-05-19T13:06:43Z",
        "merged_at": "2021-05-19T13:06:43Z",
        "body": "I recently saw this failure:\r\n[err]: lazy free a stream with all types of metadata in tests/unit/lazyfree.tcl\r\nExpected '2' to be equal to '1' (context: type eval line 23 cmd {assert_equal [s lazyfreed_objects] 1} proc ::test)\r\n\r\nThe only explanation for such a thing is that the async flushdb wasn't\r\ndone before we did the resetstat",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-19T00:13:23Z",
        "closed_at": "2021-05-19T11:49:38Z",
        "merged_at": "2021-05-19T11:49:38Z",
        "body": "redis-cli clusterManagerCommandCreate calculate interleaved_len wrong\r\nwhich make interleaved_len bigger and may access array out of range.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-18T13:52:06Z",
        "closed_at": "2021-05-19T11:42:33Z",
        "merged_at": "2021-05-19T11:42:33Z",
        "body": "`SENTINEL SIMULATE-FAILURE HELP` reply with different format\r\n```\r\n127.0.0.1:26379> SENTINEL SIMULATE-FAILURE help\r\n1) \"crash-after-election\"\r\n2) \"crash-after-promotion\"\r\n127.0.0.1:26379> SENTINEL SIMULATE-FAILURE help\r\nOK\r\n```\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-18T12:20:45Z",
        "closed_at": "2021-11-07T03:48:56Z",
        "merged_at": null,
        "body": "I accidentally found that a low version of 'redis-check-rdb' would report an error when checking a high version of rdb file. The prompt message is that the rdb file of this version cannot be processed. When I went to check the 'redis-check-rdb' version, I found that it couldn't be checked. So add 'redis-check-rdb' version support.\r\n\r\n[root@node src]# ./redis-check-rdb /root/dump.rdb \r\n[offset 0] Checking RDB file /root/dump.rdb\r\n--- RDB ERROR DETECTED ---\r\n[offset 9] Can't handle RDB format version 9",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-18T06:28:54Z",
        "closed_at": "2021-05-18T14:10:06Z",
        "merged_at": "2021-05-18T14:10:06Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-05-17T22:39:14Z",
        "closed_at": "2021-05-23T11:31:01Z",
        "merged_at": "2021-05-23T11:31:01Z",
        "body": "Before this commit, when user set sentinel-user/pass to empty string using SENTINEL CONFIG SET command, sentinel will consider \"\" as a vaild sernanme/password and dump into config file, therefore, the following error happens when start server again using same config file:\r\n\r\n```\r\n*** FATAL CONFIG FILE ERROR (Redis 255.255.255) ***\r\nReading the configuration file, at line 24\r\n>>> 'sentinel sentinel-user'\r\nUnrecognized sentinel configuration statement.\r\n```\r\n\r\nIn this commit, when user calling set sentinel-user/pass and pass into an empty string, we set sentinel.sentinel_auth_user/pass to NULL to avoid the above rewite issue.\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-16T11:21:31Z",
        "closed_at": "2021-05-18T15:33:01Z",
        "merged_at": "2021-05-18T15:33:01Z",
        "body": "There may be an extra `defragged++` by mistake\r\n\r\nSeems to be a bad title and commit message...\r\n\r\nAlso `de->key = newsds`, should we use `dictSetKey` replace? \r\n```\r\nlong activeDefragZsetEntry(zset *zs, dictEntry *de) {\r\n    sds newsds;\r\n    double* newscore;\r\n    long defragged = 0;\r\n    sds sdsele = dictGetKey(de);\r\n    if ((newsds = activeDefragSds(sdsele)))\r\n        defragged++, de->key = newsds;\r\n    newscore = zslDefrag(zs->zsl, *(double*)dictGetVal(de), sdsele, newsds);\r\n    if (newscore) {\r\n        dictSetVal(zs->dict, de, newscore);  // here is use dictSetVal, but other places just use `de->v.val`\r\n        defragged++;\r\n    }\r\n    return defragged;\r\n}\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-16T02:02:32Z",
        "closed_at": "2021-08-08T06:03:53Z",
        "merged_at": "2021-08-08T06:03:53Z",
        "body": "Fix the wrong method used in quicklistTest.\r\n\r\nThe test try to test `insert before 1 element`, but it use quicklist\r\nInsertAfter, a copy-paste typo.\r\n\r\nThe commit also add an assert to verify results in some tests\r\nto make sure it is as expected.\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2021-05-15T03:17:49Z",
        "closed_at": "2021-06-27T03:52:01Z",
        "merged_at": null,
        "body": "This one is a cleanup\r\n- `if (sdslen(value) > 0) {` . This judgment is redundant. Because `sdslen(value) == 0` has been checked before.\r\n- sdslen(value) will be called several times, so I think it might be clearer to pull it out\r\n\r\nIn order to avoid mess up the blame log. Also can just remove the redundant if judgment",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-15T01:55:08Z",
        "closed_at": "2021-05-15T12:43:09Z",
        "merged_at": "2021-05-15T12:43:09Z",
        "body": "This is the rebased PR for  https://github.com/redis/redis/pull/5585.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 74,
        "deletions": 23,
        "changed_files": 6,
        "created_at": "2021-05-14T19:42:14Z",
        "closed_at": "2021-08-03T08:45:27Z",
        "merged_at": "2021-08-03T08:45:27Z",
        "body": "Add SINTERCARD and ZINTERCARD commands that are similar to\r\nZINTER and SINTER but only return the cardinality with minimum\r\nprocessing and memory overheads.\r\n\r\n----\r\n\r\nFor set-based operations, a painful consequence of requiring only the resulting cardinality is a substantial memory overhead in either returning the entire resulting set or storing it in another key. This adds SINTERCARD/ZINTERCARD commands, which have zero memory overhead and return only the resulting cardinality. With these commands, performing Jaccard-type calculations on two sets is substantially faster and less resource-intensive - it's simply an SCARD of both sets and one SINTERCARD. Unfortunately, there is no easy way to implement a similar cardinality for unions given the underlying implementation. ZINTERCARD is kinda nasty from a factoring perspective given zunionInterDiffGenericCommand's handling of all use-cases.\r\n\r\nAnyway, interested in thoughts on this as they are required for a good amount of recommendation system work and, while they could be done with modules, it seems nasty to copy out the logic of redis core into a module. If there's a pro-command response, I'll clean-up the zset variant some.\r\n",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-05-14T14:14:50Z",
        "closed_at": "2021-06-02T14:30:09Z",
        "merged_at": "2021-06-02T14:30:09Z",
        "body": "In clusterManagerCommandImport strcat was used to concat COPY and\r\nREPLACE, the space maybe not enough.\r\nIf we use --cluster-replace but not --cluster-copy, the MIGRATE\r\ncommand contained COPY **instead of** REPLACE.\r\n\r\nI just fix bug.  Don't do multiple key MIGRATE optimization.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 54,
        "deletions": 15,
        "changed_files": 2,
        "created_at": "2021-05-14T09:40:43Z",
        "closed_at": "2021-08-07T02:55:20Z",
        "merged_at": null,
        "body": "Reduce the samples from server.maxmemory_samples*dbnum to server.maxmemory_samples for optimize performance; By change maxmemory_samples can adjust the accuracy of evict andperformance.\r\n\r\n\r\nRun test on dbs when memory is Insufficient.(policy lru/lfu).\r\ntest: redis-benchmark -t set -n 2000000 -r 2000000 -d 100 --dbnum -15\r\n\r\nbefore optimize evict \r\n```\r\nthroughput summary: 27047.13 requests per second\r\nlatency summary (msec):\r\n        avg       min       p50       p95       p99       max\r\n      1.715     0.208     1.687     2.367     3.055    54.399\r\n```\r\n\r\nafter optimize evict \r\n```\r\nthroughput summary: 66390.04 requests per second\r\nlatency summary (msec):\r\n        avg       min       p50       p95       p99       max\r\n      0.643     0.184     0.615     0.975     1.207    57.823\r\n```",
        "comments": 31
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-14T08:31:21Z",
        "closed_at": "2021-06-15T09:52:36Z",
        "merged_at": "2021-06-15T09:52:36Z",
        "body": "Hi guys,\r\n\r\nIn `hashTypeConvertZiplist` from t_hash.c, when try to convert a hashtype from encoding `ziplist` to`hash` , redis just creates an empty dict and do the `dictAdd` one by one. \r\n\r\nAs `hash_max_ziplist_entries` is set default to 512, this could lead to some unwanted rehashing process(4->8, 8->16, 16->32, 32->64, 64->128, 128->256, 256->512). \r\n\r\nAnd I also found that in `t_set.c` and `t_zset.c` they all do the presize to avoid rehashing. So I think do the hash table presize in `t_hash.c` should be fine.   \r\n\r\nThe only problem might be the time complexity of `ziplistLen`. As `hash_max_ziplist_entries` is set default to be 512, I think less time would be consumed than rehashing process.\r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2021-05-13T14:27:42Z",
        "closed_at": "2021-05-18T14:14:34Z",
        "merged_at": "2021-05-18T14:14:34Z",
        "body": "This parameter of type GeoHashRadius is 192 bytes - passing a `const` pointer instead.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 54,
        "changed_files": 2,
        "created_at": "2021-05-13T05:31:11Z",
        "closed_at": "2021-05-18T14:13:10Z",
        "merged_at": "2021-05-18T14:13:10Z",
        "body": "As far as i can tell it shows up in redis-cli in both HELP, e.g.\r\n`help client list`, and also in the command completion tips, but it is\r\nunclear what it was needed for.\r\nIt exists since the very first commit that added this mechanism.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 22,
        "changed_files": 3,
        "created_at": "2021-05-12T22:55:11Z",
        "closed_at": "2021-05-14T04:16:27Z",
        "merged_at": "2021-05-14T04:16:27Z",
        "body": "Adding this information to security.md adds the info to the the security tab for people to review and also adds a tab on the new issue button linking to the security policy.\r\n\r\nAWS Security was confused and thought we didn't have a security policy because they looked at the security tab, so presumably other people would also be confused.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 47,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2021-05-11T05:18:14Z",
        "closed_at": "2021-05-17T13:54:37Z",
        "merged_at": "2021-05-17T13:54:37Z",
        "body": "And also add tests to cover lazy free of streams with various types of\r\nmetadata (see #8932)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 135,
        "deletions": 17,
        "changed_files": 3,
        "created_at": "2021-05-09T13:00:48Z",
        "closed_at": "2021-08-02T11:59:08Z",
        "merged_at": "2021-08-02T11:59:08Z",
        "body": "When redis-cli received ASK, it used string matching wrong and didn't\r\nhandle it. \r\n\r\nWhen we access a slot which is in migrating state, it maybe\r\nreturn ASK. After redirect to the new node, we need send ASKING\r\ncommand before retry the command.  In this PR after redis-cli receives \r\nASK, we send a ASKING command before send the origin command \r\nafter reconnecting.\r\n\r\nOther changes:\r\n* Make redis-cli -u and -c (unix socket and cluster mode) incompatible \r\n  with one another.\r\n* When send command fails, we avoid the 2nd reconnect retry and just\r\n  print the error info. Users will decide how to do next. \r\n  See #9277.\r\n* Add a test faking two redis nodes in TCL to just send ASK and OK in \r\n  redis protocol to test ASK behavior. ",
        "comments": 22
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2021-05-08T15:33:35Z",
        "closed_at": "2021-06-27T03:51:52Z",
        "merged_at": null,
        "body": "A cleanup like https://github.com/redis/redis/pull/8835\r\n- the anetTcpNonBlockBestEffortBindConnect / anetTcpNonBlockConnect return ANET_ERR but outside use constant -1\r\n- anetBlock / anetNonBlock / connEnableTcpNoDelay / connDisableTcpNoDelay / connKeepAlive return ANET_ERR when error occurs. So the outside function also should use ANET_ERR? (Although these functions do not make return value judgments)\r\n\r\nalso anetUnixGenericConnect function in anet.c is unused. Maybe  missing in https://github.com/redis/redis/pull/8660",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2021-05-08T07:52:45Z",
        "closed_at": "2021-05-19T06:24:26Z",
        "merged_at": "2021-05-19T06:24:26Z",
        "body": "try a more concise rdbTryIntegerEncoding using string2ll rather than strtoll.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 23,
        "changed_files": 5,
        "created_at": "2021-05-07T05:34:08Z",
        "closed_at": "2021-05-07T09:22:21Z",
        "merged_at": null,
        "body": "Add dbGenericDelete method. \r\nMay reduce the number of lines and simplify the code to make code more readable?",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-06T18:06:21Z",
        "closed_at": "2021-06-07T11:31:57Z",
        "merged_at": "2021-06-07T11:31:57Z",
        "body": "I was just reading this lovely HyperLogLog implementation and noticed a small typo in a comment.\r\n\r\nKeep up the great work. Redis is awesome!",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-06T16:29:41Z",
        "closed_at": "2021-05-07T12:34:36Z",
        "merged_at": "2021-05-07T12:34:36Z",
        "body": "Already use port_getn instead of port_get. But the error meesage is wrong",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 22,
        "changed_files": 4,
        "created_at": "2021-05-06T12:43:19Z",
        "closed_at": "2021-06-02T05:54:31Z",
        "merged_at": "2021-06-02T05:54:31Z",
        "body": "Fix the misspelling of backup",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-05-06T06:49:21Z",
        "closed_at": "2021-05-06T14:45:49Z",
        "merged_at": "2021-05-06T14:45:49Z",
        "body": "Redis Enterprise supports the CONFIG GET command, but it replies with am\r\nempty array since the save and appendonly configs are not supported.\r\nbefore this fix redis-benchmark would segfault for trying to access the\r\nerror string on an array type reply.\r\nsee #8869",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2021-05-06T05:35:36Z",
        "closed_at": "2021-05-10T11:02:26Z",
        "merged_at": "2021-05-10T11:02:26Z",
        "body": "1. `quicklistEntry *node`, This may cause some confusion. In the code, we usually use `quicklistNode *node` and `quicklistEntry *entry`\r\n2. fix some typos\r\n3. D missing a `;`  (btw, shoud we delete all the D?)\r\n\r\ndid not fix any bugs or something . Feel free to close if unneeded",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 26,
        "changed_files": 1,
        "created_at": "2021-05-06T04:34:15Z",
        "closed_at": "2021-05-17T13:52:41Z",
        "merged_at": "2021-05-17T13:52:41Z",
        "body": "There are two bugs in redis-cli hints:\r\n* The hints of commands with subcommands lack first params.\r\n* When search matching command of currently input, we should find the\r\ncommand with longest matching prefix. If not COMMAND INFO will always\r\nmatch COMMAND and display no hints.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-05T19:07:48Z",
        "closed_at": "2021-05-06T05:24:44Z",
        "merged_at": null,
        "body": "The _REG_RSP offset includes the red zone (128 bits due to the x86_64 ABI)\r\nthis is how _UC_MACHINE_SP arch agnostic macro calculates with handful\r\n of archs (e.g. sparc too).",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-05T04:04:14Z",
        "closed_at": "2021-08-08T11:30:48Z",
        "merged_at": "2021-08-08T11:30:47Z",
        "body": "i saw a ref issue: https://github.com/redis/redis/issues/4110\r\n\r\nI did some tests, the test code:\r\n```\r\n    sds max_s = sdsfromlonglong(LLONG_MAX);\r\n    printf(\"LLONG_MAX: %lld\\n\", LLONG_MAX);\r\n    printf(\"LLONG_MAX to sds: %s\\n\", max_s);\r\n\r\n    sds min_s = sdsfromlonglong(LLONG_MIN);\r\n    printf(\"LLONG_MIN: %lld\\n\", LLONG_MIN);\r\n    printf(\"LLONG_MIN to sds: %s\\n\", min_s);\r\n\r\n    unsigned long long v;\r\n    v = -LLONG_MIN;\r\n    printf(\"-LLONG_MIN %llu\\n\", v);\r\n```\r\n\r\noutput: The output is fine\r\n```\r\nLLONG_MAX: 9223372036854775807\r\nLLONG_MAX to sds: 9223372036854775807\r\nLLONG_MIN: -9223372036854775808\r\nLLONG_MIN to sds: -9223372036854775808\r\n-LLONG_MIN 9223372036854775808\r\n```\r\n\r\nBut there will be a warning when compiling. So i think better have this fixed?\r\n```\r\nwarning: integer overflow in expression \u2018-9223372036854775808\u2019 of type \u2018long long int\u2019 results in \u2018-9223372036854775808\u2019 [-Woverflow]\r\n\r\nunsigned long long v;\r\nv = -LLONG_MIN \r\n```\r\n\r\nMaybe. I realize it's not very needed or necessary... Feel free to close it and the ref issue",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-04T15:49:45Z",
        "closed_at": "2021-05-04T17:21:14Z",
        "merged_at": "2021-05-04T17:21:14Z",
        "body": "There is a ploblem when using --intrinsic-latency. If we type a number greater than 2147, there is a int overflow ploblem. The run_time become a negative number...\r\n\r\n2147483647 / 1000000 = 2147.483647\r\n```\r\n// intrinsic_latency_duration is a int\r\nstatic struct config {\r\n    int intrinsic_latency_duration; \r\n}\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/22811481/117031266-fc6e7700-ad32-11eb-99ad-f812c4c3aa0c.png)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2021-05-03T18:23:24Z",
        "closed_at": "2021-06-07T13:47:58Z",
        "merged_at": "2021-06-07T13:47:58Z",
        "body": "Implements support for `SET key value ... NX GET` (closes #8900)\r\n\r\nTill now GET and NX were mutually exclusive.\r\nthis change make their combination mean a \"Get or Set\" command.\r\n\r\nIf the key exists it returns the old value and avoids setting,\r\nand if it does't exist it returns nil and sets it to the new value (possibly with expiry time)",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-05-03T16:31:27Z",
        "closed_at": "2021-05-07T09:22:58Z",
        "merged_at": null,
        "body": "This commit did not fix any bugs. I think using it will be more standardized...\r\n```\r\ntypedef struct redisObject {\r\n    unsigned type:4;\r\n    unsigned encoding:4;\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 168,
        "deletions": 91,
        "changed_files": 25,
        "created_at": "2021-05-03T16:10:54Z",
        "closed_at": "2021-05-03T19:57:00Z",
        "merged_at": "2021-05-03T19:57:00Z",
        "body": "Upgrade urgency: SECURITY, Contains fixes to security issues that affect\r\nauthenticated client connections. LOW otherwise.\r\n\r\nInteger overflow in STRALGO LCS command (CVE-2021-29477):\r\nAn integer overflow bug in Redis version 6.0 or newer could be exploited using\r\nthe STRALGO LCS command to corrupt the heap and potentially result in remote\r\ncode execution. The integer overflow bug exists in all versions of Redis\r\nstarting with 6.0.\r\n\r\nInteger overflow in COPY command for large intsets (CVE-2021-29478):\r\nAn integer overflow bug in Redis 6.2 could be exploited to corrupt the heap and\r\npotentially result with remote code execution. The vulnerability involves\r\nchanging the default set-max-intset-entries configuration value, creating a\r\nlarge set key that consists of integer values and using the COPY command to\r\nduplicate it. The integer overflow bug exists in all versions of Redis starting\r\nwith 2.6, where it could result with a corrupted RDB or DUMP payload, but not\r\nexploited through COPY (which did not exist before 6.2).\r\n\r\nBug fixes that are only applicable to previous releases of Redis 6.2:\r\n* Fix memory leak in moduleDefragGlobals (#8853)\r\n* Fix memory leak when doing lazy freeing client tracking table (#8822)\r\n* Block abusive replicas from sending command that could assert and crash redis (#8868)\r\n\r\nOther bug fixes:\r\n* Use a monotonic clock to check for Lua script timeout (#8812)\r\n* redis-cli: Do not use unix socket when we got redirected in cluster mode (#8870)\r\n\r\nModules:\r\n* Fix RM_GetClusterNodeInfo() to correctly populate master id (#8846)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 67,
        "deletions": 25,
        "changed_files": 16,
        "created_at": "2021-05-03T16:10:48Z",
        "closed_at": "2021-05-03T19:56:49Z",
        "merged_at": "2021-05-03T19:56:49Z",
        "body": "Upgrade urgency: SECURITY, Contains fixes to security issues that affect\r\nauthenticated client connections. LOW otherwise.\r\n\r\nInteger overflow in STRALGO LCS command (CVE-2021-29477):\r\nAn integer overflow bug in Redis version 6.0 or newer could be exploited using\r\nthe STRALGO LCS command to corrupt the heap and potentially result in remote\r\ncode execution. The integer overflow bug exists in all versions of Redis\r\nstarting with 6.0.\r\n\r\nInteger overflow in COPY command for large intsets (CVE-2021-29478):\r\nAn integer overflow bug in Redis 6.2 could be exploited to corrupt the heap and\r\npotentially result with remote code execution. The vulnerability involves\r\nchanging the default set-max-intset-entries configuration value, creating a\r\nlarge set key that consists of integer values and using the COPY command to\r\nduplicate it. The integer overflow bug exists in all versions of Redis starting\r\nwith 2.6, where it could result with a corrupted RDB or DUMP payload, but not\r\nexploited through COPY (which did not exist before 6.2).\r\n\r\nBug fixes:\r\n* Cluster: Skip unnecessary check which may prevent failure detection (#8585)\r\n* Fix not starting on alpine/libmusl without IPv6 (#8655)\r\n\r\nImprovements:\r\n* Fix performance regression in BRPOP on Redis 6.0 (#8689)\r\n\r\nModules:\r\n* Fix edge-case when a module client is unblocked (#8618)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-03T09:13:12Z",
        "closed_at": "2021-05-03T10:39:07Z",
        "merged_at": "2021-05-03T10:39:07Z",
        "body": "I think there may be a leak if dup succeed but listAddNodeTail zmalloc failed.\r\nBetter call copy->free if it exist?",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2021-05-02T15:49:30Z",
        "closed_at": "2021-05-03T12:18:52Z",
        "merged_at": "2021-05-03T12:18:52Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2021-05-02T13:18:53Z",
        "closed_at": "2021-05-06T15:34:46Z",
        "merged_at": "2021-05-06T15:34:46Z",
        "body": "redis-cli when SELECT fails, we should reset dbnum to 0, so the\r\nprompt will not display incorrectly.\r\n\r\nAdditionally when SELECT and HELLO fail, we output message to inform\r\nit.\r\n\r\nIf we select wrong db in redis-cli, it will output nothing and display we are \r\nat db 20. In fact we are at db 0.\r\n```\r\n$ ./redis-cli -p 6383 -n 20\r\n127.0.0.1:6383[20]>\r\n```\r\n\r\nFor RESP3 if we connect a server before 6.0.\r\n```\r\n$ ./redis-cli -p 6379 -3 set hello world\r\n$\r\n```\r\nThe command is not executed. I think we should display an error message.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-05-02T02:19:10Z",
        "closed_at": "2021-05-07T09:24:59Z",
        "merged_at": null,
        "body": "Sometime i use `--pass` to connect the redis just for test. And i add it in the warning message.\r\n\r\n--pass <password>  Alias of -a for consistency with the new --user option.\r\n\r\nbefore:\r\n```\r\n[root@binblog redis]# src/redis-cli --pass 123456\r\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\r\n127.0.0.1:6379>\r\n```\r\n\r\nafter:\r\n```\r\n[root@binblog redis]# src/redis-cli --pass 123456\r\nWarning: Using a password with '-a' or '-u' or '--pass' option on the command line interface may not be safe.\r\n127.0.0.1:6379>\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-05-01T13:52:51Z",
        "closed_at": "2021-06-10T13:20:54Z",
        "merged_at": null,
        "body": "* replace incorrect `REDISMODULE_NODE_ID_SIZE` with the correct `REDISMODULE_NODE_ID_LEN` in the documentation for the module api `RM_GetClusterNodesList()`\r\n* fix the call to `RedisModule_Log()` with the correct arguments in the given example for `RM_GetClusterNodesList()`\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 117,
        "deletions": 75,
        "changed_files": 1,
        "created_at": "2021-05-01T13:21:13Z",
        "closed_at": "2021-05-18T07:30:24Z",
        "merged_at": "2021-05-18T07:30:24Z",
        "body": "Currently in redis-cli only AUTH and ACL SETUSER bypass history\r\nfile. We add CONFIG SET masterauth/masteruser/requirepass,\r\nHELLO with AUTH, MIGRATE with AUTH or AUTH2 to bypass history\r\nfile too.\r\n\r\nThe drawback is HELLO and MIGRATE's code is a mess. Someday if\r\nwe change these commands, we have to change here too.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-05-01T07:21:37Z",
        "closed_at": "2021-05-01T15:49:51Z",
        "merged_at": null,
        "body": "The first time active expiration checking, it will assign `db->avg_ttl`, then do the progressive calculation which is unnecessary, because integers division, we may loss a little accuracy of `avg_ttl` stat from the beginning.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-05-01T03:29:00Z",
        "closed_at": "2021-05-01T10:34:25Z",
        "merged_at": null,
        "body": "Forget to close(fd) when fsync(fd) got error in redis-cli.c",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-30T14:18:44Z",
        "closed_at": "2021-05-03T14:13:47Z",
        "merged_at": "2021-05-03T14:13:47Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 74,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2021-04-30T14:13:46Z",
        "closed_at": "2021-08-18T08:32:40Z",
        "merged_at": null,
        "body": "Before this commit, in Sentinel mode, when the already known sentinel instance switch address, there is a chance making same sentinel instance being considered as two separate sentinel instances which may causing problem. \r\nFor example in the following scenario:\r\n1. we are deploying three sentinels (sentinel1: 127.0.0.1:26379, sentinel2: 127.0.0.1:26380,sentinel3:127.0.0.1:26381)monitoring same redis master/replica instance\r\n2. we shutdown sentinel 1 down for maintainance, and we start a new sentinel 4 (127.0.0.1:26379) from scrach, which will generate a new runid, in sentinel 2 and sentinel 3's view, when it get the hello package from sentinel 4, sentinel1 port number will be set to 0 temporatorily, but not been deleted.\r\n3. after sometime we shutdown sentinel 4 and make sentinel 1 back online, in that case, sentinel1 and sentinel 4 will both exist in sentinel 2/3's list, **with same ip and port**, and this causing the duplication issue. In sentinel 2/3's point of view the sentinel 4 is even ONLINE since the health check (ping/pong) package doesnt contain the runid.\r\n\r\nThe root cause of this issue is when removing sentinel with same runid, we still need to invalidate other known sentinels with same ip/port.\r\nhttps://github.com/redis/redis/blob/unstable/src/sentinel.c#L2874",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 315,
        "deletions": 269,
        "changed_files": 84,
        "created_at": "2021-04-30T04:16:06Z",
        "closed_at": "2021-06-10T12:39:33Z",
        "merged_at": "2021-06-10T12:39:33Z",
        "body": "This PR adds a spell checker CI action that will fail future PRs if they introduce typos and spelling mistakes.\r\nThis spell checker is based on blacklist of common spelling mistakes, so it will not catch everything,\r\nbut at least it is also unlikely to cause false positives.\r\n\r\nBesides that, the PR also fixes many spelling mistakes and types, not all are a result of the spell checker we use.\r\n\r\nHere's a summary of other changes:\r\n1. Scanned the entire source code and fixes all sorts of typos and spelling mistakes (including missing or extra spaces).\r\n2. Outdated function / variable / argument names in comments\r\n3. Fix outdated keyspace masks error log when we check `config.notify-keyspace-events` in loadServerConfigFromString.\r\n4. Trim the white space at the end of line in `module.c`. Check: https://github.com/redis/redis/pull/7751\r\n5. Some outdated https link URLs.\r\n6. Fix some outdated comment. Such as:\r\n    - In README: about the rdb, we used to said create a `thread`, change to `process`\r\n    - dbRandomKey function coment (about the dictGetRandomKey, change to dictGetFairRandomKey)\r\n    - notifyKeyspaceEvent fucntion comment (add type arg)\r\n    - Some others minor fix in comment (Most of them are incorrectly quoted by variable names)\r\n7. Modified the error log so that users can easily distinguish between TCP and TLS in `changeBindAddr`\r\n",
        "comments": 19
    },
    {
        "merged": false,
        "additions": 45739,
        "deletions": 10330,
        "changed_files": 368,
        "created_at": "2021-04-30T02:35:27Z",
        "closed_at": "2021-04-30T10:46:37Z",
        "merged_at": null,
        "body": "When a quicklist has quicklist->compress * 2 nodes, then call\r\n__quicklistCompress, all nodes will be decompressed and the middle\r\ntwo nodes will be recompressed again. This violates the fact that\r\nquicklist->compress * 2 nodes are uncompressed. It's harmless\r\nbecause when visit a node, we always try to uncompress node first.\r\nThis only happened when a quicklist has quicklist->compress * 2 + 1\r\nnodes, then delete a node. For other scenarios like insert node and\r\niterate this will not happen.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1489,
        "deletions": 321,
        "changed_files": 25,
        "created_at": "2021-04-29T15:26:58Z",
        "closed_at": "2021-08-10T06:18:49Z",
        "merged_at": "2021-08-10T06:18:49Z",
        "body": "Part one of implementing #8702 (taking hashes first before other types)\r\n\r\n## Description of the feature\r\n1. Change ziplist encoded hash objects to listpack encoding.\r\n2. Convert existing ziplists on RDB loading time. an O(n) operation.\r\n\r\n## Rdb format changes\r\n1. Add RDB_TYPE_HASH_LISTPACK rdb type.\r\n2. Bump RDB_VERSION to 10\r\n\r\n## Interface changes\r\n1. New `hash-max-listpack-entries` config is an alias for `hash-max-ziplist-entries` (same with `hash-max-listpack-value`)\r\n2. OBJECT ENCODING will return `listpack` instead of `ziplist`\r\n\r\n## Listpack improvements:\r\n1. Support direct insert, replace integer element (rather than convert back and forth from string)\r\n3. Add more listpack capabilities to match the ziplist ones (like `lpFind`, `lpRandomPairs` and such)\r\n4. Optimize element length fetching, avoid multiple calculations\r\n5. Use inline to avoid function call overhead.\r\n\r\n## Tests\r\n1. Add a new test to the RDB load time conversion\r\n2. Adding the listpack unit tests. (based on the one in ziplist.c)\r\n3. Add a few \"corrupt payload: fuzzer findings\" tests, and slightly modify existing ones.",
        "comments": 34
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-29T13:29:11Z",
        "closed_at": "2021-05-03T18:00:59Z",
        "merged_at": null,
        "body": "I changed WITHCOORDS to WITHCOORD when improving the err string. I'm changing back to WITHCOORDS so the original error is the same as it was before.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2021-04-29T10:57:49Z",
        "closed_at": "2021-05-07T09:24:02Z",
        "merged_at": null,
        "body": "This one closed because https://github.com/redis/redis/pull/8890\r\n\r\nThe typos error has been handled in 8890 (except for method rdbSaveTime)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 32,
        "changed_files": 1,
        "created_at": "2021-04-29T08:49:22Z",
        "closed_at": "2021-05-18T07:09:46Z",
        "merged_at": "2021-05-18T07:09:46Z",
        "body": "reference: https://github.com/redis/redis/pull/8870\r\nInspired by @https://github.com/huangzhw/redis/commit/f7e5ecd588b4a4eab2a376f3cadf195e46a9ee4f . So i check the code again and found another infinite loop.... (my fault, did not double check last time) and thank @huangzhw \r\n\r\nExcept for the error message, if we close the target server, it will stuck `while (1)` . Also a infinite loop.\r\n\r\nAlso ping @oranagra . Take a look for me again  :) ... Thanks\r\n\r\nbefore:\r\n![image](https://user-images.githubusercontent.com/22811481/116525619-49091b00-a90b-11eb-8dff-1f1b4261b298.png)\r\n\r\nafter:\r\n![image](https://user-images.githubusercontent.com/22811481/116525629-4c040b80-a90b-11eb-8352-2b7334d05fa1.png)\r\n\r\n",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-28T20:37:52Z",
        "closed_at": "2021-05-18T14:19:30Z",
        "merged_at": "2021-05-18T14:19:30Z",
        "body": "These parameters of `RedisModule_CreateCommand` were previously undocumented but they are needed for ACL to check permission on keys and also by Redis Cluster to figure our how to route the command.\r\n\r\nPreviously submitted to redis-doc: https://github.com/redis/redis-doc/pull/1562",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 9,
        "changed_files": 6,
        "created_at": "2021-04-28T13:29:25Z",
        "closed_at": "2021-04-29T05:25:10Z",
        "merged_at": "2021-04-29T05:25:10Z",
        "body": "delete some dead code\r\n\r\nplease check again for me @itamarhaber  thanks!",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1767,
        "deletions": 430,
        "changed_files": 24,
        "created_at": "2021-04-28T12:50:47Z",
        "closed_at": "2021-05-06T10:01:29Z",
        "merged_at": null,
        "body": "Implement #8702\r\n\r\n## Description of the feature\r\n1) Add multi-container support to quicklist\r\n    quicklsit only supports ziplist container, when replacing it with listpack container, it needs to replace a lot of code, if \r\n    we need to add new container type in the future, you still need to do the same thing.\r\n    And quicklist can support multiple container easy at the same time, just add the new QuicklistContainerType.\r\n\r\n2) Convert ziplist to listpack at runtime when the quicklsit node is accessed.\r\n    If convert ziplist to listpack is done in rdb loading, it may slow down rdb loading.\r\n\r\n2) Add some methods to the listpack to make it easier to support quicklist migration.\r\n3) Add unit tests to listpack, and fix some bugs.\r\n\r\n## Fix bugs\r\n1) #8773  Fix lost of quicklistNodeUpdateSz.\r\n2) Fix wrongly cal count in lpValidateIntegrity.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-28T11:53:32Z",
        "closed_at": "2021-04-29T09:08:52Z",
        "merged_at": "2021-04-29T09:08:52Z",
        "body": "Improve redis-cli help. When help command, we only match command\r\nprefix args not all args. So when we help commands with subcommands,\r\nall subcommands will be output.\r\n\r\nBefore this:\r\n```\r\n127.0.0.1:6380> help config\r\n\r\n\r\n```\r\n\r\nAfter this:\r\n```\r\n127.0.0.1:6380> help config\r\n\r\n  CONFIG GET parameter\r\n  summary: Get the value of a configuration parameter\r\n  since: 2.0.0\r\n  group: server\r\n\r\n  CONFIG RESETSTAT -\r\n  summary: Reset the stats returned by INFO\r\n  since: 2.0.0\r\n  group: server\r\n\r\n  CONFIG REWRITE -\r\n  summary: Rewrite the configuration file with the in memory configuration\r\n  since: 2.8.0\r\n  group: server\r\n\r\n  CONFIG SET parameter value\r\n  summary: Set a configuration parameter to the given value\r\n  since: 2.0.0\r\n  group: server\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2021-04-28T07:55:41Z",
        "closed_at": "2021-05-09T12:21:18Z",
        "merged_at": "2021-05-09T12:21:18Z",
        "body": "Smply reuse some function in dict.c\r\ndelete dead code (dictType declarations)",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 40,
        "changed_files": 8,
        "created_at": "2021-04-28T02:20:35Z",
        "closed_at": "2021-04-28T06:25:58Z",
        "merged_at": null,
        "body": "This change does not fix any bugs, as RedisModule_OpenKey determines REDISMODULE_WRITE first, but the way it is written may result in the order of RM_OpenKey never being changed again, which may result in existing RedisModules not being backwards compatible.\r\nAnd I've seen some redis modules that use the same writing style.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-27T10:48:11Z",
        "closed_at": "2021-04-27T15:02:23Z",
        "merged_at": "2021-04-27T15:02:23Z",
        "body": "Fix part of https://github.com/redis/redis/issues/8871 and  https://github.com/redis/redis/issues/8828\r\n\r\nWhen test CONFIG bind with non valid address, some server\r\nconfigure search domain. So whatever addresses will bind success.\r\nUsing an invalid ip will always fail.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2021-04-27T10:05:00Z",
        "closed_at": "2021-04-27T11:25:02Z",
        "merged_at": "2021-04-27T11:25:02Z",
        "body": "Found some typos when reading the source code of basic data structures.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-27T05:12:15Z",
        "closed_at": "2021-04-28T13:03:24Z",
        "merged_at": "2021-04-28T13:03:24Z",
        "body": "I think there is missing zfree(data) in redis-benchmark.\r\n\r\nAnd also correct the wrong size in lrange.\r\nthe text mentioned 500, but size was 450, changed to 500",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-26T16:12:47Z",
        "closed_at": "2021-04-28T10:19:56Z",
        "merged_at": "2021-04-28T10:19:56Z",
        "body": "when i test in cluster, i found: when redis-cli connecting through redis.sock, it will stuck into a dead loop\r\n\r\nFollow the steps below:\r\n- set up unixsocket in conf\r\n- set up six node\uff1asrc/redis-cli --cluster create 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384 --cluster-replicas 1\r\n- src/redis-cli -s /var/run/redis/redis.sock get name   .  it got an MOVED error\r\n- src/redis-cli -c -s /var/run/redis/redis.sock get name   .  it won't stop, stuck into a dead loop\r\n\r\nredis-cli call `cliConnect` when we got redirected in cluster mode. And in `cliConnect `it will always call `context = redisConnectUnix(config.hostsocket);` Always use hostsocket so it ran into a dead loop\r\n\r\nSome pictures I debugged:\r\n![image](https://user-images.githubusercontent.com/22811481/116115411-f3efbe00-a6ec-11eb-842e-8843143a1d6a.png)\r\n![image](https://user-images.githubusercontent.com/22811481/116115434-f9e59f00-a6ec-11eb-88f5-bb2acbc1e478.png)\r\n\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 42,
        "changed_files": 1,
        "created_at": "2021-04-26T15:11:26Z",
        "closed_at": "2021-04-28T06:51:08Z",
        "merged_at": "2021-04-28T06:51:07Z",
        "body": "This PR fixes #5854 and #2337 and addresses some minor issue introduced on #8855. \r\nIt:\r\n- Immediately exit on errors that are not related to topology updates. ( fixes #5854 ).\r\n- Deprecates the `-e` option ( retro compatible ) and warns that we now exit immediately on errors that are not related to topology updates.\r\n- Fixed wrongfully failing on config fetch error (warning only). This only affects RE. ( reverts #8855 )\r\n\r\nBottom line:\r\n- MOVED and ASK errors will not show any warning (unlike the throttled error with `-e` before).\r\n- CLUSTERDOWN still prints an error unconditionally and sleeps for 1 second.\r\n- other errors are fatal.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-26T14:21:51Z",
        "closed_at": "2021-04-27T05:15:10Z",
        "merged_at": "2021-04-27T05:15:10Z",
        "body": "This solves an issue reported in #8712 in which a replica would bypass\r\nthe client write pause check and cause an assertion due to executing a\r\nwrite command during failover.\r\n\r\nThe fact is that we don't expect replicas to execute any command other\r\nthan maybe REPLCONF and PING, etc. but matching against the ADMIN\r\ncommand flag is insufficient, so instead i just block keyspace access\r\nfor now.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-04-26T14:09:45Z",
        "closed_at": "2021-04-26T15:43:58Z",
        "merged_at": "2021-04-26T15:43:57Z",
        "body": "Fixes #8839",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-04-26T12:09:17Z",
        "closed_at": "2021-04-26T18:11:39Z",
        "merged_at": "2021-04-26T18:11:38Z",
        "body": "When [WITHCOORD] [WITHDIST] [WITHHASH] are used, the error mentioned GEORADIUS command\r\nFixes issue #8864",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-04-26T06:37:40Z",
        "closed_at": "2021-04-26T23:40:59Z",
        "merged_at": "2021-04-26T23:40:58Z",
        "body": "When ```s == \"0\"```, the following two conditional judgements are repeated.\r\n```\r\nelse if (p[0] == '0' && slen == 1)\r\n```\r\n```\r\n/* Special case: first and only digit is 0. */\r\n    if (slen == 1 && p[0] == '0') {\r\n        if (value != NULL) *value = 0;\r\n        return 1;\r\n    }\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-04-26T03:55:37Z",
        "closed_at": "2021-04-26T06:03:12Z",
        "merged_at": null,
        "body": "Add script dump subcommand",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 124,
        "deletions": 55,
        "changed_files": 12,
        "created_at": "2021-04-25T18:09:57Z",
        "closed_at": "2021-05-19T15:23:55Z",
        "merged_at": "2021-05-19T15:23:55Z",
        "body": "Extension of previous work to make sure we don't accidentally log passwords.\r\n\r\nhttps://github.com/redis/redis/commit/e1d98bca5a54bb71ff10a69337863d5543ad4373 ",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-04-25T12:41:21Z",
        "closed_at": "2021-05-10T15:29:12Z",
        "merged_at": "2021-05-10T15:29:12Z",
        "body": "As document said: all other packets will be discarded by the receiving node if the sending node is not considered part of the cluster.\r\n\r\nBut the publish messages don't do this check.\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-04-25T08:19:46Z",
        "closed_at": "2021-04-26T09:55:54Z",
        "merged_at": "2021-04-26T09:55:54Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-04-25T07:42:06Z",
        "closed_at": "2021-05-02T07:32:57Z",
        "merged_at": "2021-05-02T07:32:57Z",
        "body": "Forget to call ```dictReleaseIterator```.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 40,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-04-25T06:39:38Z",
        "closed_at": "2021-04-25T07:59:24Z",
        "merged_at": null,
        "body": "Add validation for `active-defrag-cycle-min` <= `active-defrag-cycle-max` and `active-defrag-threshold-lower` <= `active-defrag-threshold-upper` config items.\r\n\r\n```\r\n#define INTERPOLATE(x, x1, x2, y1, y2) ( (y1) + ((x)-(x1)) * ((y2)-(y1)) / ((x2)-(x1)) )\r\n...\r\n    /* Calculate the adaptive aggressiveness of the defrag */\r\n    int cpu_pct = INTERPOLATE(frag_pct,\r\n            server.active_defrag_threshold_lower,\r\n            server.active_defrag_threshold_upper,\r\n            server.active_defrag_cycle_min,\r\n            server.active_defrag_cycle_max);\r\n```\r\nabove the `cpu_pc`t in `computeDefragCycles` will be a invalid value if `x1>x2` or `y1>y2`.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-24T13:19:59Z",
        "closed_at": "2021-04-25T07:53:54Z",
        "merged_at": "2021-04-25T07:53:54Z",
        "body": "After sorting, each item in picks is sorted according\r\nto its index.\r\n\r\nIn the original code logic, we traverse from the first\r\nelement of ziplist until `zipindex == picks[pickindex].index`.\r\n\r\nWe may be able to start directly in `picks[0].index`,\r\nthis will bring performance improvements.\r\n\r\nSigned-off-by: ZheNing Hu <adlternative@gmail.com>",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-04-23T08:44:33Z",
        "closed_at": "2021-04-26T02:58:54Z",
        "merged_at": "2021-04-26T02:58:54Z",
        "body": "fix https://github.com/redis/redis/issues/8847\r\nclusterProcessPacket log only distinguish between ping and pong not meet.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-23T07:02:37Z",
        "closed_at": "2021-04-25T07:05:12Z",
        "merged_at": "2021-04-25T07:05:12Z",
        "body": "**Describe the bug**\r\nIn a module loaded to each node running in cluster mode (e.g., 3 primaries, 3 replicas),\r\nRedisModule_GetClusterNodeInfo() does not return the `master_id` field for a replica node.\r\n\r\n**To reproduce**\r\n\r\nCreate a module with the following function, load it to each node in a cluster with at least one replica and call the following function (e.g., in some dummy function for the module).\r\n\r\n```\r\nvoid get_cluster_info(RedisModuleCtx *ctx)\r\n{\r\n    char ip[REDISMODULE_NODE_ID_LEN];\r\n    char master_id[REDISMODULE_NODE_ID_LEN];\r\n    int port;\r\n    int flags;\r\n\r\n    size_t count, j;\r\n    char **ids = RedisModule_GetClusterNodesList(NULL, &count);\r\n\r\n    for (j = 0; j < count; j++) {\r\n        RedisModule_Log(NULL, \"notice\", \"Node %.*s\", REDISMODULE_NODE_ID_LEN, ids[j]);\r\n        RedisModule_GetClusterNodeInfo(NULL, ids[j], ip, master_id, &port, &flags);\r\n        RedisModule_Log(NULL, \"notice\", \"info %.*s, %.*s, %d, %d\", REDISMODULE_NODE_ID_LEN, ip, REDISMODULE_NODE_ID_LEN, master_id, port, flags);\r\n    }\r\n\r\n    RedisModule_FreeClusterNodesList(ids);\r\n}\r\n```\r\n\r\n**Expected behavior**\r\n\r\nFor replica nodes, the `master_id` field would be populated.\r\n\r\n**Additional information**\r\n\r\nThe condition to decide whether to populate the master id (https://github.com/redis/redis/blob/f61c37cec900ba391541f20f7655aad44a26bafc/src/module.c#L6171) is performing the wrong bitwise operation (i.e., checking whether it is a master rather than a replica).",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-23T03:47:17Z",
        "closed_at": "2021-04-24T11:19:25Z",
        "merged_at": "2021-04-24T11:19:25Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2021-04-23T03:27:10Z",
        "closed_at": "2021-04-25T07:36:02Z",
        "merged_at": "2021-04-25T07:36:02Z",
        "body": "This change does not fix any bugs.\r\n1. ```moduleUnload``` should return ```C_OK``` or ```C_ERR```, not ```REDISMODULE_ERR``` or ```REDISMODULE_OK```.\r\n2. The ```where``` parameter of ```listTypePush``` and ```listTypePop``` should be ```LIST_HEAD``` or ```LIST_TAIL```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-23T02:24:35Z",
        "closed_at": "2021-04-24T11:16:39Z",
        "merged_at": "2021-04-24T11:16:39Z",
        "body": "Small change in readme. My first pr by accident. Feel free to close this PR if un-needed :)\r\n\r\nREADME.md:\r\n- http://try.redis.io to https://try.redis.io\r\n- http://github.com/antirez/sds  to https://github.com/antirez/sds",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-22T15:00:30Z",
        "closed_at": "2021-04-25T07:11:16Z",
        "merged_at": "2021-04-25T07:11:16Z",
        "body": "Typo",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 15,
        "changed_files": 4,
        "created_at": "2021-04-21T15:05:46Z",
        "closed_at": "2021-04-25T10:08:47Z",
        "merged_at": "2021-04-25T10:08:47Z",
        "body": "I saw one case where the tests timed out in the fuzz tester:\r\n```\r\nTesting integration/corrupt-dump-fuzzer\r\n=== () Starting server 127.0.0.1:21418 ok\r\n[TIMEOUT]: clients state report follows.\r\n```\r\ni assume it sent some random erroneous command that caused redis to\r\nforget responding, which caused the test to hung.\r\nhopefully, next time it'll happen, we'll know which command it is.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-21T06:35:10Z",
        "closed_at": "2021-04-21T10:33:18Z",
        "merged_at": "2021-04-21T10:33:18Z",
        "body": "the anetGenericAccept return ANET_ERR but outside use constant -1\r\n\r\nthis is taken from #2747",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-20T17:06:36Z",
        "closed_at": "2021-04-21T10:42:46Z",
        "merged_at": "2021-04-21T10:42:46Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 87,
        "deletions": 64,
        "changed_files": 4,
        "created_at": "2021-04-20T16:30:25Z",
        "closed_at": "2021-05-04T10:45:08Z",
        "merged_at": "2021-05-04T10:45:08Z",
        "body": "See #8803.\r\nWhen client breached the output buffer soft limit but then went idle, we didn't disconnect on soft limit timeout, now we do.\r\nNote this also resolves some sporadic test failures in #8803 due to Linux buffering data which caused tests to fail if during the test we went back under the soft COB limit.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2021-04-20T13:27:26Z",
        "closed_at": "2021-04-25T13:16:09Z",
        "merged_at": null,
        "body": "In some cases, we need to capture specific operation logs. When AOF is on, we can scan the AOF, and if AOF is off, we can use the monitor command. But the monitor will return lots of replies when the ops is high. The usual practice is to filter data on the client side, of course, we can also use other tools for data analysis, but undoubtedly we need to transmit a large amount of redundant data through the network. So consider whether it would be better if redis can support filtering command when it feed to it's monitors.\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-04-20T13:14:43Z",
        "closed_at": "2021-04-22T05:32:44Z",
        "merged_at": "2021-04-22T05:32:44Z",
        "body": "I think modules event subscribers may get wrong things in `notifyKeyspaceEvent` callback such as wrong keys number.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-20T12:01:17Z",
        "closed_at": "2021-04-21T10:43:06Z",
        "merged_at": "2021-04-21T10:43:06Z",
        "body": "fiil -> fill",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-04-20T06:02:57Z",
        "closed_at": "2021-04-20T18:51:24Z",
        "merged_at": "2021-04-20T18:51:24Z",
        "body": "The redis-check-aof tool can quickly help detect and repair AOF abnormalities, but in some cases, we need to analyze the context of the abnormal position of the AOF, so we need to split the AOF by byte, but If it can directly print the abnormal line number in the AOF file, can we troubleshoot faster?\r\n\r\nThis PR allows redis-check-aof to support printing the line number of the abnormal location.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 159,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2021-04-19T21:26:24Z",
        "closed_at": "2021-04-27T01:23:04Z",
        "merged_at": null,
        "body": "I am working with Redis Modules API to implement authorization module. There is a missing functionality.\r\nHere is a short description.\r\n\r\n1. I need to block a client to allow analyzing auth credentials in background. When a client is blocked, I allocate a \u201crequest\u201d object for background processing on a working thread.\r\n\r\n2. The client can be unblocked in two ways:\r\n   - Calling RedisModule_UnblockClient(bc, req) on a working thread\r\n   - Timeout of client\u2019s blocking\r\n\r\n3. When authorization request is completed on the working thread, I can pass information about allocated \u201crequest\u201d object in the RedisModule_UnblockClient. Thus, when a \u201cFreeData\u201d callback is called, the pointer to the \u201crequest\u201d object is passed as a parameter and I can de-allocate it.\r\n\r\n4. There is no way to pass this information from the callback handling timeout. It means \u201cFreeData\u201d callback will not get this information and \u201crequest\u201d object is not de-allocated.\r\n\r\nIn this case I need to associate a \u201cprivate data\u201d pointer with a blocked client, so it will be available in \u201cFreeData\u201d callback independently of the way the client was unblocked.\r\n\r\nThere are two new Module API functions:\r\n-\tRedisModule_SetBlockedClientPrivData\r\n-\tRedisModule_UnblockClientKeepPrivData\r\n\r\nFunction RedisModule_SetBlockedClientPrivData allows setting a \u201cprivate data\u201d pointer in internal module structures. This pointer will be processed by FreeData callback even if a client\u2019s block timed out.\r\n\r\nFunction RedisModule_UnblockClientKeepPrivData allows unblocking a client without changing a pointer to \u201cprivate data\u201d set using RedisModule_SetBlockedClientPrivData. \r\n\r\nNew example in helloblock.c module shows usage of these functions.\r\n\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2021-04-19T17:17:28Z",
        "closed_at": "2021-05-13T04:07:34Z",
        "merged_at": "2021-05-13T04:07:34Z",
        "body": "Added new readonly versions of EVAL and EVALSHA. Commands are implemented\r\nas `no-script @scripting`. We have avoided adding `read-only` as it may result\r\nin client key tracking which is not part of `EVAL/EVALSHA`.\r\n\r\nThese commands can not become unkillable, can't force replication, and can \r\nbe consistently routed to replicas in cluster mode. \r\n\r\nFixes - https://github.com/redis/redis/issues/8537\r\n\r\n# Update:\r\nin 7.0.1 we change the flags of this command.\r\nsee: https://github.com/redis/redis/pull/8820#issuecomment-1127287325 https://github.com/redis/redis/pull/10728",
        "comments": 16
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-19T17:05:58Z",
        "closed_at": "2021-04-20T03:23:49Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-19T15:21:06Z",
        "closed_at": "2021-04-19T18:35:14Z",
        "merged_at": "2021-04-19T18:35:14Z",
        "body": "See these comments:\r\n\r\n* https://github.com/redis/redis/pull/8442/files#r615932389\r\n* https://github.com/redis/redis-doc/pull/1556#pullrequestreview-638983937\r\n\r\nIt's a minor glitch in the appearance. The links still work so there is no hurry.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3298,
        "deletions": 1289,
        "changed_files": 119,
        "created_at": "2021-04-19T14:24:33Z",
        "closed_at": "2021-04-20T05:03:58Z",
        "merged_at": "2021-04-20T05:03:58Z",
        "body": "Upgrade urgency: HIGH, if you're using ACL and pub/sub, CONFIG REWRITE, or\r\nsuffering from performance regression. see below.\r\n\r\nBug fixes for regressions in previous releases of Redis 6.2:\r\n* Fix BGSAVE, AOFRW, and replication slowdown due to child reporting CoW (#8645)\r\n* Fix short busy loop when a timer event is about to fire (#8764)\r\n* Fix default user, overwritten and reset users losing Pub/Sub channel permissions (#8723)\r\n* Fix config rewrite with an empty `save` config resulting in default `save` values (#8719)\r\n* Fix not starting on alpine/libmusl without IPv6 (#8655)\r\n* Fix issues with propagation and MULTI/EXEC in modules (#8617)\r\n  Several issues around nested calls and thread-safe contexts\r\n\r\nBug fixes that are only applicable to previous releases of Redis 6.2:\r\n* ACL Pub/Sub channels permission handling for save/load scenario (#8794)\r\n* Fix early rejection of PUBLISH inside MULTI-EXEC transaction (#8534)\r\n* Fix missing SLOWLOG records for blocked commands (#8632)\r\n* Allow RESET command during busy scripts (#8629)\r\n* Fix some error replies that were not counted on stats (#8659)\r\n\r\nBug fixes:\r\n* Add a timeout mechanism for replicas stuck in fullsync (#8762)\r\n* Process HELLO command even if the default user has no permissions (#8633)\r\n* Client issuing a long-running script and using a pipeline, got disconnected (#8715)\r\n* Fix script kill to work also on scripts that use `pcall` (#8661)\r\n* Fix list-compress-depth may compress more node than required (#8311)\r\n* Fix redis-cli handling of rediss:// URL scheme (#8705)\r\n* Cluster: Skip unnecessary check which may prevent failure detection (#8585)\r\n* Cluster: Fix hang manual failover when replica just started (#8651)\r\n* Sentinel: Fix info-refresh time field before sentinel gets a first response (#8567)\r\n* Sentinel: Fix possible crash on failed connection attempt (#8627)\r\n* Systemd: Send the readiness notification when a replica is ready to accept connections (#8409)\r\n\r\nCommand behavior changes:\r\n* ZADD: fix awrong reply when INCR used with GT/LT which blocked the update (#8717)\r\n  It was responding with the incremented value rather than nil\r\n* XAUTOCLAIM: fix response to return the next available id as the cursor (#8725)\r\n  Previous behavior was returning the last one which was already scanned\r\n* XAUTOCLAIM: fix JUSTID to prevent incrementing delivery_count (#8724)\r\n\r\nNew config options:\r\n* Add cluster-allow-replica-migration config option (#5285)\r\n* Add replica-announced config option (#8653)\r\n* Add support for plaintext clients in TLS cluster (#8587)\r\n* Add support for reading encrypted keyfiles (#8644)\r\n\r\nImprovements:\r\n* Fix performance regression in BRPOP on Redis 6.0 (#8689)\r\n* Avoid adding slowlog entries for config with sensitive data (#8584)\r\n* Improve redis-cli non-binary safe string handling (#8566)\r\n* Optimize CLUSTER SLOTS reply (#8541)\r\n* Handle remaining fsync errors (#8419)\r\n\r\nInfo fields and introspection changes:\r\n* Strip % sign from current_fork_perc info field (#8628)\r\n* Fix RSS memory info on FreeBSD (#8620)\r\n* Fix client_recent_max_input/output_buffer in 'INFO CLIENTS' when all clients drop (#8588)\r\n* Fix invalid master_link_down_since_seconds in info replication (#8785)\r\n\r\nPlatform and deployment-related changes:\r\n* Fix FreeBSD <12.x builds (#8603)\r\n\r\nModules:\r\n* Add macros for RedisModule_log logging levels (#4246)\r\n* Add RedisModule_GetAbsExpire / RedisModule_SetAbsExpire (#8564)\r\n* Add a module type for key space notification (#8759)\r\n* Set module eviction context flag only in masters (#8631)\r\n* Fix unusable RedisModule_IsAOFClient API (#8596)\r\n* Fix missing EXEC on modules propagation after failed EVAL execution (#8654)\r\n* Fix edge-case when a module client is unblocked (#8618)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-04-19T12:56:50Z",
        "closed_at": "2021-04-19T14:16:03Z",
        "merged_at": "2021-04-19T14:16:03Z",
        "body": "Before this commit using RM_Call without \"!\" could cause the master\r\nto lazy-expire a key (delete it) but without replicating to replicas.\r\nThis could cause the replica's memory usage to gradually grow and\r\ncould also cause consistency issues if the master and replica have\r\na clock diff.\r\nThis bug was introduced in https://github.com/redis/redis/pull/8617\r\n\r\nAdded a test which demonstrates that scenario.\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-19T11:20:21Z",
        "closed_at": "2021-04-20T07:59:45Z",
        "merged_at": "2021-04-20T07:59:45Z",
        "body": "This scene is hard to happen. When first attempt some keys expired,\r\nonly kv position is updated not ov. Then socket err happens, second\r\nattempt is taken. This time kv items may be mismatching with ov items.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2021-04-19T07:39:51Z",
        "closed_at": "2021-04-22T05:59:10Z",
        "merged_at": "2021-04-22T05:59:10Z",
        "body": "We use EVAL extensively on a device without RTC and rely on NTP to keep the device time up. Sometimes the EVAL commands failes with error `BUSY Redis is busy running a script. You can only call SCRIPT KILL or SHUTDOWN NOSAVE.` but we are certainly sure the script won't run longer than our configured timeout value. We have seen following output from Redis, reporting an unreasonable execution time, too: \r\n\r\n```\r\nLua slow script detected: still in execution after 140612837289 milliseconds. You can try killing the script using the SCRIPT KILL command.\r\n```\r\n\r\nBy looking into Redis source code, we have found that Lua scripting relies on `mstime()` to check for script timeout, which is a wrapper of `gettimeofday`. `gettimeofday` is not a steady clock source and might jump forward or back. It should use a monotonic time source.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 99,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2021-04-18T20:28:24Z",
        "closed_at": "2021-04-19T09:20:28Z",
        "merged_at": null,
        "body": "DRAFT (to be folded into https://github.com/redis/redis/pull/8794):\r\n\r\nIn the initial release of Redis 6.2 setting a user to only allow pubsub access to\r\na specific, and doing ACL SAVE, resulted in an assertion when ACL LOAD was used.\r\nthis was later changed by #8723 (not yet released), but still not properly resolved.\r\n\r\nThe problem is that the server that generates an ACL file, doesn't know what\r\nwould be the setting of the `acl-pubsub-default` config in the server that will load it.\r\nso it needs to always start with `resetchannels` directive.\r\n\r\nThis should still be compatible with old acl files (from redis 6.0), and ones from earlier\r\nversions of 6.2 that didn't mess with channels.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 34,
        "changed_files": 2,
        "created_at": "2021-04-18T14:01:51Z",
        "closed_at": "2021-05-09T15:45:45Z",
        "merged_at": "2021-05-09T15:45:45Z",
        "body": "Cleanup: remove duplicated code from sentinel/lazyfree, and remove\r\nan extra time call in latency.\r\n\r\nI found in `loadSentinelConfigFromQueue`, load config from three items in queue are the same.\r\nIn lazyfree.c, `lazyfreeFreeSlotsMap` and `lazyFreeTrackingTable` are the same.\r\nWe can reduce duplicated code.\r\n\r\nIn `latencyAddSample` second call `time(NULL)` is unnecessary.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-18T13:35:59Z",
        "closed_at": "2021-04-21T10:43:38Z",
        "merged_at": "2021-04-21T10:43:38Z",
        "body": "recieved -> received",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-04-18T09:16:06Z",
        "closed_at": "2021-04-18T12:12:35Z",
        "merged_at": "2021-04-18T12:12:35Z",
        "body": "In github actions CI with valgrind, i saw that even the fast replica\r\n(one that wasn't paused), didn't get to complete the replication fast\r\nenough, and ended up getting disconnected by timeout.\r\n\r\nAdditionally, due to a typo in uname, we didn't get to actually run the\r\nCPU efficiency part of the test.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-04-18T08:33:38Z",
        "closed_at": "2021-04-18T17:51:09Z",
        "merged_at": "2021-04-18T17:51:09Z",
        "body": "Reverts #8649 and subsequent attempts to stabilize the test.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-18T05:53:04Z",
        "closed_at": "2021-04-18T09:05:05Z",
        "merged_at": null,
        "body": "This test failed twice in the recent days in the freebsd CI.\r\nThe test sets a softlimit timeout of 3 seconds, waits 10 seconds for redis to disconnect the client, but fails the test if it took more than 6 seconds (changing to 9)\r\n\r\n```\r\n*** [err]: Client output buffer soft limit is enforced if time is overreached in tests/unit/obuf-limits.tcl\r\nExpected 102520 >= 100000 && 6 < 6 (context: type eval line 23 cmd {assert {$omem >= 100000 && $time_elapsed < 6}} proc ::test)\r\n```",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-18T05:22:27Z",
        "closed_at": "2021-04-19T07:08:07Z",
        "merged_at": "2021-04-19T07:08:07Z",
        "body": "## Problem\r\n\r\nSince the tail size of c->reply is PROTO_REPLY_CHUNK_BYTES(16 * 1024bytes), but in the test only publish foo bar each time (subscribe foo will receive message foo bar, only 10 more bytes), when the soft limit is triggered for the first time, it will take almost 16 * 1024 / 10 times to publish foo bar, which may cause the soft_limit_seconds to be triggered for a very long time in a slow environment.\r\n\r\n## Solution\r\n\r\nIncrease the length of the publish data to fill the time needed for c->reply.\r\n\r\n## Other\r\nThe duration of the soft_limit_seconds trigger may be twice as long as the soft_limit_seconds, I'm not sure if this is a bug, because it only occurs in slow environments.\r\n\r\nThis is a \"regression\" from the change in #8699",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 18,
        "changed_files": 4,
        "created_at": "2021-04-17T11:46:47Z",
        "closed_at": "2021-04-25T12:50:15Z",
        "merged_at": "2021-04-25T12:50:15Z",
        "body": "fix comments.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2021-04-16T22:43:35Z",
        "closed_at": "2021-04-19T21:05:22Z",
        "merged_at": null,
        "body": "I am working with Redis Modules API to implement authorization module. There is a missing functionality.\r\nHere is a short description.\r\n\r\n1. I need to block a client to allow analyzing auth credentials in background. When a client is blocked, I allocate a \u201crequest\u201d object for background processing on a working thread.\r\n\r\n2. The client can be unblocked in two ways:\r\n        - Calling RedisModule_UnblockClient(bc, req) on a working thread\r\n        - Timeout of client\u2019s blocking\r\n\r\n3. When authorization request is completed on the working thread, I can pass information about allocated \u201crequest\u201d object in the RedisModule_UnblockClient. Thus, when a \u201cFreeData\u201d callback is called, the pointer to the \u201crequest\u201d object is passed as a parameter and I can de-allocate it.\r\n\r\n4. There is no way to pass this information from the callback handling timeout. It means \u201cFreeData\u201d callback will not get this information and \u201crequest\u201d object is not de-allocated.\r\n\r\nIn this case I need to associate \u201cprivdata\u201d pointer with a blocked client, so it will be available in \u201cFreeData\u201d callback independently of the way the client was unblocked.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-16T10:05:20Z",
        "closed_at": "2021-04-20T04:37:37Z",
        "merged_at": "2021-04-20T04:37:37Z",
        "body": "@madolson  PTAL\ud83d\ude04",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 99,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2021-04-15T18:58:36Z",
        "closed_at": "2021-04-19T10:27:44Z",
        "merged_at": "2021-04-19T10:27:44Z",
        "body": "In the initial release of Redis 6.2 setting a user to only allow pubsub access to\r\na specific channel, and doing ACL SAVE, resulted in an assertion when\r\nACL LOAD was used. This was later changed by #8723 (not yet released),\r\nbut still not properly resolved (now it errors instead of crash).\r\n\r\nThe problem is that the server that generates an ACL file, doesn't know what\r\nwould be the setting of the acl-pubsub-default config in the server that will load it.\r\nso ACL SAVE needs to always start with resetchannels directive.\r\n\r\nThis should still be compatible with old acl files (from redis 6.0), and ones from earlier\r\nversions of 6.2 that didn't mess with channels.\r\n\r\nScenario:\r\n1. Create user with access to single channel.\r\n```\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass sanitize-payload ~* +@all\"\r\n127.0.0.1:6379> acl setuser foo resetchannels &test\r\nOK\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass sanitize-payload ~* +@all\"\r\n2) \"user foo off &test -@all\"\r\n```\r\n\r\n2. Persist to `aclfile`.\r\n```\r\n127.0.0.1:6379> acl save\r\nOK\r\n```\r\n\r\n`aclfile` content after saving it.\r\n\r\n```\r\ncat /tmp/users.acl\r\nuser default on nopass sanitize-payload ~* +@all\r\nuser foo off &test -@all\r\n```\r\n\r\n3. Load the aclfile\r\n```\r\n127.0.0.1:6379> acl load\r\n```\r\n\r\nPossible scenarios:\r\n\r\n```\r\nScenario 1:(acl-pubsub-default (allchannels) \u2192 default redis behaviour)\r\nresetchannels -> acl-pubsub-default (allchannels)\r\n\r\n\"user icaro off resetchannels &test -@all\"\r\nOutput: (error) ERR Error in ACL SETUSER modifier '&test': Adding a pattern after the * pattern (or the 'allchannels' flag) is not valid and does not have any effect. Try 'nochannels' to start with an empty list of channels\r\n\r\n\"user icaro off allchannels &test -@all\"\r\nOutput: (error) ERR Error in ACL SETUSER modifier '&test': Adding a pattern after the * pattern (or the 'allchannels' flag) is not valid and does not have any effect. Try 'nochannels' to start with an empty list of channels\r\n\r\n\"user icaro off nochannels &test -@all\" --> work(only &test permission)\r\nOutput: \"user icaro off nochannels &test -@all\"\r\n\r\n\"user icaro off -@all\" --> work(default behaviour -> all permissions for channels)\r\nOutput: \"user icaro off *& -@all\"\r\n\r\n\"user icaro &* off -@all\" --> work(all permissions for channels)\r\nOutput: \"user icaro off *& -@all\"\r\n\r\n\"user icaro allchannels off -@all\" --> work(all permissions for channels)\r\nOutput: \"user icaro off *& -@all\"\r\n\r\n\"user icaro resetchannels off -@all\" --> work(default behaviour -> all permissions for channels)\r\nOutput: \"user icaro off *& -@all\"\r\n\r\n\"user icaro nochannels off -@all\" --> work(no channels permissions)\r\nOutput: \"user icaro off nochannels -@all\"\r\n \r\nScenario 2:(acl-pubsub-default (nochannels))\r\nacl-pubsub-default nochannels\r\n\r\nresetchannels -> acl-pubsub-default (nochannels)\r\n\r\n\"user icaro off resetchannels &test -@all\" --> work(only &test permission)\r\nOutput: \"user icaro off nochannels &test -@all\"\r\n\r\n\"user icaro off allchannels &test -@all\"\r\nOutput: (error) ERR Error in ACL SETUSER modifier '&test': Adding a pattern after the * pattern (or the 'allchannels' flag) is not valid and does not have any effect. Try 'nochannels' to start with an empty list of channels\r\n\r\n\"user icaro off nochannels &test -@all\" --> work(only &test permission)\r\nOutput: \"user icaro off nochannels &test -@all\"\r\n\r\n\"user icaro off -@all\" --> work(default behaviour -> no channels permissions)\r\nOutput: \"user icaro off nochannels -@all\"\r\n\r\n\"user icaro &* off -@all\" --> work(all permissions for channels)\r\nOutput: \"user icaro off *& -@all\"\r\n\r\n\"user icaro allchannels off -@all\" --> work(all permissions for channels)\r\nOutput: \"user icaro off *& -@all\"\r\n\r\n\"user icaro resetchannels off -@all\" --> work(default behaviour -> no channels permissions)\r\nOutput: \"user icaro off nochannels -@all\"\r\n\r\n\"user icaro nochannels off -@all\" --> work(no channels permissions)\r\nOutput: \"user icaro off nochannels -@all\"\r\n```",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-04-15T15:20:34Z",
        "closed_at": "2021-06-08T09:36:05Z",
        "merged_at": "2021-06-08T09:36:05Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-04-15T02:58:27Z",
        "closed_at": "2021-04-15T05:44:08Z",
        "merged_at": "2021-04-15T05:44:08Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-04-14T15:50:30Z",
        "closed_at": "2021-04-19T06:34:21Z",
        "merged_at": "2021-04-19T06:34:21Z",
        "body": "When replica never successfully connect to master, server.repl_down_since\r\nwill be initialized to 0, therefore, the info master_link_down_since_seconds\r\nwas showing the current unix timestamp, which does not make much sense.\r\n\r\nThis commit fixes the issue by showing master_link_down_since_seconds to -1.\r\nmeans the replica never connect to master before.\r\n\r\nThis commit also resets this variable back to 0 when a replica is turned into\r\na master, so that it'll behave the same if the master is later turned into a\r\nreplica again.\r\n\r\nThe implication of this change is that if some app is checking if the value is > 60\r\ndo something, like conclude the replica is stale, this could case harm (changing\r\na big positive number with a small one).",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-04-14T07:21:10Z",
        "closed_at": "2021-04-14T08:49:00Z",
        "merged_at": "2021-04-14T08:49:00Z",
        "body": "replace the hardcoded after 2000, with wait_for_sync and\r\nwait_for_condition",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 104,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2021-04-13T21:10:14Z",
        "closed_at": "2023-05-10T17:09:19Z",
        "merged_at": null,
        "body": "Will rebase with #8759 after it is merged to be able to reuse the pubsub methods across test files.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-13T07:33:42Z",
        "closed_at": "2021-04-21T10:41:25Z",
        "merged_at": "2021-04-21T10:41:25Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 44,
        "changed_files": 4,
        "created_at": "2021-04-13T07:32:43Z",
        "closed_at": "2021-04-13T14:41:46Z",
        "merged_at": "2021-04-13T14:41:46Z",
        "body": "This reverts commit 808f3004f0de8c129b3067d8b2ce5002fa703e77.\r\nsee discussion in #8670 (change that never made it to any released version)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-13T06:58:49Z",
        "closed_at": "2021-04-13T14:35:10Z",
        "merged_at": "2021-04-13T14:35:10Z",
        "body": "failed on MacOS\r\n```\r\n*** [err]: EXPIRE precision is now the millisecond in tests/unit/expire.tcl\r\nExpected 'somevalue {}' to equal or match '{} {}'\r\n```\r\nhttps://github.com/redis/redis/runs/2329125455?check_suite_focus=true",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-13T06:00:24Z",
        "closed_at": "2021-04-13T13:28:54Z",
        "merged_at": "2021-04-13T13:28:54Z",
        "body": "In Sentinel mode no config file is not allowed, so there is not necessary to keep no config file warning for sentinel anymore. Also this lline will not be executed since empty config file check for sentinel will happen earlier than that.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-04-13T01:05:02Z",
        "closed_at": "2021-07-11T10:00:17Z",
        "merged_at": "2021-07-11T10:00:17Z",
        "body": "Do not install a file event to send data to rewrite child when parent\r\nstop sending diff to child in aof rewrite.\r\n\r\nIn aof rewrite, when parent stop sending data to child, if there is\r\nnew rewrite data, aofChildWriteDiffData write event will be installed.\r\nThen this event is issued and deletes the file event without do anyting.\r\nThis will happen over and over again until aof rewrite finish.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-04-12T22:29:12Z",
        "closed_at": "2021-04-12T23:36:05Z",
        "merged_at": null,
        "body": "i am adding a random file because I feel like it. hopefully the U.S. gets hacked again.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-04-12T20:09:17Z",
        "closed_at": "2021-04-13T05:41:13Z",
        "merged_at": "2021-04-13T05:41:13Z",
        "body": "i noticed server.client_pause_end_time is uninitialized, or actually 0, at\r\nstartup, which means this method would think the timerout was reached\r\nand go look for paused clients.\r\n\r\nmaybe there's a cleaner fix by setting it to 0, and then checking for a special\r\n0 value, but i suppose an early exit in that function when we're not in pause,\r\navoiding access to that var is just as good.\r\n\r\nThis causes no harm since unpauseClients will not find any paused clients.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2021-04-12T19:59:28Z",
        "closed_at": "2021-04-13T04:35:04Z",
        "merged_at": "2021-04-13T04:35:04Z",
        "body": "The code used to decide on the next time to wake on a timer with\r\nmicrosecond accuracy, but when deciding to go to sleep it used\r\nmilliseconds accuracy (with truncation), this means that it would wake\r\nup too early, see that there's no timer to process, and go to sleep\r\nagain for 0ms again and again until the right microsecond arrived.\r\n\r\ni.e. a timer for 100ms, would sleep for 99ms, but then do a busy loop\r\nthrough the kernel in the last millisecond, triggering many calls to\r\nbeforeSleep.\r\n\r\nThe fix is to change all the logic in ae.c to work with microseconds,\r\nwhich is good since most of the ae backends support micro (or even nano)\r\nseconds. however the epoll backend, doesn't support micro, so to avoid\r\nthis problem it needs to round upwards, rather than truncate.\r\n\r\nIssue created by the monotonic timer PR #7644 (redis 6.2)\r\nBefore that, all the timers in ae.c were in milliseconds (using\r\nmstime), so when it requested the backend to sleep till the next timer\r\nevent, it would have worked ok.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2021-04-12T19:45:12Z",
        "closed_at": "2021-04-12T21:00:58Z",
        "merged_at": "2021-04-12T21:00:58Z",
        "body": "Disable replica migration to avoid a race condition where the\r\nmigrated-from node turns into a replica.\r\n\r\nLong term, this test should probably be improved to handle multiple\r\nslots and accept such auto migrations but this is a quick fix to\r\nstabilize the CI without completely dropping this test.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2021-04-12T14:49:04Z",
        "closed_at": "2021-04-15T14:18:52Z",
        "merged_at": "2021-04-15T14:18:51Z",
        "body": "Starting redis 6.0 (part of the TLS feature), diskless master uses pipe from the fork\r\nchild so that the parent is the one sending data to the replicas.\r\nThis mechanism has an issue in which a hung replica will cause the master to wait\r\nfor it to read the data sent to it forever, thus preventing the fork child from terminating\r\nand preventing the creations of any other forks.\r\n\r\nThis PR adds a timeout mechanism, much like the ACK-based timeout,\r\nwe disconnect replicas that aren't reading the RDB file fast enough.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2798,
        "deletions": 1248,
        "changed_files": 44,
        "created_at": "2021-04-12T14:03:44Z",
        "closed_at": "2021-05-06T10:01:45Z",
        "merged_at": null,
        "body": "Implement #8702\r\n\r\n## Problem\r\n1) When inserting or deleting elements in the middle of ziplist, ziplist may have a cascading effect.\r\n2) ziplist code is too complex, prone to bugs and not easy to find.\r\n\r\n## Description of the feature\r\n1) Use listpack instead of ziplist.\r\n2) Modify rdb loading.\r\n3) Fix some bugs.\r\n4) Fix some typos.\r\n\r\n## Changes of config\r\nhash-max-ziplist-entries \t-> hash-max-listpack-entries\r\nhash-max-ziplist-value \t-> hash-max-listpack-value\r\nlist-max-ziplist-size \t\t-> list-max-listpack-size\r\nzset-max-ziplist-entries \t-> zset-max-listpack-entries\r\nzset-max-ziplist-value \t-> zset-max-listpack-value\r\n\r\n## Change of listpack\r\n1) Add some methods to support muli-comtainer quicklist.\r\n\r\n## Change of quicklist\r\n1) Add multi-container support.\r\n2) Add runtime conversion from ziplist to listpack to avoid slowdown due to rdb loading old quicklist.\r\n\r\n## Change of hash and zset\r\n1) Modify hash, zset's ziplist encoding to listpack encoding.\r\n\r\n## BugFix\r\n\r\n1)  Fix the error in determining the number of elements when deep enable in lpValidateIntegrity.\r\n     lpValidateNext will return EOF when iterator to the end, resulting in 1 more element.\r\n2) Fix missing quicklistNode's sz update\r\n    When inserting an element using _quicklistInsert and quicklistNode is NULL, quicklistNodeUpdateSz is not executed, resulting in node->sz being 0, which will cause the quicklist data to be corrupted or lost.\r\n\r\n## Command incompatibility changes\r\n1) ```debug ziplist``` command is modified to ```debug listpack```\r\n\r\n## Test\r\n1) Add listpack unit tests\r\n\r\n## TODO\r\nCorrupted listpack data testing.\r\n\r\nP.S. The change is too big, it feels like it can be split into multiple pr to deal with.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 143,
        "deletions": 52,
        "changed_files": 9,
        "created_at": "2021-04-11T15:29:38Z",
        "closed_at": "2021-04-19T18:33:27Z",
        "merged_at": "2021-04-19T18:33:27Z",
        "body": "Adding a new type mask \u200bfor key space notification, REDISMODULE_NOTIFY_MODULE, to enable unique notifications from commands on REDISMODULE_KEYTYPE_MODULE type keys (which is currently unsupported).\r\n\r\nModules can subscribe to a module key keyspace notification by RM_SubscribeToKeyspaceEvents,\r\nand clients by notify-keyspace-events of redis.conf or via the CONFIG SET, with the characters 'd' or 'A' \r\n(REDISMODULE_NOTIFY_MODULE type mask is part of the '**A**ll' notation for key space notifications).\r\n\r\nRefactor: move some pubsub test infra from pubsub.tcl to util.tcl to be re-used by other tests.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-04-11T12:09:32Z",
        "closed_at": "2021-04-12T07:03:23Z",
        "merged_at": null,
        "body": "In fact sentinels only send hello messages to masters and replicas,\r\nnot other sentinels. The old code will always try to send hello\r\nmessages and terminated because of no existence of link. So we make\r\nthis more clear by sending no messages. And the wrong comments are\r\nfixed.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-11T09:32:02Z",
        "closed_at": "2021-04-13T17:14:13Z",
        "merged_at": "2021-04-13T17:14:13Z",
        "body": "In a code example, using RedisModule_FreeString instead of\r\nRedisModule_Free makes it behave correctly regardless of whether\r\nautomatic memory is used or not.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2021-04-07T06:58:55Z",
        "closed_at": "2021-04-11T05:14:32Z",
        "merged_at": "2021-04-11T05:14:32Z",
        "body": "Issue created with #8419 (not released yet)\r\nThe bio aof fsync fd may be closed by main thread (AOFRW done handler)\r\nand even possibly reused for another socket, pipe, or file.\r\nThis can can an EBADF or EINVAL fsync error, which will lead to -MISCONF errors failing all writes.\r\nWe just ignore these errno because aof fsync did not really fail.\r\n\r\nWe handle errno when fsyncing aof in bio, so we could know the real reason\r\nwhen users get `-MISCONF Errors writing to the AOF file` error\r\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 65,
        "deletions": 18,
        "changed_files": 9,
        "created_at": "2021-04-07T04:29:59Z",
        "closed_at": "2021-04-26T12:52:06Z",
        "merged_at": "2021-04-26T12:52:06Z",
        "body": "Most of the ae.c backends didn't explicitly handle errors, and instead ignored all errors and did an implicit retry.\r\nThis is desired for EAGAIN and EINTER, but in case of other systematic errors, we prefer to fail and log the error we got rather than get into a busy loop.\r\n\r\n`ae_evport.c` handled `EINTR`:\r\n\r\nhttps://github.com/redis/redis/blob/fb66e2e24943018961321d13e46ee2ab66de882a/src/ae_evport.c#L288-L296\r\n\r\nTherefore we should also do that in other polls.",
        "comments": 22
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2021-04-07T03:38:34Z",
        "closed_at": "2021-04-21T06:04:36Z",
        "merged_at": "2021-04-21T06:04:36Z",
        "body": "As title.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2021-04-06T02:36:43Z",
        "closed_at": "2021-04-07T07:01:28Z",
        "merged_at": "2021-04-07T07:01:28Z",
        "body": "Actually, we don not need `ERR value is out of range, value must between 0 and 9223372036854775807`, this will cause user to continue using `COUNT = 0`, but will get `COUNT must be> 0`.\r\n\r\n![image](https://user-images.githubusercontent.com/13137470/113650898-9656eb80-96c3-11eb-8c8b-3effd161a6a6.png)\r\n\r\nFix out of range error messages to be clearer (avoid mentioning 9223372036854775807)\r\n* Fix XAUTOCLAIM COUNT option confusing error msg\r\n* Fix other (RPOP and alike) error message",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 66,
        "deletions": 25,
        "changed_files": 4,
        "created_at": "2021-04-05T20:37:25Z",
        "closed_at": "2021-04-06T08:57:57Z",
        "merged_at": "2021-04-06T08:57:57Z",
        "body": "This is work in progress, focusing on two main areas:\r\n* Avoiding race conditions with cluster configuration propagation.\r\n* Ignoring limitations with redis-cli --cluster fix which makes it hard\r\n  to distinguish real errors (e.g. failure to fix) from expected\r\n  conditions in this test (e.g. nodes not agreeing on configuration).",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-03T06:59:37Z",
        "closed_at": "2021-04-13T17:19:43Z",
        "merged_at": "2021-04-13T17:19:43Z",
        "body": "The params is more than the formatter.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-04-02T04:31:17Z",
        "closed_at": "2021-04-05T05:30:42Z",
        "merged_at": "2021-04-05T05:30:42Z",
        "body": "About https://github.com/redis/redis/issues/8736\r\nI checked other places, `DBSIZE` is only needed to update.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-04-02T03:09:02Z",
        "closed_at": "2021-04-20T02:32:09Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-04-01T08:26:49Z",
        "closed_at": "2021-06-14T13:57:59Z",
        "merged_at": null,
        "body": "fix #8572\r\n\r\nIf a command (not only the \"save\") is in the included file, the command is also in the redis.conf and the included file is behind the command,\r\nwhen set the command again with \"config set\", issue the \"config rewrite\" and restart the server,\r\nthe new command option doesn't work.\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 10945,
        "deletions": 1707,
        "changed_files": 115,
        "created_at": "2021-04-01T07:50:04Z",
        "closed_at": "2021-04-06T09:21:26Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-31T14:43:44Z",
        "closed_at": "2021-03-31T20:56:04Z",
        "merged_at": "2021-03-31T20:56:04Z",
        "body": "In case the module's digest function doesn't modify\r\n'md' it'll contain garbage and result in wrong\r\nDEBUG DIGEST",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2021-03-31T14:30:03Z",
        "closed_at": "2021-04-01T06:01:05Z",
        "merged_at": "2021-04-01T06:01:05Z",
        "body": "Before this commit, the sentinel config check when server start is not consistant when we configring different number of args when start, if we start with argc >=2, such as ./redis-server --sentinel (without config file provided). the config check or sentinel will appen in this block, before server initiaization happens:\r\nhttps://github.com/redis/redis/blob/e138698e54e97bfaababf56507026bf92dd4deb4/src/server.c#L6271\r\nHowever for argc == 1 case, eg. ./redis-sentinel the check will happen in https://github.com/redis/redis/blob/91f4f41665c4e9e0ad248ce0b528644de28d0acd/src/sentinel.c#L557\r\nwhich cause unnecessary start server and exit after all init work is done:\r\n\r\n```Hwware-MacBook-Pro:src hwware$ ./redis-sentinel\r\n15518:X 30 Mar 2021 16:46:28.085 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n15518:X 30 Mar 2021 16:46:28.085 # Redis version=255.255.255, bits=64, commit=1ccfd6a1, modified=0, pid=15518, just started\r\n15518:X 30 Mar 2021 16:46:28.085 # Warning: no config file specified, using the default config. In order to specify a config file use ./redis-sentinel /path/to/sentinel.conf\r\n15518:X 30 Mar 2021 16:46:28.086 * Increased maximum number of open files to 10032 (it was originally set to 256).\r\n15518:X 30 Mar 2021 16:46:28.086 * monotonic clock: POSIX clock_gettime\r\n                _._                                                  \r\n           _.-``__ ''-._                                             \r\n      _.-``    `.  `_.  ''-._           Redis 255.255.255 (1ccfd6a1/0) 64 bit\r\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \r\n (    '      ,       .-`  | `,    )     Running in sentinel mode\r\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 26379\r\n |    `-._   `._    /     _.-'    |     PID: 15518\r\n  `-._    `-._  `-./  _.-'    _.-'                                   \r\n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \r\n |    `-._`-._        _.-'_.-'    |           http://redis.io        \r\n  `-._    `-._`-.__.-'_.-'    _.-'                                   \r\n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \r\n |    `-._`-._        _.-'_.-'    |                                  \r\n  `-._    `-._`-.__.-'_.-'    _.-'                                   \r\n      `-._    `-.__.-'    _.-'                                       \r\n          `-._        _.-'                                           \r\n              `-.__.-'                                               \r\n\r\n15518:X 30 Mar 2021 16:46:28.090 # Sentinel started without a config file. Exiting...\r\n```\r\n\r\nThis commit provides a generalize way to check sentinel config early for all cases. After parsing config file path configured.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-03-31T11:49:04Z",
        "closed_at": "2021-03-31T21:07:19Z",
        "merged_at": "2021-03-31T21:07:19Z",
        "body": "this is a followup PR for #8699\r\ninstead of copying the deferred reply data to the previous node only if it has room for the entire thing,\r\nwe can now split the new payload, put part of it into the spare space in the prev node, and the rest may fit into the next node.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 6,
        "changed_files": 5,
        "created_at": "2021-03-31T08:53:14Z",
        "closed_at": "2021-04-01T06:44:44Z",
        "merged_at": "2021-04-01T06:44:44Z",
        "body": "5629dbe7155f2534cd44404743003dc37c444b2a added a change that configures\r\nthe tcp (plaintext) port alongside the tls port, this causes the INFO\r\ncommand for tcp_port to return that instead of the tls port when running\r\nin tls, and that broke the sentinel tests that query it.\r\n\r\nthe fix is to add a method that gets the right port from CONFIG instead\r\nof relying on the tcp_port info field.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2021-03-30T21:09:13Z",
        "closed_at": "2021-04-04T20:48:38Z",
        "merged_at": null,
        "body": "This makes 'quoted-string\\' a valid string that ends with a backslash\r\ncharacter rather than return an error.\r\n\r\nThis is somewhat inconsistent as one would expect backslash to be a\r\nspecial character that needs to be quoted by itself, but doing this more\r\nconventional fix will introduce a backwards compatibility breakage we'd\r\nlike to avoid.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-03-30T15:44:30Z",
        "closed_at": "2021-04-01T09:13:56Z",
        "merged_at": "2021-04-01T09:13:56Z",
        "body": "This command used to return the last scanned entry id as the cursor, instead of the next one to be scanned.\r\nso in the next call, the user could / should have sent `(cursor` and not just `cursor` if he wanted to avoid scanning the same record twice.\r\n\r\nscanning the record twice would look odd if someone is checking what exactly was scanned, but it also has a side effect of incrementing the delivery count twice.\r\nsince documentation was correct, and didn't mention using `(`, so we hope won't cause anyone to miss a record.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-30T11:32:31Z",
        "closed_at": "2021-03-30T20:48:02Z",
        "merged_at": "2021-03-30T20:48:02Z",
        "body": "To align with XCLAIM and the XAUTOCLAIM docs",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 50,
        "deletions": 7,
        "changed_files": 5,
        "created_at": "2021-03-30T10:49:05Z",
        "closed_at": "2021-04-05T20:13:20Z",
        "merged_at": "2021-04-05T20:13:20Z",
        "body": "Background:\r\nRedis 6.2 added ACL control for pubsub channels (#7993), which were supposed\r\nto be permissive by default to retain compatibility with redis 6.0 ACL. \r\nBut due to a bug, only newly created users got this `acl-pubsub-default` applied,\r\nwhile overwritten (updated) users got reset to `resetchannels` (denied).\r\n\r\nSince the \"default\" user exists before loading the config file,\r\nany ACL change to it, results in an update / overwrite.\r\n\r\nSo when a \"default\" user is loaded from config file or include ACL\r\nfile with no channels related rules, the user will not have any\r\npermissions to any channels. But other users will have default\r\npermissions to any channels.\r\n\r\nWhen upgraded from 6.0 with config rewrite, this will lead to\r\n\"default\" user channels permissions lost.\r\nWhen users are loaded from include file, then call \"acl load\", users\r\nwill also lost channels permissions.\r\n\r\nSimilarly, the `reset` ACL rule, would have reset the user to be denied\r\naccess to any channels, ignoring `acl-pubsub-default` and breaking\r\ncompatibility with redis 6.0.\r\n\r\nThe implication of this fix is that it regains compatibility with redis 6.0,\r\nbut breaks compatibility with redis 6.2.0 and 6.2.1. e.g. after the upgrade,\r\nthe default user will regain access to pubsub channels.\r\n\r\nOther changes:\r\nAdditionally this commit rename server.acl_pubusub_default to\r\nserver.acl_pubsub_default and fix typo in acl tests.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-03-29T20:50:26Z",
        "closed_at": "2021-03-30T19:49:06Z",
        "merged_at": "2021-03-30T19:49:06Z",
        "body": "fix #8572\r\nthe bug was also discussed in #8716, and was solved in #8719, but incompletely :\r\n\r\nwhen the server is started, and the save option is default, if you issue the \" config set save \"\" \"\r\nto change the save option, and then issue the \u201cconfig rewrite\u201d command, the save option change\r\n( from the default to \" save \"\" \" ) won't be stored.\r\n\r\nthe bug maybe has no connection with \"include\" command.",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-29T19:09:41Z",
        "closed_at": "2021-03-30T08:31:32Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-03-29T15:08:34Z",
        "closed_at": "2021-03-30T08:41:06Z",
        "merged_at": "2021-03-30T08:41:06Z",
        "body": "Another test race condition in the macos tests.\r\nthe test was waiting for PINGs to be generated and put on the replication stream,\r\nbut waiting for 1 or 2 seconds doesn't really guarantee that.\r\n\r\nthen the test that expected 6 full syncs, found only 4:\r\n*** [err]: test various edge cases of repl topology changes with missing pings at the end in tests/integration/psync2-pingoff.tcl\r\nExpected '4' to be equal to '6' (context: type eval line 64 cmd {assert_equal [status $master sync_full] 6} proc ::test)\r\n\r\nsee\r\nhttps://github.com/redis/redis/runs/2214692167?check_suite_focus=true",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 92,
        "deletions": 55,
        "changed_files": 2,
        "created_at": "2021-03-29T14:17:37Z",
        "closed_at": "2021-04-01T06:33:54Z",
        "merged_at": "2021-04-01T06:33:54Z",
        "body": "Fixes #8718 \r\n\r\nIf GT/LT fails the operation we need to reply with\r\nnill (like failure due to NX).\r\n\r\nOther changes:\r\nAdd the missing $encoding suffix to many zset tests\r\n\r\nNote: there's a behavior change just in case of INCR + GT/LT that fails.\r\nThe old code was replying with the wrong (rejected) score, and now it'll reply with nil.\r\n\r\nNote that that's anyway a corner case so this \"behavior change\" shouldn't have too much affect.\r\nUsing GT/LT with INCR has a predictable result even before we run the command (INCR GT will only only / always fail if the increment is negative).",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-03-29T08:05:24Z",
        "closed_at": "2021-03-29T10:34:16Z",
        "merged_at": "2021-03-29T10:34:16Z",
        "body": "`processCommandAndResetClient` returns 1 if client is dead. It does it by checking if `serve.current_client` is `NULL`. On script timeout, Redis will re-enter `processCommandAndResetClient` and when finished we will set `server.current_client` to `NULL`. This will cause later to falsely return 1 and think that the client that sent the timed-out\u00a0script is dead (Redis will stop reading from the client buffer).",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2021-03-29T04:52:21Z",
        "closed_at": "2021-04-01T05:50:23Z",
        "merged_at": "2021-04-01T05:50:23Z",
        "body": "There are 2 common range comparators for skiplist: `zslValueGteMin` and `zslValueLteMax`, but they're not being reused in `zslDeleteRangeByScore`\r\n\r\nThis is a small change to make code cleaner.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 182,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-03-27T12:56:39Z",
        "closed_at": "2022-03-13T08:46:24Z",
        "merged_at": null,
        "body": "I always use the following commands to initialize the sentinel configuration:\r\n\r\n```c\r\nSENTINEL monitor test master_ip master_port 2\r\nSENTINEL set test auth-pass passwd\r\nSENTINEL set ....\r\n```\r\n\r\nIt worked well before https://github.com/redis/redis/commit/8631e6477904f3d8f87662fd93a1ba294615654a, because the send `INFO` command blocks sending `PING`, the condition of this case (https://github.com/redis/redis/blob/6.2.1/src/sentinel.c#L4178) will be satisfied in 15 seconds, the unauthorized connection will be released and the authenticated connection will be reestablished between sentinel and the instance.\r\n\r\nHowever, in the current implementation, because of the refresh of [`link->last_pong_time`](https://github.com/redis/redis/blob/6.2.1/src/sentinel.c#L2815), the above case cannot be satisfied, unauthorized connection cannot be released, sentinel thinks the instance is `sdown`.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 13,
        "changed_files": 4,
        "created_at": "2021-03-27T10:14:56Z",
        "closed_at": "2021-04-01T05:20:15Z",
        "merged_at": "2021-04-01T05:20:15Z",
        "body": "**Problem**\r\n\r\nCurrently, when performing random distribution verification, we determine the probability of each element occurring in the sum, but the probability is only an estimate, these tests had rare sporadic failures, and we cannot verify what the probability of failure will be.\r\n\r\n**Solution**\r\n\r\nUsing the chi-square distribution instead of the original random distribution validation makes the test more reasonable and easier to find problems.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2021-03-26T21:11:39Z",
        "closed_at": "2021-04-21T10:51:53Z",
        "merged_at": "2021-04-21T10:51:53Z",
        "body": "This PR updates the redis.conf by:\r\n\r\n1. Removing trailing white space\r\n2. Fixing a typo in `compatable`.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-03-26T17:59:56Z",
        "closed_at": "2021-03-28T21:25:30Z",
        "merged_at": "2021-03-28T21:25:30Z",
        "body": "Fixes #8704",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-26T15:40:21Z",
        "closed_at": "2021-04-15T18:11:41Z",
        "merged_at": "2021-04-15T18:11:41Z",
        "body": "In sentinelFlushConfig function, we should call close after logging the error in order to avoid it overwrite the original errno.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2021-03-25T16:43:22Z",
        "closed_at": "2021-03-30T20:06:29Z",
        "merged_at": "2021-03-30T20:06:29Z",
        "body": "* Avoid checking output limits if buffer didn't grow.\r\n* Use previouse node in case it has room in deferred output node.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-03-25T07:24:54Z",
        "closed_at": "2021-03-25T13:09:12Z",
        "merged_at": "2021-03-25T13:09:12Z",
        "body": "Now we always save expire time of key whatever it is expired or not",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-24T17:23:02Z",
        "closed_at": "2021-03-25T02:39:31Z",
        "merged_at": "2021-03-25T02:39:31Z",
        "body": "REPLICAOF actually specifies which master to attach.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-03-24T13:56:41Z",
        "closed_at": "2021-03-25T14:16:03Z",
        "merged_at": "2021-03-25T14:16:03Z",
        "body": "This is a proposed fix for https://github.com/redis/redis/issues/8668. It is expected to significantly improve CPU utilization for BRPOP heavy workloads (e.g. sidekiq).\r\n\r\nA performance regression was introduced in https://github.com/redis/redis/commit/215b72c0ba1f42d15dcfe6fa60abb414275296ba that affects `BRPOP` heavy workloads. This patch introduces a fast path that bypasses the expensive new code path if no clients are blocked by type `BLOCKED_MODULE`.\r\n\r\nSince the issue affects Redis 6.0 as well, we'd love to be able to backport this to the 6.0 series. I checked, and `6.0` already includes `server.blocked_clients_by_type` -- so I expect a backport would be quite straight forward.\r\n\r\nI profiled using the reproduce workload from https://github.com/redis/redis/issues/8668#issuecomment-804350847, and here are the results.\r\n\r\n**Before (current `unstable`):**\r\n\r\n![Screenshot 2021-03-24 at 14 53 24](https://user-images.githubusercontent.com/11158255/112322109-e8097a00-8cb0-11eb-814a-0761f0974953.png)\r\n\r\n**After (`blocked-module-fast-path`):**\r\n\r\n![Screenshot 2021-03-24 at 14 54 47](https://user-images.githubusercontent.com/11158255/112322145-f061b500-8cb0-11eb-9dae-293c05ef962a.png)\r\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-23T15:18:09Z",
        "closed_at": "2021-03-24T13:14:47Z",
        "merged_at": "2021-03-24T13:14:47Z",
        "body": "This fixes a race where a bgsave can start during the test after we verified no bgsave is running.\r\nIs this what happened here: https://github.com/redis/redis/runs/2144641244?check_suite_focus=true?",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1168,
        "deletions": 131,
        "changed_files": 18,
        "created_at": "2021-03-23T14:51:16Z",
        "closed_at": "2021-09-23T11:02:16Z",
        "merged_at": "2021-09-23T11:02:16Z",
        "body": "### Description\r\nA mechanism for disconnecting clients when the sum of all connected clients is above a configured limit. This prevents eviction or OOM caused by accumulated used memory between all clients. It's a complimentary mechanism to the `client-output-buffer-limit` mechanism which takes into account not only a single client and not only output buffers but rather all memory used by all clients.\r\n\r\n#### Design\r\nThe general design is as following:\r\n* We track memory usage of each client, taking into account all memory used by the client (query buffer, output buffer, parsed arguments, etc...). This is kept up to date after reading from the socket, after processing commands and after writing to the socket.\r\n* Based on the used memory we sort all clients into buckets. Each bucket contains all clients using up up to x2 memory of the clients in the bucket below it. For example up to 1m clients, up to 2m clients, up to 4m clients, ...\r\n* Before processing a command and before sleep we check if we're over the configured limit. If we are we start disconnecting clients from larger buckets downwards until we're under the limit.\r\n\r\n#### Config\r\n`maxmemory-clients` max memory all clients are allowed to consume, above this threshold we disconnect clients.\r\nThis config can either be set to 0 (meaning no limit), a size in bytes (possibly with MB/GB suffix),\r\nor as a percentage of `maxmemory` by using the `%` suffix (e.g. setting it to `10%` would mean 10% of `maxmemory`).\r\n\r\n#### Important code changes\r\n* During the development I encountered yet more situations where our io-threads access global vars. And needed to fix them. I also had to handle keeps the clients sorted into the memory buckets (which are global) while their memory usage changes in the io-thread. To achieve this I decided to simplify how we check if we're in an io-thread and make it much more explicit. I removed the `CLIENT_PENDING_READ` flag used for checking if the client is in an io-thread (it wasn't used for anything else) and just used the global `io_threads_op` variable the same way to check during writes.\r\n* I optimized the cleanup of the client from the `clients_pending_read` list on client freeing. We now store a pointer in the `client` struct to this list so we don't need to search in it (`pending_read_list_node`).\r\n* Added `evicted_clients` stat to `INFO` command.\r\n* Added `CLIENT NO-EVICT ON|OFF` sub command to exclude a specific client from the client eviction mechanism. Added corrosponding 'e' flag in the client info string.\r\n* Added `multi-mem` field in the client info string to show how much memory is used up by buffered multi commands.\r\n* Client `tot-mem` now accounts for buffered multi-commands, pubsub patterns and channels (partially), tracking prefixes (partially).\r\n* CLIENT_CLOSE_ASAP flag is now handled in a new `beforeNextClient()` function so clients will be disconnected between processing different clients and not only before sleep. This new function can be used in the future for work we want to do outside the command processing loop but don't want to wait for all clients to be processed before we get to it. Specifically I wanted to handle output-buffer-limit related closing before we process client eviction in case the two race with each other.\r\n* Added a `DEBUG CLIENT-EVICTION` command to print out info about the client eviction buckets.\r\n* Each client now holds a pointer to the client eviction memory usage bucket it belongs to and listNode to itself in that bucket for quick removal.\r\n* Global `io_threads_op` variable now can contain a `IO_THREADS_OP_IDLE` value indicating no io-threading is currently being executed.\r\n* In order to track memory used by each clients in real-time we can't rely on updating these stats in `clientsCron()` alone anymore. So now I call `updateClientMemUsage()` (used to be `clientsCronTrackClientsMemUsage()`) after command processing, after writing data to pubsub clients, after writing the output buffer and after reading from the socket (and maybe other places too). The function is written to be fast.\r\n* Clients are evicted if needed (with appropriate log line) in `beforeSleep()` and before processing a command (before performing oom-checks and key-eviction).\r\n* All clients memory usage buckets are grouped as follows:\r\n  * All clients using less than 64k.\r\n  * 64K..128K\r\n  * 128K..256K\r\n  * ...\r\n  * 2G..4G\r\n  * All clients using 4g and up.\r\n* Added client-eviction.tcl with a bunch of tests for the new mechanism.\r\n* Extended maxmemory.tcl to test the interaction between maxmemory and maxmemory-clients settings.\r\n* Added an option to flag a numeric configuration variable as a \"percent\", this means that if we encounter a '%' after the number in the config file (or config set command) we consider it as valid. Such a number is store internally as a negative value. This way an integer value can be interpreted as either a percent (negative) or absolute value (positive). This is useful for example if some numeric configuration can optionally be set to a percentage of something else.\r\n\r\n----------------------\r\nOrigianl PR description:\r\n\r\nSee #7676\r\n\r\n### Description\r\nWe track how much memory each client uses. The value is updated after each `processInputBuffer` and after writing to the socket. When the sum of all clients exceeds configuration we disconnect the fat clients. This is checked after each command.\r\n\r\nThis is more or less done given the current state of my discussions with @oranagra.\r\nWe need a larger forum to review this solution and decide if it's good enough. \r\n### Important/Open issues:\r\n1. @oranagra and I thought it **won't** be right to evict clients based on some rolling avg of memory usage, rather to look at the current state. This is because we evict clients until we're back under some threshold and looking at **past** memory usage while aiming to go below some **current** memory usage might cause unexpected results. Originally we thought some past avg will be needed to avoid disconnecting a bursting client which uses up a lot of memory for a short time, but in reality such a client might eventually also be disconnected and it might not be what we want if not disconnecting it will cause all other clients to disconnect as well. There's no one right solution here, but I think adding a past based rolling avg just complicates things and makes them less predictable and tougher to tune for the user.\r\n2. How does this relate to the already existing single client COB threshold. Do we still need it? Or is it redundant and should be deprecated. From the user's point of view it's easier to simply define a threshold for `maxmemory-clients` which is also enforced on the individual client level (and also includes the query buffer size) and there might be no real reason to configure the `client-output-buffer-limit`. Also need to consider how this relates to replication client limits, currently `maxmemory-clients` ignores these clients.\r\n3. Soft vs hard thresholds: the old `client-output-buffer-limit` had a timer based soft threshold. What is it really good for? Isn't this just over complicating things? If not perhaps we want something like this for `maxmemory-clients` as well? I have a feeling this is an overkill and can probably be deprecated.\r\n4. One difference between the `client-output-buffer-limit` implementation and this is that here we check and evict clients only before processing a command and before sleep but not during the command processing. We might want to change this and add the code checking the limit and evicting clients inside `_addReplyProtoToList` where `asyncCloseClientOnOutputBufferLimitReached` is called. I think this won't affect performance that much. If we're thinking this is the future replacement for `client-output-buffer-limit` then we need to do this.\r\n5. Should client no-evict flag also guard against client-output-buffer-limit protection?\r\n\r\nTODO:\r\n- [x] remove client loop in info command: https://github.com/redis/redis/pull/8687#discussion_r619832078\r\n- [x] remove big todo comment in server.h: https://github.com/redis/redis/pull/8687#discussion_r619833195\r\n- [x] Account `MULTI` command buffer size as clients used memory.\r\n- [x] There's the issue of accounting watched keys: they are kind of per-client but not really because in reality there's a list of clients per watched key. How do we handle this? Do we add **another** mechanism for limiting memory used by watched keys?\r\n\r\n### Tests to be added:\r\n- [x] Decrease maxmemory-clients in runtime causes client eviction.\r\n- [x] Only the required number of clients are evicted to achieve maxmemory-clients\r\n- [x] First larger clients are evicted and then smaller ones.\r\n- [x] Client eviction works on both large query, large args and large output buffers, and large multi buffers and watched keys list.\r\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2021-03-23T10:21:57Z",
        "closed_at": "2021-03-24T06:22:12Z",
        "merged_at": "2021-03-24T06:22:12Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-22T20:38:22Z",
        "closed_at": "2021-03-24T06:33:24Z",
        "merged_at": "2021-03-24T06:33:24Z",
        "body": "We sometimes see the crash report saying we were killed by a random\r\nprocess even in cases where the crash was spontanius in redis.\r\nfor instance, crashes found by the corrupt-dump test.\r\n\r\nIt looks like this si_pid is sometimes left uninitialized, and a good\r\nway to tell if the crash originated in redis or trigged by outside is to\r\nlook at si_code, real signal codes are always > 0, and ones generated by\r\nkill are have si_code of 0 or below.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-03-22T15:15:08Z",
        "closed_at": "2021-03-22T18:13:31Z",
        "merged_at": "2021-03-22T18:13:31Z",
        "body": "In addReplySentinelRedisInstance function, which will be called when we do sentinel masters/slaves/sentinels, there are two flags which were missing for the instance state representation, SRI_FORCE_FAILOVER and SRI_SCRIPT_KILL_SENT",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-03-22T12:57:48Z",
        "closed_at": "2021-03-24T09:33:50Z",
        "merged_at": "2021-03-24T09:33:50Z",
        "body": "the corrupt-dump-fuzzer test found a case where an access to a corrupt\r\nstream would have caused accessing to uninitialized memory.\r\nnow it'll panic instead.\r\n\r\nThe issue was that there was a stream that says it has more than 0\r\nrecords, but looking for the max ID came back empty handed.\r\n\r\np.s. when sanitize-dump-payload is used, this corruption is detected,\r\nand the RESTORE command is gracefully rejected.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2021-03-22T11:46:45Z",
        "closed_at": "2021-03-22T14:13:25Z",
        "merged_at": "2021-03-22T14:13:25Z",
        "body": "sentinelGetInstanceTypeString is the same as sentinelRedisInstanceTypeStr,\r\nand not referenced by any functions.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-03-22T07:06:01Z",
        "closed_at": "2021-03-22T08:50:39Z",
        "merged_at": "2021-03-22T08:50:39Z",
        "body": "Since redis 6.2, redis immediately tries to connect to the master, not\r\nwaiting for replication cron.\r\n\r\nin the slow freebsd CI, this test failed and master_link_status was\r\nalready \"up\" when INFO was called.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-03-21T16:26:00Z",
        "closed_at": "2021-05-11T11:27:09Z",
        "merged_at": "2021-05-11T11:27:09Z",
        "body": "Check for errors in inet_ntop and snprintf rather than ignore them and return success (with garbage output).\r\nThe check for `ip_len == 0` seems like dead code, removed.\r\n\r\n----------\r\n\r\nOld title and subject:\r\nMove ip_len validation up in anetFDToString func\r\nAvoid expensive socket operation when not necessary. If `ip` is of size `0` then `goto error` immediately instead of attempting to get peer or sock name against the file descriptor. Changing the order of this validation should be safe and have no side effects.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-21T12:24:40Z",
        "closed_at": "2021-03-22T07:18:56Z",
        "merged_at": "2021-03-22T07:18:56Z",
        "body": "Change the order of start startLoadingFile call in redis-check-rdb\r\n\r\nredis_check_rdb in some scenarios only stopLoading is called because\r\nstartLoadingFile is called too late.\r\n\r\n-----------------------------------------------------------------------------\r\nOld description:\r\n\r\nloadAppendOnlyFile when load failed, only startLoading is called.\r\nredis_check_rdb in some scenarios only stopLoading is called.\r\nAnd we will miss some module server event.\r\n\r\nThere are many `exit(1)` `stopLoading` missed. I'm not sure whether we should add it all.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-21T06:35:45Z",
        "closed_at": "2021-03-22T08:51:13Z",
        "merged_at": "2021-03-22T08:51:13Z",
        "body": "```\r\n01:11:52> Right to restore backups when fail to diskless load : LOADING Redis is loading the dataset in memory\r\n    while executing\r\n\"$replica get $slot0_key\"\r\n    (\"uplevel\" body line 61)\r\n    invoked from within\r\n\"uplevel 1 $code\"\r\n    (procedure \"test\" line 6)\r\n    invoked from within\r\n\"test \"Right to restore backups when fail to diskless load \" {\r\n```\r\nhttps://github.com/redis/redis/runs/2157560289?check_suite_focus=true",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 7,
        "changed_files": 4,
        "created_at": "2021-03-19T04:19:40Z",
        "closed_at": "2021-04-06T09:09:37Z",
        "merged_at": "2021-04-06T09:09:37Z",
        "body": "Hi: \r\n\r\nI have implement some native data types in redis-module for helping my module retrieve data easier.  And I don't need to backoff data. Thus I simply set ` moduleTypeLoadFunc rdb_load; moduleTypeSaveFunc rdb_save` to null. Everything is ok . But I forget to close AUTO RDBDUMP in redis.conf.  \r\n\r\n\r\nThen after 60 seconds, Server crash because rdb auto save. \r\n\r\nWith this fix, module data type registration will fail if the load or save callbacks are not defined, or the optional aux load and save callbacks are not either both defined or both missing.",
        "comments": 20
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-03-18T12:19:27Z",
        "closed_at": "2021-04-10T18:25:53Z",
        "merged_at": "2021-04-10T18:25:53Z",
        "body": "make install displays an error. The problem with the original code lies in the handling of the special variable $@ when using the 'make install' command.\r\nerror display:\r\n...\r\n    INSTALL install\r\n    INSTALL install\r\n    INSTALL install\r\ncorrect display:\r\n...\r\n    INSTALL redis-server\r\n    INSTALL redis-benchmark\r\n    INSTALL redis-cli",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 12,
        "changed_files": 6,
        "created_at": "2021-03-17T21:29:00Z",
        "closed_at": "2021-03-24T15:41:05Z",
        "merged_at": "2021-03-24T15:41:05Z",
        "body": "In the method checkChildrenDone(), remove usage for the obsolete `wait3()` in favor of `waitpid()`, and properly check for the exit status code.\r\n\r\nAlso there was a Valgrind warning for uninitialized variable for `exitcode` in that method, fix that by properly initializing it. ",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-17T11:29:18Z",
        "closed_at": "2021-03-17T13:39:52Z",
        "merged_at": null,
        "body": "Add missed param LADDR ip:port in CLIENT KILL help.h. It will affect redis-cli.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-17T07:59:51Z",
        "closed_at": "2021-03-17T14:40:19Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-03-17T06:24:31Z",
        "closed_at": "2021-03-17T13:45:38Z",
        "merged_at": "2021-03-17T13:45:38Z",
        "body": "Some operating systems (e.g., NetBSD and OpenBSD) have switched to using a 64-bit integer for `time_t` on all platforms. This results in currently harmless compiler warnings due to potential truncation. The two unintrusive commits fix these minor portability concerns.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-03-16T19:46:08Z",
        "closed_at": "2021-03-17T16:52:12Z",
        "merged_at": "2021-03-17T16:52:12Z",
        "body": "pcall function runs another LUA function in protected mode, this means that any error will be caught by this function and will not stop the LUA execution. The script kill mechanism uses error to stop the running script. Scripts that uses pcall can catch the error raise by the script kill mechanism, this will cause a script like this to be unkillable:\r\n```\r\nlocal f = function()\r\n\u00a0 \u00a0 \u00a0 \u00a0 while 1 do\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 redis.call('ping')\r\n\u00a0 \u00a0 \u00a0 \u00a0 end\r\nend\r\nwhile 1 do\r\n\u00a0 \u00a0 \u00a0 \u00a0 pcall(f)\r\nend\r\n```\r\nThe fix is, when we want to kill the script, we set the hook function to be invoked\u00a0after each line. This will promise that the execution will get another error before it is able to enter the pcall function again.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 80,
        "changed_files": 2,
        "created_at": "2021-03-16T15:11:48Z",
        "closed_at": "2021-03-17T10:52:24Z",
        "merged_at": "2021-03-17T10:52:24Z",
        "body": "In the long term we may want to move away from anet completely and have\r\neverything implemented natively in connection.c, instead of having an\r\nextra layer.\r\n\r\nFor now, just get rid of unused code.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-03-16T10:53:34Z",
        "closed_at": "2021-03-16T12:49:59Z",
        "merged_at": "2021-03-16T12:49:59Z",
        "body": "`lookupKeyReadOrReply` and `lookupKeyWriteOrReply` might decide to reply to the user with the given robj. This reply might be an error reply and if so addReply function is used instead of addReplyErrorObject which will cause the error reply not\r\nto be counted on stats. The fix checks the first char in the reply and if its `-` (error) it uses `addReplyErrorObject`.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-03-16T05:35:38Z",
        "closed_at": "2021-03-26T01:59:53Z",
        "merged_at": "2021-03-26T01:59:53Z",
        "body": "### Problem\r\n1) When `hz 0`, LRU_CLOCK and run_with_period function will crash due to divide by 0.\r\n```\r\nif (1000/server.hz <= LRU_CLOCK_RESOLUTION) {\r\n```\r\n```\r\n_ms_ <= 1000/server.hz\r\n```\r\n\r\n2) When hz > 1000, `1000/server.hz` will be 0, also crash.\r\n```\r\n!(server.cronloops%((_ms_)/(1000/server.hz))))\r\n```\r\n\r\n### Solution\r\nTo backward compatible, set the bounds of hz after load config.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 78,
        "deletions": 71,
        "changed_files": 1,
        "created_at": "2021-03-16T01:29:15Z",
        "closed_at": "2021-04-21T08:43:23Z",
        "merged_at": null,
        "body": "This is just a refactoring commit. Like https://github.com/redis/redis/commit/9bd212cf241154d22849ef2addd0d5a7551b3a55\r\n\r\nThis function had two modes, send and receive PSYNC command. It had\r\nto take extra arguments that are not relevant for the other. When\r\nread code, it is not clear.\r\n\r\nSo slaveTryPartialResynchronization is refactored into separate\r\nsending and receiving APIs. The APIs have only one argument, conection.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-03-15T19:57:45Z",
        "closed_at": "2021-03-16T09:20:03Z",
        "merged_at": "2021-03-16T09:20:03Z",
        "body": "`listenToPort` attempts to gracefully handle and ignore certain errors but does not store `errno` prior to logging, which in turn calls several libc functions that may overwrite `errno`.\r\n\r\nThis has been discovered due to libmusl strftime() always returning with `errno` set to `EINVAL`, which resulted with docker-library/redis#273.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 62,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2021-03-15T17:48:29Z",
        "closed_at": "2021-03-15T19:19:57Z",
        "merged_at": "2021-03-15T19:19:57Z",
        "body": "1. moduleReplicateMultiIfNeeded should use server.in_eval like\r\n   moduleHandlePropagationAfterCommandCallback\r\n2. server.in_eval could have been set to 1 and not reset back\r\n   to 0 (a lot of missed early-exits after in_eval is already 1)\r\n\r\nNote: The new assertions in processCommand cover (2) and I added\r\ntwo module tests to cover (1)\r\n\r\nImplications:\r\nIf an EVAL that failed (and thus left server.in_eval=1) runs before a module\r\ncommand that replicates, the replication stream will contain MULTI (because\r\nmoduleReplicateMultiIfNeeded used to check server.lua_caller which is NULL\r\nat this point) but not EXEC (because server.in_eval==1)\r\nThis only affects modules as module.c the only user of server.in_eval.\r\n\r\nAffects versions 6.2.0, 6.2.1",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 107,
        "deletions": 3,
        "changed_files": 6,
        "created_at": "2021-03-15T15:53:30Z",
        "closed_at": "2021-03-30T20:40:23Z",
        "merged_at": "2021-03-30T20:40:23Z",
        "body": "The 'sentinel replicas <master>' command will ignore replicas with\r\nreplica-announced set to no.\r\n\r\nThe goal of disabling the config setting replica-announced is to allow ghost\r\nreplicas. The replica is in the cluster, synchronize with its master, can be\r\npromoted to master and is not exposed to sentinel clients. This way, it is\r\nacting as a live backup or living ghost.\r\n\r\nIn addition, to prevent the replica to be promoted as master, set\r\nreplica-priority to 0.\r\n\r\nThe initial use case was the following:\r\n\r\n    we have a 3 nodes redis cluster, with 1 master and 2 replicas\r\n    we have a 3 nodes sentinel cluster\r\n    clients ask sentinel which redis is the master for writes\r\n    clients ask sentinel which redis are the replicas for reads\r\n    we have real time high load writes to the masters\r\n    we have real time high load reads from replicas\r\n    we need async, low frequency, heavy read load using complex and CPU consuming LUA scripts.\r\n    we want to ensure the async load will not impact performances on the replicas from which the LUA scripts are running\r\n    we want another replica to be replicated (#captainObvious) but we don't want it to be master nor to accept requests from normal clients\r\n    so we add replica-announced\r\n    \r\n This PR replace https://github.com/redis/redis/pull/8437",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-03-15T11:40:03Z",
        "closed_at": "2021-03-23T03:00:33Z",
        "merged_at": "2021-03-23T03:00:33Z",
        "body": "Under certain circumstance master node repl_offset may be zero, the master node just started for example. The replica will always receive 0 offset in the manual failover process so it hangs. This initializes the value to -1.\r\n\r\nrecommit code https://github.com/redis/redis/pull/8435#issuecomment-799095498\r\n\r\n@trevor211 ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 189,
        "deletions": 7,
        "changed_files": 5,
        "created_at": "2021-03-15T08:05:59Z",
        "closed_at": "2021-03-29T10:52:03Z",
        "merged_at": "2021-03-29T10:52:03Z",
        "body": "Add couple of tests for quirk situations:\r\n- half migrated slots\r\n- many simultaneous migrations.\r\n\r\n(Originally in #5270)\r\n\r\nNote that second test is not passed in 5.0 and 6.0 branch currently.\r\n6.2 passes this test due to commits 4faad81 and e966264 (I believe).",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-03-15T04:27:23Z",
        "closed_at": "2021-03-16T15:25:31Z",
        "merged_at": "2021-03-16T15:25:31Z",
        "body": "In certain scenario start_server may think it failed to start a redis server\r\nalthough it started successfully. in these cases, it'll not terminate it, and\r\nit'll remain running when the test is over.\r\n\r\nIn start_server if config doesn't have bind (the minimal.conf in introspection.tcl),\r\nit will try to bind ipv4 and ipv6. One may success while other fails. It will\r\noutput \"Could not create server TCP listening socket\".\r\nwait_server_started uses this message to check whether instance started\r\nsuccessfully. So it will consider that it failed even though redis started successfully.\r\n\r\nAdditionally, in some cases it wasn't clear to users why the server exited,\r\nsince the warning message printed to the log, could in some cases be harmless,\r\nand in some cases fatal.\r\n\r\nThis PR adds makes a clear distinction between a warning log message and\r\na fatal one, and changes the test suite to look for the fatal message.",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 53,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2021-03-14T14:41:49Z",
        "closed_at": "2021-03-22T11:25:58Z",
        "merged_at": "2021-03-22T11:25:58Z",
        "body": "Reading CoW from `/proc/<pid>/smaps` can be slow with large processes on\r\nsome platforms.\r\n\r\nThis measures the time it takes to read CoW info and limits the duty\r\ncycle of future updates to roughly 1/100.\r\n\r\nAs current_cow_size no longer represnets a current, fixed interval value\r\nthere is also a new current_cow_size_age field that provides information\r\nabout the age of the size value, in seconds.\r\n\r\nFixes #8609 ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 53,
        "deletions": 4,
        "changed_files": 5,
        "created_at": "2021-03-14T14:41:18Z",
        "closed_at": "2021-03-22T11:27:46Z",
        "merged_at": "2021-03-22T11:27:46Z",
        "body": "Fixes #8055",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-03-13T08:41:45Z",
        "closed_at": "2021-06-24T06:28:28Z",
        "merged_at": null,
        "body": "When the current node is full, if the passed offset is -1 or the length of the node, it will cause the pre or next node to fail to determine full, and eventually, the current node will be split and split invalid(nothing to split), and eventually insert into the current node, resulting in the length of the node exceeding the maximum value set by fill.\r\nThis problem does not create any bug, because the existing code does not iterate from tail to head, and the offset never has a negative value when insert.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 34,
        "deletions": 17,
        "changed_files": 6,
        "created_at": "2021-03-13T06:46:14Z",
        "closed_at": "2021-03-14T07:41:43Z",
        "merged_at": "2021-03-14T07:41:43Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-12T14:38:11Z",
        "closed_at": "2021-03-24T09:11:38Z",
        "merged_at": "2021-03-24T09:11:38Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-03-09T23:32:29Z",
        "closed_at": "2021-03-11T05:19:36Z",
        "merged_at": "2021-03-11T05:19:36Z",
        "body": "hello and auth command both have special flag `no-auth` which allows them to processed even if the client is not authenticated. During process command, REDIS should let hello command irrespective of the status of default user in the server.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 72,
        "deletions": 49,
        "changed_files": 6,
        "created_at": "2021-03-09T22:52:12Z",
        "closed_at": "2021-03-25T08:20:28Z",
        "merged_at": "2021-03-25T08:20:28Z",
        "body": "* SLOWLOG didn't record anything for blocked commands because the client\r\n  was reset and argv was already empty. there was a fix for this issue\r\n  specifically for modules, now it works for all blocked clients.\r\n* The original command argv (before being re-written) was also reset\r\n  before adding the slowlog on behalf of the blocked command.\r\n* Latency monitor is now updated regardless of the slowlog flags of the\r\n  command or its execution (their purpose is to hide sensitive info from\r\n  the slowlog, not hide the fact the latency happened).\r\n* Latency monitor now uses real_cmd rather than c->cmd (which may be\r\n  different if the command got re-written, e.g. GEOADD)\r\n\r\nChanges:\r\n* Unify shared code between slowlog insertion in call() and\r\n  updateStatsOnUnblock(), hopefully prevent future bugs from happening\r\n  due to the later being overlooked.\r\n* Reset CLIENT_PREVENT_LOGGING in resetClient rather than after command\r\n  processing.\r\n* Add a test for SLOWLOG and BLPOP\r\n\r\nNotes:\r\n- real_cmd == c->lastcmd, except inside MULTI and Lua.\r\n- blocked commands never happen in these cases (MULTI / Lua)\r\n- real_cmd == c->cmd, except for when the command is rewritten (e.g.\r\n  GEOADD)\r\n- blocked commands (currently) are never rewritten\r\n- other than the command's CLIENT_PREVENT_LOGGING, and the\r\n  execution flag CLIENT_PREVENT_LOGGING, other cases that we want to\r\n  avoid slowlog are on AOF loading (specifically CMD_CALL_SLOWLOG will\r\n  be off when executed from execCommand that runs from an AOF)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-09T21:16:48Z",
        "closed_at": "2021-03-16T10:17:15Z",
        "merged_at": "2021-03-16T10:17:15Z",
        "body": "I'm not certain this is right, so let's discuss it.\r\nThe question is if REDISMODULE_CTX_FLAGS_EVICT and REDISMODULE_CTX_FLAGS_MAXMEMORY should be set when the module is run inside a replica that doesn't do eviction.\r\n\r\none may argue that the database is under eviction (the master does eviction and sends DELs to the replica).\r\nbut on the other hand, we don't really know the master's configuration.\r\n\r\nin my eyes, the current instance doesn't do eviction, and that's all that matters.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-09T21:08:48Z",
        "closed_at": "2021-03-10T11:20:07Z",
        "merged_at": "2021-03-10T11:20:07Z",
        "body": "The implication is that OBJECT command would was not updating stat_keyspace_misses",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-03-09T21:02:58Z",
        "closed_at": "2021-03-11T05:52:28Z",
        "merged_at": "2021-03-11T05:52:28Z",
        "body": "same as DISCARD, this command (RESET) is about client state, not server state..",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-09T20:55:57Z",
        "closed_at": "2021-03-10T13:25:17Z",
        "merged_at": "2021-03-10T13:25:17Z",
        "body": "`master_sync_perc` and `loading_loaded_perc` don't have that sign, and i think the info field should be a raw floating point number (the name suggests its units).\r\n\r\nwe already have `used_memory_peak_perc` and `used_memory_dataset_perc` which do add the `%` sign, but:\r\n1) i think it was a mistake but maybe too late to fix now, and maybe not too late to fix for `current_fork_perc`\r\n2) it is more important to be consistent with the two other \"progress\" \"prec\" metrics, and not with the \"utilization\" metric.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-03-09T16:06:23Z",
        "closed_at": "2021-03-16T10:23:19Z",
        "merged_at": "2021-03-16T10:23:19Z",
        "body": "Fixes https://github.com/redis/redis/issues/7583\r\nThis commit fixes potentical server crash issue if redisAsyncConnectBind return NULL in Sentinel instance connection, this could happen if the context initialization failed or some other reasons when setting up socket connection.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-09T14:51:27Z",
        "closed_at": "2021-03-10T14:19:48Z",
        "merged_at": "2021-03-10T14:19:48Z",
        "body": "Ref: https://github.com/redis/redis-doc/pull/1529",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-03-09T09:03:08Z",
        "closed_at": "2021-03-09T10:35:42Z",
        "merged_at": "2021-03-09T10:35:42Z",
        "body": "Fixes couple of workflows to only run on redis/redis repo. It has been running for the forked repositories as well not sure if it is intended to be.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1343,
        "deletions": 106,
        "changed_files": 28,
        "created_at": "2021-03-08T23:18:11Z",
        "closed_at": "2022-01-03T00:54:48Z",
        "merged_at": "2022-01-03T00:54:47Z",
        "body": "This is a PR to support a sharded pubsub implementation based on cluster hashing.\r\n\r\nAPI Definitions can be found [here](https://github.com/redis/redis/issues/8029#issuecomment-793160824)\r\n \r\nNew commands introduced:\r\n* SPUBLISH\r\n* SSUBSCRIBE\r\n* SUNSUBSCRIBE\r\n\r\nPrefix `S` denotes `SHARD`.\r\n\r\nNew Sub commands\r\n* PUBSUB SHARDCHANNELS\r\n* PUBSUB SHARDNUMSUB\r\n\r\nNew Configs\r\n\r\n* cluster-allow-pubsubshard-when-down: Controls whether local pubsub is allowed when the cluster is down (when a majority of slots can't be served or we can't reach a majority of primaries.) Unlike key based commands, which defaults to rejecting commands, pubusub by default allows pubsub messages, since they are best effort. \r\n\r\nConsiderations made:\r\n* Slot Migration and cleanup are handled. Slot migration works by having the migrating node continue to serve the pubsub channels, and at the very end of the migration clients will be redirected to the importing node. \r\n* Local channels are allocated to the same node as keys (same hashing logic).\r\n* Client(s) will be redirected to the master node of the slot if they try to connect to a node not serving the slot.\r\n* Client(s) can connect to replica node(s) serving the slot and perform operations.\r\n* READ_ONLY flag doesn't impact pubsublocal feature. As pubsub operations are neither read/write.\r\n* Global channels and local channels share the ACL rule(s).\r\n* Client(s) are sent a \"unsubscribe <channel>\" for each channel in the slot when on slot migration after applying `SETSLOT <slot> NODE <node-id>`. Clients will need to handle this message, and choose to either reconnect to the new target OR throw an exception up to the client. (Alternative to this is to disconnect the client)\r\n* A single subscribe can only connect to channels within a single slot. This simplifies various pieces of internal logic. Client(s) can connect to channels across multi slot over separate `SUBSCRIBELOCAL` calls though.\r\n* Client(s) can connect to both local and global channels together once in the subscribe state.\r\n* Client(s) would enter pubsub mode either on local/global channels subscription.\r\n* During the pubsub mode, only certain commands are allowed i.e. pingCommand, subscribeCommand, subscribeLocalCommand, unsubscribeCommand, unsubscribeLocalCommand, psubscribeCommand, punsubscribeCommand and resetCommand.\r\n\r\nAdditional considerations:\r\n* PSUBSCRIBELOCAL (Pattern subscribe) is currently not implemented, but has no technical blockers. The motivation is that the behavior is less obvious which pattern should be associated with which slot. The decision for now what to omit it for now.\r\n\r\n- [x] APIs\r\n- [x] Slot Migration\r\n- [x] ACL\r\n- [x] Message propagation across slot\r\n- [x] Treat independently than a key\r\n- [x] Tests",
        "comments": 23
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-03-08T18:53:32Z",
        "closed_at": "2021-03-09T09:33:32Z",
        "merged_at": "2021-03-09T09:33:32Z",
        "body": "The obtained process_rss was incorrect (the OS reports pages, not\r\nbytes), resulting with many other fields getting corrupted.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 170,
        "deletions": 19,
        "changed_files": 6,
        "created_at": "2021-03-08T13:33:19Z",
        "closed_at": "2021-03-10T16:02:18Z",
        "merged_at": "2021-03-10T16:02:17Z",
        "body": "NOTE: If anyone relied on the buggy behavior before this commit (specifically \"Bug 2\") this may seem like a breaking change, which it technically is because it changes user-facing behavior but it is in fact something that needed to be fixed.\r\n\r\nBug 1:\r\nWhen a module ctx is freed moduleHandlePropagationAfterCommandCallback\r\nis called and handles propagation. We want to prevent it from propagating\r\ncommands that were not replicated by the same context. Example:\r\n1. module1.foo does: RM_Replicate(cmd1); RM_Call(cmd2); RM_Replicate(cmd3)\r\n2. RM_Replicate(cmd1) propagates MULTI and adds cmd1 to also_propagagte\r\n3. RM_Call(cmd2) create a new ctx, calls call() and destroys the ctx.\r\n4. moduleHandlePropagationAfterCommandCallback is called, calling\r\n   alsoPropagates EXEC (Note: EXEC is still not written to socket),\r\n   setting server.in_trnsaction = 0\r\n5. RM_Replicate(cmd3) is called, propagating yet another MULTI (now\r\n   we have nested MULTI calls, which is no good) and then cmd3\r\n\r\nWe must prevent RM_Call(cmd2) from resetting server.in_transaction.\r\nREDISMODULE_CTX_MULTI_EMITTED was revived for that purpose.\r\n\r\nBug 2:\r\nFix issues with nested RM_Call where some have '!' and some don't.\r\nExample:\r\n1. module1.foo does RM_Call of module2.bar without replication (i.e. no '!')\r\n2. module2.bar internally calls RM_Call of INCR with '!'\r\n3. at the end of module1.foo we call RM_ReplicateVerbatim\r\n\r\nWe want the replica/AOF to see only module1.foo and not the INCR from module2.bar\r\n\r\nIntroduced a global replication_allowed flag inside RM_Call to determine\r\nwhether we need to replicate or not (even if '!' was specified)\r\n\r\nOther changes:\r\nSplit beforePropagateMultiOrExec to beforePropagateMulti afterPropagateExec\r\njust for better readability",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-03-08T11:35:01Z",
        "closed_at": "2021-03-08T19:22:08Z",
        "merged_at": "2021-03-08T19:22:08Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-08T10:29:08Z",
        "closed_at": "2021-03-09T12:50:04Z",
        "merged_at": "2021-03-09T12:50:04Z",
        "body": "when run benchmark with multi threads(easy to reproduce), the output message cannot erase the longger output at the tail.\r\n\r\n```\r\n# ./redis-benchmark -q --threads 3\r\nPING_INLINE: 132802.12 requests per second, p50=0.343 msec                   \r\nPING_MBULK: 132802.12 requests per second, p50=0.335 msec                    \r\nSET: 132978.73 requests per second, p50=0.351 msec               358)\r\nGET: 132978.73 requests per second, p50=0.343 msec                 3)\r\nINCR: 132978.73 requests per second, p50=0.343 msec               348)\r\nLPUSH: 99700.90 requests per second, p50=0.367 msec                 )))\r\nRPUSH: 132978.73 requests per second, p50=0.351 msec              .351)\r\nLPOP: 99700.90 requests per second, p50=0.367 msec               .369)\r\nRPOP: 132978.73 requests per second, p50=0.359 msec                    \r\nSADD: 132978.73 requests per second, p50=0.351 msec                56)\r\nHSET: 99700.90 requests per second, p50=0.359 msec                  1)\r\nSPOP: 132978.73 requests per second, p50=0.343 msec                43)\r\nZADD: 132802.12 requests per second, p50=0.359 msec               365)\r\nZPOPMIN: 132978.73 requests per second, p50=0.343 msec              .359)\r\n```\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 41,
        "deletions": 54,
        "changed_files": 1,
        "created_at": "2021-03-05T15:25:42Z",
        "closed_at": "2021-03-24T09:41:31Z",
        "merged_at": null,
        "body": "In large redis clusters, cluster slots command Is very inefficient, because for each master node, we traverse all CLUSTER_SLOTS slots,  but  in fact we only have to traverse CLUSTER_SLOTS slots once to figure out  each master node handled slot range,  this PR purpose is reduce time complexity for clusterReplyMultiBulkSlots()(a 256 sharded cluster in the test, before optimize cluster slots cost 5000us+, after optimize can keep it within 200us)",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2021-03-05T00:58:37Z",
        "closed_at": "2021-03-05T17:54:34Z",
        "merged_at": "2021-03-05T17:54:34Z",
        "body": "I don't find any references of this code. So I think they are obsolete.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-03-04T20:20:15Z",
        "closed_at": "2021-03-08T10:57:28Z",
        "merged_at": "2021-03-08T10:57:28Z",
        "body": "The result of `sdscatprintf` is doubled when using argument twice.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-03-04T19:31:54Z",
        "closed_at": "2021-03-07T12:14:23Z",
        "merged_at": "2021-03-07T12:14:23Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-04T11:50:04Z",
        "closed_at": "2021-03-05T17:32:11Z",
        "merged_at": "2021-03-05T17:32:11Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-03T19:18:39Z",
        "closed_at": "2021-03-05T17:29:52Z",
        "merged_at": "2021-03-05T17:29:52Z",
        "body": "After [#7600](https://github.com/redis/redis/pull/7600) it is no longer used.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-03T19:04:05Z",
        "closed_at": "2021-03-04T11:02:24Z",
        "merged_at": "2021-03-04T11:02:24Z",
        "body": "Since the API declared (as #define) in redismodule.h and uses\r\nthe CLIENT_ID_AOF that declared in the server.h, when\r\na module will want to make use of this API, it will get a compilation\r\nerror (module doesn't include the server.h).\r\n\r\nThe API was broken by d6eb3af (failed attempt for a cleanup).\r\nRevert to the original version of RedisModule_IsAOFClient\r\nthat uses UINT64_MAX instead of CLIENT_ID_AOF",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2021-03-03T11:27:43Z",
        "closed_at": "2021-04-06T09:29:02Z",
        "merged_at": "2021-04-06T09:29:02Z",
        "body": "The actual URL has changed.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-03T00:53:29Z",
        "closed_at": "2021-03-16T14:41:45Z",
        "merged_at": "2021-03-16T14:41:45Z",
        "body": "arguements -> arguments",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2021-03-02T21:26:37Z",
        "closed_at": "2021-03-03T06:36:08Z",
        "merged_at": "2021-03-03T06:36:08Z",
        "body": "Since sentinelCheckCreateInstanceErrors function was introduced, we can use it to generalize the error check for create master instances in SENTINEL MONITOR command.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-03-02T16:15:32Z",
        "closed_at": "2021-03-03T08:08:06Z",
        "merged_at": "2021-03-03T08:08:06Z",
        "body": "This solves the problem of /dev/random and /dev/urandom open file\r\ndescriptors leaking to childs with some versions of OpenSSL.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 22,
        "changed_files": 1,
        "created_at": "2021-03-02T16:07:45Z",
        "closed_at": "2021-03-03T07:36:28Z",
        "merged_at": "2021-03-03T07:36:28Z",
        "body": "1. Global `ClientsPeakMemOutput` and `ClientsPeakMemInput` uninitialized.\r\n2. When no connected clients variables stopped being updated instead of being zeroed over time.\r\n3. index and zero index where calculated for each client instead of once per cron.\r\n\r\nEven though I think this fixes the problem I highly doubt this was ever used due to the above bugs **and suggest considering removing this feature altogether**.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 143,
        "deletions": 44,
        "changed_files": 10,
        "created_at": "2021-03-02T14:14:51Z",
        "closed_at": "2021-03-30T20:11:33Z",
        "merged_at": "2021-03-30T20:11:32Z",
        "body": "The cluster bus is established over TLS or non-TLS depending on the configuration `tls-cluster`. The client ports distributed in the cluster and sent to clients are assumed to be TLS or non-TLS also depending on `tls-cluster`.\r\n\r\nThe cluster bus is now extended to also contain the non-TLS port of clients in a TLS cluster, when available. The non-TLS port of a cluster node, when available, is sent to clients connected without TLS in responses to CLUSTER SLOTS, CLUSTER NODES, CLUSTER SLAVES and MOVED and ASK redirects, instead of the TLS port.\r\n\r\nThe user was able to override the client port by defining `cluster-announce-port`. Now `cluster-announce-tls-port` is added, so the user can define an alternative announce port for both TLS and non-TLS clients.\r\n\r\nFixes #8134",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-01T23:12:16Z",
        "closed_at": "2021-03-25T02:37:38Z",
        "merged_at": "2021-03-25T02:37:38Z",
        "body": "An active ping is indicated by node->ping_sent being non-zero. When a pong is received, node->ping_sent is reset to zero, so it is redundant to check both node->ping_sent and pong_received. \r\n\r\nThis resolves an issue when a dropped ping may prevent failure detection if the pong time was updated for some other reason. \r\n\r\nCloses #8057",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 93,
        "deletions": 43,
        "changed_files": 7,
        "created_at": "2021-03-01T22:23:51Z",
        "closed_at": "2021-03-16T05:00:30Z",
        "merged_at": "2021-03-16T05:00:30Z",
        "body": "Followup to this: https://github.com/redis/redis/pull/7360\r\n\r\nThe big LOC change is for updating the config type to be flags instead of a boolean for modifiable. I don't want to blow away the merge history, but this seems like a more future proof change so we can add more metadata in the future.\r\n\r\nThe other change is that ACL + HELLO now show up in slowlog if they don't contain sensitive information.\r\n\r\nThe three sensitive configs are masterauth, requirepass, and masteruser.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2021-03-01T21:00:37Z",
        "closed_at": "2021-03-02T16:12:11Z",
        "merged_at": "2021-03-02T16:12:11Z",
        "body": "```\r\n*** [err]: With min-slaves-to-write: master not writable with lagged slave in tests/integration/replication-4.tcl\r\nExpected 'NOREPLICAS*' to equal or match 'OK'\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 589,
        "deletions": 193,
        "changed_files": 36,
        "created_at": "2021-03-01T17:19:12Z",
        "closed_at": "2021-03-02T06:14:39Z",
        "merged_at": "2021-03-02T06:14:39Z",
        "body": "Upgrade urgency: LOW.\r\n\r\nHere is a comprehensive list of changes in this release compared to 6.2.0,\r\neach one includes the PR number that added it, so you can get more details\r\nat https://github.com/redis/redis/pull/<number>\r\n\r\nBug fixes:\r\n* Fix sanitize-dump-payload for stream with deleted records (#8568)\r\n* Prevent client-query-buffer-limit config from being set to lower than 1mb (#8557)\r\n\r\nImprovements:\r\n* Make port, tls-port and bind config options modifiable at runtime (#8510)\r\n\r\nPlatform and deployment-related changes:\r\n* Fix compilation error on non-glibc systems if jemalloc is not used (#8533)\r\n* Improved memory consumption and memory usage tracking on FreeBSD (#8545)\r\n* Fix compilation on ARM64 MacOS with jemalloc (#8458)\r\n\r\nModules:\r\n* New Module API for getting user name of a client (#8508)\r\n* Optimize RM_Call by utilizing a shared reusable client (#8516)\r\n* Fix crash running CLIENT INFO via RM_Call (#8560)\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2021-03-01T16:59:51Z",
        "closed_at": "2021-03-02T07:39:37Z",
        "merged_at": "2021-03-02T07:39:37Z",
        "body": "Fixes #8574",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 82,
        "deletions": 25,
        "changed_files": 9,
        "created_at": "2021-03-01T16:14:06Z",
        "closed_at": "2021-03-02T06:13:08Z",
        "merged_at": "2021-03-02T06:13:08Z",
        "body": "Upgrade urgency: LOW, fixes a compilation issue.\r\n\r\nBug fixes:\r\n* Fix compilation error on non-glibc systems if jemalloc is not used (#8533)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2021-03-01T16:12:56Z",
        "closed_at": "2021-03-02T06:11:52Z",
        "merged_at": "2021-03-02T06:11:52Z",
        "body": "Upgrade urgency: LOW, fixes a compilation issue.\r\n\r\nBug fixes:\r\n* Fix compilation error on non-glibc systems if jemalloc is not used (#8533)",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-03-01T07:24:11Z",
        "closed_at": "2021-03-09T01:37:22Z",
        "merged_at": null,
        "body": "like this:\r\nCONTRIBUTING:63:    e. Done **:)**\r\ndeps/hiredis/dict.c:6: * chaining. See the source code for more information... **:)**\r\ndeps/hiredis/dict.h:6: * chaining. See the source code for more information... **:)**",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 276,
        "deletions": 229,
        "changed_files": 25,
        "created_at": "2021-03-01T05:45:47Z",
        "closed_at": "2021-03-10T07:13:11Z",
        "merged_at": "2021-03-10T07:13:11Z",
        "body": "1. Add `redis-server test all` support to run all tests.\r\n2. Add redis test to daily ci.\r\n3. Add `--accurate` option to run slow tests for more iterations (so that\r\n   by default we run less cycles (shorter time, and less prints).\r\n4. Move dict benchmark to REDIS_TEST.\r\n5. fix some leaks in tests\r\n6. make quicklist tests run on a specific fill set of options rather than huge ranges\r\n7. move some prints in quicklist test outside their loops to reduce prints\r\n8. removing sds.h from dict.c since it is now used in both redis-server and\r\n   redis-cli (uses hiredis sds)",
        "comments": 20
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 16,
        "changed_files": 2,
        "created_at": "2021-03-01T03:17:24Z",
        "closed_at": "2021-03-01T10:39:32Z",
        "merged_at": null,
        "body": "* Remove container parameter\r\nThe container parameter is only set at initialization and is not used anywhere else.\r\n\r\n* Change `QUICKLIST_NODE_ENCODING_RAW` to `QUICKLIST_NODE_ENCODING_ZIPLIST`\r\nWhen encoding is `QUICKLIST_NODE_ENCODING_RAW`, the encoding of quicklist is actually ziplist.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-02-28T16:22:58Z",
        "closed_at": "2021-03-01T15:23:29Z",
        "merged_at": "2021-03-01T15:23:29Z",
        "body": "When sanitizing the stream listpack, we need to count the deleted records too.\r\notherwise the last line that checks the `next` pointer fails.\r\n\r\nAdd test to cover that state in the stream tests.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-02-26T16:01:56Z",
        "closed_at": "2021-03-11T20:28:21Z",
        "merged_at": "2021-03-11T20:28:21Z",
        "body": "Before this commit, when sentinel doesn't sucessfully get response from monitored master/replicas the info-refresh field will show current unix timestamps in milliseconds, which does not have valid meaning IMHO.. Similarly to `last-ping-sent` field, if sentinel has not get response for info command, we should reply value as 0 to indicate the reply has never been received.\r\n\r\n```\r\n127.0.0.1:26379> sentinel masters\r\n****************\r\n   25) \"info-refresh\"\r\n   26) \"1614269775017\"\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 113,
        "deletions": 44,
        "changed_files": 2,
        "created_at": "2021-02-26T13:25:55Z",
        "closed_at": "2021-03-04T13:03:49Z",
        "merged_at": "2021-03-04T13:03:49Z",
        "body": "* The `redis-cli --scan` output should honor output mode (set explicitly or implicitly), and quote key names when not in raw mode.\r\n  * Technically this is a breaking change, but it should be very minor since raw mode is by default on for non-tty output.\r\n  * It should only affect  TTY output (human users) or non-tty output if `--no-raw` is specified.\r\n\r\n* Added `--quoted-input` option to treat all arguments as potentially quoted strings.\r\n* Added `--quoted-pattern` option to accept a potentially quoted pattern.\r\n\r\nUnquoting is applied to potentially quoted input only if single or double quotes are used. \r\n\r\nFixes #8561, #8563",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2021-02-26T12:35:08Z",
        "closed_at": "2021-03-09T19:57:14Z",
        "merged_at": "2021-03-09T19:57:14Z",
        "body": "After #8216 change, since we check for CMD_WRITE instead of CMD_READONLY, no need to check for evalCommand/evalShaCommand.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-02-26T03:36:32Z",
        "closed_at": "2021-03-24T09:10:15Z",
        "merged_at": "2021-03-24T09:10:15Z",
        "body": "Similar to the `SET key value [PXAT <milliseconds-timestamp>]`, it is often necessary to set and get the absolute expire time of the key in the module. Otherwise I can only do it by this:\r\n```\r\nlong long milliseconds = expire - RedisModule_Milliseconds();  \r\nRedisModule_SetExpire(key, milliseconds);\r\n```\r\nNow i can do it by this:\r\n```\r\nRedisModule_SetAbsExpire(key, expire);\r\n```\r\n\r\nAt the same time, in the previous implementation of `RM_SetExpire`, expire is not forced to be a positive value, so it is possible to get a value that conflicts with REDISMODULE_NO_EXPIRE(-1) in `RM_GetExpire`, so I Add a check to ensure that the expire parameters in `RM_SetExpire` and `RM_SetAbsExpire` must be positive.\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-02-25T20:25:58Z",
        "closed_at": "2021-05-18T07:27:10Z",
        "merged_at": null,
        "body": "As hello command might have auth related information in it, it shouldn't be persisted into the redis-cli history for security concerns.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-02-25T15:03:26Z",
        "closed_at": "2021-03-01T06:18:14Z",
        "merged_at": "2021-03-01T06:18:14Z",
        "body": "HI, Mr.Big @oranagra \r\n\r\nIn #8508 , I wanna get client username by `redis-module` api. But it's not release. So I need to find a way....\r\nHere's what i think... Firstly call `client info`, Secondly parse the `reply` string.  Of course this approach \r\ndoes not work. Because when the `RM_CALL` is invoked. It will create a faker client.\r\n\r\nThe point is client connection is NULL, so server will crash in `connGetInfo `\r\n\r\nMy solution is set the `fd` to an illegal value(-1) to avoid crashing.  What do you think about \ud83d\ude04\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 67,
        "deletions": 78,
        "changed_files": 3,
        "created_at": "2021-02-25T14:13:31Z",
        "closed_at": "2021-03-10T14:09:43Z",
        "merged_at": "2021-03-10T14:09:43Z",
        "body": "Have a clear seperation between in and out flags\r\n\r\nOther changes:\r\n\r\ndelete dead code in RM_ZsetIncrby: if zsetAdd returned error (happens only if the result of the operation is NAN or if `score` is NAN) we return immediately so there is no way that zsetAdd succeeded and returned NAN in the out-flags\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 80,
        "changed_files": 4,
        "created_at": "2021-02-25T03:39:20Z",
        "closed_at": "2021-02-26T05:00:27Z",
        "merged_at": "2021-02-26T05:00:27Z",
        "body": "When the config infra was originally written, requirepass wasn't supported since it did a bunch of manipulation of requirepass with default user, now it doesn't. I also removed the max password length, because we only compare hashes, so the upper bound is no long strictly needed.\r\n\r\nclient_max_querybuf_len used to be an atomic, which was why it wasn't in the original conversion, but now it's just normal.\r\n\r\nNote: Note requirepass will stay in sync with ACLs, but ACLs don't stay in sync with requirepass.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-02-25T01:46:35Z",
        "closed_at": "2021-03-01T05:54:52Z",
        "merged_at": "2021-03-01T05:54:52Z",
        "body": "DB ID used to be parsed as a `long` for SWAPDB command, now make it into an `int` to be consistent with other commands that parses the DB ID argument like `SELECT, MOVE, COPY`. See https://github.com/redis/redis/pull/8085",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 53,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2021-02-24T20:40:58Z",
        "closed_at": "2021-02-25T07:24:41Z",
        "merged_at": "2021-02-25T07:24:41Z",
        "body": "* Allow explicit control of HAVE_MALLOC_SIZE when using libc.\r\n* zmalloc(0) should align with jemalloc and never return NULL, panic, or return something valgrind will consider a dangling pointer (and report false leaks).\r\n* Move alpine-libc daily to use malloc_usable_size.\r\n* Create an explicit no-malloc-usable-size daily with valgrind.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-02-24T19:50:06Z",
        "closed_at": "2021-02-28T13:28:10Z",
        "merged_at": "2021-02-28T13:28:09Z",
        "body": "```\r\n redis-cli --cluster create 127.0.0.1:6379 127.0.0.1:6379 127.0.0.1:6379\r\n >>> Performing hash slots allocation on 3 nodes...\r\n Master[0] -> Slots 0 - 5460\r\n Master[1] -> Slots 5461 - 10922\r\n Master[2] -> Slots 10923 - 16383\r\n >>> Trying to optimize slaves allocation for anti-affinity\r\n Floating point exception (core dumped)\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-02-24T18:13:12Z",
        "closed_at": "2021-02-24T22:26:17Z",
        "merged_at": "2021-02-24T22:26:16Z",
        "body": "If a test fails while a node is stopped, we intercept the signal but since it's stopped it never exits until the timeout kills it. This just executes a continue signal to all nodes to cover that case, so tests exit more quickly.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-02-24T16:23:07Z",
        "closed_at": "2021-02-25T18:08:25Z",
        "merged_at": "2021-02-25T18:08:24Z",
        "body": "Before this commit, when redis-cli interactive mode perform a reconnect, the dbnum and transaction state hasn't been reset. This will cause incoonsistencies after the reconnect is successful. Examples:\r\n\r\n127.0.0.1:6379> select 10\r\nOK\r\n127.0.0.1:6379[10]> set foo bar\r\nOK\r\n\r\n//Disconnect happens:\r\n\r\n127.0.0.1:6379[10]> set foo bar\r\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\r\nnot connected> set foo bar\r\nOK\r\n127.0.0.1:6379[10]> ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2021-02-24T12:46:28Z",
        "closed_at": "2021-02-24T14:41:50Z",
        "merged_at": "2021-02-24T14:41:50Z",
        "body": "This aligns better with other commands, specifically XADD",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-02-24T12:41:27Z",
        "closed_at": "2021-02-24T14:41:00Z",
        "merged_at": "2021-02-24T14:41:00Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-02-24T05:43:10Z",
        "closed_at": "2021-02-24T16:45:13Z",
        "merged_at": "2021-02-24T16:45:13Z",
        "body": "Use addReplyErrorObject instead of addReply to reply shared.noscripterr, and add test for it.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 58,
        "changed_files": 2,
        "created_at": "2021-02-24T02:14:31Z",
        "closed_at": "2021-03-12T06:40:35Z",
        "merged_at": "2021-03-12T06:40:35Z",
        "body": "fix [#8523](https://github.com/redis/redis/issues/8523)\r\noptimize for command `cluster slots`",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-02-23T12:32:07Z",
        "closed_at": "2021-02-23T18:28:04Z",
        "merged_at": "2021-02-23T18:28:03Z",
        "body": "server may still be LOADING the RDB when receiving the ping",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-02-23T12:16:41Z",
        "closed_at": "2021-02-24T00:55:10Z",
        "merged_at": "2021-02-24T00:55:10Z",
        "body": "In luaRedisGenericCommand check channel return is ACL_DENIED_CHANNEL not ACL_DENIED_AUTH.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 33,
        "changed_files": 7,
        "created_at": "2021-02-23T12:07:51Z",
        "closed_at": "2021-03-26T11:10:02Z",
        "merged_at": "2021-03-26T11:10:02Z",
        "body": "Add publish channel permissions check in processCommand.\r\n\r\nprocessCommand didn't check publish channel permissions, so we can\r\nqueue a publish command in a transaction. But when exec the transaction,\r\nit will fail with -NOPERM.\r\n\r\nWe also union keys/commands/channels permissions check togegher in\r\nACLCheckAllPerm. Remove pubsubCheckACLPermissionsOrReply in \r\npublishCommand/subscribeCommand/psubscribeCommand. Always \r\ncheck permissions in processCommand/execCommand/\r\nluaRedisGenericCommand.",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-02-23T11:28:02Z",
        "closed_at": "2021-02-23T15:08:50Z",
        "merged_at": "2021-02-23T15:08:49Z",
        "body": "Fixes #8531",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 18,
        "changed_files": 5,
        "created_at": "2021-02-23T09:49:19Z",
        "closed_at": "2021-02-23T10:57:46Z",
        "merged_at": "2021-02-23T10:57:46Z",
        "body": "Fixes #8530 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 396,
        "deletions": 46,
        "changed_files": 17,
        "created_at": "2021-02-22T15:21:04Z",
        "closed_at": "2021-02-22T21:22:53Z",
        "merged_at": "2021-02-22T21:22:53Z",
        "body": "Upgrade urgency: SECURITY if you use 32bit build of redis (see bellow), LOW\r\notherwise.\r\n\r\nInteger overflow on 32-bit systems (CVE-2021-21309):\r\nRedis 4.0 or newer uses a configurable limit for the maximum supported bulk\r\ninput size. By default, it is 512MB which is a safe value for all platforms.\r\nIf the limit is significantly increased, receiving a large request from a client\r\nmay trigger several integer overflow scenarios, which would result with buffer\r\noverflow and heap corruption.\r\n\r\nBug fixes:\r\n* Avoid 32-bit overflows when proto-max-bulk-len is set high (#8522)\r\n* Fix an issue where a forked process deletes the parent's pidfile (#8231)\r\n* Fix flock cluster config may cause failure to restart after kill -9 (#7674)\r\n* Avoid an out-of-bounds read in the redis-sentinel (#7443)\r\n\r\nPlatform and deployment-related changes:\r\n* Fix setproctitle related crashes. (#8150, #8088)\r\n  Caused various crashes on startup, mainly on Apple M1 chips or under\r\n  instrumentation.\r\n* Add a check for an ARM64 Linux kernel bug (#8224)\r\n  Due to the potential severity of this issue, Redis will refuse to run on\r\n  affected platforms by default.\r\n\r\nModules:\r\n* RM_ZsetRem: Delete key if empty, the bug could leave empty zset keys (#8453)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 507,
        "deletions": 64,
        "changed_files": 24,
        "created_at": "2021-02-22T14:36:31Z",
        "closed_at": "2021-02-22T23:35:37Z",
        "merged_at": "2021-02-22T23:35:37Z",
        "body": "Upgrade urgency: SECURITY if you use 32bit build of redis (see bellow), LOW\r\notherwise.\r\n\r\nInteger overflow on 32-bit systems (CVE-2021-21309):\r\nRedis 4.0 or newer uses a configurable limit for the maximum supported bulk\r\ninput size. By default, it is 512MB which is a safe value for all platforms.\r\nIf the limit is significantly increased, receiving a large request from a client\r\nmay trigger several integer overflow scenarios, which would result with buffer\r\noverflow and heap corruption.\r\n\r\nBug fixes:\r\n* Avoid 32-bit overflows when proto-max-bulk-len is set high (#8522)\r\n* Fix handling of threaded IO and CLIENT PAUSE (failover), could lead to data loss or a crash (#8520)\r\n* Fix the selection of a random element from large hash tables (#8133)\r\n* Fix broken protocol in client tracking tracking-redir-broken message (#8456)\r\n* XINFO able to access expired keys on a replica (#8436)\r\n* Fix broken protocol in redis-benchmark when used with -a or --dbnum (#8486)\r\n* Avoid assertions (on older kernels) when testing arm64 CoW bug (#8405)\r\n* CONFIG REWRITE should honor umask settings (#8371)\r\n* Fix firstkey,lastkey,step in COMMAND command for some commands (#8367)\r\n\r\nModules:\r\n* RM_ZsetRem: Delete key if empty, the bug could leave empty zset keys (#8453)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1853,
        "deletions": 653,
        "changed_files": 89,
        "created_at": "2021-02-22T13:57:55Z",
        "closed_at": "2021-02-22T21:23:58Z",
        "merged_at": "2021-02-22T21:23:58Z",
        "body": "Upgrade urgency: SECURITY if you use 32bit build of redis (see bellow), MODERATE\r\nif you used earlier versions of Redis 6.2, LOW otherwise.\r\n\r\nInteger overflow on 32-bit systems (CVE-2021-21309):\r\nRedis 4.0 or newer uses a configurable limit for the maximum supported bulk\r\ninput size. By default, it is 512MB which is a safe value for all platforms.\r\nIf the limit is significantly increased, receiving a large request from a client\r\nmay trigger several integer overflow scenarios, which would result with buffer\r\noverflow and heap corruption.\r\n\r\nHere is a comprehensive list of changes in this release compared to 6.2 RC3,\r\neach one includes the PR number that added it, so you can get more details\r\nat https://github.com/redis/redis/pull/<number>\r\n\r\nBug fixes:\r\n* Avoid 32-bit overflows when proto-max-bulk-len is set high (#8522)\r\n* Fix broken protocol in client tracking tracking-redir-broken message (#8456)\r\n* Avoid unsafe field name characters in INFO commandstats, errorstats, modules (#8492)\r\n* XINFO able to access expired keys during CLIENT PAUSE WRITE (#8436)\r\n* Fix allowed length for REPLCONF ip-address, needed due to Sentinel's support for hostnames (#8517)\r\n* Fix broken protocol in redis-benchmark when used with -a or --dbnum (#8486)\r\n* XADD counts deleted records too when considering switching to a new listpack (#8390)\r\n\r\nBug fixes that are only applicable to previous releases of Redis 6.2:\r\n* Fixes in GEOSEARCH bybox (accuracy and mismatch between width and height) (#8445)\r\n* Fix risk of OOM panic in HRANDFIELD, ZRANDMEMBER commands with huge negative count (#8429)\r\n* Fix duplicate replicas issue in Sentinel, needed due to hostname support (#8481)\r\n* Fix Sentinel configuration rewrite, an improvement of #8271 (#8480)\r\n\r\nCommand behavior changes:\r\n* SRANDMEMBER uses RESP3 array type instead of set type (#8504)\r\n* EXPIRE, EXPIREAT, SETEX, GETEX: Return error when provided expire time overflows (#8287)\r\n\r\nOther behavior changes:\r\n* Remove ACL subcommand validation if fully added command exists. (#8483)\r\n\r\nImprovements:\r\n* Optimize sorting in GEORADIUS / GEOSEARCH with COUNT (#8326)\r\n* Optimize HRANDFIELD and ZRANDMEMBER case 4 when ziplist encoded (#8444)\r\n* Optimize in-place replacement of elements in HSET, HINCRBY, LSET (#8493)\r\n* Remove redundant list to store pubsub patterns (#8472)\r\n* Add --insecure option to command line tools (#8416)\r\n\r\nInfo fields and introspection changes:\r\n* Add INFO fields to track progress of BGSAVE, AOFRW, replication (#8414)\r\n\r\nModules:\r\n* RM_ZsetRem: Delete key if empty, the bug could leave empty zset keys (#8453)\r\n* RM_HashSet: Add COUNT_ALL flag and set errno (#8446)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-02-22T10:06:41Z",
        "closed_at": "2021-02-22T13:41:32Z",
        "merged_at": "2021-02-22T13:41:32Z",
        "body": "On 32-bit systems, setting the `proto-max-bulk-len` config parameter to a high value may result with integer overflow and a subsequent heap overflow when parsing an input bulk (CVE-2021-21309).\r\n\r\nThis fix has two parts: \r\n* Set a reasonable limit to the config parameter.\r\n* Add additional checks to prevent the problem in other potential but unknown code paths.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 486,
        "deletions": 39,
        "changed_files": 6,
        "created_at": "2021-02-22T09:14:01Z",
        "closed_at": "2021-08-03T08:37:19Z",
        "merged_at": "2021-08-03T08:37:19Z",
        "body": "Add new Module APS for RESP3 responses:\r\n- RM_ReplyWithMap\r\n- RM_ReplyWithSet\r\n- RM_ReplyWithAttribute\r\n- RM_ReplySetMapLength\r\n- RM_ReplySetSetLength\r\n- RM_ReplySetAttributeLength\r\n- RM_ReplyWithBool\r\n\r\nDeprecate REDISMODULE_POSTPONED_ARRAY_LEN in favor of a generic REDISMODULE_POSTPONED_LEN\r\n\r\nImprove documentation\r\nAdd tests",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 10,
        "changed_files": 3,
        "created_at": "2021-02-22T04:32:25Z",
        "closed_at": "2021-02-22T21:05:00Z",
        "merged_at": "2021-02-22T21:05:00Z",
        "body": "clientsArePaused isn't thread safe because it has a side effect of attempting to unpause, which may cause multiple threads concurrently updating the unblocked_clients global list. This change resolves this issue by no longer postponing client for threaded reads when clients are paused and then skipping the check for client paused for threaded reads, incase one is postponed and then clients are paused. (I don't think this is strictly possible, but being defensive seems better here)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-02-22T02:17:49Z",
        "closed_at": "2021-02-22T06:45:26Z",
        "merged_at": "2021-02-22T06:45:26Z",
        "body": "When `milliseconds == LLONG_MAX / 1000`,  `milliseconds *= 1000` no overflow will occur.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 25,
        "changed_files": 4,
        "created_at": "2021-02-20T20:46:17Z",
        "closed_at": "2021-02-21T09:22:37Z",
        "merged_at": "2021-02-21T09:22:36Z",
        "body": "Originally this was limited to IPv6 address length, but effectively it\r\nhas been used for host names and now that Sentinel accepts that as well\r\nwe need to be able to store full hostnames.\r\n\r\nAlso saves a few bytes per active client.\r\n\r\nFixes #8507",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 11,
        "changed_files": 4,
        "created_at": "2021-02-19T20:31:32Z",
        "closed_at": "2021-02-28T12:11:18Z",
        "merged_at": "2021-02-28T12:11:18Z",
        "body": "A single client pointer is added in the server struct. This is\r\ninitialized by the first RM_Call() and reused for every subsequent\r\nRM_Call() except if it's already in use, which means that it's not\r\nused for (recursive) module calls to modules. For these, a new\r\n\"fake\" client is created each time.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2021-02-19T18:08:26Z",
        "closed_at": "2021-02-20T10:56:31Z",
        "merged_at": "2021-02-20T10:56:31Z",
        "body": "The `dict` field `iterators` is misleading and incorrect.  This variable is used for 1 purpose - to pause rehashing.\r\n\r\nThe current `iterators` field doesn't actually count \"iterators\".  It counts \"safe iterators\".  But - it doesn't actually count safe iterators either.  For one, it's only incremented once the safe iterator begins to iterate, not when it's created.  It's also incremented in `dictScan` to prevent rehashing (and commented to make it clear why `iterators` is being incremented during a scan).\r\n\r\nThis update renames the field as `pauserehash` and creates 2 helper macros `dictPauseRehashing(d)` and `dictResumeRehashing(d)`",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-02-19T03:11:01Z",
        "closed_at": "2021-02-19T11:01:25Z",
        "merged_at": "2021-02-19T11:01:25Z",
        "body": "I typed some logs and found that the background save process may not have finished executing setOOMScoreAdj after executing the get_oom_score_adj method, resulting in the obtained oom_score_adj being 15.\r\nIncreased the number of attempts to get_oom_score_adj, ran for half an hour without fail.\r\n\r\n![get_child_oom_score_adj](https://user-images.githubusercontent.com/965798/108452040-c462a680-72a2-11eb-8272-502d6ab4dd24.png)\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 282,
        "deletions": 56,
        "changed_files": 8,
        "created_at": "2021-02-18T20:24:09Z",
        "closed_at": "2021-03-01T14:04:44Z",
        "merged_at": "2021-03-01T14:04:44Z",
        "body": "Add ability to modify port, tls-port and bind configurations by CONFIG SET command.\r\n\r\nTo simplify the code and make it cleaner, a new structure\r\nadded, socketFds, which contains the file descriptors array and its counter,\r\nand used for TCP, TLS and Cluster sockets file descriptors.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-02-18T11:59:27Z",
        "closed_at": "2021-02-18T18:15:34Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-02-18T09:12:00Z",
        "closed_at": "2021-02-28T12:36:37Z",
        "merged_at": "2021-02-28T12:36:37Z",
        "body": "I'am developing some pure memory service base on `redis-module`. It's very hard to get client user name in redis module. \r\n\r\n`ID` is always change when upper application(my service consumer) recreate connection or use connection pool. But client user name is almost never change in upper layer application.  Add  `client name` can help module developer to do some business logic by consumer identity",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-02-17T08:22:08Z",
        "closed_at": "2021-02-17T10:30:29Z",
        "merged_at": "2021-02-17T10:30:29Z",
        "body": "Valgrind warns about `write` accessing uninitialized memory, which was the struct padding.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2021-02-16T20:00:40Z",
        "closed_at": "2021-02-22T13:00:59Z",
        "merged_at": "2021-02-22T13:00:59Z",
        "body": "SRANDMEMBER with negative count (non unique) can return the same member\r\nmultiple times, and the order of elements in the returned collection matters.\r\nFor these reasons returning a RESP3 Set type is not valid for the negative\r\ncount, but also not really valid for the positive (unique) variant either (the\r\ncommand returns an array of random picks, not a set)\r\n\r\nThis PR also contains a minor optimization for SRANDMEMBER, HRANDFIELD,\r\nand ZRANDMEMBER, to avoid the temporary dict from being rehashed while it grows.\r\n\r\nFixes https://github.com/redis/redis/issues/8503",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-02-15T14:07:39Z",
        "closed_at": "2021-02-15T15:20:03Z",
        "merged_at": "2021-02-15T15:20:03Z",
        "body": "There are two tests in other.tcl that were dependant of the sha1 package\r\nimport which meant that they didn't usually run.\r\nThe reason it was like that was that prior to the creation of DEBUG\r\nDIGEST, the test suite used to have an equivalent function, but that's\r\nno longer the case and this dependency isn't needed.\r\n\r\nThe other change is to revert config changes done by the test before the\r\ntest suite continues. can be useful if using `--host` to run multiple\r\nunits against the same server",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2021-02-15T14:03:33Z",
        "closed_at": "2021-02-16T20:19:31Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 14,
        "changed_files": 6,
        "created_at": "2021-02-15T13:51:22Z",
        "closed_at": "2021-03-10T17:11:17Z",
        "merged_at": "2021-03-10T17:11:16Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 84,
        "deletions": 16,
        "changed_files": 5,
        "created_at": "2021-02-15T10:46:38Z",
        "closed_at": "2021-02-16T11:01:14Z",
        "merged_at": "2021-02-16T11:01:14Z",
        "body": "* Avoid useless copying in LINDEX command for reply\r\n* Use stack for decoding integer-encoded values in list push (used by LMOVE)\r\n* Add ziplistReplace, in-place optimized for elements of same size (HSET, HINCRBY, LSET)\r\n\r\n----\r\n\r\nThe commits are independent optimizations. They can be merged without squashing to keep separation.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 79,
        "deletions": 5,
        "changed_files": 9,
        "created_at": "2021-02-15T09:49:26Z",
        "closed_at": "2021-02-15T15:08:53Z",
        "merged_at": "2021-02-15T15:08:53Z",
        "body": "Fixes #8489",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-02-15T07:04:57Z",
        "closed_at": "2021-02-15T10:50:23Z",
        "merged_at": "2021-02-15T10:50:23Z",
        "body": "the fix in #8461 wasn't good enough, the test is still failing\r\nhttps://github.com/redis/redis/runs/1899772314?check_suite_focus=true\r\nthis should make it timing independent and also faster in most cases",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2021-02-13T09:37:23Z",
        "closed_at": "2021-03-07T16:09:12Z",
        "merged_at": "2021-03-07T16:09:12Z",
        "body": "In `dictEncObjHash` `else` statement, the `encoding` can only be `OBJ_ENCODING_INT`.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-02-12T15:09:39Z",
        "closed_at": "2021-02-14T12:42:42Z",
        "merged_at": "2021-02-14T12:42:42Z",
        "body": "Fix the pointers to the slot hash tags in the case of prefixed commands usage (AUTH / SELECT)\r\n\r\nThis PR fixes #8477 . \r\nBottom line, it adjusts the pointers to the slot hash tags in the case of prefixed commands usage as soon as we get the 1st reply ( same like we already did for the random strings within the command )",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 19,
        "changed_files": 2,
        "created_at": "2021-02-11T19:53:43Z",
        "closed_at": "2021-02-22T13:22:25Z",
        "merged_at": "2021-02-22T13:22:25Z",
        "body": "Currently there is a validation to disallow subcommand addition to a fully added command. This is to avoid if a customer has wrongly added it. Due to this there is a breaking change across versions, due to the addition of PFDEBUG into hyperloglog category.\r\n\r\n```\r\n// Redis 6.0.5\r\n127.0.0.1:6379> acl setuser user1 +@hyperloglog +pfdebug|test\r\nOK\r\n```\r\n```\r\n// Redis 6.2\r\n127.0.0.1:6379> acl setuser user1 +@hyperloglog +pfdebug|test\r\n(error) ERR Error in ACL SETUSER modifier '+pfdebug|test': Adding a subcommand of a command already fully added is not allowed. Remove the command to start. Example: -DEBUG +DEBUG|DIGEST\r\n```\r\n\r\nHowever, we can ignore the subcommand instead of throwing the error. \r\n\r\nCurrent behaviour:\r\n```\r\n127.0.0.1:6380> ACL SETUSER newuser +PFCOUNT +PFCOUNT|DEBUG\r\n(error) ERR Error in ACL SETUSER modifier '+PFCOUNT|DEBUG': Adding a subcommand of a command already fully added is not allowed. Remove the command to start. Example: -DEBUG +DEBUG|DIGEST\r\n```\r\n\r\nProposed changes:\r\n```\r\n127.0.0.1:6379> ACL SETUSER newuser +PFCOUNT +PFCOUNT|DEBUG\r\nOK\r\n127.0.0.1:6379> ACL LIST\r\n1) \"user default on nopass ~* &* +@all\"\r\n2) \"user newuser off &* -@all +pfcount\"\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2021-02-11T11:27:58Z",
        "closed_at": "2021-02-11T13:25:02Z",
        "merged_at": "2021-02-11T13:25:02Z",
        "body": "* Don't run test script on non-Linux.\r\n* Verify that reported fds do indeed exist also in parent, to avoid\r\n  false negatives on some systems (namely CentOS).\r\n\r\nCo-authored-by: Andy Pan <panjf2000@gmail.com>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2021-02-11T08:30:50Z",
        "closed_at": "2021-02-11T09:50:48Z",
        "merged_at": "2021-02-11T09:50:48Z",
        "body": "We need to store replicas referenced by their announced address (IP or\r\naddress). Before that, if hostnames were used and the IP address\r\nchanged, duplicate entries would have been created.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-02-10T14:47:58Z",
        "closed_at": "2021-02-10T16:38:11Z",
        "merged_at": "2021-02-10T16:38:11Z",
        "body": "The `resolve-hostnames` and `announce-hostnames` parameters were not\r\nspecified correctly according to the new convention introduced by\r\n1aad55b66.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 19,
        "changed_files": 4,
        "created_at": "2021-02-09T22:53:14Z",
        "closed_at": "2021-02-10T06:59:07Z",
        "merged_at": "2021-02-10T06:59:07Z",
        "body": "The following PR removes time sensitive checks from block on background tests during leak checks.\r\nIt also an uninitialized variable on RedisModuleBlockedClient() when calling RM_BlockedClientMeasureTimeEnd() without RM_BlockedClientMeasureTimeStart()",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2021-02-09T06:40:41Z",
        "closed_at": "2021-02-11T13:27:17Z",
        "merged_at": null,
        "body": "Disable printing needless outputs on non-Linux platform, otherwise, it goes like this:\r\n![image](https://user-images.githubusercontent.com/7496278/107325359-97fd9c00-6ae4-11eb-83a1-23f604f0b924.png)\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-02-09T05:50:04Z",
        "closed_at": "2021-02-09T10:36:10Z",
        "merged_at": "2021-02-09T10:36:10Z",
        "body": "1. Rename 18-cluster-nodes-slots.tcl to 19-cluster-nodes-slots.tcl.\r\n2. Fix trival memory leak in redis-cli.c.\r\n3. Fix freeConvertedSds indentation.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 390,
        "deletions": 214,
        "changed_files": 10,
        "created_at": "2021-02-08T23:37:08Z",
        "closed_at": "2021-05-30T06:20:32Z",
        "merged_at": "2021-05-30T06:20:32Z",
        "body": "Related to #https://github.com/redis/redis/issues/8433\r\n\r\n---------------------\r\n\r\n## Part I\r\n\r\nThis commit is to always replicate time-to-live(TTL) values as absolute UNIX timestamps in milliseconds. With this commit, all mutations in Redis would start to be propagated the same way in both AOF and replication stream. No more special command rewrite/translation for AOF only.\r\n\r\nThis commit aims to mitigate two issues, as discussed here #https://github.com/redis/redis/issues/8433:\r\n\r\n1. When TTLs are replicated as relative values, TTL keys might outlive the their intended expiration deadline on replicas due to significant replication lag. Suppose in a two-node shard with one primary and one replica:\r\n- At T1, a client issues `SET K V EX 10` on the primary, setting TTL of key K to be 10 seconds.\r\n- At T2 where T2 > T1, replica received the SET command due to replication lag.\r\n- At T1+9s, 1 second before the key would get expired, this primary dies. Replica takes over.\r\n- At T2+10s, the key finally expires by the new primary.\r\n\r\nIn aggregate, the key lived for T2+10s-T1 in wall time, where T2-T1 is the replication lag between old primary and new primary. The larger the replication lag is, the more the key outlives its intended lifetime. \r\n\r\n2. TTLs are replicated as absolute values in RDB but was replicated as relative values in replication stream. As result, after full resynchronization, keys with similar TTLs could end up having very different lifespans on replicas. Suppose the following sequence:\r\n- At wall time T1, a client consecutively sets two keys A and B with the same relative TTL, say 3600 seconds(1 hour) on a primary.  E.g. `SET A a EX 3600` and `SET B b EX 3600`\r\n- Now the primary creates a RDB to full sync a replica. The RDB\u2019s boundary happens to fall between the two SETs, so A is captured in RDB with absolute expiration timestamp T1+3600, but B is not captured.\r\n- Now suppose it takes the replica 2 hours to receive this RDB and starts to replicate the data. A would be immediately expired, but B would live on for another hour.\r\n\r\nThis is a counter-intuitive experience for clients. The client set two keys to expire after 1 hour at the relatively same wall time, but one outlived another for 1 hour.\r\n\r\n---------------------\r\n\r\n## Part II\r\n\r\nIntroduced a new command `EXPIRETIME` that returns the absolute Unix timestamp of an expire:\r\n\r\n```\r\nEXPIRETIME key TIME|PTIME\r\n\r\nAvailable since 6.?\r\n\r\nTime complexity: O(1)\r\n```\r\n\r\nReturns the absolute Unix timestamp(since January 1, 1970) at which the given key will expire.\r\n\r\n**Options**\r\n\r\nThe `EXPIRETIME` command supports a set of options that modify its behavior:\r\n* TIME -- Returns the Unix timestamp in seconds. \r\n* PTIME -- Returns the Unix timestamp in milliseconds. \r\n\r\n**Return value**\r\n\r\nInteger reply: TTL in milliseconds, or a negative value in order to signal an error (see the description above).\r\n* The command returns -2 if the key does not exist.\r\n * The command returns -1 if the key exists but has no associated expire.",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 46,
        "changed_files": 5,
        "created_at": "2021-02-08T18:22:47Z",
        "closed_at": "2021-02-17T22:13:50Z",
        "merged_at": "2021-02-17T22:13:50Z",
        "body": "This [commit](https://github.com/redis/redis/commit/dfb12f06283f22c157d99830de21700a7f86c139) introduced a dictionary on the server side to efficiently handle the pub sub pattern matching. However, there is another list maintaining the same information which is redundant as well as expensive to operate on. Hence removing it.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 96,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-02-08T14:12:13Z",
        "closed_at": "2021-02-09T12:38:09Z",
        "merged_at": "2021-02-09T12:38:09Z",
        "body": "Replaces #7499",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-02-08T12:47:02Z",
        "closed_at": "2021-02-08T14:24:01Z",
        "merged_at": "2021-02-08T14:24:01Z",
        "body": "We think it's possible that on the module's blocking timeout time tracking test, the timeout is happening prior we issue the RedisModule_BlockedClientMeasureTimeStart(bc) on the background thread. If that is the case one possible solution is to increase the timeout. Increasing to 200ms to 500ms to see if nightly stops failing.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 91,
        "deletions": 36,
        "changed_files": 5,
        "created_at": "2021-02-08T11:45:46Z",
        "closed_at": "2021-02-08T15:02:47Z",
        "merged_at": "2021-02-08T15:02:47Z",
        "body": "* For consistency, use tclsh for the script as well\r\n* Ignore leaked fds that originate from grandparent process, since we\r\n  only care about fds redis-sentinel itself is responsible for\r\n* Check every test iteration to catch problems early\r\n* Some cleanups, e.g. parameterization of file name, etc.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 204,
        "deletions": 37,
        "changed_files": 2,
        "created_at": "2021-02-08T09:51:54Z",
        "closed_at": "2021-03-22T13:16:40Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 122,
        "deletions": 104,
        "changed_files": 21,
        "created_at": "2021-02-08T00:57:38Z",
        "closed_at": "2021-02-08T18:06:56Z",
        "merged_at": null,
        "body": "This [commit](https://github.com/redis/redis/commit/dfb12f06283f22c157d99830de21700a7f86c139) introduced a dictionary on the server side to efficiently handle the pub sub pattern matching. However, there is another list maintaining the same information which is redundant as well as expensive to operate on. Hence removing it.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2021-02-07T12:37:51Z",
        "closed_at": "2021-02-07T13:41:50Z",
        "merged_at": "2021-02-07T13:41:50Z",
        "body": "Github started shifting some repositoreis to use ubuntu 20.04 by default\r\ntcl8.5 is missing in these, but 8.6 exists in both 20.04 and 18.04",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-02-07T08:25:10Z",
        "closed_at": "2021-03-01T06:15:26Z",
        "merged_at": "2021-03-01T06:15:26Z",
        "body": "The `arm_thread_state64_get_pc` used later in the file is defined in mach kernel headers. Apparently they get included if you use the system malloc but not if you use jemalloc. This patch guarantees their inclusion and successful compilation.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 87,
        "deletions": 39,
        "changed_files": 3,
        "created_at": "2021-02-07T01:31:31Z",
        "closed_at": "2021-02-21T07:34:47Z",
        "merged_at": "2021-02-21T07:34:47Z",
        "body": "When redis responds with tracking-redir-broken push message (RESP3),\r\nit was responding with a broken protocol: an array of 3 elements, but only\r\npushes 2 elements.\r\n\r\nSome bugs in the test make this pass. Read the push reply\r\nwill consume an extra reply, because the reply length is 3, but there\r\nare only two elements, so the next reply will be treated as third\r\nelement. \r\n\r\nThis fix is simple, but I can't pass the test. It takes me a lot of time to figure out what's wrong with the test.\r\n\r\n```\r\n204         set res -1\r\n205         for {set i 0} {$i <= $MAX_TRIES && $res < 0} {incr i} {\r\n206             set res [lsearch -exact [r PING] \"tracking-redir-broken\"]\r\n207         }\r\n\r\n```\r\n\r\nWe should set res -1 explicitly, because res currently is `key1`, this test will always pass.\r\nIn for loop condition $res < 0, can't equal 0.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-02-05T16:42:21Z",
        "closed_at": "2021-02-05T17:51:32Z",
        "merged_at": "2021-02-05T17:51:32Z",
        "body": "Following the warnings on building the test module:\r\nhttps://github.com/redis/redis/pull/7491#issuecomment-771158966\r\n\r\nThis PR addresses it:\r\n- by defining _XOPEN_SOURCE with a value of 700 ( nanosleep only needs `_POSIX_C_SOURCE >= 199309L` ) but I believe there is arm on using a more recent macro:\r\n>  Defining _XOPEN_SOURCE with a value of 700 or greater produces\r\n          the same effects as defining _POSIX_C_SOURCE with a value of\r\n          200809L or greater.  Where one sees _POSIX_C_SOURCE >= 200809L\r\n- Removing the unused variable.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 50,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2021-02-05T13:06:27Z",
        "closed_at": "2021-02-05T17:54:02Z",
        "merged_at": "2021-02-05T17:54:02Z",
        "body": "Without this fix, RM_ZsetRem can leave empty sorted sets which are\r\nnot allowed to exist.\r\n\r\nFixes #4859.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 15,
        "changed_files": 9,
        "created_at": "2021-02-04T10:42:45Z",
        "closed_at": "2021-02-08T17:29:33Z",
        "merged_at": "2021-02-08T17:29:33Z",
        "body": "Fix typo and some comments.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 155,
        "deletions": 11,
        "changed_files": 6,
        "created_at": "2021-02-04T08:50:14Z",
        "closed_at": "2021-02-15T09:40:05Z",
        "merged_at": "2021-02-15T09:40:05Z",
        "body": "The added flag affects the return value of RM_HashSet() to include\r\nthe number of inserted fields, in addition to updated and deleted\r\nfields.\r\n\r\nErrno is set on errors, tests are added and documentation updated.\r\n\r\nFixes #6914.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 225,
        "deletions": 60,
        "changed_files": 4,
        "created_at": "2021-02-04T07:27:34Z",
        "closed_at": "2021-02-04T16:08:36Z",
        "merged_at": "2021-02-04T16:08:36Z",
        "body": "Fix errors of GEOSEARCH bybox search due to:\r\n1. projection of the box to a trapezoid (when the meter box is converted to long / lat it's no longer a box).\r\n2. width and height mismatch\r\n\r\nChanges:\r\n- New GEOSEARCH point in rectangle algorithm\r\n- Fix GEOSEARCH bybox width and height mismatch bug\r\n- Add GEOSEARCH bybox testing to the existing \"GEOADD + GEORANGE randomized test\"\r\n- Add new fuzzy test to stress test the bybox corners and edges\r\n- Add some tests for edge cases of the bybox algorithm\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 129,
        "deletions": 44,
        "changed_files": 4,
        "created_at": "2021-02-03T20:02:22Z",
        "closed_at": "2021-02-07T14:55:11Z",
        "merged_at": "2021-02-07T14:55:11Z",
        "body": "It is inefficient to repeatedly pick a single random element from a\r\nziplist.\r\nFor CASE4, which is when the user requested a low number of unique\r\nrandom picks from the collectoin, we used thta pattern.\r\n\r\nNow we use a different algorithm that picks unique elements from a\r\nziplist, and guarentee no duplicate but doesn't provide random order\r\n(which is only needed in the non-unique random picks case)\r\n\r\nUnrelated changes:\r\n* change ziplist count and indexes variables to unsigned\r\n* solve compilation warnings about uninitialized vars in gcc 10.2\r\n\r\nCo-authored-by: xinluton <xinluton@qq.com>",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 259,
        "deletions": 67,
        "changed_files": 2,
        "created_at": "2021-02-03T09:07:45Z",
        "closed_at": "2021-04-13T21:58:05Z",
        "merged_at": "2021-04-13T21:58:05Z",
        "body": "Sections are added to the modules API ref, by extracting comments from module.c starting with a level 2 markdown heading. These are rendered in the order as they appear in module.c. A TOC is added at the top of the page.\r\n\r\nAPI function names are linked to their definitions within the page. A function index is added to the bottom of the page.\r\n\r\nLinebreaks are added in long API function prototypes.",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-02-03T06:47:29Z",
        "closed_at": "2021-02-03T08:21:58Z",
        "merged_at": null,
        "body": "update help info\r\n\r\n#",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 635,
        "deletions": 22,
        "changed_files": 10,
        "created_at": "2021-02-02T12:07:46Z",
        "closed_at": "2021-09-14T14:48:06Z",
        "merged_at": "2021-09-14T14:48:06Z",
        "body": "List functions operating on elements by index:\r\n\r\n* RM_ListGet\r\n* RM_ListSet\r\n* RM_ListInsert\r\n* RM_ListDelete\r\n\r\n<del>List iterator functions: RM_ListIteratorStart, RM_ListIteratorStop, RM_ListIteratorNext, RM_ListIteratorInsert, RM_ListIteratorDelete</del>\r\n\r\nIteration is done using a simple for loop over indices. The index based functions use an internal iterator as an optimization. This is explained in the docs:\r\n\r\n```\r\n * Many of the list functions access elements by index. Since a list is in\r\n * essence a doubly-linked list, accessing elements by index is generally an\r\n * O(N) operation. However, if elements are accessed sequentially or with\r\n * indices close together, the functions are optimized to seek the index from\r\n * the previous index, rather than seeking from the ends of the list.\r\n *\r\n * This enables iteration to be done efficiently using a simple for loop:\r\n *\r\n *     long n = RM_ValueLength(key);\r\n *     for (long i = 0; i < n; i++) {\r\n *         RedisModuleString *elem = RedisModule_ListGet(key, i);\r\n *         // Do stuff...\r\n *     }\r\n```",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 19,
        "changed_files": 4,
        "created_at": "2021-02-02T11:15:07Z",
        "closed_at": "2021-02-03T15:35:29Z",
        "merged_at": "2021-02-03T15:35:29Z",
        "body": "* Add bash temporarily to allow sentinel fd leaks test to run.\r\n* Use vmactions-freebsd rdist sync to work around bind permission denied\r\n  and slow execution issues.\r\n* Upgrade to tcl8.6 to be aligned with latest Ubuntu envs.\r\n* Concat all command executions to avoid ignoring failures.\r\n* Skip intensive fuzzer on FreeBSD. For some yet unknown reason, generate_fuzzy_traffic_on_key causes TCL to significantly bloat on FreeBSD resulting with out of memory.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 91,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2021-02-02T07:44:59Z",
        "closed_at": "2021-03-15T15:53:42Z",
        "merged_at": null,
        "body": "The `sentinel replicas <master>` command will not expose replicas with a priority of -1.\r\n\r\nAs with a value of 0, the replica is marked as not able to perform the role of master.\r\n\r\nThe goal of adding a value (`-1`) to the config setting `replica-priority` is to allow _ghost replicas_. The replica is in the cluster, synchronize with its master, can never be promoted to master and is not exposed to sentinel clients. This way, it is acting as a `live backup` or `living ghost`.\r\n\r\nThe initial use case was the following:\r\n- we have a 3 nodes redis cluster, with 1 master and 2 replicas\r\n- we have a 3 nodes sentinel cluster\r\n- clients ask sentinel which redis is the master for writes\r\n- clients ask sentinel which redis are the replicas for reads\r\n- we have real time high load writes to the masters\r\n- we have real time high load reads from replicas\r\n- we need async, low frequency, heavy read load using complex and CPU consuming LUA scripts.\r\n- we want to ensure the async load will not impact performances on the replicas from which the LUA scripts are running\r\n- we want another replica to be replicated (#captainObvious) but we don't want it to be master nor to accept requests from normal clients\r\n- so we add the `-1` value to `replica-priority`\r\n\r\nthis code is running in production on a 6.0.10 cluster and we are happy with it.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-02-02T03:03:58Z",
        "closed_at": "2021-02-02T08:51:20Z",
        "merged_at": "2021-02-02T08:51:20Z",
        "body": "now we support client pause write, in case of read command read stale data during write pausing.\r\n\r\nCOPY is a write command, but the source key is just read.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-02-02T02:43:28Z",
        "closed_at": "2021-03-23T02:57:00Z",
        "merged_at": null,
        "body": "Under certain circumstance(the master node just started)\uff0cmaster node repl_offset may be zero. replica will always receive 0 offset in the manual failover process.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2021-02-01T10:58:49Z",
        "closed_at": "2021-02-02T08:54:20Z",
        "merged_at": "2021-02-02T08:54:20Z",
        "body": "In `addReplyAggregateLen` and `addReplyBulkLen`, it will check whether use static objects or not. But in `addReplyLongLongWithPrefix`, this check is already done. It is redundant.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 141,
        "deletions": 36,
        "changed_files": 7,
        "created_at": "2021-02-01T03:07:59Z",
        "closed_at": "2021-02-05T13:56:20Z",
        "merged_at": "2021-02-05T13:56:20Z",
        "body": "Changes to HRANDFIELD and ZRANDMEMBER:\r\n* Fix risk of OOM panic when client query a very big negative count (avoid allocating huge temporary buffer).\r\n* Fix uneven random distribution in HRANDFIELD with negative count (wasn't using dictGetFairRandomKey).\r\n* Add tests to check an even random distribution (HRANDFIELD, SRANDMEMBER, ZRANDMEMBER).",
        "comments": 23
    },
    {
        "merged": true,
        "additions": 6051,
        "deletions": 921,
        "changed_files": 91,
        "created_at": "2021-01-31T15:54:35Z",
        "closed_at": "2021-02-01T18:11:42Z",
        "merged_at": "2021-02-01T18:11:42Z",
        "body": "Upgrade urgency LOW: This is the third Release Candidate of Redis 6.2.\r\n\r\nHere is a comprehensive list of changes in this release compared to 6.2 RC2,\r\neach one includes the PR number that added it, so you can get more details\r\nat https://github.com/redis/redis/pull/<number>\r\n\r\nNew commands / args:\r\n* Add HRANDFIELD and ZRANDMEMBER commands (#8297)\r\n* Add FAILOVER command (#8315)\r\n* Add GETEX, GETDEL commands (#8327)\r\n* Add PXAT/EXAT arguments to SET command (#8327)\r\n* Add SYNC arg to FLUSHALL and FLUSHDB, and ASYNC/SYNC arg to SCRIPT FLUSH (#8258)\r\n\r\nSentinel:\r\n* Add hostname support to Sentinel (#8282)\r\n* Prevent file descriptors from leaking into Sentinel scripts (#8242)\r\n* Fix config file line order dependency and config rewrite sequence (#8271)\r\n\r\nNew configuration options:\r\n* Add set-proc-title config option to disable changes to the process title (#3623)\r\n* Add proc-title-template option to control what's shown in the process title (#8397)\r\n* Add lazyfree-lazy-user-flush config option to control FLUSHALL, FLUSHDB and SCRIPT FLUSH (#8258)\r\n\r\nBug fixes:\r\n* AOF: recover from last write error by turning on/off appendonly config (#8030)\r\n* Exit on fsync error when the AOF fsync policy is 'always' (#8347)\r\n* Avoid assertions (on older kernels) when testing arm64 CoW bug (#8405)\r\n* CONFIG REWRITE should honor umask settings (#8371)\r\n* Fix firstkey,lastkey,step in COMMAND command for some commands (#8367)\r\n\r\nSpecial considerations:\r\n* Fix misleading description of the save configuration directive (#8337)\r\n\r\nImprovements:\r\n* A way to get RDB file via replication without excessive replication buffers (#8303)\r\n* Optimize performance of clusterGenNodesDescription for large clusters (#8182)\r\n\r\nInfo fields and introspection changes:\r\n* SLOWLOG and LATENCY monitor include unblocking time of blocked commands (#7491)\r\n\r\nModules:\r\n* Add modules API for streams (#8288)\r\n* Add event for fork child birth and termination (#8289)\r\n* Add RM_BlockedClientMeasureTime* etc, to track background processing in commandstats (#7491)\r\n* Fix bug in v6.2, wrong value passed to the new unlink callback (#8381)\r\n* Fix bug in v6.2, modules blocked on keys unblock on commands like LPUSH (#8356)\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-01-31T07:44:45Z",
        "closed_at": "2021-01-31T10:14:37Z",
        "merged_at": "2021-01-31T10:14:37Z",
        "body": "Temporarily disabling this print until #8383 is solved and merged.\r\nI rather these error message not show up for now when people run the tests.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-01-30T18:30:57Z",
        "closed_at": "2021-01-31T10:13:45Z",
        "merged_at": "2021-01-31T10:13:45Z",
        "body": "* The corrupt dump fuzzer found a division by zero.\r\n* in some cases the random fields from the HRANDFIELD tests produced\r\n  fields with newlines and other special chars (due to \\ char), this caused\r\n  the TCL tests to see a bulk response that has a newline in it and add {}\r\n  around it, later it can think this is a nested list. in fact the `alpha` random\r\n  string generator isn't using spaces and newlines, so it should not use `\\`\r\n  either.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-01-29T21:36:39Z",
        "closed_at": "2021-01-30T09:18:59Z",
        "merged_at": "2021-01-30T09:18:59Z",
        "body": "This commit fixes sentinel  announces hostnames test error in certain linux environment. Before this commit, we only check localhost is resolved into 127.0.0.1, however in ubuntu or some other linux environments \"localhost\" will be resolved into ::1 ipv6 address first if the network stack is capable. \r\n\r\nIn my ubuntu environment default /etc/hosts file:\r\n```\r\n# The following lines are desirable for IPv6 capable hosts\r\n::1     localhost ip6-localhost ip6-loopback\r\nff02::1 ip6-allnodes\r\nff02::2 ip6-allrouters\r\n```\r\n\r\n```\r\nxxxxx@xxxxx:~/hwware/redis_unstable/redis-unstable$ getent hosts localhost\r\n::1             localhost ip6-localhost ip6-loopback\r\n```\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 18,
        "changed_files": 2,
        "created_at": "2021-01-29T08:22:39Z",
        "closed_at": "2021-12-17T06:13:13Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 53,
        "deletions": 16,
        "changed_files": 7,
        "created_at": "2021-01-29T03:32:11Z",
        "closed_at": "2021-04-01T09:45:15Z",
        "merged_at": "2021-04-01T09:45:15Z",
        "body": "For https://github.com/redis/redis/issues/8111, actually, we already have handled most `fsync` errors, but there still be some places we don't handle.\r\n\r\nIn `aof.c`, we call fsync when stop aof, and now print a log to let user know that if fail.\r\nIn `cluster.c`, we now return error, the calling function already handles these write errors.\r\nIn `redis-cli.c`, users hope to save rdb, we now print a message if fsync failed.\r\nIn `rio.c`, we now treat fsync errors like we do for write errors. \r\nIn `server.c`, we try to fsync aof file when shutdown redis, we only can print one log if fail.\r\nIn `bio.c`, if failing to fsync aof file, we will set `aof_bio_fsync_status` to error , and reject writing just like last writing aof error,  moreover also set INFO command field `aof_last_write_status` to error.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-01-28T16:35:57Z",
        "closed_at": "2021-01-28T18:49:47Z",
        "merged_at": "2021-01-28T18:49:47Z",
        "body": "* Indicate address can also be a unix socket path name.\r\n* Document the LADDR option as well.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2021-01-28T13:02:27Z",
        "closed_at": "2021-02-07T10:36:57Z",
        "merged_at": "2021-02-07T10:36:57Z",
        "body": "Disable certificate validation, making it possible to connect to servers\r\nwithout configuring full trust chain.\r\n\r\nThe use of this option is insecure and makes the connection vulnerable\r\nto man in the middle attacks.\r\n\r\nFixes #8404 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 134,
        "deletions": 80,
        "changed_files": 8,
        "created_at": "2021-01-28T10:17:50Z",
        "closed_at": "2021-02-16T14:06:52Z",
        "merged_at": "2021-02-16T14:06:52Z",
        "body": "Extend send data from child processes back to the parent to also send the current status of the BGSave (number of keys processed).\r\nThis will allow us to see the progress of the save process.",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 61,
        "deletions": 80,
        "changed_files": 11,
        "created_at": "2021-01-28T06:52:43Z",
        "closed_at": "2021-02-09T19:52:28Z",
        "merged_at": "2021-02-09T19:52:28Z",
        "body": "The main attempt is just to remove all the unnecessary allocations and deallocations during client rewrites. Hopefully moving forward it's clearer how to rewrite arguments. There are a couple of static allocations not in shared structs now:\r\n1. Tracking table invalidation table, it is a static variable\r\n2. the cluster mode migrate will create a static DEL, which uses the replaceClientVector, so would get deleted without an extra explicit incrRefcount. I could also move everything to actually be shared refcounts, but that seemed like a lot for not much value.\r\n\r\nAlso removed resetRefCount as it's no longer used, and just seems like something easy to misuse. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-01-27T22:49:57Z",
        "closed_at": "2021-03-14T06:46:26Z",
        "merged_at": "2021-03-14T06:46:26Z",
        "body": "On a replica we do accept connections, even though commands accessing the database will operate in read-only mode. But the server is still already operational and processing commands.\r\n\r\nNot sending the readiness notification means that on a HA setup where the nodes all start as replicas (with replicaof in the config) with a replica that cannot connect to the master server and which might not come back in a predictable amount of time or at all, the service supervisor will end up timing out the service and terminating it, with no option to promote it to be the main instance. This seems counter to what the readiness notification is supposed to be signaling.\r\n\r\nInstead send the readiness notification when we start accepting commands, and then send the various server status changes as that.\r\n\r\nFixes: commit 641c64ada10404356fc76c0b56a69b32c76f253c\r\nFixes: commit dfb598cf3304818e608ceb6b5d9529a293345c4a",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 68,
        "deletions": 33,
        "changed_files": 1,
        "created_at": "2021-01-27T12:59:32Z",
        "closed_at": "2021-01-28T08:33:46Z",
        "merged_at": "2021-01-28T08:33:46Z",
        "body": "At least in one case the arm64 cow kernel bug test triggers an assert, which is a problem because it cannot be ignored like cases where the bug is found.\r\n\r\nOn older systems (Linux <4.5) madvise fails because MADV_FREE is not supported. We treat these failures as an indication the system is not affected.\r\n\r\nFixes #8351, #8406",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-27T11:42:36Z",
        "closed_at": "2021-01-27T16:56:43Z",
        "merged_at": "2021-01-27T16:56:43Z",
        "body": "In activeDefragSdsListAndDict when `dict_val_type` is `DEFRAG_SDS_DICT_VAL_VOID_PTR`, it should update `de->v.val` not `ln->value`.\r\nBecause this code path will never be executed, so this bug never happened.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-01-26T18:33:10Z",
        "closed_at": "2021-01-26T22:29:38Z",
        "merged_at": null,
        "body": "instead.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2021-01-26T03:36:43Z",
        "closed_at": "2021-01-26T04:58:04Z",
        "merged_at": "2021-01-26T04:58:04Z",
        "body": "I noticed in `src/t_zset.c` that a ternary conditional evaluated to the same result in either case. The condition also has no side-effects. This pull request removes the ternary conditional, to improve readability.\r\n\r\nThis also meant that the `withscores` parameter was unused. So I removed it from the function so that the compiler wouldn't emit a warning.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 244,
        "deletions": 12,
        "changed_files": 7,
        "created_at": "2021-01-25T22:32:32Z",
        "closed_at": "2021-01-28T16:17:40Z",
        "merged_at": "2021-01-28T16:17:40Z",
        "body": "Make it possible to customize the process title, i.e. include custom\r\nstrings, immutable configuration like port/tls-port/unix socket name,\r\netc.\r\n\r\nThe default template format is backwards compatible, with the exception\r\nof Redis instances that have only Unix domain sockets (no port/tls-port)\r\nwhich will now report their listening socket.\r\n\r\nFixes #694, #2081",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2021-01-25T20:53:03Z",
        "closed_at": "2021-06-22T10:46:53Z",
        "merged_at": null,
        "body": "this is a minor change for setproctitle implementation. For the alloc/dealloc call the memory should be tracked using zmalloc library, but currently the untracked memory is very small through.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-01-25T11:33:42Z",
        "closed_at": "2021-01-26T01:45:54Z",
        "merged_at": "2021-01-26T01:45:54Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2021-01-24T14:28:24Z",
        "closed_at": "2021-01-26T15:55:33Z",
        "merged_at": "2021-01-26T15:55:33Z",
        "body": "It was confusing as to why these don't return a map type.\r\nthe reason is that order matters, so we need to make sure the client\r\nlibrary knows to respect it.\r\nAdded comments in the implementation and tests to cover it.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-24T12:38:00Z",
        "closed_at": "2021-02-22T13:08:16Z",
        "merged_at": "2021-02-22T13:08:16Z",
        "body": "If we set stream-node-max-bytes = 0, then we insert entry then delete,\r\ndo this many times, the last stream node will be very big.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 115,
        "deletions": 25,
        "changed_files": 1,
        "created_at": "2021-01-24T12:00:06Z",
        "closed_at": "2021-01-26T17:34:57Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2021-01-23T16:47:11Z",
        "closed_at": "2021-01-23T19:53:59Z",
        "merged_at": "2021-01-23T19:53:59Z",
        "body": "1. Valgrind leak in a recent change in a module api test\r\n2. Increase treshold of a RESTORE TTL test\r\n3. Change assertions to use assert_range which prints the values",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-01-23T04:35:41Z",
        "closed_at": "2021-02-08T14:52:03Z",
        "merged_at": null,
        "body": "Leftover fd leaks of #8242 :\r\n\r\n1. /dev/urandom\r\n2. socket: /tmp/dotnet-diagnostic-1195-1499-socket type=STREAM\r\n3. pipe",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-22T06:19:57Z",
        "closed_at": "2021-01-22T07:37:25Z",
        "merged_at": "2021-01-22T07:37:25Z",
        "body": "In `dbOverwrite` when unlink an entry from db, call `moduleNotifyKeyUnlink`\uff0cthe `robj` should be the old object not new object.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-01-21T16:16:47Z",
        "closed_at": "2021-01-25T05:50:41Z",
        "merged_at": null,
        "body": "**In the scenario of parsing certain strings, the parsing process will crash**\r\n\r\nCrash occurs when parsing the string `*3\\r\\n$3\\r\\nSET\\r\\n$5\\r\\nhello\\r\\n$`, the reason is that in some scenarios, `r->pos` and `r->len` will be equal, leading to crash.\r\n\r\nThe related problem code is as follows:\r\n\r\n```c\r\nstatic int processBulkItem(redisReader *r) {\r\n    redisReadTask *cur = r->task[r->ridx];\r\n    void *obj = NULL;\r\n    char *p, *s;\r\n    long long len;\r\n    unsigned long bytelen;\r\n    int success = 0;\r\n\r\n    p = r->buf+r->pos;\r\n    s = seekNewline(p,r->len-r->pos);\r\n    if (s != NULL) {\r\n        p = r->buf+r->pos;\r\n        bytelen = s-(r->buf+r->pos)+2; /* include \\r\\n */\r\n        \r\n       /* omit part of the code .... */\r\n    }\r\n\r\n    return REDIS_ERR;\r\n}\r\n```\r\n\r\n- `_len` has the risk of overflow\r\n- `len` may be 0\r\n\r\n```c\r\n/* Find pointer to \\r\\n. */\r\nstatic char *seekNewline(char *s, size_t len) {\r\n    int pos = 0;\r\n    int _len = len-1;\r\n\r\n    /* omit part of the code .... */\r\n}\r\n\r\n```\r\n\r\nhiredis related modifications: https://github.com/redis/hiredis/pull/916",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-01-21T15:29:59Z",
        "closed_at": "2021-01-21T17:55:43Z",
        "merged_at": "2021-01-21T17:55:43Z",
        "body": "The flag should be set before TLS negotiation begins to avoid a race\r\ncondition where a fork+exec before it is completed ends up leaking the\r\nfile descriptor.\r\n\r\nFixes failed tests following #8242",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-01-21T12:34:45Z",
        "closed_at": "2021-01-22T10:11:58Z",
        "merged_at": "2021-01-22T10:11:58Z",
        "body": "Use `lsof` to get more details about fd leaks, for better debugging, in case https://github.com/redis/redis/pull/8242#issuecomment-764548840",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 418,
        "deletions": 87,
        "changed_files": 5,
        "created_at": "2021-01-21T11:23:36Z",
        "closed_at": "2021-02-04T14:23:48Z",
        "merged_at": null,
        "body": "[Edit]\r\n\r\n- Fix GEOSEARCH bybox width and height mismatch\r\n- Fix errors of GEOSEARCH bybox search due to projection of the box to a\r\n  trapezoid (when the meter box is converted to long / lat it's no longer a box).\r\n- Add GEOSEARCH bybox testing to the existing \"GEOADD + GEORANGE randomized test\"\r\n- Add new fuzzy test to stress test the bybox corners and edges\r\n- Add some tests for edge cases of the bybox algorithm\r\n\r\n\r\n---\r\n\r\nThis PR add fuzzy test for `GEOSEARCH` command and fix the wrong order of widht and height parameters. As shown in the [document](https://redis.io/commands/geosearch), width is before height.\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-01-20T13:05:03Z",
        "closed_at": "2021-01-20T19:57:25Z",
        "merged_at": "2021-01-20T19:57:25Z",
        "body": "This is a regression introduced due to a new (safer) way of rewriting configuration files. In the past the file was simply overwritten (same inode), but now Redis creates a new temporary file and later renames it over the old one.\r\n\r\nThe temp file typically has `0600` permissions and a later chmod fixes that, but it needs to explicitly consider the `umask`.\r\n\r\nFixes #8369 ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2021-01-20T10:16:50Z",
        "closed_at": "2021-01-20T11:56:46Z",
        "merged_at": "2021-01-20T11:56:46Z",
        "body": "1. GEORADIUS*_RO used to have \"movablekeys\"\r\n2. XREAD and XREADGROUP used to have (1,1,1) but it's only movablekeys\r\n3. Z*STORE used to have (0,0,0) but dstkey is at (1,1,1)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2021-01-20T10:08:49Z",
        "closed_at": "2021-01-20T12:03:39Z",
        "merged_at": "2021-01-20T12:03:39Z",
        "body": "the test was misleading because the module would actually woke up on a wrong type and re-blocked, while the test name suggests the module doesn't not wake up at all on a wrong type..\r\n\r\ni changed the name of the test + added verification that indeed the module wakes up and gets re-blocked after it understand it's the wrong type",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-20T03:17:41Z",
        "closed_at": "2021-01-21T05:20:16Z",
        "merged_at": null,
        "body": "When free large string, the time to free memory is much longer than the command processing time.\r\nIn the example in #8336 , when free a 512M string, the time consumed is almost 8000 microseconds, after modification it becomes 30 microseconds.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-19T23:52:38Z",
        "closed_at": "2021-01-20T08:17:20Z",
        "merged_at": "2021-01-20T08:17:20Z",
        "body": "Move the logic of detecting OS from bash script to TCL script.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-19T23:11:10Z",
        "closed_at": "2021-01-20T07:58:03Z",
        "merged_at": "2021-01-20T07:58:03Z",
        "body": "It turns out that the `exec` only executes the `cat` command without printing the file content on stdout, thus we have to always wrap the `exec cat $file` into a `puts` to get outputs printed on stdout in TCL.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-19T23:06:02Z",
        "closed_at": "2021-01-21T01:36:59Z",
        "merged_at": "2021-01-21T01:36:59Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-01-19T04:46:17Z",
        "closed_at": "2021-01-19T09:36:22Z",
        "merged_at": "2021-01-19T09:36:21Z",
        "body": "Avoid redundant calls to `fcntl` with `O_NONBLOCK` flag set/unset if this flag has been already set/unset.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 60,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2021-01-18T21:51:25Z",
        "closed_at": "2021-01-19T16:49:27Z",
        "merged_at": "2021-01-19T16:49:27Z",
        "body": "This commit adds tests to make sure that relative and absolute expire commands\r\nare propagated as is to replicas and stop any future attempt to change that without\r\na proper discussion. see #8327 and #5171\r\n\r\nAdditionally it slightly improve the AOF test that tests the opposite (always\r\npropagating absolute times), by covering more commands, and shaving 2\r\nseconds from the test time.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 120,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2021-01-18T20:49:33Z",
        "closed_at": "2021-01-19T11:15:34Z",
        "merged_at": "2021-01-19T11:15:34Z",
        "body": "This makes it possible to implement blocking list and zset commands\r\nusing the modules API.\r\n\r\nIt is already supposed to work like this according to the Modules API\r\nreference in the documentation for RedisModule_BlockClientOnKeys():\r\n\r\n> If you block on a key of a type that has blocking operations\r\n> associated, like a list, a sorted set, a stream, and so forth,\r\n> the client may be unblocked once the relevant key is targeted\r\n> by an operation that normally unblocks the native blocking\r\n> operations for that type.\r\n\r\nThis commit also includes a test case for the reverse: A module\r\nunblocks a client blocked on BLPOP by inserting elements using\r\nRedisModule_ListPush(). This already works, but it was untested.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 46,
        "changed_files": 4,
        "created_at": "2021-01-17T15:48:28Z",
        "closed_at": "2021-01-21T09:56:08Z",
        "merged_at": "2021-01-21T09:56:08Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2021-01-17T08:55:27Z",
        "closed_at": "2021-01-28T14:25:35Z",
        "merged_at": "2021-01-28T14:25:35Z",
        "body": "Before this commit, to respect the contract with the user that on\r\nacknowledged write data is synced on disk, redis will exit for AOF\r\nwrite error, but we don't care about fsync error. So to guarantee\r\ndata safe, redis should exit  for fsync error when the AOF fsync\r\npolicy is 'always'.",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2021-01-15T23:54:23Z",
        "closed_at": "2021-01-18T09:37:06Z",
        "merged_at": "2021-01-18T09:37:06Z",
        "body": "getTimeZone with unsigned long can be negative so changing it to signed long. \r\n\r\nFixes https://github.com/redis/redis/issues/8237",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 187,
        "deletions": 180,
        "changed_files": 2,
        "created_at": "2021-01-15T15:48:06Z",
        "closed_at": "2021-01-20T09:47:07Z",
        "merged_at": "2021-01-20T09:47:07Z",
        "body": "Follow-up of #8325.\r\n\r\nFix broken formatting in `RM_Call` and `RM_CreateDataType`,\r\n`RM_SubscribeToServerEvent` (nested lists, etc. in list items).\r\n\r\nUnhide docs of `RM_LoadDataTypeFromString` and\r\n`RM_SaveDataTypeToString` by removing blank line between docs and\r\nfunction.\r\n\r\nClarification added to `RM__Assert`: Recommentation to use the\r\n`RedisModule_Assert` macro instead.\r\n\r\nVarious trivial changes (typos, backticks, etc.).\r\n\r\nRuby script:\r\n\r\n* Replace `RM_Xyz` with `RedisModule_Xyz` in docs. (RM is correct\r\n  when refering to the C code but RedisModule is correct in the\r\n  API docs.)\r\n* Automatic backquotes around C functions like `malloc()`.\r\n* Turn URLs into links. The link text is the URL itself.",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-01-15T13:05:01Z",
        "closed_at": "2021-01-21T06:08:05Z",
        "merged_at": "2021-01-21T06:08:05Z",
        "body": "lower than 55535 doesn't include 55535, but 55535 is ok for this condition.\r\nso 55535 or less is better message for error ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 21,
        "changed_files": 5,
        "created_at": "2021-01-14T12:23:24Z",
        "closed_at": "2021-01-15T08:44:47Z",
        "merged_at": null,
        "body": "This is just optimization and does not fix any bugs or performance improvements.\r\n\r\n* The return type of listAddNodeHead, listAddNodeTail, listInsertNode is `list*`, but in these three methods, the memory address of the list does not change, so the return value is meaningless.\r\n* Return `listNode*` should make more sense. The code often call listAddNodeTail, then call listLast to get the node of current added.\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 17,
        "changed_files": 1,
        "created_at": "2021-01-14T12:11:44Z",
        "closed_at": "2021-01-19T18:31:31Z",
        "merged_at": "2021-01-19T18:31:31Z",
        "body": "The line that said:\r\n\r\n> Note: you can disable saving completely by commenting out all \"save\" lines\r\n\r\nwas incorrect.\r\n\r\nAlso, note that the `save` values used in the default redis.conf file are different than the server's defaults, so this PR also addresses this and introduces a potential behavior change for some users.\r\n\r\nFixes #8318 ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2021-01-14T06:43:13Z",
        "closed_at": "2021-01-17T13:43:22Z",
        "merged_at": "2021-01-17T13:43:22Z",
        "body": "This is a follow up PR from https://github.com/redis/redis/issues/8313\r\n\r\nBefore this commit, like the linked issue, the config error `Wrong hostname or port for replica.` may bring some confusion to users if they configed duplicate known-replica or known-sentinel entry.. This commit providing a more specific error case when duplicate to avoid this confusion.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 476,
        "deletions": 110,
        "changed_files": 7,
        "created_at": "2021-01-13T21:57:30Z",
        "closed_at": "2021-01-27T17:47:27Z",
        "merged_at": "2021-01-27T17:47:27Z",
        "body": "This commit introduces two new command and two options for an existing command\r\n\r\n```\r\nGETEX <key> [PERSIST][EX seconds][PX milliseconds] [EXAT seconds-timestamp][PXAT milliseconds-timestamp]\r\n```\r\nThe getexCommand() function implements extended options and variants of the GET command. Unlike GET command this command is not read-only. Only one of the options can be used at a given time.\r\n\r\n1. PERSIST removes any TTL associated with the key.\r\n2. EX Set expiry TTL in seconds.\r\n3. PX Set expiry TTL in milliseconds.\r\n4. EXAT Same like EX instead of specifying the number of seconds representing the TTL (time to live), it takes an absolute Unix timestamp\r\n5. PXAT Same like PX instead of specifying the number of milliseconds representing the TTL (time to live), it takes an absolute Unix timestamp\r\n\r\nCommand would return either the bulk string, error or nil.\r\n\r\n```\r\nGETDEL <key>\r\n```\r\nWould delete the key after getting.\r\n\r\n```\r\nSET key value [NX] [XX] [KEEPTTL] [GET] [EX <seconds>] [PX <milliseconds>] [EXAT <seconds-timestamp>][PXAT <milliseconds-timestamp>]\r\n```\r\n\r\nTwo new options added here are EXAT and PXAT\r\n\r\nKey implementation notes\r\n\r\n- `SET` with `PX/EX/EXAT/PXAT` is always translated to `PXAT` in `AOF`. When relative time is specified (`PX/EX`), replication will always use `PX`.\r\n- `setexCommand` and `psetexCommand` would no longer need translation in `feedAppendOnlyFile` as they are modified to invoke `setGenericCommand ` with appropriate flags which will take care of correct AOF translation.\r\n- `GETEX` without any optional argument behaves like `GET`.\r\n- `GETEX` command is never propagated, It is either propagated as `PEXPIRE[AT], or PERSIST`.\r\n- `GETDEL` command is propagated as `DEL`\r\n- Combined the validation for `SET` and `GETEX` arguments. \r\n- Test cases to validate AOF/Replication propagation\r\n\r\nIssue - https://github.com/redis/redis/issues/2762",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-01-13T16:51:57Z",
        "closed_at": "2021-02-02T08:57:13Z",
        "merged_at": "2021-02-02T08:57:13Z",
        "body": "This PR provides an optimization, in terms of time, for all GEORADIUS/GEORADIUSBYMEMBER searches which utilize the default, sorted, COUNT clause. This is commonly used for nearest-neighbor (top-K points closest to a given lat/lon) searches. While the current implementation appends all matching points to the geoPoint array and performs pruning after-the-fact via a full sort and [0, count)-based for-loop, this PR sorts only the required number of elements.\r\n\r\nThis optimization provides a 5-20% improvement in runtime depending on the density of points of interest (POI) as well as the radius searched. No performance degradation has been observed.\r\n\r\n# Benchmark\r\n\r\nThe following are simple real-world benchmark results of GEORADIUS search (in microseconds). Likewise, it includes a test of this PR (replacing the call to qsort in georadiusGeneric with one to Redis' pqsort to do a partial sort of only the desired COUNT number of results.)\r\n\r\n## Member Search\r\n\r\nClosest 90 members in a 100 mile radius based on 1000 individual requests in 5 different countries (a real-world large-scale social network search.) Numbers are 50th/95th/99th percentiles.\r\n\r\nCountry | Current | pqsort\r\n------------- | ------------ | -------------\r\n1 | 5962/27154/29907 | **4968/22276/25437**\r\n2 | 7026/27555/31774 | **5815/23016/27283**\r\n3 | 6804/28140/31347 | **5608/23334/26499**\r\n4 | 2726/22584/28495 | **2575/19428/23794**\r\n5 | 4196/18290/32663 | **3492/15988/27720**\r\n\r\n## City Search\r\n\r\nClosest 10 cities in 100 miles. Format is median (average) over 1000 requests.\r\n\r\nLocation | Current | pqsort\r\n------------- | ------------ | -------------\r\nAmbler, PA | 596 (634) | **520 (551)**\r\nNew York, NY | 691 (738) | **608 (645)**\r\nSan Francisco, CA | 153 (161) | **138 (144)**\r\n\r\n## Restaurant Search\r\n\r\nClosest 25 restaurants in 5 miles. Format is median (average) over 1000 requests.\r\n\r\nLocation | Current | pqsort\r\n------------- | ------------ | -------------\r\nAmbler, PA | 114 (122) | **113 (120)**\r\nNew York, NY | 1197 (1260) | **1022 (1104)**\r\n\r\nClosest 25 restaurants in 20 miles. In this database, there are 4090 restaurants in a 25 mile radius from the Times Square lat/lon used for New York; we're just looking for the closest 25.\r\n\r\nLocation | Current | pqsort\r\n------------- | ------------ | -------------\r\nAmbler, PA | 747 (801) | **646 (694)**\r\nNew York, NY | 1853 (1959) | **1513 (1637)**\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 284,
        "deletions": 268,
        "changed_files": 2,
        "created_at": "2021-01-13T14:37:32Z",
        "closed_at": "2021-01-15T11:33:57Z",
        "merged_at": "2021-01-15T11:33:57Z",
        "body": "Fixes markdown formatting errors and some functions not showing\r\nup in the generated documentation at all.\r\n\r\nRuby script (gendoc.rb) fixes:\r\n\r\n* Modified automatic instertion of backquotes:\r\n  * Don't add backquotes around names which are already preceded by a\r\n    backquote. Fixes for example occurrences of \\`RedisModule_Reply\\*\\` which turned\r\n    into \\`\\`RedisModule_Reply\\`\\*\\` and messed up the formatting.\r\n  * Add backquotes around types such as RedisModuleString (in addition\r\n    to function names `RedisModule_[A-z()]*` and macro names\r\n    `REDISMODULE_[A-z]*`).\r\n  * Require 4 spaces indentation for disabling automatic backquotes, i.e.\r\n    code blocks. Fixes continuations of list items (indented 2 spaces).\r\n* More permissive extraction of doc comments:\r\n  * Allow doc comments starting with `/**`.\r\n  * Make space before `*` on each line optional.\r\n  * Make space after `/*` and `/**` optional (needed when appearing on\r\n    its own line).\r\n\r\nMarkdown fixes in module.c:\r\n\r\n* Fix code blocks not indented enough (4 spaces needed).\r\n* Add blank line before code blocks and lists where missing (needed).\r\n* Escape special markdown characters `_*^<>` messing up formatting.\r\n* Lists with `1)` changed to `1.` for proper markdown lists.\r\n* Remove excessive indentation which causes text to be unintentionally\r\n  rendered as code blocks.\r\n* Other minor formatting fixes.\r\n\r\nOther fixes in module.c:\r\n\r\n* Remove blank lines between doc comment and function definition. A blank\r\n  line here makes the Ruby script exclude the function in docs.\r\n\r\n**Note:** The diff is large but much of it is whitespace changes and similar. Should be very easy to review.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1522,
        "deletions": 373,
        "changed_files": 11,
        "created_at": "2021-01-13T12:28:22Z",
        "closed_at": "2021-09-15T08:10:29Z",
        "merged_at": "2021-09-15T08:10:29Z",
        "body": "Edit: the interfaces here later changed, see #10056\r\n\r\nFix #7297\r\n\r\nThe problem:\r\n\r\nToday, there is no way for a client library or app to know the key name indexes for commands such as ZUNIONSTORE/EVAL and others with \"numkeys\", since COMMAND INFO returns no useful info for them.\r\n\r\nFor cluster-aware redis clients, this requires to 'patch' the client library code specifically for each of these commands or to resolve each execution of these commands with COMMAND GETKEYS.\r\n\r\nThe solution:\r\n\r\nIntroducing key specs other than the legacy \"range\" (first,last,step)\r\n\r\nThe 8th element of the command info array, if exists, holds an array of key specs. The array may be empty, which indicates the command doesn't take any key arguments or may contain one or more key-specs, each one may leads to the discovery of 0 or more key arguments.\r\n\r\nA client library that doesn't support this key-spec feature will keep using the first,last,step and movablekeys flag which will obviously remain unchanged.\r\n\r\nA client that supports this key-specs feature needs only to look at the key-specs array. If it finds an unrecognized spec, it must resort to using COMMAND GETKEYS if it wishes to get all key name arguments, but if all it needs is one key in order to know which cluster node to use, then maybe another spec (if the command has several) can supply that, and there's no need to use GETKEYS.\r\n\r\nEach spec is an array of arguments, first one is the spec name, the second is an array of flags, and the third is an array containing details about the spec (specific meaning for each spec type)\r\nThe initial flags we support are \"read\" and \"write\" indicating if the keys that this key-spec finds are used for read or for write. clients should ignore any unfamiliar flags.\r\n\r\nIn order to easily find the positions of keys in a given array of args we introduce keys specs. There are two logical steps of key specs:\r\n1. `start_search`: Given an array of args, indicate where we should start searching for keys\r\n2. `find_keys`: Given the output of start_search and an array of args, indicate all possible indices of keys.\r\n\r\n### start_search step specs\r\n- `index`: specify an argument index explicitly\r\n  - `index`: 0 based index (1 means the first command argument)\r\n- `keyword`: specify a string to match in `argv`. We should start searching for keys just after the keyword appears.\r\n  - `keyword`: the string to search for\r\n  - `start_search`: an index from which to start the keyword search (can be negative, which means to search from the end)\r\n\r\nExamples:\r\n- `SET` has start_search of type `index` with value `1`\r\n- `XREAD` has start_search of type `keyword` with value `[\u201cSTREAMS\u201d,1]`\r\n- `MIGRATE` has start_search of type `keyword` with value `[\u201cKEYS\u201d,-2]`\r\n\r\n### find_keys step specs\r\n- `range`: specify `[count, step, limit]`.\r\n  - `lastkey`: index of the last key. relative to the index returned from begin_search. -1 indicating till the last argument, -2 one before the last\r\n  - `step`: how many args should we skip after finding a key, in order to find the next one\r\n  - `limit`: if count is -1, we use limit to stop the search by a factor. 0 and 1 mean no limit. 2 means \u00bd of the remaining args, 3 means \u2153, and so on.\r\n- \u201ckeynum\u201d: specify `[keynum_index, first_key_index, step]`.\r\n  - `keynum_index`: is relative to the return of the `start_search` spec.\r\n  - `first_key_index`: is relative to `keynum_index`.\r\n  - `step`: how many args should we skip after finding a key, in order to find the next one\r\n\r\nExamples:\r\n- `SET` has `range` of `[0,1,0]`\r\n- `MSET` has `range` of `[-1,2,0]`\r\n- `XREAD` has `range` of `[-1,1,2]`\r\n- `ZUNION` has `start_search` of type `index` with value `1` and `find_keys` of type `keynum` with value `[0,1,1]`\r\n- `AI.DAGRUN` has `start_search` of type `keyword` with value `[\u201cLOAD\u201c,1]` and `find_keys` of type `keynum` with value `[0,1,1]` (see https://oss.redislabs.com/redisai/master/commands/#aidagrun)\r\n\r\nNote: this solution is not perfect as the module writers can come up with anything, but at least we will be able to find the key args of the vast majority of commands.\r\nIf one of the above specs can\u2019t describe the key positions, the module writer can always fall back to the `getkeys-api` option.\r\n\r\nSome keys cannot be found easily (`KEYS` in `MIGRATE`: Imagine the argument for `AUTH` is the string \u201cKEYS\u201d - we will start searching in the wrong index). \r\nThe guarantee is that the specs may be incomplete (`incomplete` will be specified in the spec to denote that) but we never report false information (assuming the command syntax is correct). \r\nFor `MIGRATE` we start searching from the end - `startfrom=-1` - and if one of the keys is actually called \"keys\" we will report only a subset of all keys - hence the `incomplete` flag.\r\nSome `incomplete` specs can be completely empty (i.e. UNKNOWN begin_search) which should tell the client that COMMAND GETKEYS (or any other way to get the keys) must be used (Example: For `SORT` there is no way to describe the STORE keyword spec, as the word \"store\" can appear anywhere in the command).\r\n\r\nWe will expose these key specs in the `COMMAND` command so that clients can learn, on startup, where the keys are for all commands instead of holding hardcoded tables or use `COMMAND GETKEYS` in runtime.\r\n\r\nComments:\r\n1. Redis doesn't internally use the new specs, they are only used for COMMAND output.\r\n2. In order to support the current COMMAND INFO format (reply array indices 4, 5, 6) we created a synthetic range, called legacy_range, that, if possible, is built according to the new specs.\r\n3. Redis currently uses only getkeys_proc or the legacy_range to get the keys indices (in COMMAND GETKEYS for example).\r\n\r\n\"incomplete\" specs:\r\nthe command we have issues with are MIGRATE, STRALGO, and SORT\r\nfor MIGRATE, because the token KEYS, if exists, must be the last token, we can search in reverse. it one of the keys is actually the string \"keys\" will return just a subset of the keys (hence, it's \"incomplete\")\r\nfor SORT and STRALGO we can use this heuristic (the keys can be anywhere in the command) and therefore we added a key spec that is both \"incomplete\" and of \"unknown type\"\r\n\r\nif a client encounters an \"incomplete\" spec it means that it must find a different way (either COMMAND GETKEYS or have its own parser) to retrieve the keys.\r\nplease note that all commands, apart from the three mentioned above, have \"complete\" key specs",
        "comments": 17
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2021-01-13T08:55:36Z",
        "closed_at": "2021-01-13T13:59:12Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-13T06:46:26Z",
        "closed_at": "2021-01-13T09:38:03Z",
        "merged_at": "2021-01-13T09:38:03Z",
        "body": "ref [https://github.com/redis/redis/commit/ea930a352ca748c0fa49a1b2ee894ea92bc83b0e](https://github.com/redis/redis/commit/ea930a352ca748c0fa49a1b2ee894ea92bc83b0e)\r\n\r\noutput:\r\n```shell\r\n30450:C 13 Jan 2021 14:27:44.072 * RDB: 1052672 MB of memory used by copy-on-write\r\n```\r\n\r\nFeels like it was deleted by mistake.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2021-01-13T06:23:44Z",
        "closed_at": "2021-01-14T00:44:05Z",
        "merged_at": "2021-01-14T00:44:05Z",
        "body": "I think it is not frequent called.\r\nbut when the master has many slaves.\r\nIt is better to call just one zmalloc instead of calling zrealloc.\r\n\r\nwe can know the maximum size of dict with dictSize.\r\nand there is only 1 or 0 slave that has newAddr.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2021-01-13T04:45:44Z",
        "closed_at": "2021-02-04T10:35:22Z",
        "merged_at": null,
        "body": "Like title",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-01-13T02:39:41Z",
        "closed_at": "2021-01-13T09:58:13Z",
        "merged_at": "2021-01-13T09:58:13Z",
        "body": "1. Use lookupKeyWrite if dstKey isn't NULL,  even input keys. ref: [https://github.com/redis/redis/commit/747174388f305148b0832dd97b9754e2a64bdfef](https://github.com/redis/redis/commit/747174388f305148b0832dd97b9754e2a64bdfef)\r\n2. Use lookupKeyRead if dstKey is NULL.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 679,
        "deletions": 16,
        "changed_files": 7,
        "created_at": "2021-01-12T20:53:58Z",
        "closed_at": "2021-01-28T21:18:06Z",
        "merged_at": "2021-01-28T21:18:06Z",
        "body": "This should eliminate dataloss and reduce the \"no master\" outage period of planned failovers.\r\n\r\nShould solve this issue: #6062\r\n\r\nNew API:\r\n`FAILOVER [ABORT] [TO <HOST> <IP>] [FORCE] [TIMEOUT <timeout>] `\r\n\r\nThis command will coordinate a failover between the master and one\r\nof its replicas. The happy path contains the following steps:\r\n1) The master will initiate a client pause write, to stop replication\r\ntraffic.\r\n2) The master will periodically check if the target replica has\r\nconsumed the entire replication stream through acks. \r\n3) Once the replica has caught up, the master will itself become a replica.\r\n4) The master will send a PSYNC FAILOVER request to target replica, which\r\nif accepted will cause the replica to become the new master and start a sync.\r\n\r\nThe master will be unpaused automatically only if the PSYNC FAILOVER command\r\nis explicitly accepted or rejected, all other grey/retryable states will cause the\r\nmaster to continue to attempt to failover. FAILOVER ABORT is the only way to abort \r\na failover command, as replicaof will be disabled. This is to make this an explicit decision\r\nsince many automatic processes may already use replicaof.\r\n\r\nThe arguments [TO <host> <ip>] will designate a specific replica to failover to.\r\n\r\n`FORCE` flag indicates that even if the target replica is not caught up,\r\nfailover to it anyway. This must be specified with a timeout. (You can do\r\nsomething like timeout 1 if you want it to force failover immediately). Force\r\nwill still rollback if the target explicitly rejects the command. \r\n\r\n`TIMEOUT <timeout>` indicates how long should the primary wait for \r\na replica to sync up before aborting. If not specified, the failover\r\nwill wait forever.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 31,
        "changed_files": 1,
        "created_at": "2021-01-12T01:27:04Z",
        "closed_at": "2021-03-08T19:43:09Z",
        "merged_at": "2021-03-08T19:43:09Z",
        "body": "When a quicklist has quicklist->compress * 2 nodes, then call\r\n__quicklistCompress, all nodes will be decompressed and the middle\r\ntwo nodes will be recompressed again. This violates the fact that\r\nquicklist->compress * 2 nodes are uncompressed. It's harmless\r\nbecause when visit a node, we always try to uncompress node first.\r\nThis only happened when a quicklist has quicklist->compress * 2 + 1\r\nnodes, then delete a node. For other scenarios like insert node and\r\niterate this will not happen.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2021-01-11T20:23:14Z",
        "closed_at": "2021-01-12T07:46:25Z",
        "merged_at": "2021-01-12T07:46:25Z",
        "body": "This fixes three issues:\r\n1.  Using debug SLEEP was impacting the subsequent test, and causing it to pass reliably even though it should have failed. There was exactly 5 seconds of artificial pause (after 1000, wait 3000, wait 1000) between the debug sleep 5 and when we needed to unblock the client in the subsequent test. Now the test properly makes sure the client is unblocked, and the subsequent test is fixed.\r\n2. Minor, the client pause types were using & comparisons instead of ==, since it was previously a flag.\r\n3. Test is faster now that some of the hand wavy time is removed. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1218,
        "deletions": 286,
        "changed_files": 42,
        "created_at": "2021-01-11T19:19:13Z",
        "closed_at": "2021-01-12T14:25:38Z",
        "merged_at": "2021-01-12T14:25:38Z",
        "body": "Upgrade urgency MODERATE: several bugs with moderate impact are fixed,\r\nHere is a comprehensive list of changes in this release compared to 6.0.9.\r\n\r\nCommand behavior changes:\r\n* SWAPDB invalidates WATCHed keys (#8239)\r\n* SORT command behaves differently when used on a writable replica (#8283)\r\n* EXISTS should not alter LRU (#8016)\r\n  In Redis 5.0 and 6.0 it would have touched the LRU/LFU of the key.\r\n* OBJECT should not reveal logically expired keys (#8016)\r\n  Will now behave the same TYPE or any other non-DEBUG command.\r\n* GEORADIUS[BYMEMBER] can fail with -OOM if Redis is over the memory limit (#8107)\r\n\r\nOther behavior changes:\r\n* Sentinel: Fix missing updates to the config file after SENTINEL SET command (#8229)\r\n* CONFIG REWRITE is atomic and safer, but requires write access to the config file's folder (#7824, #8051)\r\n  This change was already present in 6.0.9, but was missing from the release notes.\r\n\r\nBug fixes with compatibility implications (bugs introduced in Redis 6.0):\r\n* Fix RDB CRC64 checksum on big-endian systems (#8270)\r\n  If you're using big-endian please consider the compatibility implications with\r\n  RESTORE, replication and persistence.\r\n* Fix wrong order of key/value in Lua's map response (#8266)\r\n  If your scripts use redis.setresp() or return a map (new in Redis 6.0), please\r\n  consider the implications.\r\n\r\nBug fixes:\r\n* Fix an issue where a forked process deletes the parent's pidfile (#8231)\r\n* Fix crashes when enabling io-threads-do-reads (#8230)\r\n* Fix a crash in redis-cli after executing cluster backup (#8267)\r\n* Handle output buffer limits for module blocked clients (#8141)\r\n  Could result in a module sending reply to a blocked client to go beyond the limit.\r\n* Fix setproctitle related crashes. (#8150, #8088)\r\n  Caused various crashes on startup, mainly on Apple M1 chips or under instrumentation.\r\n* Backup/restore cluster mode keys to slots map for repl-diskless-load=swapdb (#8108)\r\n  In cluster mode with repl-diskless-load, when loading failed, slot map wouldn't\r\n  have been restored.\r\n* Fix oom-score-adj-values range, and bug when used in config file (#8046)\r\n  Enabling setting this in the config file in a line after enabling it, would\r\n  have been buggy.\r\n* Reset average ttl when empty databases (#8106)\r\n  Just causing misleading metric in INFO\r\n* Disable rehash when Redis has child process (#8007)\r\n  This could have caused excessive CoW during BGSAVE, replication or AOFRW.\r\n* Further improved ACL algorithm for picking categories (#7966)\r\n  Output of ACL GETUSER is now more similar to the one provided by ACL SETUSER.\r\n* Fix bug with module GIL being released prematurely (#8061)\r\n  Could in theory (and rarely) cause multi-threaded modules to corrupt memory.\r\n* Reduce effect of client tracking causing feedback loop in key eviction (#8100)\r\n* Fix cluster access to unaligned memory (SIGBUS on old ARM) (#7958)\r\n* Fix saving of strings larger than 2GB into RDB files (#8306)\r\n\r\nAdditional improvements:\r\n* Avoid wasteful transient memory allocation in certain cases (#8286, #5954)\r\n\r\nPlatform / toolchain support related improvements:\r\n* Fix crash log registers output on ARM. (#8020)\r\n* Add a check for an ARM64 Linux kernel bug (#8224)\r\n  Due to the potential severity of this issue, Redis will print log warning on startup.\r\n* Raspberry build fix. (#8095)\r\n\r\nNew configuration options:\r\n* oom-score-adj-values config can now take absolute values (besides relative ones) (#8046)\r\n\r\nModule related fixes:\r\n* Moved RMAPI_FUNC_SUPPORTED so that it's usable (#8037)\r\n* Improve timer accuracy (#7987)\r\n* Allow '\\0' inside of result of RM_CreateStringPrintf (#6260)\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2021-01-11T18:44:13Z",
        "closed_at": "2021-01-12T07:41:57Z",
        "merged_at": "2021-01-12T07:41:57Z",
        "body": "The test was trying to wait for the replica to start loading the rdb\r\nfrom the master before it kills the master, but it was actually waiting\r\nfor ROLE to be in \"sync\" mode, which corresponds to REPL_STATE_TRANSFER\r\nthat starts before the actual loading starts.\r\nnow instead it waits for the loading flag to be set.\r\n\r\nBesides, the test was dependent on the previous configuration of the\r\nservers, relying on the fact the replica is configured to persist\r\n(either RDB of AOF), now it is set explicitly.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 18,
        "changed_files": 1,
        "created_at": "2021-01-11T12:05:14Z",
        "closed_at": "2021-06-24T06:28:38Z",
        "merged_at": null,
        "body": "1. zrange_result_handler add store variable, simplifies the use of zrangeGenericCommand method\u3002\r\n2. Delete the withscores parameter of the genericZrangebyrankCommand, genericZrangebyscoreCommand, and genericZrangebylexCommand methods to avoid confusion withscores and store.\r\n3. Remove the space after zrangeGenericCommand  function.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-11T05:53:55Z",
        "closed_at": "2021-01-12T06:22:53Z",
        "merged_at": "2021-01-12T06:22:53Z",
        "body": "Fixes #8299 \r\n\r\nJust change the rdbWriteRaw return value type from `int` to `ssize_t`.\r\n\r\nThis function is called about 20 places. Most of caller's input parameter `len` is in the range of int, so this change will just act on the function `rdbSaveLzfBlob` and function `rdbSaveRawString` when a string(>2GB) need to be written to rdb.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 5135,
        "deletions": 1685,
        "changed_files": 89,
        "created_at": "2021-01-10T20:06:10Z",
        "closed_at": "2021-01-12T14:21:04Z",
        "merged_at": "2021-01-12T14:21:04Z",
        "body": "@redis/core-team please approve.\r\nThis is basically what's in unstable with additional release notes.\r\n\r\n--------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nUpgrade urgency LOW: This is the second Release Candidate of Redis 6.2.\r\n\r\nIMPORTANT: If you're running Redis on ARM64 or a big-endian system, upgrade may\r\nhave significant implications. Please be sure to read the notes below.\r\n\r\nHere is a comprehensive list of changes in this release compared to 6.2 RC1,\r\neach one includes the PR number that added it, so you can get more details\r\nat https://github.com/redis/redis/pull/<number>\r\n\r\nNew commands / args:\r\n* Add the REV, BYLEX and BYSCORE arguments to ZRANGE, and the ZRANGESTORE command (#7844)\r\n* Add the XAUTOCLAIM command (#7973)\r\n* Add the MINID trimming strategy and the LIMIT argument to XADD and XTRIM (#8169)\r\n* Add the ANY argument to GEOSEARCH and GEORADIUS (#8259)\r\n* Add the CH, NX, XX arguments to GEOADD (#8227)\r\n* Add the COUNT argument to LPOP and RPOP (#8179)\r\n* Add the WRITE argument to CLIENT PAUSE for pausing write commands exclusively (#8170)\r\n* Change the proto-ver argument of HELLO to optional (#7377)\r\n* Add the CLIENT TRACKINGINFO subcommand (#7309)\r\n\r\nCommand behavior changes:\r\n* CLIENT TRACKING yields an error when given overlapping BCAST prefixes (#8176)\r\n* SWAPDB invalidates WATCHed keys (#8239)\r\n* SORT command behaves differently when used on a writable replica (#8283)\r\n\r\nOther behavior changes:\r\n* Avoid propagating MULTI/EXEC for read-only transactions (#8216)\r\n* Remove the read-only flag from TIME, ECHO, ROLE, LASTSAVE (#8216)\r\n* Fix the command flags of PFDEBUG (#8222)\r\n* Tracking clients will no longer receive unnecessary key invalidation messages after FLUSHDB (#8039)\r\n* Sentinel: Fix missing updates to the config file after SENTINEL SET command (#8229)\r\n\r\nBug fixes with compatibility implications (bugs introduced in Redis 6.0):\r\n* Fix RDB CRC64 checksum on big-endian systems (#8270)\r\n  If you're using big-endian please consider the compatibility implications with\r\n  RESTORE, replication and persistence.\r\n* Fix wrong order of key/value in Lua's map response (#8266)\r\n  If your scripts use redis.setresp() or return a map (new in Redis 6.0), please\r\n  consider the implications.\r\n* Fix saving of strings larger than 2GB into RDB files (#8306)\r\n\r\nBug fixes that are only applicable to previous releases of Redis 6.2:\r\n* Resolve rare assertions in active defragmentation while loading (#8284, #8281)\r\n\r\nBug fixes:\r\n* Fix the selection of a random element from large hash tables (#8133)\r\n* Fix an issue where a forked process deletes the parent's pidfile (#8231)\r\n* Fix crashes when enabling io-threads-do-reads (#8230)\r\n* Fix a crash in redis-cli after executing cluster backup (#8267)\r\n* Fix redis-benchmark to use an IP address for the first cluster node (#8154)\r\n\r\nAdditional improvements:\r\n* Improve replication handshake time (#8214)\r\n* Release client tracking table memory asynchronously in cases where the DB is also freed asynchronously (#8039)\r\n* Avoid wasteful transient memory allocation in certain cases (#8286, #5954)\r\n* Handle binary string values by the 'requirepass' and 'masterauth' configs (#8200)\r\n\r\nPlatform and deployment-related changes:\r\n* Install redis-check-rdb and redis-check-aof as symlinks to redis-server (#5745)\r\n* Add a check for an ARM64 Linux kernel bug (#8224)\r\n  Due to the potential severity of this issue, Redis will refuse to run on\r\n  affected platforms by default.\r\n\r\nInfo fields and introspection changes:\r\n* Add the errorstats section to the INFO command (#8217)\r\n* Add the failed_calls and rejected_calls fields INFO's commandstats section (#8217)\r\n* Report child copy-on-write metrics continuously (#8264)\r\n\r\nModule API changes:\r\n* Add the RedisModule_SendChildCOWInfo API (#8264)\r\n* Add the may-replicate command flag (#8170)",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 36,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2021-01-10T15:42:54Z",
        "closed_at": "2021-09-12T11:42:10Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2021-01-10T13:27:06Z",
        "closed_at": "2021-01-27T15:13:11Z",
        "merged_at": "2021-01-27T15:13:11Z",
        "body": "In some scenarios, such as remote backup, we only want to get remote\r\nredis server db snapshot. Currently, redis-cli acts as a replica and\r\nsends SYNC to redis, but redis still accumulates replication buffer\r\nin the replica client output buffer, that may result in using vast\r\nmemory, or failing to transfer RDB because of client-output-buffer-limit.\r\nIn this commit, we add 'replconf rdb-only 0|1', redis doesn't send\r\nincremental replication buffer to them if they send 'replconf rdb-only 1',\r\nso we can reduce used memory and improve success of getting RDB.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 27,
        "changed_files": 4,
        "created_at": "2021-01-08T18:49:41Z",
        "closed_at": "2021-01-08T21:35:31Z",
        "merged_at": "2021-01-08T21:35:31Z",
        "body": "- the last COW report wasn't always read from the pipe\r\n  (receiveLastChildInfo wasn't used)\r\n- but in fact, there's no reason we won't always try to drain that pipe\r\n  so i'm unifying receiveLastChildInfo with receiveChildInfo\r\n- adjust threshold of the COW test when run in accurate mode\r\n- add some prints in case this test fails again\r\n- fix indentation, page size, and PID! in MacOS proc info\r\n\r\np.s. it seems that pri_pages_dirtied is always 0",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1002,
        "deletions": 23,
        "changed_files": 11,
        "created_at": "2021-01-07T12:40:49Z",
        "closed_at": "2021-01-29T08:47:29Z",
        "merged_at": "2021-01-29T08:47:29Z",
        "body": "New commands:\r\n`HRANDFIELD [<count> [WITHVALUES]]`\r\n`ZRANDMEMBER [<count> [WITHSCORES]]`\r\nAlgorithms are similar to the one in SRANDMEMBER.\r\n\r\nBoth return a simple bulk response when no arguments are given, and an array otherwise.\r\nIn case values/scores are requested, RESP2 returns a long array, and RESP3 a nested array.\r\nnote: in all 3 commands, the only option that also provides random order is the one with negative count.\r\n\r\nChanges to SRANDMEMBER\r\n* Optimization when count is 1, we can use the more efficient algorithm of non-unique random\r\n* optimization: work with sds strings rather than robj\r\n\r\nOther changes:\r\n* zzlGetScore: when zset needs to convert string to double, we use safer memcpy (in\r\n  case the buffer is too small)\r\n* Solve a \"bug\" in SRANDMEMBER test: it intended to test a positive count (case 3 or\r\n  case 4) and by accident used a negative count",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2021-01-07T06:59:58Z",
        "closed_at": "2021-01-09T18:24:59Z",
        "merged_at": "2021-01-09T18:24:59Z",
        "body": "I'm not sure this change is necessary since the clusterAddNode return value is unused anywhere.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-01-06T17:17:52Z",
        "closed_at": "2021-01-06T19:20:53Z",
        "merged_at": "2021-01-06T19:20:53Z",
        "body": " a potential memory leaking was found in processInlineBuffer, valgrind message:\r\n\r\n==10359== 32 (16 direct, 16 indirect) bytes in 1 blocks are definitely lost in loss record 401 of 669\r\n==10359==    at 0x4C31D2F: realloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n==10359==    by 0x14FEF2: zrealloc (zmalloc.c:158)\r\n==10359==    by 0x14FA4A: sdssplitargs (sds.c:1049)\r\n==10359==    by 0x15A831: processInlineBuffer (networking.c:1578)\r\n==10359==    by 0x15B71A: processInputBuffer (networking.c:1908)\r\n==10359==    by 0x15BC39: readQueryFromClient (networking.c:2026)\r\n==10359==    by 0x203464: callHandler (connhelpers.h:79)\r\n==10359==    by 0x203B0D: connSocketEventHandler (connection.c:296)\r\n==10359==    by 0x13DC8E: aeProcessEvents (ae.c:479)\r\n==10359==    by 0x13DEBC: aeMain (ae.c:539)\r\n==10359==    by 0x14D009: main (server.c:5349)\r\n\r\ncode base: 6.0.9\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-06T16:43:10Z",
        "closed_at": "2021-01-08T08:03:22Z",
        "merged_at": "2021-01-08T08:03:21Z",
        "body": "The defragger works well on these systems, but the tests and their\r\nthresholds are not adjusted for these big pages, so the defragger isn't\r\nable to get down the fragmentation to the levels the test expects and it\r\nfails on \"defrag didn't stop\".\r\n\r\nRandomly choosing 8k as the threshold for the skipping\r\n\r\nFixes #8265 (which had 65k pages)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2021-01-06T08:39:08Z",
        "closed_at": "2021-01-20T12:07:10Z",
        "merged_at": "2021-01-20T12:07:10Z",
        "body": "",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2021-01-05T12:48:26Z",
        "closed_at": "2021-01-28T14:38:50Z",
        "merged_at": "2021-01-28T14:38:50Z",
        "body": "some modules may need to behave in a certain way when a fork child exists, and undo that behavior when the child exits.\r\nfor example, `updateDictRehashingPolicy` to prevent CoW",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1089,
        "deletions": 76,
        "changed_files": 8,
        "created_at": "2021-01-05T10:25:58Z",
        "closed_at": "2021-01-28T14:19:44Z",
        "merged_at": "2021-01-28T14:19:43Z",
        "body": "APIs added for these stream operations: add, delete, iterate and\r\ntrim (by ID or maxlength). The functions are prefixed by RM_Stream.\r\n\r\nThe type RedisModuleStreamID is added and functions for converting\r\nfrom and to RedisModuleString.\r\n\r\nWhenever the stream functions return REDISMODULE_ERR, errno is set to\r\nprovide additional error information.\r\n\r\nRefactoring: The zset iterator fields in the RedisModuleKey struct\r\nare wrapped in a union, to allow the same space to be used for type-\r\nspecific info for streams and allow future use for other key types.\r\n\r\nFixes #5760.\r\n",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 88,
        "deletions": 24,
        "changed_files": 3,
        "created_at": "2021-01-05T10:23:05Z",
        "closed_at": "2021-02-21T07:09:54Z",
        "merged_at": "2021-02-21T07:09:54Z",
        "body": "TL:DR: Respond with error if expire time overflows from positive to negative of vice versa.\r\n* `SETEX`, `SET EX`, `GETEX` etc would have already error on negative value,\r\nbut now they would also error on overflows (i.e. when the input was positive but\r\nafter the manipulation it becomes negative, which would have passed before)\r\n* `EXPIRE` and `EXPIREAT` was ok taking negative values (would implicitly delete\r\nthe key), we keep that, but we do error if the user provided a value that changes\r\nsign when manipulated (except the case of changing sign when `basetime` is added)\r\n\r\n---------------\r\n\r\nIf we set the expiry of a key to 10^15, the key is automatically expired due to an overflow while trying to convert the value from seconds to milliseconds. \r\n\r\nExpire time overflows (GDB):\r\n```c\r\nThread 1 \"redis-server\" hit Breakpoint 1, expireGenericCommand (c=0x7ffff751c480, basetime=1609840928734, unit=0) at expire.c\r\n501         robj *key = c->argv[1], *param = c->argv[2];\r\n(gdb) n\r\n504         if (getLongLongFromObjectOrReply(c, param, &when, NULL) != C_OK)\r\n(gdb) n\r\n507         if (unit == UNIT_SECONDS) when *= 1000;\r\n(gdb) n\r\n508         when += basetime;\r\n(gdb) p when\r\n$1 = -8446744073709551616\r\n```\r\n\r\nEarlier:\r\n```\r\n127.0.0.1:6379> SET A B EX 10000000000000000\r\nOK\r\n127.0.0.1:6379> GET A\r\n(nil)\r\n127.0.0.1:6379> SET A B\r\nOK\r\n127.0.0.1:6379> GET A\r\n\"B\"\r\n127.0.0.1:6379> EXPIRE A 10000000000000000\r\n(integer) 1\r\n(48.60s)\r\n127.0.0.1:6379> GET A\r\n(nil)\r\n```\r\n\r\nNow:\r\n```\r\n127.0.0.1:6379> SET A B EX 10000000000000000\r\n(error) ERR invalid expire time in set\r\n127.0.0.1:6379> SET A B\r\nOK\r\n127.0.0.1:6379> GET A\r\n\"B\"\r\n127.0.0.1:6379> EXPIRE A 10000000000000000\r\n(error) ERR invalid expire time in expire\r\n127.0.0.1:6379> GET A\r\n\"B\"\r\n```",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-05T06:20:27Z",
        "closed_at": "2021-01-05T16:41:58Z",
        "merged_at": "2021-01-05T16:41:58Z",
        "body": "In sdscatfmt when call sdsMakeRoomFor, it required more space than needed.\r\nIf the initlen is very big, it will allocate a lot of memory than needed.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-04T14:33:13Z",
        "closed_at": "2021-01-04T21:16:20Z",
        "merged_at": "2021-01-04T21:16:20Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2021-01-04T02:34:20Z",
        "closed_at": "2021-01-04T08:28:48Z",
        "merged_at": "2021-01-04T08:28:48Z",
        "body": "When storekey is not null, it is a write command.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 411,
        "deletions": 80,
        "changed_files": 8,
        "created_at": "2021-01-03T09:22:45Z",
        "closed_at": "2021-01-28T10:09:12Z",
        "merged_at": "2021-01-28T10:09:12Z",
        "body": "This is both a bugfix and an enhancement.\r\n\r\nInternally, Sentinel relies entirely on IP addressess to identify\r\ninstances. When configured with a new master, it also requires users to\r\nspecify and IP and not hostname.\r\n\r\nHowever, replicas may use the replica-announce-ip configuration to\r\nannounce a hostname. When that happens, Sentinel fails to match the\r\nannounced hostname with the expected IP and considers that a different\r\ninstance, trigerring reconfiguration, etc.\r\n\r\nAnother use case is where TLS is used and clients are expected to match\r\nthe hostname to connect to with the certificate's SAN attribute. To\r\nproperly implement this configuration, it is necessary for Sentinel to\r\nredirect clients to a hostname rather than an IP address.\r\n\r\nThe new 'resolve-hostnames' configuration parameter determines if\r\nSentinel is willing to accept hostnames. It is set by default to no,\r\nwhich maintains backwards compatibility and avoids unexpected DNS\r\nresolution delays on systems with DNS configuration issues.\r\n\r\nInternally, Sentinel contineus to identify instances by their resolved\r\nIP address and will also report the IP by default. The new\r\n'announce-hostnames' parameter determines if Sentinel should prefer to\r\nannounce a hostname, when available, rather than an IP address. This\r\napplies to addresses returned to clients, as well as their\r\nrepresentation in the configuration file, REPLICAOF configuration\r\ncommands, etc.\r\n\r\nThis commit also introduces SENTINEL CONFIG GET and SENTINEL CONFIG SET\r\nwhich can be used to introspect or configure global Sentinel\r\nconfiguration that was previously was only possible by directly\r\naccessing the configuration file and possibly restarting the instance.\r\n\r\nFixes #7758, #7393, #2118, #7928\r\n\r\nCo-authored-by: myl1024 <myl92916@qq.com>",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2021-01-03T08:43:47Z",
        "closed_at": "2021-01-03T14:09:30Z",
        "merged_at": "2021-01-03T14:09:30Z",
        "body": "In #7726 (part of 6.2), we added a mechanism for whileBlockedCron, this\r\nmechanism has an assertion to make sure the timestamp in\r\nwhileBlockedCron was always set correctly before the blocking operation\r\nstarts.\r\n\r\nI now found (thanks to our CI) two bugs in that area:\r\n1) CONFIG RESETSTAT (if it was allowed during loading) would have\r\n   cleared this var\r\n2) the call stopLoading (which calls whileBlockedCron) was made too\r\n   early, while the rio is still in use, in which case the update_cksum\r\n   (rdbLoadProgressCallback) may still be called and whileBlockedCron\r\n   can assert.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2021-01-03T07:11:33Z",
        "closed_at": "2021-01-03T14:14:53Z",
        "merged_at": "2021-01-03T14:14:53Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2021-01-01T07:38:15Z",
        "closed_at": "2021-01-02T06:37:20Z",
        "merged_at": "2021-01-02T06:37:20Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 278,
        "deletions": 29,
        "changed_files": 9,
        "created_at": "2020-12-30T20:55:07Z",
        "closed_at": "2021-01-26T07:31:54Z",
        "merged_at": "2021-01-26T07:31:54Z",
        "body": "This PR fixes https://github.com/redis/redis/issues/5388 . This is a well known and an annoying issue in Sentinel mode, and has left for 2 years hasn't been fixed. \r\n**Cause of this issue:**\r\nCurrently, Redis rewrite process works well in server mode, however in sentinel mode, the sentinel config has variant semantics for different configurations, in example configuration https://github.com/redis/redis/blob/unstable/sentinel.conf, we put comments on these. However the rewrite process only treat the sentinel config as a single option. During rewrite process, it will mess up with the lines and comments.\r\n**Approaches:**\r\nIn order to solve this issue, we need to differentiate different subconfig options in sentinel separately, for example, `sentinel monitor <master-name> <ip> <redis-port> <quorum>\r\n`\r\nwe can treat it as `sentinel monitor` option, instead of the `sentinel` option.\r\n\r\nThis PR also fixes the dependency issue when putting configurations in sentinel.conf. For example before this commit, \r\nwe must put `sentinel monitor <master-name> <ip> <redis-port> <quorum>` before `sentinel auth-pass <master-name> <password>` for a single master, otherwise the server cannot start and will return error. This commit fixes this issue, as long as the monitoring master was configured, no matter the sequence is, the sentinel can start and run properly.\r\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-30T19:15:30Z",
        "closed_at": "2021-01-05T07:15:10Z",
        "merged_at": "2021-01-05T07:15:10Z",
        "body": "Turns out the RDB checksum in Redis 6.0 on bigendian is broken.\r\nIt always returned 0, so the RDB files are generated as if checksum is\r\ndisabled, and will be loaded ok on littleendian, and on bigendian.\r\nBut it'll not be able to load RDB files generated on littleendian or older versions.\r\n\r\nSimilarly DUMP and RESTORE will work on the same version (0==0),\r\nbut will be unable to exchange dump payloads with littleendian or old versions.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-29T13:25:50Z",
        "closed_at": "2021-01-03T09:56:27Z",
        "merged_at": "2021-01-03T09:56:26Z",
        "body": "getRDB is \"designed\" to work in two modes: one for redis-cli --rdb and\r\none for redis-cli --cluster backup.\r\nin the later case it uses the hiredis connection from the cluster nodes\r\nand it used to free it without nullifying the context, so a later\r\nattempt to free the context would crash.\r\n\r\nI suppose the reason it seems to want to free the hiredis context ASAP\r\nis that it wants to disconnect the replica link, so that replication\r\nbuffers will not be accumulated.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 22,
        "changed_files": 4,
        "created_at": "2020-12-29T12:03:20Z",
        "closed_at": "2021-01-05T06:29:21Z",
        "merged_at": "2021-01-05T06:29:21Z",
        "body": "When a Lua script returns a map to redis (a feature which was added in\r\nredis 6 together with RESP3), it would have returned the value first and\r\nthe key second.\r\n\r\nIf the client was using RESP2, it was getting them out of order, and if\r\nthe client was in RESP3, it was getting a map of value => key.\r\nThis was happening regardless of the Lua script using redis.setresp(3)\r\nor not.\r\n\r\nThis also affects a case where the script was returning a map which it got\r\nfrom from redis by doing something like: redis.setresp(3); return redis.call()\r\n\r\nThis fix is a breaking change for redis 6.0 users who happened to rely\r\non the wrong order (either ones that used redis.setresp(3), or ones that\r\nreturned a map explicitly).\r\n\r\nThis commit also includes other two changes in the tests:\r\n1. The test suite now handles RESP3 maps as dicts rather than nested\r\n   lists\r\n2. Remove some redundant (duplicate) tests from tracking.tcl",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 396,
        "deletions": 156,
        "changed_files": 12,
        "created_at": "2020-12-28T18:58:27Z",
        "closed_at": "2021-01-07T14:14:30Z",
        "merged_at": "2021-01-07T14:14:30Z",
        "body": "This PR has two commits:\r\n1. A refactory commit to unify the 3 pid variables in the server struct into one, which brings some benefits\r\n2. Have the fork children monitor their COW and send it to the parent to be shown in a new INFO field.\r\neach commit has a detailed commit comment that lists all the changes and their implications.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 27,
        "changed_files": 5,
        "created_at": "2020-12-28T12:12:26Z",
        "closed_at": "2021-01-09T17:52:43Z",
        "merged_at": "2021-01-09T17:52:43Z",
        "body": "1. Change `__FUNCTION__` to `__func__`\r\n2. Fix printf format.\r\n3. Replace the assert in quicklist and intset with the assert of redisAssert.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-28T11:54:00Z",
        "closed_at": "2021-01-01T08:23:31Z",
        "merged_at": "2021-01-01T08:23:31Z",
        "body": "The crash log attempts to print the current client info, and when it\r\ndoes that it attempts to check if the first argument happens to be a key\r\nbut it did so for commands with no arguments too, which caused the crash\r\nlog to crash half way and not reach its end.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2020-12-28T09:40:26Z",
        "closed_at": "2021-01-08T16:29:45Z",
        "merged_at": "2021-01-08T16:29:45Z",
        "body": "[Edit]\r\nSupport ANY option to return some results that match the criteria ASAP, without a complete search and implicit sorting.\r\n\r\n\r\n---\r\nSupport negative COUNT argument to return some results that match the criteria ASAP, without a complete search and implicit sorting.\r\n\r\nrefer: #8245",
        "comments": 20
    },
    {
        "merged": true,
        "additions": 72,
        "deletions": 15,
        "changed_files": 7,
        "created_at": "2020-12-28T08:42:36Z",
        "closed_at": "2021-01-15T13:32:59Z",
        "merged_at": "2021-01-15T13:32:59Z",
        "body": "* Adds ASYNC and SYNC arguments to SCRIPT FLUSH\r\n* Adds SYNC argument to FLUSHDB and FLUSHALL\r\n* Adds new config to control the default behavior of FLUSHDB, FLUSHALL and SCRIPT FLUASH.\r\n\r\nthe new behavior is as follows:\r\n* FLUSH[ALL|DB],SCRIPT FLUSH: Determine sync or async according to the\r\n  value of lazyfree-lazy-user-flush.\r\n* FLUSH[ALL|DB],SCRIPT FLUSH ASYNC: Always flushes the database in an async manner.\r\n* FLUSH[ALL|DB],SCRIPT FLUSH SYNC: Always flushes the database in a sync manner.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-28T06:39:09Z",
        "closed_at": "2021-02-24T11:18:55Z",
        "merged_at": "2021-02-24T11:18:55Z",
        "body": "In quicklistDelRange when delete entry from entry.offset to node tail, extent only need gte node->count - entry.offset, not node->count.\r\n```\r\n        } else if (entry.offset >= 0 && extent >= node->count) {\r\n            /* If deleting more nodes after this one, calculate delete based\r\n             * on size of current node. */\r\n            del = node->count - entry.offset;\r\n```\r\n\r\nFor now, this code path nerver happen, because when use this function the first `entry.offset` is always <= 0.\r\nBut if we write a command like `lremrange key begin end` and use this function, errors will occur.\r\nBecause extent >= node->count - entry.offset will report comparison between signed and unsigned integer expressions error, we  \r\nuse extent + entry.offset >= node->count instead.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 47,
        "changed_files": 1,
        "created_at": "2020-12-28T03:14:45Z",
        "closed_at": "2020-12-30T06:37:38Z",
        "merged_at": "2020-12-30T06:37:38Z",
        "body": "in commit  [https://github.com/redis/redis/commit/fb2feae599e2a190cfc6906ca9a03abc54528b61](https://github.com/redis/redis/commit/fb2feae599e2a190cfc6906ca9a03abc54528b61), the null judgment of the list needs to be placed in a loop to avoid empty list key, but now it is no longer necessary.\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 39,
        "changed_files": 12,
        "created_at": "2020-12-28T01:57:11Z",
        "closed_at": "2021-06-24T06:28:15Z",
        "merged_at": null,
        "body": "This fixes six issues:\r\n\r\n1. Change key variable type to unsigned long\r\nBecause the maximum number of redis keys is greater than 2^32, so if the type is long or int, it will overflow.\r\n\r\n2. Change sum of keys variable type to unsigned long long.\r\nIf it is the sum of N(>2) keys, using unsigned long to store it may cause an overflow.\r\n\r\n3. Optimize performEvictions funcion, there is no need to calculate the sum of all keys(the original code may also overflow).\r\n\r\n4. Fix dictGetRandomKey function implicit conversion overflow\r\nIf `count == server.maxmemory_samples` and server.maxmemory_samples is INT_MAX, `count*10` will overflow.\r\n\r\n5. Change return type of emptyDb to unsigned long long\r\nReturn -1 in emptyDb is not used anywhere, and we can determine the error by `error == EINVAL`, so change it to 0.\r\nAnd if -1 is returned, `server.dirty += emptyDb()` will result in server.dirty--, which obviously shouldn't happen.\r\n\r\n6. Change variable type of sdslen to size_t.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 18,
        "changed_files": 1,
        "created_at": "2020-12-27T20:15:50Z",
        "closed_at": "2021-01-19T15:24:50Z",
        "merged_at": "2021-01-19T15:24:50Z",
        "body": "This PR removes some unnecessary runtime debug checks from 2 years ago regarding io-threads.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2020-12-27T07:40:55Z",
        "closed_at": "2020-12-27T19:40:13Z",
        "merged_at": "2020-12-27T19:40:13Z",
        "body": "Recently efaf09ee4 started using addReplyErrorSds in place of\r\naddReplySds the later takes ownership of the string but the former did\r\nnot.\r\nThis introduced memory leaks when a script returns an error to redis,\r\nand also in clusterRedirectClient (two new usages of\r\naddReplyErrorSds which was mostly unused till now.\r\n\r\nThis commit changes two thanks.\r\n1. change addReplyErrorSds to take ownership of the error string.\r\n2. scripting.c doesn't actually need to use addReplyErrorSds, it's a\r\nperfect match for addReplyErrorFormat (replaces newlines with spaces)",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2020-12-25T11:26:40Z",
        "closed_at": "2020-12-25T15:30:35Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2020-12-24T11:45:25Z",
        "closed_at": "2020-12-25T03:09:21Z",
        "merged_at": null,
        "body": "In replicationCron, mater sends PING according to ping_slave_period,\r\nThe reason why master sends PING is to update interaction time in slaves,\r\nso master needn't send PING to slaves if already sent other synchronization\r\nstream in the past repl_ping_slave_period time.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 79,
        "deletions": 6,
        "changed_files": 15,
        "created_at": "2020-12-24T10:05:18Z",
        "closed_at": "2021-01-19T20:57:31Z",
        "merged_at": "2021-01-19T20:57:31Z",
        "body": "epoll_create1() may fail either because it's not implemented (in an old kernel) or it doesn't recognize/understand the EPOLL_CLOEXEC flag, in which case we ought to fall back to the conventional epoll_create(), which makes the creation of epoll instance more robust.\r\n\r\nBesides, the `FD_CLOEXEC` flag should be enabled on this epoll instance to avoid fd leaks.",
        "comments": 72
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-12-24T07:10:13Z",
        "closed_at": "2020-12-24T09:58:44Z",
        "merged_at": "2020-12-24T09:58:44Z",
        "body": "In ziplist.c `zipStorePrevEntryLength` and `zipStorePrevEntryLengthLarge`, when encode large prev entry length, we should use 4 instead of `sizeof(len)`:\r\n```\r\nint zipStorePrevEntryLengthLarge(unsigned char *p, unsigned int len) {\r\n    if (p != NULL) {\r\n        p[0] = ZIP_BIG_PREVLEN;\r\n        memcpy(p+1,&len,sizeof(len));\r\n        memrev32ifbe(p+1);\r\n    }\r\n    return 1+sizeof(len);\r\n}\r\n\r\n/* Encode the length of the previous entry and write it to \"p\". Return the\r\n * number of bytes needed to encode this length if \"p\" is NULL. */\r\nunsigned int zipStorePrevEntryLength(unsigned char *p, unsigned int len) {\r\n    if (p == NULL) {\r\n        return (len < ZIP_BIG_PREVLEN) ? 1 : sizeof(len)+1;\r\n    } else {\r\n        if (len < ZIP_BIG_PREVLEN) {\r\n            p[0] = len;\r\n            return 1;\r\n        } else {\r\n            return zipStorePrevEntryLengthLarge(p,len);\r\n        }\r\n    }\r\n}\r\n```\r\nOn most compilers `sizeof(unsigned int) == 4` and entry length is small, so this never happened. But I think it's better to use 4 instead of `sizeof(len)`.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 68,
        "deletions": 24,
        "changed_files": 4,
        "created_at": "2020-12-24T02:17:46Z",
        "closed_at": "2021-01-04T12:48:28Z",
        "merged_at": "2021-01-04T12:48:28Z",
        "body": "This PR not only fixes the problem that swapdb does not make the transaction fail, but also optimizes the FLUSHALL and FLUSHDB command to set the CLIENT_DIRTY_CAS flag to avoid unnecessary traversal of clients.\r\n\r\nsee: #8236",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-12-23T08:39:44Z",
        "closed_at": "2020-12-23T10:55:06Z",
        "merged_at": "2020-12-23T10:55:06Z",
        "body": "This isn't a leak, just an warning due to unreachable\r\nallocation on the fork child.\r\nProblem created by 92a483bca",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2020-12-23T07:05:27Z",
        "closed_at": "2020-12-24T01:25:16Z",
        "merged_at": null,
        "body": "If a non-oom command or config command is executed while eviction is in progress, performEvictions will be executed twice.",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 130,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2020-12-22T14:46:02Z",
        "closed_at": "2020-12-22T19:18:58Z",
        "merged_at": null,
        "body": "Build steps:\r\n```\r\ngit clone https://github.com/ithewei/libhv\r\ncd libhv\r\n./configure && make && sudo make install\r\n\r\ngit clone -b libhv https://github.com/ithewei/redis\r\ncd redis/deps/hiredis\r\nmkdir build\r\ncd build\r\ncmake .. -DENABLE_EXAMPLES=ON\r\nmake\r\n./examples/example-libhv\r\n```\r\n\r\noutput:\r\n```\r\nConnected...\r\nargv[end-1]: ./examples/example-libhv\r\nDisconnected...\r\n```\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 81,
        "deletions": 25,
        "changed_files": 14,
        "created_at": "2020-12-22T14:04:18Z",
        "closed_at": "2021-01-17T13:48:49Z",
        "merged_at": "2021-01-17T13:48:49Z",
        "body": "",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2020-12-22T07:37:23Z",
        "closed_at": "2020-12-22T14:14:16Z",
        "merged_at": "2020-12-22T14:14:16Z",
        "body": "The Sentinel set command accepts multiple configurations at same time for single master, however when we do eg.\r\n`sentinel set mymaster parallel-syncs 3 notification-script <script-name>` with deny-scripts-reconfig configured, the parallel-syncs config set successfully but notification-script not, in this case, sentinel should still trigger a sentinelFlushConfig call since the first config was changed.\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-12-22T05:50:18Z",
        "closed_at": "2020-12-24T08:42:52Z",
        "merged_at": "2020-12-24T08:42:52Z",
        "body": "Just like it says, easy peasy!",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 71,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2020-12-22T05:40:05Z",
        "closed_at": "2021-01-03T15:13:37Z",
        "merged_at": "2021-01-03T15:13:37Z",
        "body": "geoadd same member with diff longitude or latitude return updated members.\r\n\r\ne.g:\r\ngeoadd test 1.0 2.0 same will return 1\r\ngeoadd test 1.0 3.0 same will return 0\r\n\r\nThe actual value has been updated\uff0cbecause the score is update\u3002",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 103,
        "deletions": 103,
        "changed_files": 21,
        "created_at": "2020-12-22T00:16:40Z",
        "closed_at": "2020-12-24T03:06:26Z",
        "merged_at": "2020-12-24T03:06:26Z",
        "body": "Centralize most of the error calls so that they actually pass through the addReplyError* type commands. Although there are some fringe benefits, like reducing memory allocations, the main benefit is to catch errors thrown from the replica and detect replication divergence.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 145,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2020-12-21T14:56:01Z",
        "closed_at": "2021-01-07T15:06:06Z",
        "merged_at": "2021-01-07T15:06:06Z",
        "body": "Older arm64 Linux kernels have a bug that could lead to data corruption during\r\nbackground save under the following scenario:\r\n\r\n1) jemalloc uses MADV_FREE on a page,\r\n2) jemalloc reuses and writes the page,\r\n3) Redis forks the background save process, and\r\n4) Linux performs page reclamation.\r\n\r\nUnder these conditions, Linux will reclaim the page wrongfully and the\r\nbackground save process will read zeros when it tries to read the page.\r\n\r\nThe bug has been fixed in Linux with commit:\r\nff1712f953e27f0b0718762ec17d0adb15c9fd0b (\"arm64: pgtable: Ensure dirty bit is\r\npreserved across pte_wrprotect()\")",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 13,
        "changed_files": 8,
        "created_at": "2020-12-21T11:53:33Z",
        "closed_at": "2020-12-22T06:57:46Z",
        "merged_at": "2020-12-22T06:57:46Z",
        "body": "Similarity Fix #7832 ",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 258,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2020-12-21T08:04:20Z",
        "closed_at": "2021-01-27T11:34:20Z",
        "merged_at": null,
        "body": "Last week, I met the same problem as [this question posted on stackoverflow](https://stackoverflow.com/questions/17211639/get-random-any-value-from-redis-hash) . It seems that it's a common feature which is needed since a long time ago.\r\n\r\nI know the code may looks like a shit, but as a user of redis, I hope the feature will be added before long.\r\n\r\nAnd the new command `hrandmember` for now doesn't behave like `srandmember` when it comes to negative count, as you may concern",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-12-21T03:28:00Z",
        "closed_at": "2020-12-23T16:37:34Z",
        "merged_at": "2020-12-23T16:37:34Z",
        "body": "incrRefCount(touchedkey) because rpoplpushHandlePush may change the client command argument vector, rpoplpushHandlePush has been modify to lmoveHandlePush, lmoveHandlePush does not change the client command argument vector.\r\n\r\nrelated to [Fix for bug 561 and other related problems](https://github.com/redis/redis/commit/c1c9d551da6dd534c8dae051a3a7e64bf7db6bfb)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 294,
        "deletions": 18,
        "changed_files": 9,
        "created_at": "2020-12-21T00:54:46Z",
        "closed_at": "2020-12-31T14:53:44Z",
        "merged_at": "2020-12-31T14:53:44Z",
        "body": "this PR pushes forward the observability on overall error statistics and command statistics within redis-server:\r\n\r\n- It extends `INFO COMMANDSTATS` to have\r\n  - failed_calls in - so we can keep track of errors that happen from the command itself, broken by command.\r\n  - rejected_calls - so we can keep track of errors that were triggered outside the commmand processing per se ( example of wrong arity, oom, etc... )\r\nSample ( notice rejected_calls ):\r\n  ```\r\n  127.0.0.1:6379> set k \r\n  (error) ERR wrong number of arguments for 'set' command\r\n  127.0.0.1:6379> info commandstats\r\n  # Commandstats\r\n  cmdstat_set:calls=0,usec=0,usec_per_call=0.00,rejected_calls=1,failed_calls=0\r\n  cmdstat_info:calls=8,usec=1077,usec_per_call=134.62,rejected_calls=0,failed_calls=0\r\n  ```\r\n\r\n- Adds a new section to INFO, named `ERRORSTATS` that enables keeping track of the different errors that occur within redis ( within processCommand and call ) based uppong the reply Error Prefix ( The first word after the \"-\", up to the first space ). \r\n  \r\n  Sample:\r\n  ```\r\n  127.0.0.1:6379> auth a\r\n  (error) ERR AUTH <password> called without any password configured for the default user. Are you sure your configuration is correct?\r\n  127.0.0.1:6379> set k\r\n  (error) ERR wrong number of arguments for 'set' command\r\n  127.0.0.1:6379> ACL SETUSER alice on >p1pp0 ~cached:* +get +info\r\n  OK\r\n  127.0.0.1:6379> auth alice p1pp0\r\n  OK\r\n  127.0.0.1:6379> set a b\r\n  (error) NOPERM this user has no permissions to run the 'set' command or its subcommand\r\n  127.0.0.1:6379> info errorstats\r\n  # Errorstats\r\n  errorstat_ERR:count=2\r\n  errorstat_NOPERM:count=1\r\n  ```\r\n\r\n\r\n- This PR also fixes RM_ReplyWithError so that it can be correctly identified as an error reply.\r\n\r\n---\r\nTo test the added features:\r\n```\r\ntclsh tests/test_helper.tcl --verbose --single unit/info\r\ntclsh tests/cluster/run.tcl --single tests/18-info.tcl\r\n```\r\n\r\nThe added tests are divided as follow:\r\n- **failed** call authentication error\r\n- **failed** call NOGROUP error\r\n- **rejected** call due to wrong arity\r\n- **rejected** call by OOM error\r\n- **rejected** call by authorization error\r\n- **rejected** call rejected due to MOVED Redirection",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 242,
        "deletions": 74,
        "changed_files": 9,
        "created_at": "2020-12-20T12:39:10Z",
        "closed_at": "2020-12-22T10:03:50Z",
        "merged_at": "2020-12-22T10:03:50Z",
        "body": "In the distant history there was only the read flag for commands, and whatever\r\ncommand that didn't have the read flag was a write one.\r\nThen we added the write flag, but some portions of the code still used !read\r\nAlso some commands that don't work on the keyspace at all, still have the read\r\nflag.\r\n\r\nChanges in this commit:\r\n1. remove the read-only flag from TIME, ECHO, ROLE and LASTSAVE\r\n\r\n2. EXEC command used to decides if it should propagate a MULTI by looking at\r\n   the command flags (!read & !admin).\r\n   When i was about to change it to look at the write flag instead, i realized\r\n   that this would cause it not to propagate a MULTI for PUBLISH, EVAL, and\r\n   SCRIPT, all 3 are not marked as either a read command or a write one (as\r\n   they should), but all 3 are calling forceCommandPropagation.\r\n\r\n   So instead of introducing a new flag to denote a command that \"writes\" but\r\n   not into the keyspace, and still needs propagation, i decided to rely on\r\n   the forceCommandPropagation, and just fix the code to propagate MULTI when\r\n   needed rather than depending on the command flags at all.\r\n\r\n   The implication of my change then is that now it won't decide to propagate\r\n   MULTI when it sees one of these: SELECT, PING, INFO, COMMAND, TIME and\r\n   other commands which are neither read nor write.\r\n\r\n3. Changing getNodeByQuery and clusterRedirectBlockedClientIfNeeded in\r\n   cluster.c to look at !write rather than read flag.\r\n   This should have no implications, since these code paths are only reachable\r\n   for commands which access keys, and these are always marked as either read\r\n   or write.\r\n\r\nThis commit improve MULTI propagation tests, for modules and a bunch of\r\nother special cases, all of which used to pass already before that commit.\r\nthe only one that test change that uncovered a change of behavior is the\r\none that DELs a non-existing key, it used to propagate an empty\r\nmulti-exec block, and no longer does.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-19T17:27:07Z",
        "closed_at": "2020-12-20T18:45:13Z",
        "merged_at": null,
        "body": "`ZIP_INT_MASK` doesn't seem to be used anywhere \ud83e\udd14 ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 178,
        "deletions": 159,
        "changed_files": 2,
        "created_at": "2020-12-18T20:19:00Z",
        "closed_at": "2020-12-24T09:55:29Z",
        "merged_at": "2020-12-24T09:55:29Z",
        "body": "I've always wanted to trash that code.\r\n\r\nThe first commit deals with sendSynchronousCommand:\r\nThis function was never actually used as a synchronous (do both send or receive), it was always used only ine one of the two modes, which meant it has to take extra arguments that are not relevant for the other.\r\nBesides that, a tool that sends a synchronous command, it not something we want in our toolbox (synchronous IO in single threaded app is evil).\r\n\r\nsendSynchronousCommand was now refactored into separate sending and receiving APIs, and the sending part has two variants, one taking vaargs, and the other taking argc+argv (and an optional length array which means you can use binary sds strings).\r\n\r\nThe second commit deals with the syncWithMaster and the ugly state machine in it.\r\nIt attempts to clean it a bit, but more importantly it uses pipeline for part of the work (rather than 7 round trips, we now have 4).\r\ni.e. the connect and PING are separate, then AUTH + 3 REPLCONF in one pipeline, and finally the PSYNC (must be separate since the master has to have an empty output buffer).",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-12-18T11:36:02Z",
        "closed_at": "2020-12-23T14:28:18Z",
        "merged_at": "2020-12-23T14:28:18Z",
        "body": "refer: #8205",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-18T09:45:50Z",
        "closed_at": "2020-12-18T12:55:40Z",
        "merged_at": "2020-12-18T12:55:40Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-12-17T23:37:04Z",
        "closed_at": "2021-09-20T16:28:07Z",
        "merged_at": null,
        "body": "The Redis dictionary, as defined in `dict.h/dict.c`, is a general purpose dictionary utility that should not be modified for custom changes in Redis.  The dictionary uses a `dictEntry` structure to carry each key/value pair.\r\n\r\nThis update allows the embedding of arbitrary metadata at the end of the `dictEntry` struct by allowing the allocation of a fixed amount of space at the end of each `dictEntry` in a given dictionary.\r\n\r\nThis is useful for modules that might like to maintain meta data along with a Redis defined structure which they can't modify.  This allows the storage of the metadata without the overhead of another dictionary.\r\n\r\nThis is an optional capability specified through the `dictType` and will not affect performance of the dictionary.  It will only affect space (memory) for dictionaries where this is explicitly enabled.",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-12-16T11:38:45Z",
        "closed_at": "2020-12-17T09:02:18Z",
        "merged_at": "2020-12-17T09:02:18Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 148,
        "deletions": 27,
        "changed_files": 4,
        "created_at": "2020-12-16T07:22:47Z",
        "closed_at": "2020-12-17T17:26:34Z",
        "merged_at": "2020-12-17T17:26:34Z",
        "body": "Previously neither server.requirepass or server.masterauth handles binary safe SDS strings. This leads to inconsistencies in client experience around password authentication. This CR makes REQUIREPASS and MASTERAUTH directives handle binary safe strings and added generic SDS argument support in config.c. \r\n\r\ni.e. On master\r\n\r\n```\r\nconfig set requirepass \"ab\\x00cd\"\r\nOK\r\nAUTH ab\r\nOK\r\nAUTH \"ab\\x00cd\"\r\n(error) WRONGPASS invalid username-password pair\r\n```\r\n\r\nAfter the fix.\r\n\r\n```\r\nconfig set requirepass \"ab\\x00cd\"\r\nOK\r\nAUTH ab\r\n(error) WRONGPASS invalid username-password pair\r\nAUTH \"ab\\x00cd\"\r\nOK\r\n```\r\n\r\nSame goes for MASTERAUTH.\r\n\r\nSee original related PR https://github.com/redis/redis/pull/7236",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-12-15T21:14:09Z",
        "closed_at": "2020-12-23T07:46:24Z",
        "merged_at": "2020-12-23T07:46:24Z",
        "body": "Homebrew for darwin-arm64 uses `/opt/homebrew` instead of `/usr/local` as the prefix, so that it can co-exist with x86_64 using Rosetta 2.\r\n\r\nThis patch detects the arm64 environment and chooses the proper Homebrew prefix.\r\n\r\nTested using both arm64 and x86_64 builds on a M1 Mac running macOS 11.1 and Xcode 12.3 with dual Hombrew installations.",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 17,
        "changed_files": 5,
        "created_at": "2020-12-15T11:43:27Z",
        "closed_at": "2021-01-13T19:52:03Z",
        "merged_at": null,
        "body": "> I agree that it's unnecessary, since it's a global, but I think this commit just obfuscates the git commit history. Do you think the change actually improves the code readability?\r\n\r\n@madolson  Old PR https://github.com/redis/redis/pull/8192 was unexpected closed by me, I made some changes and submitted another. I think the list of slaves and monitors is part of the function and should not be exposed. After removing these parameters, the whole function will be more concise.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-12-15T10:47:13Z",
        "closed_at": "2020-12-15T20:03:06Z",
        "merged_at": "2020-12-15T20:03:06Z",
        "body": "This small PR has by main goal to explicitly state the default TLS protocols supported and raise awareness to why TLS 1.0 and TLS 1.1 (formally [deprecated](https://tools.ietf.org/id/draft-ietf-tls-oldversions-deprecate-02.html)) should not be used.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 16,
        "changed_files": 4,
        "created_at": "2020-12-15T05:21:32Z",
        "closed_at": "2020-12-15T11:32:10Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-14T17:06:16Z",
        "closed_at": "2020-12-16T21:19:13Z",
        "merged_at": "2020-12-16T21:19:12Z",
        "body": "a small comment nit was found when i was reading https://github.com/redis/redis/pull/7958 .",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-12-14T13:21:14Z",
        "closed_at": "2020-12-14T15:10:32Z",
        "merged_at": "2020-12-14T15:10:32Z",
        "body": "found by the fuzzer with valgrind: https://github.com/redis/redis/runs/1549810809?check_suite_focus=true",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 294,
        "deletions": 25,
        "changed_files": 3,
        "created_at": "2020-12-14T11:16:10Z",
        "closed_at": "2020-12-14T18:54:11Z",
        "merged_at": "2020-12-14T18:54:11Z",
        "body": "This release is the first significant Redis release managed by the core team\r\nunder the new project governance model.\r\n\r\nRedis 6.2 includes many new commands and improvements, but no big features. It\r\nmainly makes Redis more complete and addresses issues that have been requested\r\nby many users frequently or for a long time.\r\n\r\nMany of these changes were not eligible for 6.0.x for several reasons:\r\n\r\n1. They are not backward compatible, which is always the case with new or\r\n   extended commands (that cannot be replicated to an older replica).\r\n2. They require a longer release-candidate test cycle.\r\n\r\n\r\nHere is a comprehensive list of changes in this release compared to 6.0.9,\r\neach one includes the PR number that added it, so you can get more details\r\nat https://github.com/redis/redis/pull/<number>\r\n\r\nNew commands / args:\r\n* Add SMISMEMBER command that checks multiple members (#7615)\r\n* Add ZMSCORE command that returns an array of scores (#7593)\r\n* Add LMOVE and BLMOVE commands that pop and push arbitrarily (#6929)\r\n* Add RESET command that resets client connection state (#7982)\r\n* Add COPY command that copies keys (#7953)\r\n* Add ZDIFF and ZDIFFSTORE commands (#7961)\r\n* Add ZINTER and ZUNION commands (#7794)\r\n* Add GEOSEARCH/GEOSEARCHSTORE commands for bounding box spatial queries (#8094)\r\n* Add GET parameter to SET command, for more powerful GETSET (#7852)\r\n* Add exclusive range query to XPENDING (#8130)\r\n* Add exclusive range query to X[REV]RANGE (#8072)\r\n* Add GT and LT options to ZADD for conditional score updates (#7818)\r\n* Add CLIENT INFO and CLIENT LIST for specific ids (#8113)\r\n* Add IDLE argument to XPENDING command (#7972)\r\n* Add local address to CLIENT LIST, and a CLIENT KILL filter. (#7913)\r\n* Add NOMKSTREAM option to XADD command (#7910)\r\n* Add command introspection to Sentinel (#7940)\r\n* Add SENTINEL MYID subcommand (#7858)\r\n\r\nNew features:\r\n* Dump payload sanitization: prevent corrupt payload causing crashes (#7807)\r\n  Has flags to enable full O(N) validation (disabled by default).\r\n* ACL patterns for Pub/Sub channels (#7993)\r\n* Support ACL for Sentinel mode (#7888)\r\n* Support getting configuration from both stdin and file at the same time (#7893)\r\n  Lets you avoid storing secrets on the disk.\r\n\r\nNew features in CLI tools:\r\n* redis-cli RESP3 push support (#7609)\r\n* redis-cli cluster import support source and target that require auth (#7994)\r\n* redis-cli URIs able to provide user name in addition to password (#8048)\r\n* redis-cli/redis-benchmark allow specifying the prefered ciphers/ciphersuites (#8005)\r\n* redis-cli add -e option to exit with code when command execution fails (#8136)\r\n\r\nCommand behavior changes:\r\n* EXISTS should not alter LRU (#8016)\r\n  In Redis 5.0 and 6.0 it would have touched the LRU/LFU of the key.\r\n* OBJECT should not reveal logically expired keys (#8016)\r\n  Will now behave the same TYPE or any other non-DEBUG command.\r\n* Improve db id range check for SELECT and MOVE (#8085)\r\n  Changes the error message text on a wrong db index.\r\n* Modify AUTH / HELLO error message (#7648)\r\n  Changes the error message text when the user isn't found or is disabled.\r\n* BITOPS length limited to proto_max_bulk_len rather than 512MB (#8096)\r\n  The limit is now configurable like in SETRANGE, and APPEND.\r\n* GEORADIUS[BYMEMBER] can fail with -OOM if Redis is over the memory limit (#8107)\r\n\r\nOther behavior changes:\r\n* Optionally (default) fail to start if requested bind address is not available (#7936)\r\n  If you rely on Redis starting successfully even if one of the bind addresses\r\n  is not available, you'll need to tune the new config.\r\n* Limit the main db dictionaries expansion to prevent key eviction (#7954)\r\n  In the past big dictionary rehashing could result in massive data eviction.\r\n  Now this rehashing is delayed (up to a limit), which can result in performance\r\n  loss due to hash collisions.\r\n* CONFIG REWRITE is atomic and safer, but requires write access to the config file's folder (#7824, #8051)\r\n  This change was already present in 6.0.9, but was missing from the release\r\n  notes.\r\n* A new incremental eviction mechanism that reduces latency on eviction spikes (#7653)\r\n  In pathological cases this can cause memory to grow uncontrolled and may require\r\n  specific tuning.\r\n* Not resetting \"save\" config when Redis is started with command line arguments. (#7092)\r\n  In case you provide command line arguments without \"save\" and count on it\r\n  being disabled, Now the defaults \"save\" config will kick in.\r\n* Update memory metrics for INFO during loading (#7690)\r\n* When \"supervised\" config is enabled, it takes precedence over \"daemonize\". (#8036)\r\n* Assertion and panic, print crash log without generating SIGSEGV (#7585)\r\n* Added crash log report on SIGABRT, instead of silently exiting (#8004)\r\n* Disable THP (Transparent Huge Pages) if enabled (#7381)\r\n  If you deliberately enabled it, you'll need to config Redis to keep it.\r\n\r\nBug fixes:\r\n* Handle output buffer limits for module blocked clients (#8141)\r\n  Could result in a module sending reply to a blocked client to go beyond the\r\n  limit.\r\n* Fix setproctitle related crashes. (#8150, #8088)\r\n  Caused various crashes on startup, mainly on Apple M1 chips or under\r\n  instrumentation.\r\n* A module doing RM_Call could cause replicas to get nested MULTI (#8097).\r\n* Backup/restore cluster mode keys to slots map for repl-diskless-load=swapdb (#8108)\r\n  In cluster mode with repl-diskless-load, when loading failed, slot map\r\n  wouldn't have been restored.\r\n* Fix oom-score-adj-values range, and bug when used in config file (#8046)\r\n  Enabling setting this in the config file in a line after enabling it, would\r\n  have been buggy.\r\n* Reset average ttl when empty databases (#8106)\r\n  Just causing misleading metric in INFO\r\n* Disable rehash when Redis has child process (#8007)\r\n  This could have caused excessive CoW during BGSAVE, replication or AOFRW.\r\n* Further improved ACL algorithm for picking categories (#7966)\r\n  Output of ACL GETUSER is now more similar to the one provided by ACL SETUSER.\r\n* Fix bug with module GIL being released prematurely (#8061)\r\n  Could in theory (and rarely) cause multi-threaded modules to corrupt memory.\r\n* Fix cluster redirect for module command with no firstkey. (#7539)\r\n* Reduce effect of client tracking causing feedback loop in key eviction (#8100)\r\n* Kill disk-based fork child when all replicas drop and 'save' is not enabled (#7819)\r\n* Rewritten commands (modified for propagation) are logged as their original command (#8006)\r\n* Fix cluster access to unaligned memory (SIGBUS on old ARM) #7958\r\n* If diskless repl child is killed, make sure to reap the child pid (#7742)\r\n* Broadcast a PONG message when slot's migration is over, may reduce MOVED responses (#7571)\r\n\r\nOther improvements:\r\n* TLS Support in redis-benchmark (#7959)\r\n* Accelerate diskless master connections, and general re-connections (#6271)\r\n* Run active defrag while blocked / loading (#7726)\r\n* Performance and memory reporting improvement - sds take control of its internal fragmentation (#7875)\r\n* Speedup cluster failover. (#7948)\r\n\r\nPlatform / toolchain support related improvements:\r\n* Optionally (not by default) use H/W Monotonic clock for faster time sampling (#7644)\r\n* Remove the requirements for C11 and _Atomic supporting compiler (#7707)\r\n  This would allow to more easily build and use Redis on older systems and\r\n  compilers again.\r\n* Fix crash log registers output on ARM. (#8020)\r\n* Raspberry build fix. (#8095)\r\n* Setting process title support for Haiku. (#8060)\r\n* DragonFlyBSD RSS memory sampling support. (#8023)\r\n\r\nNew configuration options:\r\n* Enable configuring OpenSSL using the standard openssl.cnf (#8143)\r\n* oom-score-adj-values config can now take absolute values (besides relative ones) (#8046)\r\n* TLS: Add different client cert support. (#8076)\r\n* Note that a few other changes listed above added their config options.\r\n\r\nInfo fields and introspection changes:\r\n* Add INFO fields to track diskless and disk-based replication progress (#7981)\r\n* Add INFO field for main thread cpu time, and scrape system time. (#8132)\r\n* Add total_forks to INFO STATS (#8155)\r\n* Add maxclients and cluster_connections to INFO CLIENTS (#7979)\r\n* Add tracking bcast flag and client redirection in client list (#7995)\r\n* Fixed INFO client_recent_max_input_buffer includes argv array (#8065, see #7874)\r\n* Note that a few other changes listed above added their info fields.\r\n\r\nModule API changes:\r\n* Add CTX_FLAGS_DENY_BLOCKING as a unified the way to know if blocking is allowed (#8025)\r\n* Add data type callbacks for lazy free effort, and unlink (#7912)\r\n* Add data type callback for COPY command (#8112)\r\n* Add callbacks for defrag support. (#8149)\r\n* Add module event for repl-diskless-load swapdb (#8153)\r\n\r\nModule related fixes:\r\n* Moved RMAPI_FUNC_SUPPORTED so that it's usable (#8037)\r\n* Improve timer accuracy (#7987)\r\n* Allow '\\0' inside of result of RM_CreateStringPrintf (#6260)\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 40,
        "changed_files": 2,
        "created_at": "2020-12-14T07:17:19Z",
        "closed_at": "2020-12-14T09:13:47Z",
        "merged_at": "2020-12-14T09:13:47Z",
        "body": "Additionally the older defrag tests are using an obsolete way to check\r\nif the defragger is suuported (the error no longer contains \"DISABLED\").\r\nthis doesn't usually makes a difference since these tests are completely\r\nskipped if the allocator is not jemalloc, but that would fail if the\r\nallocator is a jemalloc that doesn't support defrag.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 133,
        "deletions": 19,
        "changed_files": 3,
        "created_at": "2020-12-13T16:49:03Z",
        "closed_at": "2021-01-13T20:36:04Z",
        "merged_at": "2021-01-13T20:36:04Z",
        "body": "Fix #8145\r\n\r\nGenerating one node slots info is costly in clusterGenNodeDescription(), if there are many nodes in cluster, it will cost much time and block server to separately  generate every node description. We already had all slots topology of cluster, so we can generate all nodes slots info by only iterating slots topology once, that can reduce complexity to get clusterGenNodesDescription for big redis cluster.\r\n\r\nOtherwise, replace `sdscatprintf` with `sdsfmt` in clusterGenNodeDescription, since `sdsfmt` has better performance.",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 28483,
        "deletions": 14357,
        "changed_files": 114,
        "created_at": "2020-12-13T14:55:53Z",
        "closed_at": "2022-08-02T15:06:47Z",
        "merged_at": null,
        "body": null,
        "comments": 11
    },
    {
        "merged": true,
        "additions": 132,
        "deletions": 47,
        "changed_files": 4,
        "created_at": "2020-12-13T13:26:59Z",
        "closed_at": "2020-12-25T19:49:25Z",
        "merged_at": "2020-12-25T19:49:25Z",
        "body": "Implements the `[count]` variant of `L/RPOP`, provides partial resolution for #766 but leaves the blocking variants untouched for now.\r\n\r\nAdds: `L/RPOP <key> [count]`\r\n\r\nImplements no. 2 of the following strategies:\r\n\r\n1. Loop on listTypePop - this would result in multiple calls for memory freeing and allocating (see https://github.com/redis/redis/pull/8179/commits/769167a079b0e110d28e4a8099dce1ecd45682b5)\r\n2. Iterate the range to build the reply, then call quickListDelRange - this requires two iterations and **is the current choice**\r\n3. Refactor quicklist to have a pop variant of quickListDelRange - probably optimal but more complex\r\n\r\nAlso:\r\n* There's a historical check for NULL after calling listTypePop that is converted to an assert.\r\n* This refactors common logic shared between LRANGE and the new form of LPOP/RPOP into addListRangeReply (adds test for b/w compat)\r\n* Consequently, it may have made sense to have `LRANGE l -1 -2` and `LRANGE l 9 0` be legit and return a reverse reply. Due to historical reasons that would be, however, a breaking change.\r\n* Added minimal comments to existing commands to adhere to the style, make core dev life easier and get commit karma, naturally.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 48,
        "deletions": 42,
        "changed_files": 1,
        "created_at": "2020-12-13T08:47:30Z",
        "closed_at": "2021-03-24T10:26:57Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 78,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2020-12-13T04:29:32Z",
        "closed_at": "2021-01-08T08:00:36Z",
        "merged_at": "2021-01-08T08:00:36Z",
        "body": "Handle BCAST deduplications. This is mostly a POC to evaluate whether or not we should do this or throw an error: https://github.com/redis/redis/pull/8135\r\n\r\nI'm not sure if we need to do all the work to copy all the keys into the BCAST state, but it handles some edge cases where a new client had created the new bcast state.  ",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-12-13T01:20:59Z",
        "closed_at": "2020-12-13T09:11:30Z",
        "merged_at": "2020-12-13T09:11:30Z",
        "body": "Not much else to say.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-12T20:09:16Z",
        "closed_at": "2020-12-13T01:27:35Z",
        "merged_at": "2020-12-13T01:27:35Z",
        "body": "The pid of the benchmark process is used to randomize the random number generator's\r\nseed. This ensures that when multiple benchmark processes are started at the same time\r\nto generate load on a server, they use different seeds. This will ensure randomness in\r\nthe keys generated by those benchmark processes.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 32,
        "changed_files": 13,
        "created_at": "2020-12-10T18:44:49Z",
        "closed_at": "2020-12-13T15:09:55Z",
        "merged_at": "2020-12-13T15:09:55Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 435,
        "deletions": 107,
        "changed_files": 12,
        "created_at": "2020-12-10T01:35:20Z",
        "closed_at": "2021-01-08T07:36:55Z",
        "merged_at": "2021-01-08T07:36:54Z",
        "body": "Readonly implementation for: https://github.com/redis/redis/pull/6784\r\n\r\nMain Changes:\r\nThis introduces a new optional mode argument for CLIENT PAUSE, WRITE, which blocked all clients when they attempt execute a write command. There is also now a companion command, CLIENT UNPAUSE, which can be used to undo a client pause. This is targeted for failover use cases, where you use ```CLIENT PAUSE <timeout> WRITE``` to stop all traffic on the master until the replica is caught up. You can then multi-exec the ```CLIENT UNPAUSE``` + ```REPLICAOF <newip> <newport>``` to transactionally failover without two master situations.\r\n\r\nBehavior changes:\r\n* Commands will now throw basic errors (basically wrong node, wrong number of arguments) instead of just hanging.\r\n* Redis will no longer spin on the event loop during pause, but it will be doing useful work of parsing commands. (Although they might be ditched later)\r\n\r\nOther minor changes:\r\n* new may-replicate flag in the command table. The flag essentially indicates that the command should be allowed on replicas (it's not marked with write) but shouldn't be allowed during client pause. \r\n* new flag to COMMAND command: \"may_replicate\"\r\n*  Client pause will strongly enforce the guarantee that replication offset does not change while it's happening, fixed two places this can occur\r\n* * REPLCONF GETACK to replicas for WAIT command\r\n* * lazy expire",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 569,
        "deletions": 150,
        "changed_files": 2,
        "created_at": "2020-12-09T17:52:47Z",
        "closed_at": "2021-01-08T16:13:26Z",
        "merged_at": "2021-01-08T16:13:26Z",
        "body": "Fix #6640 #4450 #5951\r\n\r\nThis PR adds an additional trimming strategy to XADD and XTRIM named MINID (complements the existing MAXLEN).\r\nIt also adds a new LIMIT argument that allows incremental trimming by repeated calls (rather than all at once).\r\n\r\nThis provides the ability to trim all records older than a certain ID (which makes it possible for the user to trim by age too).\r\nExample:\r\n`XTRIM mystream MINID ~ 1608540753 ` will trim entries with id < 1608540753, but might not trim all (because of the `~` modifier)\r\n\r\nThe purpose is to ease the use of streams. many users use streams as logs and the common case is wanting a log of the last X seconds rather than a log that contains maximum X entries (new MINID vs existing MAXLEN)\r\n\r\nThe new LIMIT modifier is only supported when the trim strategy uses `~`. i.e. when the user asked for exact trimming, it all happens in one go (no possibility for incremental trimming).\r\nHowever, when `~` is provided, we trim full rax nodes, up to the limit number of records.\r\nThe default limit is `100*stream_node_max_entries` (used when LIMIT is not provided).\r\nI.e. this is a **behavior change** (even if the existing MAXLEN strategy is used).\r\nAn explicit limit of 0 means unlimited (but note that it's not the default).\r\n\r\nOther changes:\r\n1. Refactor arg parsing code for XADD and XTRIM to use common code.",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-09T15:24:40Z",
        "closed_at": "2020-12-09T18:20:56Z",
        "merged_at": "2020-12-09T18:20:56Z",
        "body": "Fixes #8165.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-12-09T02:52:01Z",
        "closed_at": "2020-12-13T09:47:24Z",
        "merged_at": "2020-12-13T09:47:24Z",
        "body": "in aof stream object rewrite, an error handling case missing raxStop call. just a small nit in the code.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-12-08T13:17:06Z",
        "closed_at": "2020-12-08T14:33:10Z",
        "merged_at": "2020-12-08T14:33:10Z",
        "body": "c4fdf09c0 added a test that now fails with valgrind\r\nit fails for two resons:\r\n1) the test samples the used memory and then limits the maxmemory to\r\n   that value, but it turns out this is not atomic and on slow machines\r\n   the background cron process that clean out old query buffers reduces\r\n   the memory so that the setting doesn't cause eviction.\r\n2) the dbsize was tested late, after reading some invalidation messages\r\n   by that time more and more keys got evicted, partially draining the\r\n   db. this is not the focus of this fix (still a known limitation)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-08T08:09:58Z",
        "closed_at": "2020-12-08T14:26:39Z",
        "merged_at": "2020-12-08T14:26:39Z",
        "body": "Happened to see this case,  I found some unused variables\r\n\r\nI remove unnecessary variables `starting`, and  add `assert_equal` check for `$before_len` and`$before_len_r`",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2020-12-08T08:03:14Z",
        "closed_at": "2020-12-13T08:01:19Z",
        "merged_at": "2020-12-13T08:01:19Z",
        "body": "Sometimes we want to know total forks of redis, that can help us analyze  redis stability and some problems, because `fork` is much costly and may block server.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-12-08T07:57:16Z",
        "closed_at": "2020-12-17T08:22:13Z",
        "merged_at": "2020-12-17T08:22:13Z",
        "body": "If we only has one node in cluster or before https://github.com/redis/redis/commit/8fdc857a9f83e59b3062d051dc0155dd53c89ea7, we don't know myself ip, so we should use config.hostip for myself.\r\n\r\nBut now, more than one node in cluster,  if we use `-h` with virtual IP or DNS, benchmark doesn't show node real ip and port of myself even though it could get right IP and port by CLUSTER NODES command.\r\n\r\nWe must use `sdsnew(ip)` to update `node->ip` if it is different from config.hostip, because we will free `ip` when leave this function. \r\nHere will free `node->ip` https://github.com/redis/redis/blob/unstable/src/redis-benchmark.c#L1084-L1088",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 80,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2020-12-07T21:21:43Z",
        "closed_at": "2020-12-13T12:36:06Z",
        "merged_at": "2020-12-13T12:36:06Z",
        "body": "When a replica uses the diskless-load swapdb approach, it backs up the old database,\r\nthen attempts to load a new one, and in case of failure, it restores the backup.\r\n\r\nthis means that modules with global out of keyspace data, must have an option to\r\nsubscribe to events and backup/restore/discard their global data too.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 39,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-12-07T15:28:54Z",
        "closed_at": "2020-12-08T09:27:31Z",
        "merged_at": "2020-12-08T09:27:31Z",
        "body": "Makes spt_init more careful with assumptions about what memory regions\r\nmay be overwritten. It will now only consider a contiguous block of argv\r\nand envp elements and mind any gaps.\r\n\r\nFixes #8062 and probably #8144.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 569,
        "deletions": 8,
        "changed_files": 9,
        "created_at": "2020-12-07T12:22:41Z",
        "closed_at": "2020-12-13T07:56:02Z",
        "merged_at": "2020-12-13T07:56:01Z",
        "body": "Add a new set of defrag functions that take a defrag context and allow\r\ndefragmenting memory blocks and RedisModuleStrings.\r\n\r\nModules can register a defrag callback which will be invoked when the\r\ndefrag process handles globals.\r\n\r\nModules with custom data types can also register a datatype-specific\r\ndefrag callback which is invoked for keys that require defragmentation.\r\nThe callback and associated functions support both one-step and\r\nmulti-step options, depending on the complexity of the key as exposed by\r\nthe free_effort callback.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2020-12-07T06:30:25Z",
        "closed_at": "2020-12-08T09:15:40Z",
        "merged_at": "2020-12-08T09:15:40Z",
        "body": "getPositiveLongFromObjectOrReply was introduced in https://github.com/redis/redis/pull/8018, we can use this for positive check for count argument in spop",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-12-06T20:11:59Z",
        "closed_at": "2020-12-07T12:30:13Z",
        "merged_at": "2020-12-07T12:30:13Z",
        "body": "This rather simple PR will enable configuring and enabling very important OpenSSL features using the standard openssl.cnf file. \r\nOne example of it are [Engines](https://github.com/openssl/openssl/blob/master/README-Engine.md) ( introduced in OpenSSL 0.9.6 ). \r\nMore information of the current bultin engines of opensll for the following crypto devices:\r\n- Microsoft CryptoAPI\r\n- VIA Padlock\r\n- nCipher CHIL\r\n\r\n\r\nIn addition, dynamic binding to external ENGINE implementations is now provided. An example of dynamic agent is [Intel\u00ae QAT OpenSSL* Engine](https://github.com/intel/QAT_Engine#using-the-openssl-configuration-file-to-loadinitialize-engines).\r\nGiven we're using `OPENSSL_config()` and  `OPENSSL_init_crypto()` we're compatible with versions >= 0.9.6. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 135,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2020-12-06T11:22:51Z",
        "closed_at": "2020-12-08T14:41:21Z",
        "merged_at": "2020-12-08T14:41:21Z",
        "body": "Module blocked clients cache the response in a temporary client,\r\nthe reply list in this client would be affected by the recent fix\r\nin #7202, but when the reply is later copied into the real client,\r\nit would have bypassed all the checks for output buffer limit, which\r\nwould have resulted in both: responding with a partial response to\r\nthe client, and also not disconnecting it at all.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 6,
        "changed_files": 5,
        "created_at": "2020-12-06T08:49:32Z",
        "closed_at": "2020-12-15T07:30:25Z",
        "merged_at": "2020-12-15T07:30:25Z",
        "body": "1. Fix wrong server.drity increment in spopWithCountCommand. If the count of spop is larger than the size of set, server.dirty should only increase the count of set, not the count.\r\n2. Fix wrong server.dirty increment in hsetCommand, pfaddCommand, ltrimCommand.",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 62,
        "deletions": 5,
        "changed_files": 6,
        "created_at": "2020-12-06T01:52:59Z",
        "closed_at": "2020-12-23T08:49:19Z",
        "merged_at": null,
        "body": "Currently during cli connect of Redis CLI, if it fails to SELECT a database (due to ACL permissions), the user lands onto database 0. However the Redis CLI prompt states otherwise which is confusing to the end user. For e.g.\r\n\r\n```\r\n$ ./redis-cli\r\n127.0.0.1:6379> acl setuser mydefault on nopass +@all -select +select|5\r\nOK\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* &* +@all\"\r\n2) \"user mydefault on nopass &* +@all -select +select|5\"\r\n127.0.0.1:6379>\r\n$ ./redis-cli --user mydefault --pass a -n 1\r\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\r\n127.0.0.1:6379[1]> client list\r\nid=8 addr=127.0.0.1:61906 laddr=127.0.0.1:6379 fd=8 name= age=10 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=45024 argv-mem=10 obl=0 oll=0 omem=0 tot-mem=62490 events=r cmd=client user=mydefault redir=-1\r\n127.0.0.1:6379[1]>\r\n```\r\n\r\nAs we can see in the above scenario, the current client is pointing to 0 db index instead of 1.\r\n\r\nAfter the proposed changes, redis-cli warns about the failure of selection of database as well as displays the current database index the client is pointing to.\r\n\r\n```\r\n$ ./redis-cli\r\n127.0.0.1:6379> acl setuser mydefault on nopass +@all -select +select|5\r\nOK\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* &* +@all\"\r\n2) \"user mydefault on nopass &* +@all -select +select|5\"\r\n$ ./redis-cli --user mydefault --pass a -n 1\r\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\r\nWarning: SELECT failed\r\n127.0.0.1:6379> client list\r\nid=10 addr=127.0.0.1:62661 laddr=127.0.0.1:6379 fd=8 name= age=5 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=45024 argv-mem=10 obl=0 oll=0 omem=0 tot-mem=62490 events=r cmd=client user=mydefault redir=-1\r\n127.0.0.1:6379> CURRDB\r\n(integer) 0\r\n127.0.0.1:6379> SELECT 1\r\n(error) NOPERM this user has no permissions to run the 'select' command or its subcommand\r\n127.0.0.1:6379> SELECT 5\r\nOK\r\n127.0.0.1:6379[5]> CURRDB\r\n(integer) 5\r\n```\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2020-12-05T12:43:52Z",
        "closed_at": "2020-12-06T13:30:30Z",
        "merged_at": "2020-12-06T13:30:30Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-12-05T05:25:41Z",
        "closed_at": "2020-12-06T01:35:03Z",
        "merged_at": null,
        "body": "Noting that you may need root permissions would have saved me, and likely many people, a few seconds. I am suggesting this change or a similar change, to acknowledge this fact, so that time is not wasted for others following spinning up their own instance in the future.\r\n\r\nFeel free to decline, debate, or describe here; As I am hopeful that this change will be made, partially because it almost certainly will only save people time, and does no harm since you need to use root permissions (sudo) or be logged in with su anyways. But I do note that this is a minor almost irrelevant bikeshed, and would only be slightly dismayed should the preference be that it not be changed. \r\n\r\nThis is such a minor request, but at the same time, I feel it could have a non-zero impact and thus made the suggestion. Best wishes. ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-12-04T06:33:06Z",
        "closed_at": "2020-12-13T08:04:46Z",
        "merged_at": "2020-12-13T08:04:46Z",
        "body": "This PR fixes https://github.com/redis/redis/issues/8127, by providing -e option, user can control whether return error code if the command execution fails. @oranagra please take a look and let me know if i miss anything, thanks\r\n\r\nExample:\r\n>\r\n./redis-cli  config set requirepass 1234\r\nOK\r\n./redis-cli  -e flushdb\r\nNOAUTH Authentication required.\r\necho $?\r\n1\r\n\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2020-12-03T21:22:36Z",
        "closed_at": "2020-12-23T00:56:02Z",
        "merged_at": null,
        "body": "This PR fixes https://github.com/redis/redis/issues/8003 . The cause of this issue is if we did not specify any prefixes, \"\" will be used as the default prefix since an empty string is a prefix for all other string. However if we call the CLIENT TRACKING ON BCAST with prefixes again both \"\" and other prefixes will be registered in the prefixes radix tree for bcast tracking, this will cause same client receive duplicate invalidation messages.\r\n\r\n For a side note regarding my comment if user specifying one prefix is a subset  of another in the same call: https://github.com/redis/redis/issues/8003#issuecomment-721314648 . There are two ways we handle this:\r\n1. Do a brute force check for each prefixes.\r\n2. Provide the Radix tree API for checking if a key's path has other keys.\r\n\r\nThe first one has O(n^2) performance and we are trying to avoid, the second is optimal but we may not doing it for now since it need to change the lower level Radix tree code for providing this API,  maybe we can do this later, but for now since we don't want to bring much complexity, we can just simply solving the original issue now first..",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 319,
        "deletions": 10,
        "changed_files": 9,
        "created_at": "2020-12-03T19:52:13Z",
        "closed_at": "2020-12-23T13:52:07Z",
        "merged_at": "2020-12-23T13:52:07Z",
        "body": "When a database on a 64 bit build grows past 2^31 keys, the underlying hash table expands to 2^32 buckets. After this point, the algorithms for selecting random elements only return elements from half of the available buckets because they use `random()` which has a range of 0 to 2^31 - 1. This causes problems for eviction policies which use `dictGetSomeKeys` or `dictGetRandomKey`. Over time they cause the hash table to become unbalanced because, while new keys are spread out evenly across all buckets, evictions come from only half of the available buckets. Eventually this half of the table starts to run out of keys and it takes longer and longer to find candidates for eviction. This continues until no more evictions can happen. This solution addresses this by using a 64 bit PRNG when needed.\r\n\r\n### Performance\r\nTo test the performance impact of this change, we used a LUA script to rapidly insert keys into databases at max memory using the affected eviction policies and compared results with and without this change. There were no meaningful differences with all databases achieving around 100k evictions per second.\r\n\r\n### Test Coverage\r\nWhile the basic eviction functionality is covered by existing tests, there isn't one that covers what happens after 2^31 keys. Because of the time and memory required to run a test like this I did not add one.\r\n",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-12-03T19:40:14Z",
        "closed_at": "2020-12-13T17:14:47Z",
        "merged_at": "2020-12-13T17:14:46Z",
        "body": "The following PR exposes the main thread CPU info via info modules ( linux specific only ) (`used_cpu_sys_main_thread` and `used_cpu_user_main_thread`). This is important for:\r\n- distinguish between main thread and io-threads cpu time total cpu time consumed ( check what is the first bottleneck on the used config )\r\n- distinguish between main thread and modules threads total cpu time consumed\r\n\r\nApart from it, this PR also exposes the `server_time_in_microseconds` within the Server section so that we can properly differentiate consecutive collection and calculate for example the CPU% and or / cpu time vs wall time, etc... \r\n\r\nNote: given that I was already touching the CPU time collection code I've moved it to be called within the cpu section ( no need to measure cpu time if we're not outputting it ). \r\n\r\nHere is a sample output of the proposed #CPU section on a 1 and 4 io-threads config ( while running a simple benchmark ):\r\n\r\n### 1 io-thread (default) \r\nWe should see a small difference between main thread / all threads cpu time. Not very usefull for this simple scenario\r\n\r\n```\r\n$ redis-cli info cpu\r\n# CPU\r\nused_cpu_sys:15.972181\r\nused_cpu_user:2.316606\r\nused_cpu_sys_children:0.000000\r\nused_cpu_user_children:0.000000\r\nused_cpu_sys_main_thread:15.972034\r\nused_cpu_user_main_thread:2.316585\r\n```\r\n\r\n### 4 io-threads \r\nWe should see a large difference between main thread / all threads cpu time, and be able to properly identify the main/bg threads cpu usage.\r\n\r\n```\r\n$ redis-cli info cpu\r\n# CPU\r\nused_cpu_sys:18.644760\r\nused_cpu_user:20.648113\r\nused_cpu_sys_children:0.000000\r\nused_cpu_user_children:0.000000\r\nused_cpu_sys_main_thread:6.978166\r\nused_cpu_user_main_thread:2.860689\r\n```\r\n\r\n-----------------------------------------------\r\n\r\n## Regarding server time\r\n\r\nSample #SERVER section output:\r\n\r\n```\r\n$ redis-cli info server\r\n# Server\r\n# Server\r\nredis_version:255.255.255\r\nredis_git_sha1:c44a11c6\r\nredis_git_dirty:0\r\nredis_build_id:f6545694f8309272\r\nredis_mode:standalone\r\nos:Linux 5.4.0-56-generic x86_64\r\narch_bits:64\r\nmultiplexing_api:epoll\r\natomicvar_api:c11-builtin\r\ngcc_version:9.3.0\r\nprocess_id:310328\r\nprocess_supervised:no\r\nrun_id:d25738d6a821c41f0cf1036078b8728c0c0eaaa8\r\ntcp_port:6379\r\nserver_time_usec:1607863602885086\r\nuptime_in_seconds:7\r\nuptime_in_days:0\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:14028082\r\nexecutable:/home/filipe/redislabs/redis/./src/redis-server\r\nconfig_file:\r\nio_threads_active:0\r\n```",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-12-03T15:28:17Z",
        "closed_at": "2020-12-08T09:43:00Z",
        "merged_at": "2020-12-08T09:43:00Z",
        "body": "Fixes #8129.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-12-02T13:07:19Z",
        "closed_at": "2020-12-02T15:24:28Z",
        "merged_at": "2020-12-02T15:24:27Z",
        "body": "Tries to fix timings in the new tests -  https://github.com/redis/redis/pull/7993#issuecomment-737044292\r\n\r\nDO NOT MERGE before reverting change to .github/ci.yml",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-12-01T02:33:40Z",
        "closed_at": "2020-12-01T19:46:46Z",
        "merged_at": "2020-12-01T19:46:46Z",
        "body": "SET with GET will throw an error but still overwrite the value on error. So checking the error code. \r\n\r\nResolves https://github.com/redis/redis/issues/8116\r\n\r\nSide note, I think there is a static code checker that will make sure we are checking return codes, might be worth looking at. (Or we could use rust)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2020-11-30T16:50:59Z",
        "closed_at": "2020-12-07T12:24:06Z",
        "merged_at": "2020-12-07T12:24:06Z",
        "body": "The output is identical to CLIENT LIST but provides a single line for\r\nthe current client only.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 84,
        "deletions": 10,
        "changed_files": 6,
        "created_at": "2020-11-30T16:32:52Z",
        "closed_at": "2020-12-09T18:22:45Z",
        "merged_at": "2020-12-09T18:22:45Z",
        "body": "This adds a copy callback for module data types, in order to make\r\nmodules compatible with the new COPY command.\r\n\r\nThe callback is optional and COPY will fail for keys with data types\r\nthat do not implement it.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-11-30T12:42:43Z",
        "closed_at": "2020-11-30T21:03:54Z",
        "merged_at": "2020-11-30T21:03:53Z",
        "body": "1. when performing the and operation, if the output is 0, we can jump out of the loop.\r\n2. when performing an or operation, if the output is 0xff, we can jump out of the loop.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-11-30T09:15:24Z",
        "closed_at": "2020-12-02T11:17:26Z",
        "merged_at": "2020-12-02T11:17:26Z",
        "body": "As described in redis-benchamrk help message 'The test names are the same as the ones produced as output.', In redis-benchmark output, we can only see PING_BULK, but the cmd `redis-benchmark -t ping_bulk` is not supported. We  have to run it with ping_mbulk which is not user friendly.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 272,
        "deletions": 118,
        "changed_files": 6,
        "created_at": "2020-11-30T08:10:29Z",
        "closed_at": "2020-12-02T11:56:12Z",
        "merged_at": "2020-12-02T11:56:12Z",
        "body": "When replica diskless-load type is swapdb in cluster mode,  we don't backup keys to slots map, so we will lose keys to slots map if fail to sync, we should backup keys to slots map at first, and restore it properly when fail.\r\n\r\nIn the fist commit, I refactor cleaning up db backups, I think backups just are big dictionaries instead of main dbs, we shouldn't call emptyDbGeneric, currently we use this function to clean main dbs. Please have a look @oranagra @guybe7 I revet it if I miss somethings.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-11-29T15:20:34Z",
        "closed_at": "2020-12-12T15:31:42Z",
        "merged_at": "2020-12-12T15:31:41Z",
        "body": "Partial resolution for #6860, item 7.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-11-29T11:46:00Z",
        "closed_at": "2020-11-30T21:15:15Z",
        "merged_at": "2020-11-30T21:15:15Z",
        "body": "If master changes into replica role, it still keep old average ttl.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2020-11-28T23:30:39Z",
        "closed_at": "2020-12-03T08:12:08Z",
        "merged_at": "2020-12-03T08:12:08Z",
        "body": "In the iterator for these functions, we'll traverse the sorted sets\r\nin a reversed way so that largest elements come first. We prefer\r\nthis order because it's optimized for insertion in a skiplist, which\r\nis the destination of the elements being iterated in these functions.\r\n\r\nI didn't modify the iteration code for sets (since it isn't ordered anyway),\r\nand I also kept the function names (didn't rename to \"reverse\" or something),\r\njust added a comment to zuiInitIterator. I feel like this should be further\r\ndocumented, but I wasn't sure where to put further explanations.\r\n\r\nI just noticed that the \"ui\" in \"zuiInitIterator\" is probably \"union\" and \"inter\".\r\nMaybe we should rename all \"zui\" functions to \"zuid\"?",
        "comments": 17
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-26T11:41:02Z",
        "closed_at": "2020-12-02T10:18:20Z",
        "merged_at": null,
        "body": "add log debug header \"debugmacro.h\" in dict.c\r\n@oranagra please have a look",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-11-25T22:15:51Z",
        "closed_at": "2020-12-06T12:51:23Z",
        "merged_at": "2020-12-06T12:51:23Z",
        "body": "When client tracking is enabled signalModifiedKey can increase memory usage,\r\nthis can cause the loop in performEvictions to keep running since it was measuring\r\nthe memory usage impact of signalModifiedKey.\r\n\r\nThe section that measures the memory impact of the eviction should be just on dbDelete,\r\nexcluding keyspace notification, client tracking, and propagation to AOF and replicas.\r\n\r\nThis resolves part of the problem described in #8069\r\np.s. fix took 1 minute, test took about 3 hours to write.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2020-11-25T16:22:43Z",
        "closed_at": "2020-12-07T19:31:36Z",
        "merged_at": "2020-12-07T19:31:36Z",
        "body": "No need to remove-and-insert if it's the same rax (same consumer)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 212,
        "deletions": 25,
        "changed_files": 7,
        "created_at": "2020-11-25T15:50:51Z",
        "closed_at": "2020-12-06T11:14:19Z",
        "merged_at": "2020-12-06T11:14:19Z",
        "body": "Fix #8049",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-11-25T14:26:46Z",
        "closed_at": "2020-11-26T08:58:02Z",
        "merged_at": "2020-11-26T08:58:02Z",
        "body": "we recently did that for SETRANGE and APPEND",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-11-25T14:05:10Z",
        "closed_at": "2020-11-25T20:15:33Z",
        "merged_at": "2020-11-25T20:15:33Z",
        "body": "__ILP32__ is 32 bits ABI not necessarily x86. thus attempt to compiler\r\n wrong registers code path.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 317,
        "deletions": 90,
        "changed_files": 7,
        "created_at": "2020-11-25T13:40:22Z",
        "closed_at": "2020-12-12T00:21:06Z",
        "merged_at": "2020-12-12T00:21:05Z",
        "body": "refer: #4417\r\n\r\nSyntax:\r\n```\r\nGEOSEARCH key [FROMMEMBER member] [FROMLOC long lat] [BYRADIUS radius\r\nunit] [BYBOX width height unit] [WITHCORD] [WITHDIST] [WITHASH] [COUNT\r\ncount] [ASC|DESC]\r\n\r\nGEOSEARCHSTORE dest_key src_key [FROMMEMBER member] [FROMLOC long lat]\r\n[BYRADIUS radius unit] [BYBOX width height unit] [WITHCORD] [WITHDIST]\r\n[WITHASH] [COUNT count] [ASC|DESC] [STOREDIST]\r\n\r\n```\r\n\r\n- Add two types of CIRCULAR_TYPE and RECTANGLE_TYPE to achieve different\r\nsearches\r\n- Judge whether the point is within the rectangle, refer to:\r\ngeohashGetDistanceIfInRectangle",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 514,
        "deletions": 78,
        "changed_files": 5,
        "created_at": "2020-11-25T12:39:30Z",
        "closed_at": "2020-11-25T18:13:00Z",
        "merged_at": null,
        "body": "- [x] reuse `georadiusGeneric` wrapper, include notion of different geo shapes\r\n- [x] command logic around GEOBBOX GEOBBOX_RO GEOBBOXBYMEMBER GEOBBOXBYMEMBER_RO\r\n- [x] calculate the great circle distance with two other variations if required ( assuming same lon/lat for the bounding box check )\r\n- [ ] increaste testing on GEORADIUS GEORADIUSBYMEMBER\r\n- [ ] add testing for the new 4 commands\r\n- [ ] benchmark the simplifications benefits on existing GEO commands\r\n- [ ] benchmark new commands\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2020-11-24T12:21:08Z",
        "closed_at": "2020-11-26T11:32:49Z",
        "merged_at": null,
        "body": "It would be better to return immediately when \"No key + XX option: nothing to do.\". Because getDoubleFromObjectOrReply may take a long time when there are many elements to be splitted in an \u00a0abnormal scene.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 50,
        "deletions": 19,
        "changed_files": 1,
        "created_at": "2020-11-24T09:20:19Z",
        "closed_at": "2020-11-24T15:58:11Z",
        "merged_at": "2020-11-24T15:58:11Z",
        "body": "Seems to have gone unnoticed for a long time, because at least with\r\nglibc it will only be triggered if setenv() was called before spt_init,\r\nwhich Redis doesn't.\r\n\r\nFixes #8064.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 19,
        "changed_files": 4,
        "created_at": "2020-11-24T02:07:19Z",
        "closed_at": "2020-12-01T19:41:27Z",
        "merged_at": "2020-12-01T19:41:27Z",
        "body": "SELECT used to read the index into a `long` variable, and then pass it to a function\r\nthat takes an `int`, possibly causing an overflow before the range check.\r\n\r\nNow all these commands use better and cleaner range check, and that also results in\r\na slight change of the error response in case of an invalid database index.\r\n\r\nSELECT:\r\nin the past it would have returned either `-ERR invalid DB index` (if not a number),\r\nor `-ERR DB index is out of range` (if not between 1..16 or alike).\r\nnow it'll return either `-ERR value is out of range` (if not a number), or\r\n`-ERR value is out of range, value must between -2147483648 and 2147483647`\r\n(if not in the range for an int), or `-ERR DB index is out of range`\r\n(if not between 0..16 or alike)\r\n\r\n\r\nMOVE:\r\nin the past it would only fail with `-ERR index out of range` no matter the reason.\r\nnow return the same errors as the new ones for SELECT mentioned above.\r\n(i.e. unlike for SELECT even for a value like 17 we changed the error message)\r\n\r\nCOPY:\r\ndoesn't really matter how it behaved in the past (new command), new behavior is\r\nlike the above two.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 45,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-11-23T10:51:46Z",
        "closed_at": "2020-11-23T12:37:58Z",
        "merged_at": "2020-11-23T12:37:58Z",
        "body": "Tests basic behavior of the default, '=' and '~' for both commands.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-11-23T06:55:20Z",
        "closed_at": "2020-11-24T05:00:45Z",
        "merged_at": null,
        "body": "when we want to run redis server on system,it is possible that system does not have any directory for /var/run/redis\r\nfor this reason we add some code to check if we do not have this directory, created this directory with given permission, rerecursively",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 15451,
        "deletions": 4910,
        "changed_files": 152,
        "created_at": "2020-11-23T04:42:06Z",
        "closed_at": "2020-11-23T06:54:55Z",
        "merged_at": null,
        "body": "when we want to run redis server on system,it is possible that system does not have any directory for /var/run/redis\r\nfor this reason we add some code to check if we do not have this directory, created this directory with given permission, rerecursively",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 33986,
        "deletions": 15248,
        "changed_files": 222,
        "created_at": "2020-11-22T22:52:21Z",
        "closed_at": "2020-12-13T14:56:23Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 220,
        "deletions": 94,
        "changed_files": 14,
        "created_at": "2020-11-22T17:36:08Z",
        "closed_at": "2020-12-11T16:31:41Z",
        "merged_at": "2020-12-11T16:31:40Z",
        "body": "This adds a new `tls-client-cert-file` and `tls-client-key-file`\r\nconfiguration directives which make it possible to use different\r\ncertificates for the TLS-server and TLS-client functions of Redis.\r\n    \r\nThis is an optional directive. If it is not specified the `tls-cert-file`\r\nand `tls-key-file` directives are used for TLS client functions as well.\r\n\r\nFixes #7946",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-22T05:53:30Z",
        "closed_at": "2020-11-22T12:56:46Z",
        "merged_at": "2020-11-22T12:56:45Z",
        "body": "It should be **and** not **or**.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-11-21T21:22:12Z",
        "closed_at": "2020-11-30T13:00:17Z",
        "merged_at": "2020-11-30T13:00:17Z",
        "body": "See https://github.com/redis/redis-doc/pull/1443\r\n\r\nAlso allows nameless commands.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-11-21T19:25:40Z",
        "closed_at": "2020-11-22T12:40:39Z",
        "merged_at": "2020-11-22T12:40:39Z",
        "body": "When USE_SYSTEMD=yes is specified, try to use pkg-config to determine\r\nlibsystemd linker flags. If not found, silently fall back to simply\r\nusing \"-lsystemd\".\r\n\r\nWe now use a LIBSYSTEMD_LIBS variable so users can explicitly override\r\nit and specify their own library.\r\n\r\nIf USE_SYSTEMD is unspecified the old behavior of auto-enabling it if\r\nboth pkg-config and libsystemd are available is retained.\r\n\r\nFixes #8059 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 100,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2020-11-21T18:23:07Z",
        "closed_at": "2020-12-03T12:36:49Z",
        "merged_at": "2020-12-03T12:36:48Z",
        "body": "Adds the ability to use exclusive (open) start and end query intervals, e.g.:\r\n\r\n```\r\nredis> XRANGE vipstream (1-0 (2-0\r\n```\r\n\r\nFixes #6562",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-21T09:03:11Z",
        "closed_at": "2020-11-22T12:16:58Z",
        "merged_at": null,
        "body": "The range of dbid has already been determined in selectDb.\r\n\r\n```c\r\nint selectDb(client *c, int id) {\r\n    if (id < 0 || id >= server.dbnum)\r\n        return C_ERR;\r\n    c->db = &server.db[id];\r\n    return C_OK;\r\n}\r\n\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2020-11-20T13:03:30Z",
        "closed_at": "2020-11-22T12:12:45Z",
        "merged_at": "2020-11-22T12:12:45Z",
        "body": "If we enable diskless replication, set `repl-diskless-sync-delay` to 0, and master has non-rdb child process such as rewrite aof child, master will start to a new BGSAVE but fails immediately when replicas ask for full synchronization, and master always fails to start a new BGSAVE and disconnects with replicas until non-rdb child process exists.\r\n\r\nmaster log\r\n```\r\n18570:M 20 Nov 2020 21:00:08.968 - Accepted 127.0.0.1:60409\r\n18570:M 20 Nov 2020 21:00:08.968 * Replica 127.0.0.1:21111 asks for synchronization\r\n18570:M 20 Nov 2020 21:00:08.968 * Full resync requested by replica 127.0.0.1:21111\r\n18570:M 20 Nov 2020 21:00:08.968 * Starting BGSAVE for SYNC with target: replicas sockets\r\n18570:M 20 Nov 2020 21:00:08.968 # BGSAVE for replication failed\r\n```\r\nreplica log\r\n```\r\n18803:S 20 Nov 2020 21:00:59.270 * MASTER <-> REPLICA sync started\r\n18803:S 20 Nov 2020 21:00:59.270 * Non blocking connect for SYNC fired the event.\r\n18803:S 20 Nov 2020 21:00:59.270 * Master replied to PING, replication can continue...\r\n18803:S 20 Nov 2020 21:00:59.270 * Partial resynchronization not possible (no cached master)\r\n18803:S 20 Nov 2020 21:00:59.270 * Master does not support PSYNC or is in error state (reply: -ERR BGSAVE failed, replication can't continue)\r\n18803:S 20 Nov 2020 21:00:59.271 * Retrying with SYNC...\r\n18803:S 20 Nov 2020 21:00:59.271 # I/O error reading bulk count from MASTER: Connection reset by peer\r\n18803:S 20 Nov 2020 21:00:59.271 * Reconnecting to MASTER 127.0.0.1:21112 after failure\r\n```\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 56,
        "changed_files": 4,
        "created_at": "2020-11-19T02:19:13Z",
        "closed_at": "2020-11-24T19:40:59Z",
        "merged_at": "2020-11-24T19:40:59Z",
        "body": "1. Avoid multiple conditional judgments\r\n2. Avoid free robj->ptr",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-18T13:54:40Z",
        "closed_at": "2020-11-25T21:39:02Z",
        "merged_at": "2020-11-25T21:39:02Z",
        "body": "this metric already includes the argv bytes, like what clientsCronTrackClientsMemUsage does, but it's missing the array itself.\r\n\r\np.s. For the purpose of tracking expensive clients we don't need to include the size of the client struct and the static reply buffer in it.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-11-17T21:40:33Z",
        "closed_at": "2020-11-22T12:00:52Z",
        "merged_at": "2020-11-22T12:00:51Z",
        "body": "This is hopefully usually harmles.\r\nThe server.ready_keys will usually be empty so the code after releasing\r\nthe GIL will soon be done.\r\nThe only case where it'll actually process things is when a module\r\nreleases a client (or module) blocked on a key, by triggering this NOT\r\nfrom within a command (e.g. a timer event).\r\n\r\nThis bug was introduced in redis 6.0.9, see #7903",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-11-17T14:31:00Z",
        "closed_at": "2020-11-23T14:14:34Z",
        "merged_at": "2020-11-23T14:14:34Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 65,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-11-16T10:04:39Z",
        "closed_at": "2020-12-08T10:19:01Z",
        "merged_at": null,
        "body": "\u2026ritable dir (e.g. /etc)\r\n\r\nUsing a subdir (/etc/redis) may help but will break existing configuration during package update.\r\n\r\nSuch a change is critical, especially in \"patch\" release (6.0.9)\r\n\r\nAn alternative way is to revert unconditionally to the old way (not atomic)\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-16T06:47:41Z",
        "closed_at": "2020-11-16T14:37:15Z",
        "merged_at": "2020-11-16T14:37:15Z",
        "body": "@felipou FYI:\r\nhttps://github.com/redis/redis/runs/1403956160?check_suite_focus=true",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-16T01:23:22Z",
        "closed_at": "2020-11-20T16:13:12Z",
        "merged_at": "2020-11-20T16:13:12Z",
        "body": "backtrace can be compile time disabled.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-15T10:08:30Z",
        "closed_at": "2020-11-20T07:51:31Z",
        "merged_at": null,
        "body": "Consider the following scenario:\r\n\r\nWe have 3 masters A,B,C. A has three slaves 1,2,3. B has one slave 4. C has two slaves 5,6 (A,B,C,1,2,3,4,5,6 are all node IDs). After a while, the slave 1 fails. Then another master D joins in. D is an orphaned master. A and C have the max (two) working slaves and the slave 1 with the smallest node ID will be chosen to perform the slave migration. But the slave 1 has failed, so the migration won't work. In the above situation, the slave 2 should be chosen(a working slave and with the smallest node ID), am I right?",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 64,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-11-14T19:00:48Z",
        "closed_at": "2021-07-23T08:59:08Z",
        "merged_at": null,
        "body": "Dear maintainers,\r\n\r\nThis PR adds a fuzzer to the project with the goal of setting Redis up to be fuzzed by way of OSS-Fuzz. OSS-Fuzz is a free service run by Google that performs continuous fuzzing of important open source projects. Essentially, OSS-Fuzz will perform the fuzzing for you and then email you bug reports, coverage reports etc. All we need is a set of email addresses that will receive this information.\r\n\r\nFor cross-referencing, the PR that adds the OSS-Fuzz logic is here: https://github.com/google/oss-fuzz/pull/4643\r\n\r\nIf you are happy to integrate it then I would be happy to continue supplying more sophisticated fuzzers to the project as well.\r\n\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 17,
        "changed_files": 4,
        "created_at": "2020-11-12T14:16:04Z",
        "closed_at": "2020-11-22T11:57:57Z",
        "merged_at": "2020-11-22T11:57:57Z",
        "body": "When oom-score-adj-values is provided in the config file after\r\noom-score-adj yes, it'll take an immediate action, before\r\nreadOOMScoreAdj was acquired, resulting in an error (out of range score\r\ndue to uninitialized value. delay the reaction the real call is made by\r\nmain().\r\n\r\nsince the values are clamped to -1000..1000, and they're\r\napplied as an offset from the value at startup (which may be -1000), we\r\nneed to allow the offsets to reach to +2000 so that a value of +1000 is\r\nachievable in case the value at startup was -1000.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-11-12T11:57:56Z",
        "closed_at": "2020-11-12T13:10:38Z",
        "merged_at": null,
        "body": "If lrem has not deleted any elements, there is no need to determine if the list is empty and send a del notification",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-11-12T05:36:17Z",
        "closed_at": "2020-11-13T13:16:41Z",
        "merged_at": "2020-11-13T13:16:41Z",
        "body": "Compiling module generates some compiler warnings.\r\n\r\n```\r\ntestmodule.c: In function \u2018TestNotifications\u2019:\r\ntestmodule.c:176:41: warning: field precision specifier \u2018.*\u2019 expects argument of type \u2018int\u2019, but argument 4 has type \u2018size_t {aka long unsigned int}\u2019 [-Wformat=]\r\n         RedisModule_Log(ctx, \"warning\", \"Failed NOTIFY Test. Reason: \" #msg, ##__VA_ARGS__); \\\r\n                                         ^\r\ntestmodule.c:223:13: note: in expansion of macro \u2018FAIL\u2019\r\n             FAIL(\"Got reply '%.*s'. expected '1'\", sz, rep);\r\n             ^~~~\r\ntestmodule.c:176:41: warning: field precision specifier \u2018.*\u2019 expects argument of type \u2018int\u2019, but argument 4 has type \u2018size_t {aka long unsigned int}\u2019 [-Wformat=]\r\n         RedisModule_Log(ctx, \"warning\", \"Failed NOTIFY Test. Reason: \" #msg, ##__VA_ARGS__); \\\r\n                                         ^\r\ntestmodule.c:238:13: note: in expansion of macro \u2018FAIL\u2019\r\n             FAIL(\"Got reply '%.*s'. expected '2'\", sz, rep);\r\n             ^~~~\r\nld -o testmodule.so testmodule.xo -shared  -lc\r\ncc -I.  -W -Wall -fno-common -g -ggdb -std=c99 -O2 -fPIC -c hellocluster.c -o hellocluster.xo\r\nld -o hellocluster.so hellocluster.xo -shared  -lc\r\ncc -I.  -W -Wall -fno-common -g -ggdb -std=c99 -O2 -fPIC -c hellotimer.c -o hellotimer.xo\r\nld -o hellotimer.so hellotimer.xo -shared  -lc\r\ncc -I.  -W -Wall -fno-common -g -ggdb -std=c99 -O2 -fPIC -c hellodict.c -o hellodict.xo\r\nld -o hellodict.so hellodict.xo -shared  -lc\r\ncc -I.  -W -Wall -fno-common -g -ggdb -std=c99 -O2 -fPIC -c hellohook.c -o hellohook.xo\r\nhellohook.c: In function \u2018clientChangeCallback\u2019:\r\nhellohook.c:47:44: warning: format \u2018%llu\u2019 expects argument of type \u2018long long unsigned int\u2019, but argument 3 has type \u2018uint64_t {aka long unsigned int}\u2019 [-Wformat=]\r\n     printf(\"Client %s event for client #%llu %s:%d\\n\",\r\n                                         ~~~^\r\n                                         %lu\r\nhellohook.c:50:9:\r\n         ci->id,ci->addr,ci->port);\r\n         ~~~~~~\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-11-11T19:39:03Z",
        "closed_at": "2020-11-11T21:58:06Z",
        "merged_at": "2020-11-11T21:58:06Z",
        "body": "For AOF client, the original client argv wasn't being initialized and was leading to memory corruption. Properly initializing that now. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 162,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2020-11-11T10:14:34Z",
        "closed_at": "2020-11-11T20:57:34Z",
        "merged_at": "2020-11-11T20:57:34Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 121,
        "deletions": 75,
        "changed_files": 9,
        "created_at": "2020-11-10T19:36:35Z",
        "closed_at": "2020-12-24T03:13:13Z",
        "merged_at": "2020-12-24T03:13:13Z",
        "body": "This change updates how we manage CSC tracking tables to improve memory efficiency and reduce unnecessary messages to clients. \r\n\r\nThe main change is we now always clear out the tracking table when flushdb is called instead instead of just for flushall. The original motivation here was back when there was a 16 million slot table, it was expensive to clean up so we didn't want to free it. However, there needed to be some way to reclaim the memory used by the table, so flushall was for that purpose. Now that it's less expensive to clear the table, we should always do it, since we are already notifying all the clients to free out their local caches. \r\n\r\nThe secondary change here is that we also free client tracking tables asynchronously when requested. The tracking table can get very large, and could block the main thread for several seconds otherwise. ",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-11-09T12:20:29Z",
        "closed_at": "2020-11-10T05:48:57Z",
        "merged_at": "2020-11-10T05:48:57Z",
        "body": "In listJoin(), if list 'o' is empty, we can return without run the following code.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 74,
        "deletions": 36,
        "changed_files": 2,
        "created_at": "2020-11-09T08:11:56Z",
        "closed_at": "2020-11-17T10:52:50Z",
        "merged_at": "2020-11-17T10:52:50Z",
        "body": "* Expose \"process_supervised\" as an info field.\r\n* Log messages improvements (clarify required systemd config, report\r\n  auto-detected supervision mode, etc.)\r\n* Set server.supervised properly, so it can take precedence of\r\n  `daemonize` configuration. In the past `daemonize` settings had to be compatible with `supervised` (e.g. `no` for systemd) which could lead to misconfiguration.\r\n* Produce clear warning if systemd is detected/requested but executable\r\n  is compiled without support for it, instead of silently ignoring.\r\n* Handle systemd notification error on startup, and turn off supervised\r\n  mode if it failed.\r\n\r\nResolved #8024 ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-08T12:33:53Z",
        "closed_at": "2020-11-10T20:02:10Z",
        "merged_at": "2020-11-10T20:02:10Z",
        "body": "This adds the `--cluster-yes` option's usage, as well as prepares the usage infrastructure for future options (couldn't find additional undocumented switches for now).\r\n\r\nFixes #8028 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 5,
        "created_at": "2020-11-07T07:20:22Z",
        "closed_at": "2020-11-08T06:32:39Z",
        "merged_at": "2020-11-08T06:32:39Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2020-11-06T10:31:49Z",
        "closed_at": "2021-01-29T06:35:11Z",
        "merged_at": "2021-01-29T06:35:11Z",
        "body": "The key point is how to recover from last AOF write error, for example:\r\n\r\n1. start redis with appendonly yes, and append some write commands\r\n\r\n2. short write or something else error happen, `server.aof_last_write_status` changed to `C_ERR`, now redis doesn't accept write commands\r\n\r\n3. execute `CONFIG SET appendonly no` to avoid the above problem, now redis can accept write commands again\r\n\r\n4. disk error resolved, and execute `CONFIG SET appendonly yes` to reopen AOF, but `server.aof_last_write_status` cannot be changed to `C_OK` (if background aof rewrite run less then 1 second, it will free `server.aof_buf` and then serverCron cannot fix `aof_last_write_status`), then redis cannot accept write commands forever.\r\n\r\nThis PR use a simple way to fix it:\r\n\r\n1. just free `server.aof_buf` when stop appendonly to save memory, if error happens in `flushAppendOnlyFile(1)`, the `server.aof_buf` may contains some data which has not be written to aof, I think we can ignore it because we turn off the appendonly.\r\n\r\n2. reset fsync status after stop appendonly and call `flushAppendOnlyFile` only when `aof_state` is ON\r\n\r\n3. reset `server.last_write_status` when reopen aof to accept write commands",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-06T02:36:46Z",
        "closed_at": "2020-11-08T07:26:26Z",
        "merged_at": "2020-11-08T07:26:26Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 236,
        "deletions": 13,
        "changed_files": 16,
        "created_at": "2020-11-05T21:26:53Z",
        "closed_at": "2020-11-17T16:58:55Z",
        "merged_at": "2020-11-17T16:58:55Z",
        "body": "Blocking command should not be used with MULTI, LUA, and RM_Call. This is because, the caller, who executes the command in this context, expects a reply.\r\n\r\nToday, LUA and MULTI have a special (and different) treatment to blocking commands:\r\n* LUA \u00a0 - Most commands are marked with no-script flag which are checked when executing and command from LUA, commands that are not marked (like XREAD) verify that their blocking mode is not used inside LUA (by checking the CLIENT_LUA client flag).\r\n* MULTI - Command that is going to block, first verify that the\u00a0client is not inside multi (by checking the CLIENT_MULTI\u00a0client flag). If the client is inside multi, they return a result which is a match to the empty key with no timeout (for example blpop inside MULTI will act as lpop)\r\n\r\nFor modules that perform RM_Call with blocking command, the returned results type is REDISMODULE_REPLY_UNKNOWN and the caller can not really know what happened.\r\n\r\nDisadvantages of the current state are:\r\n* No unified approach, LUA, MULTI, and RM_Call, each has a different treatment\r\n* Module can not safely execute blocking command (and get reply or error). Though It is true that modules are not like LUA or MULTI and should be smarter not to execute blocking commands on RM_Call, sometimes you want to execute a command base on client input (for example if you create a module that provides a new scripting language like javascript or python).\r\n* While modules (on modules command) can check for REDISMODULE_CTX_FLAGS_LUA or REDISMODULE_CTX_FLAGS_MULTI to know not to block the client, there is no way to check if the command came from another module using RM_Call. So there is no way for a module to know not to block another module RM_Call execution.\r\n\r\nThe PR suggests a way to unified the treatment for blocking clients by introducing a new CLIENT_DENY_BLOCKING client flag. On LUA, MULTI, and RM_Call the new flag turned on to signify that the client should not be blocked. A blocking command\r\nverifies that the flag is turned off before blocking. If a blocking command sees that the CLIENT_DENY_BLOCKING flag is on, it's not blocking and return results which are matches to empty key with no timeout (as MULTI does today).\r\n\r\nThe new flag is checked on the following commands:\r\n* List blocking commands: BLPOP, BRPOP, BRPOPLPUSH, BLMOVE, \r\n* Zset blocking commands: BZPOPMIN, BZPOPMAX\r\n* Stream blocking commands: XREAD, XREADGROUP\r\n* SUBSCRIBE, PSUBSCRIBE\r\n* MONITOR\r\n\r\nIn addition, the new flag is turned on inside the AOF client, we do not want to block the AOF client to prevent deadlocks and commands ordering issues (and there is also an existing assert in the code that verifies it).\r\n\r\nTo keep backward compatibility on LUA, all the no-script flags on existing commands were kept untouched. In addition, a LUA special treatment on `XREAD` and `XREADGROUP` was kept.\r\n\r\nTo keep backward compatibility on MULTI (which today allows SUBSCRIBE, and PSUBSCRIBE). We added a special treatment on those commands to allow executing them on MULTI.\r\n\r\nThe only backward compatibility issue that this PR introduces is that now MONITOR is not allowed inside MULTI.\r\n\r\nTests were added to verify blocking commands are not blocking the client on LUA, MULTI, or RM_Call. Tests were added to verify the\u00a0module can check for CLIENT_DENY_BLOCKING flag.\r\n\r\nRelated issues:\r\n* https://github.com/redis/redis/issues/7991\r\n* https://github.com/redis/redis/issues/7992 - the next step from here is async RM_Call which does not turn on the new flag and allows the command to block it.\r\n\r\n@oranagra @yossigo @guybe7 @itamarhaber let me know what you think. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-05T16:55:00Z",
        "closed_at": "2020-11-08T07:16:15Z",
        "merged_at": "2020-11-08T07:16:15Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-05T16:04:34Z",
        "closed_at": "2020-11-11T10:10:55Z",
        "merged_at": "2020-11-11T10:10:55Z",
        "body": "In `module.c`, when `moduleTimerHandler` returning `AE_NOMORE`, `aeTimer` become invalid. Next time when `RM_CreateTimer` is called, we know we must create a new aeTimer directly.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-11-05T12:32:17Z",
        "closed_at": "2020-11-05T13:43:54Z",
        "merged_at": "2020-11-05T13:43:54Z",
        "body": "Bug in passing arguments, caused registers not to be printed correctly\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-11-05T11:51:27Z",
        "closed_at": "2020-11-06T08:32:45Z",
        "merged_at": null,
        "body": "Avoid creating an useless bio fsync when aof_fsync is everysec.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2020-11-04T19:01:21Z",
        "closed_at": "2020-11-05T17:58:54Z",
        "merged_at": "2020-11-05T17:58:54Z",
        "body": "This commit fixed two overflow issue for debug populate command when passing the key num and value size as a negative number. Since the createStringObject and dictExpand the second parameter are using unsigned type. ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2020-11-04T18:36:48Z",
        "closed_at": "2020-11-04T19:43:56Z",
        "merged_at": "2020-11-04T19:43:56Z",
        "body": "This solves consistent defrag test failures on Ubuntu 20.04 arm64 (Pine64 A64+ board) as well as a minor issue where solo tests were not reported with their correct file name.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 19,
        "changed_files": 5,
        "created_at": "2020-11-04T13:37:24Z",
        "closed_at": "2020-11-18T09:16:22Z",
        "merged_at": "2020-11-18T09:16:22Z",
        "body": "Bug was intoduced by #5021.\r\n\r\nBefore that commit, dbExists was used instead of\r\nlookupKeyRead (which affects access time)",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 244,
        "deletions": 20,
        "changed_files": 7,
        "created_at": "2020-11-04T13:29:49Z",
        "closed_at": "2021-09-13T07:39:12Z",
        "merged_at": "2021-09-13T07:39:11Z",
        "body": "This PR aims to solve issue #6030, about making psync possible after master reboot, at first I wrote some codes in #6034 but it's not good enough, after a discussion with @oranagra , I modify the code(about the expire problem) and open the new one, here ping @redis/core-team I'd like to have your suggestions.\r\n\r\nThe main idea is how to allow a master to load replication info from RDB file when rebooting, if master can load replication info it means that replicas may have the chance to psync with master, it can save much traffic.\r\n\r\n\r\nThe key point is we need guarantee safety and consistency, so there\r\nare two differences between master and replica:\r\n1. master would load the replication info as secondary ID and\r\n   offset, in case other masters have the same replid.\r\n2. when master loading RDB, it would propagate expired keys as DEL\r\n   command to replication backlog, then replica can receive these\r\n   commands to delete stale keys.\r\n   p.s. the expired keys when RDB loading is useful for users, so\r\n   we show it as `rdb_last_load_keys_expired` and `rdb_last_load_keys_loaded` in info persistence.\r\n\r\nMoreover, after load replication info, master should update\r\n`no_replica_time` in case loading RDB cost too long time.",
        "comments": 23
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-11-04T13:07:28Z",
        "closed_at": "2020-11-04T14:19:19Z",
        "merged_at": "2020-11-04T14:19:19Z",
        "body": "`rdbSaveSingleModuleAux()` and `rdbSaveObject()` use RedisModuleIO's \"bytes\" field for tracking written bytes even before calling `moduleInitIOContext()` which again sets \"bytes\" to zero - thus information about written bytes of the \"header\" part is lost. This return value is not used at the moment from what I have seen, nevertheless it should return correct value.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2020-11-03T09:38:14Z",
        "closed_at": "2022-03-22T08:34:01Z",
        "merged_at": "2022-03-22T08:34:01Z",
        "body": "When rewrite the config file, we need read the old config file first, and the CONFIG_MAX_LEN is 1024, so if some lines are longer than, it will generate a wrong config file, and redis cannot reboot from the new config file.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2020-11-03T06:31:06Z",
        "closed_at": "2020-11-03T15:16:12Z",
        "merged_at": "2020-11-03T15:16:12Z",
        "body": "In `redisFork()`, we don't set child pid, so `updateDictResizePolicy()` doesn't take effect, that isn't friendly for copy-on-write.\r\n\r\nHi @oranagra We may introduce this problem from this commit https://github.com/redis/redis/pull/6247/commits/56258c6b7d26b6ee44bf31b18e9aad95d0a0142a",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 74,
        "deletions": 16,
        "changed_files": 4,
        "created_at": "2020-11-02T19:27:36Z",
        "closed_at": "2020-11-10T21:50:04Z",
        "merged_at": "2020-11-10T21:50:04Z",
        "body": "Slowlog emits whatever is on the client argv, which can be rewritten for replication. Note: They are rewritten even when there is no replica, so this occurs on standalone redis instances as well. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 87,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2020-11-01T23:27:09Z",
        "closed_at": "2020-11-04T12:49:16Z",
        "merged_at": "2020-11-04T12:49:16Z",
        "body": "The following PR enables specifying the prefered ciphers list (TLS <=v1.2) or prefered ciphersuites (TLS v1.3)  to connect to redis-server. ( Added tests for both variations )\r\nHere is a sample usage:\r\n\r\nTLS <= v1.2\r\n```\r\nredis-benchmark  --cacert <...> --tls --tls-ciphers \"DEFAULT:-AES128-SHA256\" \r\n```\r\n\r\nTLS v1.3\r\n```\r\nredis-benchmark  --cacert <...> --tls --tls-ciphersuites \"TLS_AES_128_GCM_SHA256\" \r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2020-11-01T17:02:29Z",
        "closed_at": "2020-11-03T12:59:22Z",
        "merged_at": "2020-11-03T12:59:22Z",
        "body": "The reason that we want to get a full crash report on SIGABRT\r\nis that the jmalloc, when detecting a corruption, calls abort().\r\nThis will cause the Redis to exist silently without any report\r\nand without any way to analyze what happened.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-10-31T06:42:50Z",
        "closed_at": "2020-11-02T01:57:02Z",
        "merged_at": null,
        "body": "Add a command to redis, which can return the database currently used by the redis client, and call it through dbpos",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 330,
        "deletions": 34,
        "changed_files": 2,
        "created_at": "2020-10-30T06:53:46Z",
        "closed_at": "2020-11-09T20:54:48Z",
        "merged_at": "2020-11-09T20:54:48Z",
        "body": "Test support for the new map, null and push message types. Map objects are parsed as a list of lists of key value pairs.\r\nfor instance: user => john password => 123\r\n\r\nwill be parsed to the following TCL list:\r\n\r\n{{user john} {password 123}}\r\n\r\nAlso added the following tests:\r\n\r\n* Redirection still works with RESP3\r\n\r\n* Able to use a RESP3 client as a redirection client\r\n\r\n* No duplicate invalidation messages when turning BCAST mode on after normal tracking\r\n\r\n* Server is able to evacuate enough keys when num of keys surpasses limit by more than defined initial effort\r\n\r\n* Different clients using different protocols can track the same key\r\n\r\n* OPTOUT tests\r\n\r\n* OPTIN tests\r\n\r\n* Clients can redirect to the same connection\r\n\r\n* tracking-redir-broken test\r\n\r\n* HELLO 3 checks\r\n\r\n* Invalidation messages still work when using RESP3, with and without redirection\r\n\r\n* Switching to RESP3 doesn't disturb previous tracked keys\r\n\r\n* Tracking info is correct\r\n\r\n* Flushall and flushdb produce invalidation messages\r\n\r\n**These tests achieve 100% line coverage for tracking.c using lcov.**",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 23,
        "changed_files": 2,
        "created_at": "2020-10-30T06:18:37Z",
        "closed_at": "2020-11-12T08:55:52Z",
        "merged_at": "2020-11-12T08:55:52Z",
        "body": "BLPOP when there are elements in the list works in the same way as LPOP\ndoes. Due to this they also does the same repeatitive action and logic\nfor the same is written at two different places. This is a bad code\npractice as the one needs the context to change the BLPOP list pop code\nas well when the LPOP code gets changed.\n\nSeparated the generic logic from LPOP to a function that is being used\nby the BLPOP code as well.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-10-30T05:24:41Z",
        "closed_at": "2020-11-11T06:22:18Z",
        "merged_at": "2020-11-11T06:22:18Z",
        "body": "This commit provides more tracking information in client list output. It may useful for showing current client tracking modes.",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 45,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2020-10-30T05:19:57Z",
        "closed_at": "2020-11-04T08:00:19Z",
        "merged_at": "2020-11-04T08:00:19Z",
        "body": "This commit fixes https://github.com/redis/redis/issues/6489 by providing two different flags --cluster-from-user, --cluster-from-pass and --cluster-askpass for source node authentication. Also for cluster node authentication, using existing --user and --pass flag.\r\n\r\nExample:\r\n./redis-cli --cluster import 127.0.0.1:7000 --cluster-from 127.0.0.1:6379 --pass 1234 --user default --cluster-from-user default --cluster-from-pass 123456 \r\n\r\n./redis-cli --cluster import 127.0.0.1:7000 --cluster-from 127.0.0.1:6379 --askpass --cluster-from-user default --cluster-from-askpass \r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 442,
        "deletions": 40,
        "changed_files": 11,
        "created_at": "2020-10-30T01:12:00Z",
        "closed_at": "2020-12-01T12:21:40Z",
        "merged_at": "2020-12-01T12:21:40Z",
        "body": "Fixes #7923.\r\n\r\nThis PR appropriates the special `&` symbol (because `@` and `*` are taken), followed by a literal value or pattern for describing the Pub/Sub patterns that an ACL user can interact with. It is similar to the existing key patterns mechanism in function (additive) and implementation (copy-pasta). It also adds the `allchannels` and `resetchannels` ACL keywords, naturally.\r\n\r\nThe default user is given `allchannels` permissions, whereas new users get whatever is defined by the 'acl-pubsub-default' configuration directive. For backward compatibility in 6.2, the default of this directive is `allchannels` but this is likely to be changed to `resetchannels` in the next major version for stronger default security settings.\r\n\r\nUnless `allchannels` is set for the user, channel access  permissions are checked as follows :\r\n* Calls to both `PUBLISH` and `SUBSCRIBE` will fail unless a pattern matching the argumentative channel name(s) exists for the user.\r\n* Calls to `PSUBSCRIBE` will fail unless the pattern(s) provided as an argument literally exist(s) in the user's list.\r\n* Such failures are logged to the ACL log.\r\n\r\nRuntime changes to channel permissions for a user with existing subscribing clients cause said clients to disconnect unless the new permissions permit the connections to continue. Note, however, that `PSUBSCRIBE`rs' patterns are matched literally, so given the change `bar:*` -> `b*`, pattern subscribers to `bar:*` **will be disconnected**.\r\n\r\nNotes/questions:\r\n* `UNSUBSCRIBE`, `PUNSUBSCRIBE`  and `PUBSUB` remain unprotected due to lack of reasons for touching them.\r\n\r\nTo do:\r\n* [ ] Decide whether to add/use `nochannels` and `nokeys` as aliases/instead\r\n",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-10-29T10:45:17Z",
        "closed_at": "2020-11-02T16:18:43Z",
        "merged_at": "2020-11-02T16:18:42Z",
        "body": "The bug occurs when 'callback' re-registers itself to a point\r\nin the future and the execution time in non-negligible:\r\n'now' refers to time BEFORE callback was executed and is used\r\nto calculate 'next_period'.\r\nWe must get the actual current time when calculating 'next_period'",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 119,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2020-10-28T15:51:46Z",
        "closed_at": "2020-11-05T08:51:27Z",
        "merged_at": "2020-11-05T08:51:27Z",
        "body": "Perform full reset of all client connection states, is if the client was\r\ndisconnected and re-connected. This affects:\r\n\r\n* `MULTI` state\r\n* Watched keys\r\n* `MONITOR` mode\r\n* Pub/Sub subscription\r\n* ACL/Authenticated state\r\n* Client tracking state\r\n* Cluster read-only/asking state\r\n* RESP version\r\n* Selected database\r\n* `CLIENT REPLY` state\r\n\r\nThe response is +RESET to make it easily distinguishable from other\r\nresponses.\r\n\r\nResolves #5571 ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 9,
        "changed_files": 3,
        "created_at": "2020-10-28T11:01:32Z",
        "closed_at": "2020-11-05T09:46:17Z",
        "merged_at": "2020-11-05T09:46:17Z",
        "body": "Expose new `loading_rdb_used_mem` showing the used memory of the server\r\nthat saved the RDB file we're currently using.\r\nThis is useful in diskless replication when the total size of the rdb is\r\nunkown, and can be used as a rought estimation of progres.\r\n\r\nUse that new field to calculate the \"user friendly\"\r\n`loading_loaded_perc` and `loading_eta_seconds`.\r\n\r\nExpose `master_sync_read_bytes` and `master_sync_total_bytes` to complement\r\non the existing `master_sync_left_bytes` (which cannot be used on its own\r\nto calculate progress).\r\n\r\nAdd \"user friendly\" field for `master_sync_perc`",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-10-28T09:08:45Z",
        "closed_at": "2020-11-04T07:53:44Z",
        "merged_at": "2020-11-04T07:53:44Z",
        "body": "Few config settings are also reflected by the INFO command.\r\nthese are mainly ones that are important for either an instant view of\r\nthe server status (to compare a metric to it's limit config),\r\nImportant configurations that are necessary in the crash log (which\r\ncurrently doesn't print the config),\r\nAnd things that are important for monitoring solutions (such as\r\nPrometheus), which rely on INFO to collect their data.\r\n\r\nAdd cluster_connections to INFO CLIENTS too,\r\nThis makes it possible to be combined together with connected_clients\r\nand connected_slaves and be matched against maxclients",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-10-28T07:41:10Z",
        "closed_at": "2020-11-25T16:23:19Z",
        "merged_at": null,
        "body": "Do not claim the entry if claiming consumer is identical\r\nto the consumer that already has the entry in its PEL\r\n(It affects delivery_count for no reason as the entry\r\nstays with the same consumer)",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 68,
        "deletions": 29,
        "changed_files": 1,
        "created_at": "2020-10-28T04:06:21Z",
        "closed_at": "2020-10-28T10:35:29Z",
        "merged_at": "2020-10-28T10:35:29Z",
        "body": "This commit refactored part of aof rewrite code in order to avoid potentical memory leaks when returning with error.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2020-10-28T03:29:05Z",
        "closed_at": "2020-10-28T06:51:35Z",
        "merged_at": "2020-10-28T06:51:35Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-10-28T00:54:18Z",
        "closed_at": "2020-10-28T14:00:51Z",
        "merged_at": null,
        "body": "In current implementation, slowlog get command use setDeferredArrayLen to set reply length at the end. However, we should know the length of the reply after we parse the user flag.   By setting array length at beginning will avoid the redundant potentical call for malloc and memcpy operation when we use setDeferredArrayLen to set the array actual length.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 347,
        "deletions": 59,
        "changed_files": 5,
        "created_at": "2020-10-27T17:18:31Z",
        "closed_at": "2021-01-06T08:34:28Z",
        "merged_at": "2021-01-06T08:34:28Z",
        "body": "Partial fix of #7649\r\n\r\nNew command: `XAUTOCLAIM <key> <group> <consumer> <min-idle-time> <start> [COUNT <count>] [JUSTID]`\r\n\r\nThe purpose is to claim entries from a stale consumer without the usual XPENDING+XCLAIM combo which takes two round trips.\r\nThe syntax for XAUTOCLAIM is similar to scan: A cursor is returned (streamID) by each call and should be used as `start` for the next call. 0-0 means the scan is complete.\r\n\r\nThis PR extends the deferred reply mechanism for any bulk string (not just counts)\r\n\r\nThis PR carries some unrelated test code changes:\r\n- Renames the term \"client\" into \"consumer\" in the stream-cgroups test\r\n- And also changes DEBUG SLEEP into \"after\"\r\n\r\nNote: If we had a idle-time rax we wouldn't need the SCAN-like behavior: User just specifies COUNT, if the number of returned entries is <COUNT that means there are no more entries to claim. The downsize of maintaining another idle-time rax is ~50% memory overhead for streams with cgroups",
        "comments": 36
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2020-10-27T13:35:10Z",
        "closed_at": "2020-11-29T10:08:48Z",
        "merged_at": "2020-11-29T10:08:48Z",
        "body": "Partial fix of #7649",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2020-10-27T05:07:13Z",
        "closed_at": "2020-10-27T18:15:15Z",
        "merged_at": "2020-10-27T18:15:15Z",
        "body": "When all the work is just adding logs, we could pull\r\nthe condition out so as to use less CPU time when\r\nloglevel is bigger than LL_VERBOSE.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 71,
        "deletions": 31,
        "changed_files": 2,
        "created_at": "2020-10-27T04:36:28Z",
        "closed_at": "2020-10-28T17:01:21Z",
        "merged_at": "2020-10-28T17:01:21Z",
        "body": "Built a slight better algorithm that picks categories that are closer to optimal since there were still some basic cases that were producing non-obvious results. Was impacting someone else's PR. \r\n\r\nNote, we do need to check both maxsame and mindiff for it to pass the test cases, otherwise several cases fail. e.g.\r\n\r\n```\r\nExpected '+@all -@slow' to be equal to '+@all -@geo -@scripting -@hyperloglog -@pubsub -@bitmap -@admin -@blocking -@set -@slow +smove +bzpopmax +bitfield_ro +spop +pfadd +smismember +sadd +sismember +srem +bzpopmin +publish +scard +getbit +lastsave' (context: type source line 147 file /Users/matolson/git/redis/tests/unit/acl.tcl cmd {assert_equal \"+@all -@$category\" $cmdstr} proc ::test)\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2604,
        "deletions": 834,
        "changed_files": 122,
        "created_at": "2020-10-26T13:37:08Z",
        "closed_at": "2020-10-27T07:12:02Z",
        "merged_at": "2020-10-27T07:12:02Z",
        "body": "Upgrade urgency: SECURITY if you use an affected platform (see below).\r\n                 Otherwise the upgrade urgency is MODERATE.\r\n\r\nThis release fixes a potential heap overflow when using a heap allocator other\r\nthan jemalloc or glibc's malloc. See:\r\nhttps://github.com/redis/redis/pull/7963\r\n\r\nOther fixes in this release:\r\n\r\nNew:\r\n* Memory reporting of clients argv (#7874)\r\n* Add redis-cli control on raw format line delimiter (#7841)\r\n* Add redis-cli support for rediss:// -u prefix (#7900)\r\n* Get rss size support for NetBSD and DragonFlyBSD\r\n\r\nBehavior changes:\r\n* WATCH no longer ignores keys which have expired for MULTI/EXEC (#7920)\r\n* Correct OBJECT ENCODING response for stream type (#7797)\r\n* Allow blocked XREAD on a cluster replica (#7881)\r\n* TLS: Do not require CA config if not used (#7862)\r\n\r\nBug fixes:\r\n* INFO report real peak memory (before eviction) (#7894)\r\n* Allow requirepass config to clear the password (#7899)\r\n* Fix config rewrite file handling to make it really atomic (#7824)\r\n* Fix excessive categories being displayed from ACLs (#7889)\r\n* Add fsync in replica when full RDB payload was received (#7839)\r\n* Don't write replies to socket when output buffer limit reached (#7202)\r\n* Fix redis-check-rdb support for modules aux data (#7826)\r\n* Other smaller bug fixes\r\n\r\nModules API:\r\n* Add APIs for version and compatibility checks (#7865)\r\n* Add RM_GetClientCertificate (#7866)\r\n* Add RM_GetDetachedThreadSafeContext (#7886)\r\n* Add RM_GetCommandKeys (#7884)\r\n* Add Swapdb Module Event (#7804)\r\n* RM_GetContextFlags provides indication of being in a fork child (#7783)\r\n* RM_GetContextFlags document missing flags: MULTI_DIRTY, IS_CHILD (#7821)\r\n* Expose real client on connection events (#7867)\r\n* Minor improvements to module blocked on keys (#7903)\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 412,
        "deletions": 63,
        "changed_files": 24,
        "created_at": "2020-10-26T13:21:39Z",
        "closed_at": "2020-10-27T06:49:23Z",
        "merged_at": "2020-10-27T06:49:22Z",
        "body": "Upgrade urgency: SECURITY if you use an affected platform (see below).\r\n                 Otherwise the upgrade urgency is MODERATE.\r\n\r\nThis release fixes a potential heap overflow when using a heap allocator other\r\nthan jemalloc or glibc's malloc. See:\r\nhttps://github.com/redis/redis/pull/7963\r\n\r\nOther fixes in this release:\r\n\r\n* Avoid case of Lua scripts being consistently aborted due to OOM\r\n* XPENDING will not update consumer's seen-time\r\n* A blocked XREADGROUP didn't propagated the XSETID to replicas / AOF\r\n* UNLINK support for streams\r\n* RESTORE ABSTTL won't store expired keys into the DB\r\n* Hide AUTH from MONITOR\r\n* Cluster: reduce spurious PFAIL/FAIL states upon delayed PONG receival\r\n* Cluster: Fix case of clusters mixing accidentally by gossip\r\n* Cluster: Allow blocked XREAD on a cluster replica\r\n* Cluster: Optimize memory usage CLUSTER SLOTS command\r\n* RedisModule_ValueLength support for stream data type\r\n* Minor fixes in redis-check-rdb and redis-cli\r\n* Fix redis-check-rdb support for modules aux data\r\n* Add fsync in replica when full RDB payload was received",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 352,
        "deletions": 55,
        "changed_files": 6,
        "created_at": "2020-10-25T17:53:13Z",
        "closed_at": "2020-11-15T12:14:26Z",
        "merged_at": "2020-11-15T12:14:26Z",
        "body": "Related to issue #446.\r\n\r\nSimply adds the new commands ZDIFF and ZDIFFSTORE. For now, there are 3 things missing:\r\n- More tests (for now I just used the two basic tests from the reference PR)\r\n- Add documentation with a PR for redis-doc\r\n- Include \"algorithm 2\", as in the SDIFF command. I've started something [here](https://github.com/felipou/redis/commit/2b6d29904f42e671685c355ad8ecfdb3a9034bd7). It works, but it doesn't look very good to me right now, it seems rather \"fragile\". But if this is wanted, I can add it to this PR and work on it.\r\n\r\nSide question: I noticed that the ZINTER and ZUNION commands are missing from the help.c file (only the \"STORE\" version is there), is that as expected, or should I fix it (here or in another PR)? I added both the ZDIFF and ZDIFFSTORE here.",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 337,
        "deletions": 164,
        "changed_files": 6,
        "created_at": "2020-10-25T15:54:22Z",
        "closed_at": "2020-10-28T06:00:55Z",
        "merged_at": "2020-10-28T06:00:55Z",
        "body": "This PR kicks off TLS support for redis-benchmark. When possible, it uses the same code as redis-cli namely `benchmarkSecureConnection` and `writeConn`.\r\nAdditionally it adds the tls variation to the redis-benchmark tests. Both secure/unsecure tests working as expected:\r\n\r\n### secure test variation\r\n```\r\n$ tclsh tests/test_helper.tcl --tls --single integration/redis-benchmark \r\nCleanup: may take some time... OK\r\nStarting test server at port 11111\r\n[ready]: 673644\r\nTesting integration/redis-benchmark\r\n[ready]: 673645\r\n[ready]: 673646\r\n[ready]: 673650\r\n[ready]: 673647\r\n[ready]: 673649\r\n[ready]: 673648\r\n[ready]: 673654\r\n[ready]: 673652\r\n[ready]: 673653\r\n[ready]: 673651\r\n[ready]: 673656\r\n[ready]: 673655\r\n[ready]: 673657\r\n[ready]: 673658\r\n[ready]: 673659\r\n[ok]: benchmark: set,get\r\n[ok]: benchmark: full test suite\r\n[ok]: benchmark: multi-thread set,get\r\n[ok]: benchmark: pipelined full set,get\r\n[ok]: benchmark: arbitrary command\r\n[ok]: benchmark: keyspace length\r\n[1/1 done]: integration/redis-benchmark (1 seconds)\r\n\r\n                   The End\r\n\r\nExecution time of different units:\r\n  1 seconds - integration/redis-benchmark\r\n\r\n\\o/ All tests passed without errors!\r\n\r\nCleanup: may take some time... OK\r\n```\r\n### unsecure test variation\r\n```\r\n$ tclsh tests/test_helper.tcl --single integration/redis-benchmark \r\nCleanup: may take some time... OK\r\nStarting test server at port 11111\r\n[ready]: 673772\r\nTesting integration/redis-benchmark\r\n[ready]: 673771\r\n[ready]: 673773\r\n[ready]: 673774\r\n[ready]: 673775\r\n[ready]: 673776\r\n[ready]: 673777\r\n[ready]: 673778\r\n[ready]: 673779\r\n[ready]: 673780\r\n[ready]: 673781\r\n[ready]: 673782\r\n[ready]: 673783\r\n[ready]: 673784\r\n[ready]: 673785\r\n[ready]: 673786\r\n[ok]: benchmark: set,get\r\n[ok]: benchmark: full test suite\r\n[ok]: benchmark: multi-thread set,get\r\n[ok]: benchmark: pipelined full set,get\r\n[ok]: benchmark: arbitrary command\r\n[ok]: benchmark: keyspace length\r\n[1/1 done]: integration/redis-benchmark (1 seconds)\r\n\r\n                   The End\r\n\r\nExecution time of different units:\r\n  1 seconds - integration/redis-benchmark\r\n\r\n\\o/ All tests passed without errors!\r\n\r\nCleanup: may take some time... OK\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2020-10-25T15:23:29Z",
        "closed_at": "2020-10-27T14:36:01Z",
        "merged_at": "2020-10-27T14:36:01Z",
        "body": "Turns out this was broken since version 4.0 when we added sds size\r\nclasses.\r\nThe cluster code uses sds for the receive buffer, and then casts it to a\r\nstruct and accesses a 64 bit variable.\r\nThis commit replaces the use of sds with a simple reallocated buffer.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 45,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-10-25T12:06:49Z",
        "closed_at": "2020-11-03T12:56:57Z",
        "merged_at": "2020-11-03T12:56:57Z",
        "body": "- Generates a more backwards compatible command stream\r\n- Slightly more efficient executaion in replica/AOF\r\n- Add a test for coverage",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-10-25T07:25:17Z",
        "closed_at": "2020-10-26T09:55:24Z",
        "merged_at": "2020-10-26T09:55:24Z",
        "body": "The tests sometimes fail to find a log message.\r\nRecently i added a print that shows the log files that are searched\r\nand it shows that the message was in deed there.\r\nThe only reason i can't think of for this seach to fail, is we we\r\nhappened to read an incomplete line, which didn't match our pattern and\r\nthen on the next iteration we would continue reading from the line after\r\nit.\r\n\r\nThe fix is to always re-evaluation the previous line.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 127,
        "deletions": 37,
        "changed_files": 18,
        "created_at": "2020-10-24T15:41:54Z",
        "closed_at": "2020-12-06T09:53:05Z",
        "merged_at": "2020-12-06T09:53:05Z",
        "body": "As we know, redis may reject user's requests or evict some keys if used memory is over maxmemory. Dictionaries expanding may make things worse, some big dictionaries, such as main db and expires dict, may eat huge memory  at once for allocating a new big hash table and be far more than maxmemory after expanding. There are related issues: #4213 #4583\r\n\r\nMore details, when expand dict in redis, we will allocate a new big ht[1] that generally is double of ht[0], The size of ht[1] will be very big if ht[0] already is big. For db dict, if we have more than 64 million keys, we need to cost 1GB for ht[1] when dict expands. \r\n\r\nIf the sum of used memory and new hash table of dict needed exceeds maxmemory, we shouldn't allow the dict to expand. Because, if we enable keys eviction, we still couldn't add much more keys after eviction and rehashing, what's worse, redis will keep less keys when redis only remains a little memory for storing new hash table instead of users' data. Moreover users can't write data in redis if disable keys eviction.\r\n\r\nWhat this commit changed ?\r\n\r\n- Add a new member function `expandAllowed` for  dict type, it provide a way for caller to allow expand or not. We expose two parameters for this function: more memory needed for expanding and dict current load factor, users can implement a function to make a decision by them.\r\n- For main db dict and expires dict type, these dictionaries may be very big and cost huge memory for expanding, so we implement a judgement function: we can stop dict to expand provisionally if used memory will be over maxmemory after dict expands, but to guarantee the performance of redis, we still allow dict to expand if dict load factor exceeds the safe load factor.\r\n- Add test cases to verify we don't allow main db to expand when left memory is not enough, so that avoid keys eviction.\r\n\r\nOther changes:\r\n- For new hash table size when expand.  Before this commit, the size is that double used of dict and later _dictNextPower.  Actually we aim to control a dict load factor between 0.5 and 1.0. Now we replace `*2` with `+1`, since the first check is that `used >= size`, the outcome of before will usually be the same as _dictNextPower(used+1). The only case where it'll differ is when dict_can_resize is false during fork, so that later the _dictNextPower(used*2) will cause the dict to jump to `*4` (i.e. _dictNextPower(`1025*2`) will return 4096).\r\n- Fix rehash test cases due to changing algorithm of new hash table size when expand.",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 608,
        "deletions": 0,
        "changed_files": 10,
        "created_at": "2020-10-24T07:22:59Z",
        "closed_at": "2020-11-17T10:03:06Z",
        "merged_at": "2020-11-17T10:03:05Z",
        "body": "I'm not used to using git and I'm not familiar with how to do PR, so I'm sorry if I made a mistake.\r\n\r\nSyntax:\r\n\r\n`COPY <key> <new-key> [DB <dest-db>] [REPLACE]`\r\n\r\nsee #6599 \r\n\r\nCo-authored-by: tmgauss\r\n\r\nI'm not very good at English, so there may be something wrong with the grammar of the comments in the code, etc. I'm sorry, but please check.",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-10-23T18:19:00Z",
        "closed_at": "2020-10-28T13:05:01Z",
        "merged_at": "2020-10-28T13:05:01Z",
        "body": "This commit adds acl related configuration into sentinel.conf. I mainly copied from redis.conf but with several changes:\r\n1. Add the important note part.\r\n2. Refactored the example for command categories and commands since sentinel uses a different command table and the example commands should be different.\r\n\r\n@oranagra @itamarhaber please review this and let me know if there is any other changes i should make.. Thanks!",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2020-10-23T01:48:24Z",
        "closed_at": "2020-10-27T06:14:00Z",
        "merged_at": "2020-10-27T06:14:00Z",
        "body": "This commit deals with manual failover as well as non-manual failover.\r\n\r\nWe did tests with manual failover as follows:\r\n1, Setup redis cluster which holds 16 partions, each having only\r\n   1 corresponding replica.\r\n2, Write a batch of data to redis cluster and make sure the redis is doing\r\n   a active expire in serverCron.\r\n3, Do a manual failover sequentially to each partions with a time interval\r\n   of 3 minutes.\r\n4, Collect logs and do some computaiton work.\r\n\r\nThe result:\r\ncase    avgTime    maxTime    minTime\r\nC1      95.8ms\t   227ms      25ms\r\nC2      47.9ms     96ms       12ms\r\nC3      12.6ms     27ms       7ms\r\n\r\nExplanation\r\ncase C1: All nodes use the version before optimization\r\ncase C2: Masters use the elder version while replicas use the optimized version\r\ncase C3: All nodes use the optimized version\r\nfailover time: The time between when replica got a `manual failover request` and\r\n               when it `won the failover election`.\r\navgTime: average failover time\r\nmaxTime: maximum failover time\r\nminTime: mimimum failover time\r\nms: millisecond\r\n\r\nCo-authored-by: chendq8 <c.d_q@163.com>\r\n\r\nFixes #7944",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 160,
        "deletions": 4,
        "changed_files": 5,
        "created_at": "2020-10-22T19:03:04Z",
        "closed_at": "2020-10-26T06:05:00Z",
        "merged_at": "2020-10-26T06:05:00Z",
        "body": "## added tests\r\n\r\nThis PR is a maintenance one for redis-benchmark, adding the following tests for standalone redis:\r\n- full test suite\r\n- multi-thread full test suite\r\n- pipelined commands full test suite\r\n- arbitrary command\r\n\r\n## added --version\r\nApart from it, and to make it easier to reference redis-benchmark version it applies the same version format as seen on redis-cli. \r\n```\r\n$ redis-benchmark --version\r\nredis-benchmark 255.255.255 (git:7223d1d0)\r\n```",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-10-22T14:39:52Z",
        "closed_at": "2020-10-22T16:47:33Z",
        "merged_at": "2020-10-22T16:47:33Z",
        "body": "In current sentinel code, if we do sentinel flushconfig, some redundant configuration for RDB save will be added in sentinel conf, for example:\r\nrequirepass 1234\r\nsentinel myid 4cc73269c70ebf64cc8fb18a1a37898044c30463\r\nsentinel deny-scripts-reconfig yes\r\n#Generated by CONFIG REWRITE\r\nprotected-mode no\r\nport 26379\r\nsave 3600 1\r\nsave 300 100\r\nsave 60 10000\r\n\r\nAlthough this configuration has no effect in Sentinel mode, it may make user confused and make sentinel configuration not clean enough.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-10-21T22:30:32Z",
        "closed_at": "2020-10-25T08:05:40Z",
        "merged_at": "2020-10-25T08:05:40Z",
        "body": "",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-10-21T15:35:04Z",
        "closed_at": "2020-10-25T22:37:58Z",
        "merged_at": "2020-10-25T22:37:58Z",
        "body": "This one-liner provides the below and brings Sentinel en par with the core server's introspectability abilities. Furthermore, given #7888, one needs the ability to actually review the commands (and their respective categories) for administrative purposes.\r\n\r\n```\r\n127.0.0.1:26379> COMMAND\r\n 1) 1) \"command\"\r\n    2) (integer) -1\r\n    3) 1) random\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @slow\r\n       2) @connection\r\n 2) 1) \"shutdown\"\r\n    2) (integer) -1\r\n    3) 1) admin\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @admin\r\n       2) @slow\r\n       3) @dangerous\r\n 3) 1) \"role\"\r\n    2) (integer) 1\r\n    3) 1) readonly\r\n       2) fast\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @read\r\n       2) @fast\r\n       3) @dangerous\r\n 4) 1) \"psubscribe\"\r\n    2) (integer) -2\r\n    3) 1) pubsub\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @pubsub\r\n       2) @slow\r\n 5) 1) \"subscribe\"\r\n    2) (integer) -2\r\n    3) 1) pubsub\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @pubsub\r\n       2) @slow\r\n 6) 1) \"sentinel\"\r\n    2) (integer) -2\r\n    3) 1) admin\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @admin\r\n       2) @slow\r\n       3) @dangerous\r\n 7) 1) \"auth\"\r\n    2) (integer) -2\r\n    3) 1) fast\r\n       2) no_auth\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @fast\r\n       2) @connection\r\n 8) 1) \"info\"\r\n    2) (integer) -1\r\n    3) 1) random\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @slow\r\n       2) @dangerous\r\n 9) 1) \"publish\"\r\n    2) (integer) 3\r\n    3) 1) pubsub\r\n       2) fast\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @pubsub\r\n       2) @fast\r\n10) 1) \"punsubscribe\"\r\n    2) (integer) -1\r\n    3) 1) pubsub\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @pubsub\r\n       2) @slow\r\n11) 1) \"acl\"\r\n    2) (integer) -2\r\n    3) 1) admin\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @admin\r\n       2) @slow\r\n       3) @dangerous\r\n12) 1) \"unsubscribe\"\r\n    2) (integer) -1\r\n    3) 1) pubsub\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @pubsub\r\n       2) @slow\r\n13) 1) \"ping\"\r\n    2) (integer) 1\r\n    3) 1) fast\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @fast\r\n       2) @connection\r\n14) 1) \"hello\"\r\n    2) (integer) -2\r\n    3) 1) fast\r\n       2) no_auth\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @fast\r\n       2) @connection\r\n15) 1) \"client\"\r\n    2) (integer) -2\r\n    3) 1) admin\r\n       2) random\r\n    4) (integer) 0\r\n    5) (integer) 0\r\n    6) (integer) 0\r\n    7) 1) @admin\r\n       2) @slow\r\n       3) @dangerous\r\n       4) @connection\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 10,
        "changed_files": 5,
        "created_at": "2020-10-21T15:30:44Z",
        "closed_at": "2020-10-22T08:10:54Z",
        "merged_at": "2020-10-22T08:10:54Z",
        "body": "Useful for running tests on systems which may be way slower than usual.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-10-21T11:49:24Z",
        "closed_at": "2020-10-22T08:34:55Z",
        "merged_at": "2020-10-22T08:34:55Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 39,
        "deletions": 50,
        "changed_files": 3,
        "created_at": "2020-10-21T08:20:59Z",
        "closed_at": "2020-10-28T19:09:16Z",
        "merged_at": "2020-10-28T19:09:16Z",
        "body": "Background:\r\n#3467 (redis 4.0.0), started ignoring ENOPROTOOPT, but did that only for\r\nthe default bind (in case bind config wasn't explicitly set).\r\n#5598 (redis 5.0.3), added that for bind addresses explicitly set\r\n(following bug reports in debian for redis 4.0.9 and 5.0.1), it\r\nalso ignored a bunch of other errors like EPROTONOSUPPORT which was\r\nrequested in #3894, and also added EADDRNOTAVAIL (wasn't clear why).\r\n\r\nThis (ignoring EADDRNOTAVAIL) makes redis start successfully, even if a\r\ncertain network interface isn't up yet , in which case we rather redis\r\nfail and will be re-tried when the NIC is up, see #7933.\r\n\r\nHowever, it turns out that when IPv6 is disabled (supported but unused),\r\nthe error we're getting is EADDRNOTAVAIL. and in many systems the\r\ndefault config file tries to bind to localhost for both v4 and v6 and\r\nwould like to silently ignore the error on v6 if disabled.\r\nThis means that we sometimes want to ignore EADDRNOTAVAIL and othertimes\r\nwe wanna fail.\r\n\r\nSo this commit changes these main things:\r\n1. Ignore all the errors we ignore for both explicitly requested bind\r\n   address and a default implicit one.\r\n2. Add a '-' prefix to allow EADDRNOTAVAIL be ignored (by default that's\r\n   different than the previous behavior).\r\n3. Restructure that function in a more readable and maintainable way see\r\n   below.\r\n4. Make the default behavior of listening to all achivable by setting\r\n  a bind config directive to * (previously only possible by omitting\r\n  it)\r\n5. document everything.\r\n\r\nThe old structure of this function was that even if there are no bind\r\naddresses requested, the loop that runs though the bind addresses runs\r\nat least once anyway!\r\nIn that one iteration of the loop it binds to both v4 and v6 addresses,\r\nhandles errors for each of them separately, and then eventually at the\r\nif-else chain, handles the error of the last bind attempt again!\r\nThis was very hard to read and very error prone to maintain, instead now\r\nwhen the bind info is missing we create one with two entries, and run\r\nthe simple loop twice.\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2020-10-20T00:30:20Z",
        "closed_at": "2020-10-27T04:46:51Z",
        "merged_at": "2020-10-27T04:46:51Z",
        "body": "Hopefully we never see a new git commit that is, \"fixing warnings\".\r\n\r\nJemalloc on centos 6 and old ubuntu are excluded since they already have warnings that appear to be bugs, and not real issues. There is also a 32 bit warning that was fixed by Oran in another PR: https://github.com/redis/redis/pull/7926",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 24,
        "changed_files": 1,
        "created_at": "2020-10-19T23:50:11Z",
        "closed_at": "2020-10-20T16:52:06Z",
        "merged_at": "2020-10-20T16:52:05Z",
        "body": "Following the issue described in #7930 , this PR fixes it by only including the hash tag placeholder when `--cluster` flag is passed. \r\nBellow is a quick example of the impact on a Redis Enterprise POC setup with 4 master shards. \r\n\r\n## current load pattern of redis-benchmark\r\n![image](https://user-images.githubusercontent.com/5832149/96523545-e0f7bf00-126d-11eb-8981-b1a8d78ce952.png)\r\n\r\n## fixed load pattern of redis-benchmark\r\n![image](https://user-images.githubusercontent.com/5832149/96523510-d5a49380-126d-11eb-91aa-80f55d55cc7a.png)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-10-18T18:29:53Z",
        "closed_at": "2020-10-20T06:12:25Z",
        "merged_at": "2020-10-20T06:12:25Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2020-10-18T10:51:59Z",
        "closed_at": "2020-10-18T12:02:05Z",
        "merged_at": null,
        "body": "Just use a temporary variable instead.\r\n\r\nSigned-off-by: Hanif Bin Ariffin <hanif.ariffin.4326@gmail.com>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-10-17T20:28:41Z",
        "closed_at": "2020-10-18T05:14:46Z",
        "merged_at": "2020-10-18T05:14:46Z",
        "body": "Reference the correct \"case\", case 4, in the comment explaining the need\r\nfor case 3, when the number of request items is too close to the\r\ncardinality of the set. Case 4 is indeed the \"natural approach\"\r\nreferenced earlier in that sentence.\r\n\r\n---\r\n\r\nPS: I want to take advantage of this PR for really thanking all the contributors for the quality of code throughout the codebase, it is extremely easy to read and comments, such as the one updated in this PR, make it really easy to browse through the implementation. Kudos!",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-10-17T05:28:02Z",
        "closed_at": "2020-10-18T05:11:18Z",
        "merged_at": "2020-10-18T05:11:18Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-10-16T06:23:12Z",
        "closed_at": "2020-10-22T09:57:47Z",
        "merged_at": "2020-10-22T09:57:46Z",
        "body": "WATCH no longer ignores keys which have expired for MULTI/EXEC. expire.c now calls signalModifiedKey() to invalidate watch as well. This fixes #7918 and Issue 9 in #6860 ",
        "comments": 37
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-10-16T02:04:26Z",
        "closed_at": "2020-10-19T12:32:19Z",
        "merged_at": "2020-10-19T12:32:19Z",
        "body": "Similar to https://github.com/redis/redis/pull/7307, in aof rewrite we also did double fclose, this PR fixed issue: https://github.com/redis/redis/issues/3834",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2020-10-16T01:24:15Z",
        "closed_at": "2020-10-23T12:26:31Z",
        "merged_at": "2020-10-23T12:26:31Z",
        "body": "Support `info Persistence` with rdb_last_bgsave_time_sec considering rdb\r\nbackground saving using sockets.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-10-15T20:02:36Z",
        "closed_at": "2020-10-18T11:50:30Z",
        "merged_at": "2020-10-18T11:50:30Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 70,
        "deletions": 64,
        "changed_files": 9,
        "created_at": "2020-10-15T11:08:43Z",
        "closed_at": "2020-10-28T19:13:45Z",
        "merged_at": "2020-10-28T19:13:45Z",
        "body": "Useful when you want to know through which bind address the client connected to the server in case of multiple bind addresses.\r\n\r\nAlso added \"target\" support  to `CLIENT KILL`.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 338,
        "deletions": 16,
        "changed_files": 11,
        "created_at": "2020-10-15T03:44:24Z",
        "closed_at": "2020-11-16T08:34:04Z",
        "merged_at": "2020-11-16T08:34:04Z",
        "body": "Currently, 'dbAsyncDelete' does not specifically deal with the module data type, so the memory allocated in the module can only be freed synchronously. This is unfriendly for modules that will allocate a large amount of memory. In extreme cases, synchronous release of memory is easy to cause RT jitter. Therefore, this PR is to make the memory allocated by the module data type also have the opportunity to be freed asynchronously.\r\n\r\nThis PR adds an optional API to the RedisModuleTypeMethods structure, which is free_effort (its type is RedisModuleTypeFreeEffortFunc), which indicates the effort required to free a module memory. Currently, if the effort exceeds LAZYFREE_THRESHOLD, the module memory It may be released asynchronously.\r\n\r\nAt the same time, this PR also added a member'lazyfreed_objects' to INFO Memory (behind the 'lazyfree_pending_objects' \r\n member), which represents the number of objects that have been lazyfree since redis was started, and 'lazyfree_pending_objects'  indicates the pressure of the current memory asynchronous free queue. I think the indicator 'lazyfreed_objects' is still very useful in certain situations. For example, using the 'lazyfreed_objects'  information in the TCL test corresponding to this PR can confirm that the memory of this module is freed asynchronously, rather than simply judging the memory drop as in 'lazyfree.tcl' . There is also the lazyfree mechanism that uses 'bio_mutex' to synchronize with the BIO thread, which will have some lock overhead, so we usually don\u2019t want too many objects to release memory through lazyfree, and we can provide us with monitoring the growth trend of 'lazyfreed_objects' Good reference information, and if LAZYFREE_THRESHOLD becomes a configurable parameter in the future, the growth rate of 'lazyfreed_objects' can also be controlled by adjusting the value of LAZYFREE_THRESHOLD.",
        "comments": 39
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-10-14T05:05:25Z",
        "closed_at": "2020-10-18T07:15:44Z",
        "merged_at": "2020-10-18T07:15:43Z",
        "body": "This PR introduces a NOMKSTREAM option for xadd command, this would be useful for some use cases when we do not want to create new stream by default:\r\n\r\n`XADD key [MAXLEN [~|=] <count>] <ID or *> <NOMKSTREAM> [field value] [field value]`\r\n\r\nFixes: https://github.com/redis/redis/issues/6204",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 4,
        "created_at": "2020-10-13T14:39:16Z",
        "closed_at": "2020-12-17T11:00:48Z",
        "merged_at": "2020-12-17T11:00:48Z",
        "body": "Please check if they are accurate!\r\n\r\nSigned-off-by: Hanif Bin Ariffin <hanif.ariffin.4326@gmail.com>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-10-13T12:14:18Z",
        "closed_at": "2020-10-13T16:50:58Z",
        "merged_at": "2020-10-13T16:50:58Z",
        "body": "If 'delta' is negative 'mem_freed' may underflow and cause\r\nthe while loop to exit prematurely (And not evicting enough\r\nmemory)\r\n\r\nmem_freed can be negative when:\r\n1. We use lazy free (consuming memory by appending to a list)\r\n2. Thread doing an allocation between the two calls to zmalloc_used_memory.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-10-13T06:08:47Z",
        "closed_at": "2020-10-13T09:42:53Z",
        "merged_at": "2020-10-13T09:42:53Z",
        "body": "See https://en.cppreference.com/w/c/io/fprintf\r\n\r\nSigned-off-by: Hanif Bin Ariffin <hanif.ariffin.4326@gmail.com>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2020-10-13T05:10:19Z",
        "closed_at": "2020-10-13T07:05:06Z",
        "merged_at": "2020-10-13T07:05:06Z",
        "body": "#5021 fixed exists command on replica, but it did not delete dbExists(), which is never used after that.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-10-13T04:33:13Z",
        "closed_at": "2020-10-13T06:34:08Z",
        "merged_at": "2020-10-13T06:34:08Z",
        "body": "If we fail or stop to rewrite aof, we need to remove temporary aof. We also remove temporary rdb when replicas abort to receive rdb. But currently we delete them in main thread, to avoid unnecessary latency, we should use bg_unlink to remove them in a background thread.\r\n\r\nBtw, we have already used this way to removed child process temporary rdb.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 192,
        "deletions": 97,
        "changed_files": 2,
        "created_at": "2020-10-13T00:20:55Z",
        "closed_at": "2020-10-13T08:03:41Z",
        "merged_at": null,
        "body": "Implementation notes:\r\n\r\n1. For sds(len/setlen/etc) methods, we technically do not need to mask the for the extra bits whenever we\r\nwe want to shift the given value because the caller of all of these functions should have guaranteed that\r\nthe given `size_t` does not exceed what the current SDS_TYPE_* can handle.\r\n\r\n2. I think this is all that needs to be done. There might be edge cases where code is accessing the values directly?\r\n\r\nEdit: Getting connection refused error for tests. Is that me? :V\r\n\r\nSigned-off-by: Hanif Bin Ariffin <hanif.ariffin.4326@gmail.com>",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 486,
        "deletions": 21,
        "changed_files": 9,
        "created_at": "2020-10-11T14:51:59Z",
        "closed_at": "2020-11-16T13:51:53Z",
        "merged_at": null,
        "body": "As we know, master has heavy load when sends RDB file to replicas in disk-based mode during full synchronization, and more replicas more heavier. What's more, I find master has much heavy load if replicas come from the same machine, i think, the local back network has high speed and event loops are constantly triggered on master. So during full synchronization in disk-based mode, master has low ability to handle clients's requests.\r\n\r\nI implement a way to send RDB file by a separate thread, and one thread is used for one replica, the thread handles the job of sending RDB to replica. In my implementation, actually, the thread just is like `sendBulkToSlave` function. Create a thread when master needs to send RDB and release it when finishes sending RDB. I don't find latency to create and join threads though it  is not beautiful to create threads when we need rather than creating them when server starts. But I think this way is much easy.\r\n\r\nI also implement blocking `redis_sendfile` to send RDB file on Linux and macOS, it can exactly reduce CPU usage and may lessen RDB transfer time if on high speed network.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-10-11T13:37:56Z",
        "closed_at": "2020-10-11T15:14:02Z",
        "merged_at": "2020-10-11T15:14:02Z",
        "body": "TIL I learned that [`rediss://` is registered with IANA](https://www.iana.org/assignments/uri-schemes/prov/rediss), so I was surprised that `redis-cli -u rediss://myhost.com/` results in \"Invalid URI scheme\".\r\n\r\nApologies if this has already been discussed.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2020-10-11T11:53:18Z",
        "closed_at": "2020-10-14T06:38:48Z",
        "merged_at": "2020-10-14T06:38:48Z",
        "body": "This is a compatibility issue with redis 5.0 that was introduced by ACL.\r\nBefore this commit, setting requirepass to an empty string will result\r\nin a server that needs an empty AUTH, unlike redis 5.0 which would\r\naccept connections without an AUTH.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-10-09T11:34:46Z",
        "closed_at": "2020-11-25T21:36:22Z",
        "merged_at": "2020-11-25T21:36:22Z",
        "body": "Fixed comment typo while reading module code.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-10-08T12:46:19Z",
        "closed_at": "2020-10-18T13:56:44Z",
        "merged_at": "2020-10-18T13:56:43Z",
        "body": "In some cases one command added a very big bulk of memory, and this\r\nwould be \"resolved\" by the eviction before the next command.\r\n\r\nThe current peak memory which is only being tracked in serverCron, which\r\nmay be useful to see what was the highest steady state the server was\r\nfacing.\r\n\r\nBut in some cases seeing an unexplained mass eviction we would wish to\r\nknow the highest momentary usage too.\r\nTracking it in call() and beforeSleep() adds some hooks in AOF and RDB\r\nloading.\r\n\r\nThe fix in clientsCronTrackExpansiveClients is related to #7874",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 40,
        "changed_files": 3,
        "created_at": "2020-10-07T12:56:06Z",
        "closed_at": "2020-10-11T10:43:24Z",
        "merged_at": "2020-10-11T10:43:23Z",
        "body": "This allows supplying secret configuration (for example - masterauth) via a secure channel\r\ninstead of having it in a plaintext file / command line param, while still allowing for most\r\nof the configuration to reside there.\r\n\r\nAlso, remove 'special' case handling for --check-rdb which hasn't been relevant\r\nsince 4.0.0.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-10-07T09:59:35Z",
        "closed_at": "2020-10-07T17:28:57Z",
        "merged_at": "2020-10-07T17:28:57Z",
        "body": "1. si_code can be very useful info some day.\r\n2. a clear indication that redis was killed by an external user",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-10-06T21:47:14Z",
        "closed_at": "2020-10-08T05:09:10Z",
        "merged_at": "2020-10-08T05:09:10Z",
        "body": "When there are multiple categories that fix the criteria of \"more than half of the commands are in this group, either by inclusion or exclusion,\" we include all of them. This can result in overlapping categories that require a lot of re-adding or re-removing. For example:\r\n\r\n\"-@all +@read\" -> \"-@all +@read +@set +@sortedset +@hash +@bitmap +@geo -sdiffstore -hmset -setbit -hsetnx -zremrangebyscore -zpopmax -spop -sort -zpopmin -hset -georadiusbymember -zremrangebylex -geoadd -sunionstore -bitfield -bzpopmin -sadd -zadd -hdel -zunionstore -bitop -zinterstore -bzpopmax -hincrby -sinterstore -zremrangebyrank -zrem -smove -zincrby -srem -hincrbyfloat -georadius\"\r\n\r\nSince the set, sortedset, hash, bitmap, and geo categories have a majority of read commands, they all get included, but then need all their write to be removed. \r\n\r\nWe still might not build the \"optimal\" string, but we should only build shorter strings after this change. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 94,
        "deletions": 21,
        "changed_files": 5,
        "created_at": "2020-10-06T13:53:32Z",
        "closed_at": "2020-10-19T04:33:55Z",
        "merged_at": "2020-10-19T04:33:55Z",
        "body": "This commit implements ACL for Sentinel mode, main work of this PR includes:\r\n\r\n1. Update Sentinel command table in order to better support ACLs.\r\n2. Fix couple of things which currently blocks the support for ACL on sentinel mode.\r\n3. Provide \"sentinel sentinel-user\" and \"sentinel sentinel-pass <password>\" configuration in order to let sentinel authenticate with a specific user in other sentinels.\r\n\r\nthis should Fix:https://github.com/redis/redis/issues/7708\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2020-10-06T11:08:26Z",
        "closed_at": "2020-10-11T13:11:32Z",
        "merged_at": "2020-10-11T13:11:32Z",
        "body": "The main motivation here is to provide a way for modules to create a\r\nsingle, global context that can be used for logging.\r\n\r\nCurrently, it is possible to obtain a thread-safe context that is not\r\nattached to any blocked client by using `RM_GetThreadSafeContext`.\r\nHowever, the attached context is not linked to the module identity so\r\nlog messages produced are not tagged with the module name.\r\n\r\nIdeally we'd fix this in `RM_GetThreadSafeContext` itself but as it\r\ndoesn't accept the current context as an argument there's no way to do\r\nthat in a backwards compatible manner.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 89,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2020-10-06T00:36:59Z",
        "closed_at": "2020-10-06T13:56:05Z",
        "merged_at": null,
        "body": "Hit a hardware memory corruption fault, and redis was killed by\r\nSIGBUS with kernel log (On Linux-4.15):\r\nMemory failure: 0x43080: Killing redis-server:6718 due to hardware memory corruption\r\nMemory failure: 0x43080: recovery action for dirty LRU page: Recovered\r\nMCE: Killing redis-server:6718 due to hardware memory corruption fault at 7f8654c83fe0\r\n\r\nSupport MCE handling in this patch, always handle SIGBUS by new\r\nhandler. if not a MCE error, try to fall through to orignal processing\r\nflow, An action required MCE kills redis server currently, and an\r\naction optional MCE could be ignored and just prints notice level log.\r\n\r\nTest with this patch, redis could handle AO error, print following\r\nand continue to run:\r\nRedis 999.999.999 caught SIGBUS: hardware memory error detected in process but not consumed: action optional, see detailed message from kernel log\r\n\r\nSigned-off-by: zhenwei pi <pizhenwei@bytedance.com>",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 409,
        "deletions": 121,
        "changed_files": 12,
        "created_at": "2020-10-05T14:09:22Z",
        "closed_at": "2020-10-11T13:04:15Z",
        "merged_at": "2020-10-11T13:04:15Z",
        "body": "This is essentially the same as calling COMMAND GETKEYS but provides a more efficient interface that can be used in every context (i.e. not a Redis command).\r\n\r\nThis pull request also includes refactoring of the `getKeysFromCommand()` interface to eliminate the previously used static buffer.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-10-05T14:06:24Z",
        "closed_at": "2020-10-05T15:49:48Z",
        "merged_at": "2020-10-05T15:49:48Z",
        "body": "as discussed in https://github.com/redis/redis/pull/7873, we need to mention two configs requirepass and aclfile are not compatable in order to avoid the confusion for users. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2020-10-04T12:35:46Z",
        "closed_at": "2020-10-06T18:43:31Z",
        "merged_at": "2020-10-06T18:43:31Z",
        "body": "I suppose that it was overlooked, since till recently none of the blocked commands were readonly.\r\nFixes #7877\r\n\r\nTested manually.\r\n@madolson if you think that i'm going the right way, i'll add a test for that.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 127,
        "deletions": 14,
        "changed_files": 5,
        "created_at": "2020-10-01T08:35:33Z",
        "closed_at": "2020-10-02T05:19:45Z",
        "merged_at": "2020-10-02T05:19:45Z",
        "body": "This commit has two aspects:\r\n1) improve memory reporting for all the places that use sdsAllocSize to compute\r\n   memory used by a string, in this case it'll include the internal fragmentation.\r\n2) reduce the need for realloc calls by making the sds implicitly take over\r\n   the internal fragmentation of the block it allocated.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 5,
        "changed_files": 5,
        "created_at": "2020-10-01T07:55:06Z",
        "closed_at": "2020-10-05T08:15:37Z",
        "merged_at": "2020-10-05T08:15:37Z",
        "body": "track and report memory used by clients argv.\r\nthis is very usaful in case clients started sending a command and didn't\r\ncomplete it. in which case the first args of the command are already\r\ntrimmed from the query buffer.\r\n\r\nin an effort to avoid cache misses and overheads while keeping track of\r\nthese, i avoid calling sdsZmallocSize and instead use the sdslen /\r\nbulk-len which can at least give some insight into the problem.\r\n\r\nThis memory is now added to the total clients memory usage, as well as\r\nthe client list.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-10-01T05:46:43Z",
        "closed_at": "2020-10-05T13:26:50Z",
        "merged_at": null,
        "body": "Before this commit, when configuring redis using acl file, the requirepass of default user will be ignored, for example:\r\n\r\nin redis.conf file if we configure:\r\naclfile /etc/redis/users.acl\r\nrequirepass foobar\r\n\r\nwhen server starts the default user password was set to empty, this was not expected behaviour:\r\n\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* +@all\"\r\n2) \"user frank on #2d9c75273d72b32df726fb545c8a4edc719f0a95a6fd993950b10c474ad9c927 ~cached:* -@all +acl\"\r\n\r\nThis commit make sure the requirepass will take effect when acl file was used, so after the changes, default user password wil l be set if requirepass was configured.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-09-30T15:34:55Z",
        "closed_at": "2020-09-30T16:52:01Z",
        "merged_at": "2020-09-30T16:52:01Z",
        "body": "PROBLEM:\r\n\r\n[$rd1 read] reads invalidation messages one by one, so it's never going to see the second invalidation message produced after INCR b, whether or not it exists. Adding another read will block incase no invalidation message is produced.\r\n\r\nFIX:\r\n\r\nWe switch the order of \"INCR a\" and \"INCR b\" - now \"INCR b\" comes first. We still only read the first invalidation message produces. If an invalidation message is wrongly produces for b - then it will be produced before that of a, since \"INCR b\" comes before \"INCR a\".",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 47,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2020-09-30T14:46:02Z",
        "closed_at": "2020-10-01T08:27:47Z",
        "merged_at": "2020-10-01T08:27:46Z",
        "body": "This is a regression in 6.0 (connection abstraction)\r\nit seems it can be triggered only when setting script command\r\nreplication to no (or loading old AOF files)",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2020-09-30T13:15:05Z",
        "closed_at": "2020-10-01T07:56:24Z",
        "merged_at": "2020-10-01T07:56:24Z",
        "body": "$ make PROG_SUFFIX=foo\r\nwill build `redis-server-foo`, etc.\r\n\r\n$ make PROG_SUFFIX=-foo\r\nwill do the same.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-09-30T10:03:17Z",
        "closed_at": "2020-09-30T14:06:58Z",
        "merged_at": "2020-09-30T14:06:58Z",
        "body": "When REDISMODULE_EVENT_CLIENT_CHANGE events are delivered, modules may\nwant to mutate the client state (e.g. perform authentication).\n    \nThis change links the module context with the real client rather than a\nfake client for these events.\n\nThis also fixes an issue with RM_Authenticate*() which would silently\nfail when called for a fake client context.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 88,
        "deletions": 0,
        "changed_files": 6,
        "created_at": "2020-09-30T09:52:22Z",
        "closed_at": "2020-10-11T14:11:43Z",
        "merged_at": "2020-10-11T14:11:42Z",
        "body": "This API function makes it possible to retrieve the X.509 certificate\nused by clients to authenticate TLS connections.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 181,
        "deletions": 4,
        "changed_files": 9,
        "created_at": "2020-09-30T09:02:31Z",
        "closed_at": "2020-10-11T14:21:59Z",
        "merged_at": "2020-10-11T14:21:59Z",
        "body": "* Introduce a new API's: RM_GetContextFlagsAll, and\r\nRM_GetKeyspaceNotificationFlagsAll that will return the\r\nfull flags mask of each feature. The module writer can\r\ncheck base on this value if the Flags he needs are\r\nsupported or not.\r\n\r\n* For each flag, introduce a new value on redismodule.h,\r\nthis value represents the LAST value and should be there\r\nas a reminder to update it when a new value is added,\r\nalso it will be used in the code to calculate the full\r\nflags mask (assuming flags are incrementally increasing).\r\nIn addition, stated that the module writer should not use\r\nthe LAST flag directly and he should use the GetFlagAll API's.\r\n\r\n* Introduce a new API: RM_IsSubEventSupported, that returns for a given\r\nevent and subevent, whether or not the subevent supported.\r\n\r\n* Introduce a new macro RMAPI_FUNC_SUPPORTED(func) that returns whether\r\nor not a function API is supported by comparing it to NULL.\r\n\r\n* Introduce a new API: int RM_GetServerVersion();, that will return the\r\ncurrent Redis version in the format 0x00MMmmpp; e.g. 0x00060008;\r\n\r\n* Changed unstable version from 999.999.999 to 255.255.255\r\n\r\n\r\nPlease notice that flags of RM_HashGet, RM_HashSet, and RM_ZsetAdd are not handled in this PR and we should discuss how to handle those.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2020-09-30T07:52:25Z",
        "closed_at": "2020-10-01T08:30:23Z",
        "merged_at": "2020-10-01T08:30:22Z",
        "body": "The MEMORY command is used for debugging memory usage, so it should include internal\r\nfragmentation, same as used_memory\r\n\r\nThis PR is a portion of https://github.com/redis/redis/pull/5159",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-09-29T12:54:58Z",
        "closed_at": "2020-09-29T17:48:22Z",
        "merged_at": "2020-09-29T17:48:22Z",
        "body": "The tls-ca-cert or tls-ca-cert-dir configuration parameters are only\r\nused when Redis needs to authenticate peer certificates, in one of these\r\nscenarios:\r\n\r\n1. Incoming clients or replicas, with tls-auth-clients enabled.\r\n2. A replica authenticating the master's peer certificate.\r\n3. Cluster nodes authenticating other nodes when establishing the bus\r\n   protocol connection.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-09-28T14:25:14Z",
        "closed_at": "2020-10-05T10:37:20Z",
        "merged_at": "2020-10-05T10:37:20Z",
        "body": "This commit provides a fast way to get current sentinel id. Similar to Cluster myid subcommand. Currently there is no good way to get current sentinel id, other than jumping to the top of sentinel run log file when sentinel starts.  Getting sentinel id would be helpful when analysing sentinel behaviour in logs, such as:\r\n\r\n1958:X 15 Apr 19:36:46.426 # +try-failover master redisroute144200262587 10.133.0.56 6379\r\n1958:X 15 Apr 19:36:46.430 # +vote-for-leader b7b5e68f89e1593427afde670b7c1ff5bc1fbe8f 14\r\n1958:X 15 Apr 19:36:46.440 # 24f3ea9e48876738c8ed52a3f2fe5f0f9d9fdd81 voted for b7b5e68f89e1593427afde670b7c1ff5bc1fbe8f 14",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 39,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2020-09-27T19:29:53Z",
        "closed_at": "2020-10-02T12:07:20Z",
        "merged_at": "2020-10-02T12:07:20Z",
        "body": "Closes #2349\r\n\r\nAdd optional GET parameter to SET command in order to set a new value to\r\na key and retrieve the old key value. With this change we can deprecate\r\n`GETSET` command and use only the SET command with the GET parameter.\r\n\r\n@oranagra ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 13,
        "changed_files": 2,
        "created_at": "2020-09-26T10:55:40Z",
        "closed_at": "2020-09-27T14:13:33Z",
        "merged_at": "2020-09-27T14:13:33Z",
        "body": "Also stabilize new shutdown tests on slow machines (valgrind)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-09-25T07:03:54Z",
        "closed_at": "2020-09-25T13:25:48Z",
        "merged_at": "2020-09-25T13:25:47Z",
        "body": "We may access and modify these two variables in signal handler function, to guarantee them async-signal-safe, so we should set them to volatile sig_atomic_t type.\r\n\r\nBut don't worry, till now, we don't receive any bug report, maybe signals will be handled in main thread on most platforms. We do that since we want to follow C and POSIX standard in signal handler function.\r\n\r\nFix #7777 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 521,
        "deletions": 233,
        "changed_files": 4,
        "created_at": "2020-09-24T19:12:22Z",
        "closed_at": "2021-01-07T08:58:54Z",
        "merged_at": "2021-01-07T08:58:54Z",
        "body": "Add ZRANGESTORE command, and improve ZSTORE command to deprecated Z[REV]RANGE[BYSCORE|BYLEN].\r\n\r\nSyntax for the new ZRANGESTORE command:\r\nZRANGESTORE <dst> <src> <min> <max> [BYSCORE | BYLEX] [REV] [LIMIT offset count]\r\n\r\nNew syntax for ZRANGE:\r\nZRANGE <key> <min> <max> [BYSCORE | BYLEX] [REV] [WITHSCORES] [LIMIT offset count]\r\n\r\nOld syntax for ZRANGE:\r\nZRANGE <key> <start> <stop> [WITHSCORES]\r\n\r\nOther Z*RANGE* commands remain unchanged.\r\n\r\nThe implementation uses common code for all of these, by utilizing a consumer interface that in one\r\ncommand response to the client, and in the other command stores a zset key.",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-09-24T14:44:58Z",
        "closed_at": "2020-09-27T12:40:08Z",
        "merged_at": "2020-09-27T12:40:08Z",
        "body": "I configure slaveof no one in redis.conf by accident,and redis tried to connect to no:0.I think that's not what i expected.\r\n",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2020-09-24T14:26:09Z",
        "closed_at": "2020-10-04T14:18:17Z",
        "merged_at": "2020-10-04T14:18:17Z",
        "body": "Hi\r\nThis PR handles raw formatting unwanted addition line drop from redis-cli. This cuases the retrived raw data to become unusable as it has additional line drop symbol. Causes issue https://github.com/RedisAI/RedisAI/issues/458",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-09-24T10:52:42Z",
        "closed_at": "2020-09-25T05:08:07Z",
        "merged_at": "2020-09-25T05:08:07Z",
        "body": "We should sync temp DB file before renaming as rdb_fsync_range does not use\r\nflag `SYNC_FILE_RANGE_WAIT_AFTER`.\r\n\r\nRefer to `Linux Programmer's Manual`:\r\nSYNC_FILE_RANGE_WAIT_AFTER\r\n    Wait upon write-out of all pages in the range after performing any write.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-09-24T07:17:55Z",
        "closed_at": "2020-09-29T14:03:48Z",
        "merged_at": "2020-09-29T14:03:48Z",
        "body": "`numele` can't be more than 65k, and we already know `index` is non-negative from an earlier check",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-09-24T01:15:21Z",
        "closed_at": "2020-09-24T08:58:43Z",
        "merged_at": null,
        "body": "According to `Linux Programmer's Manual`([sync_file_range(2)](https://man7.org/linux/man-pages/man2/sync_file_range.2.html)):\r\n\r\nSYNC_FILE_RANGE_WAIT_BEFORE | SYNC_FILE_RANGE_WRITE\r\n\tEnsures that all pages in the specified range which were dirty when\r\n\tsync_file_range() was called are placed under write-out.  This is\r\n\ta start-write-for-data-integrity operation.\r\n\r\nSYNC_FILE_RANGE_WAIT_BEFORE | SYNC_FILE_RANGE_WRITE | SYNC_FILE_RANGE_WAIT_AFTER\r\n\tThis is a write-for-data-integrity operation that will ensure that all\r\n\tpages in the specified range which were dirty when sync_file_range() was\r\n\tcalled are committed to disk.\r\n\r\nWe change `SYNC_FILE_RANGE_WAIT_BEFORE | SYNC_FILE_RANGE_WRITE` to\r\n`SYNC_FILE_RANGE_WAIT_BEFORE | SYNC_FILE_RANGE_WRITE | SYNC_FILE_RANGE_WAIT_AFTER`\r\nin this commit to ensure that data in the specified range are committed\r\nto dist when we call sync_file_range().\r\n\r\nFixes #5199",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-09-23T06:50:23Z",
        "closed_at": "2020-09-23T08:30:25Z",
        "merged_at": "2020-09-23T08:30:25Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-09-23T01:23:13Z",
        "closed_at": "2020-09-23T06:09:49Z",
        "merged_at": "2020-09-23T06:09:49Z",
        "body": "Fixes #5591",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 72,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2020-09-22T22:07:49Z",
        "closed_at": "2020-09-23T07:00:32Z",
        "merged_at": "2020-09-23T07:00:32Z",
        "body": "- backtrace is also supported optionally as OpenBSD.\r\n- Dumping i386/amd64 registers in similar manner.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-09-22T07:17:24Z",
        "closed_at": "2020-09-22T08:38:52Z",
        "merged_at": "2020-09-22T08:38:52Z",
        "body": "This happens only on diskless replicas when attempting to reconnect after failing to load an RDB file. It is more likely to occur with larger datasets.\r\n\r\nAfter reconnection is initiated, `replicationEmptyDbCallback()` may get called and try to write to an unconnected socket. This triggered another issue where the connection is put into an error state and the connect handler never gets called. The problem is a regression introduced by commit c17e597d05. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 18,
        "changed_files": 1,
        "created_at": "2020-09-21T16:09:13Z",
        "closed_at": "2020-09-22T06:05:48Z",
        "merged_at": "2020-09-22T06:05:48Z",
        "body": "This commit adds streamIteratorStop call in rewriteStreamObject function in some of the return statement. Although currently this will not cause memory leak since stream id is only 16 bytes long, however I don't think this is the reason we don't call it.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2020-09-21T11:03:22Z",
        "closed_at": "2020-09-22T07:18:08Z",
        "merged_at": "2020-09-22T07:18:07Z",
        "body": "Fix #7808 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 47,
        "changed_files": 1,
        "created_at": "2020-09-20T22:17:16Z",
        "closed_at": "2020-09-25T09:55:46Z",
        "merged_at": "2020-09-25T09:55:46Z",
        "body": "",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-09-20T14:07:21Z",
        "closed_at": "2020-09-21T08:17:49Z",
        "merged_at": "2020-09-21T08:17:49Z",
        "body": "445a4b6 introudced a makefile script that detects if the toolchain\r\nsupports c11, and it looked that it was passing on MacOS and fails on\r\nUbuntu, looks like Ubuntu's Dash was spawning a background process,\r\ndeleted foo.c before gcc tried to compile it.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-09-20T10:50:09Z",
        "closed_at": "2020-09-22T09:11:19Z",
        "merged_at": "2020-09-22T09:11:19Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 95,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2020-09-19T07:17:29Z",
        "closed_at": "2020-09-22T06:47:59Z",
        "merged_at": "2020-09-22T06:47:58Z",
        "body": "#7717 \r\n\r\nIf there is no any other slave waiting dumping RDB finished, the current child process need not continue to dump RDB, then we kill it. So child process won't use more memory, and we also can fork a new child process asap to dump rdb for next full synchronization or bgsave.  But we also need to check if users enable 'save' RDB, if enable, we should not remove directly since that means RDB is important for users to keep data safe.\r\n\r\nBtw, now, `rdbRemoveTempFile` in `killRDBChild` won't block server, so we can `killRDBChild` safely.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 98,
        "deletions": 2,
        "changed_files": 6,
        "created_at": "2020-09-18T23:23:55Z",
        "closed_at": "2020-09-23T18:56:17Z",
        "merged_at": "2020-09-23T18:56:16Z",
        "body": "",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2020-09-18T09:16:05Z",
        "closed_at": "2021-03-05T09:23:21Z",
        "merged_at": null,
        "body": "We are tencent cloud redis team, recently some redis Instance takes up a lot of memory,  but user data takes up very little\uff0cwe found that most of the memory was consumed by Lua. \r\n\r\nCurrently Lua memory control does not pass through Redis\u2019s zrealloc()\\zfree(), Redis total memory cannot be effectively controlled when user maliciously uses Lua(maxmemory does not work), This will bring disaster to cloud vendors.\r\n\r\nWe want to allocate Lua\u2019s memory through zmalloc as well, it helps to control the total memory consume of the Redis, In addition, Redis uses Jemalloc to better control the memory fragmentation rate\r\n\r\n---\r\n\r\nWe loaded an RDB file with a little bit of user data, but a lot of Lua scripts:\r\n![\u622a\u5c4f2020-09-18 \u4e0b\u53484 21 20](https://user-images.githubusercontent.com/11421972/93574481-0bcec900-f9cb-11ea-8ed9-689fc445e74e.png)\r\n\r\n---\r\n\r\nAfter Lua's memory control is pass zrealloc()\\zfree(),  load the same RDB file,  the total memory consume is reduced(due to the use of Jemalloc, the memory fragmentation rate is reduced)\r\n\r\n![\u622a\u5c4f2020-09-18 \u4e0b\u53484 27 15](https://user-images.githubusercontent.com/11421972/93575113-cf4f9d00-f9cb-11ea-87eb-038d21c02061.png)\r\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2020-09-18T08:14:58Z",
        "closed_at": "2020-09-21T09:10:20Z",
        "merged_at": "2020-09-21T09:10:20Z",
        "body": "When killThreads() is triggered, we should make sure that all others thread are killable and the cancellation requests should not be deferred.\r\n\r\nThe first commit can be cherry picked to elder version.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2020-09-18T05:58:20Z",
        "closed_at": "2020-09-22T09:53:36Z",
        "merged_at": "2020-09-22T09:53:36Z",
        "body": "Method *connGetSocketError* is designed to return error status of the socket. \r\n\r\nWhen the *getsockopt* method in *connGetSocketError* is invoked successfully, and the socket does have error, *errno* won't be updated. Assigning *errno* to connection error here is misleading. As *connectGetSocketError* can get socket error number by *getsockopt* and return it, we should record the returned value instead. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-09-17T21:16:42Z",
        "closed_at": "2020-09-19T09:24:40Z",
        "merged_at": "2020-09-19T09:24:40Z",
        "body": "The symbol base address is a const on this system.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2306,
        "deletions": 253,
        "changed_files": 51,
        "created_at": "2020-09-17T09:48:51Z",
        "closed_at": "2020-12-06T12:54:36Z",
        "merged_at": "2020-12-06T12:54:35Z",
        "body": "### Sanitize dump payload: ziplist, listpack, zipmap, intset, stream\r\n\r\nWhen loading an encoded payload we will at least do a shallow validation to\r\ncheck that the size that's encoded in the payload matches the size of the\r\nallocation.\r\nThis let's us later use this encoded size to make sure the various offsets\r\ninside encoded payload don't reach outside the allocation, if they do, we'll\r\nassert/panic, but at least we won't segfault or smear memory.\r\n\r\nWe can also do 'deep' validation which runs on all the records of the encoded\r\npayload and validates that they don't contain invalid offsets. This lets us\r\ndetect corruptions early and reject a RESTORE command rather than accepting\r\nit and asserting (crashing) later when accessing that payload via some command.\r\n\r\nconfiguration:\r\n- adding ACL flag skip-sanitize-payload\r\n- adding config sanitize-dump-payload [yes/no/clients]\r\n\r\nFor now, we don't have a good way to ensure MIGRATE in cluster resharding isn't\r\nbeing slowed down by these sanitation, so i'm setting the default value to `no`,\r\nbut later on it should be set to `clients` by default.\r\n\r\nchanges:\r\n- changing rdbReportError not to `exit` in RESTORE command\r\n- adding a new stat to be able to later check if cluster MIGRATE isn't being\r\n  slowed down by sanitation.\r\n\r\n### Fuzz tester and fixes for segfaults and leaks it exposed\r\n\r\nThe test creates keys with various encodings, DUMP them, corrupt the payload and RESTORES it.\r\nIt utilizes the recently added use-exit-on-panic config to distinguish between asserts and segfaults.\r\nIf the restore succeeds, it runs random commands on the key to attempt to trigger a crash.\r\nIt runs in two modes, one with deep sanitation enabled and one without.\r\nIn the first one we don't expect any assertions or segfaults, in the second one we expect assertions, but no segfaults.\r\nWe also check for leaks and invalid reads using valgrind, and if we find them we print the commands that lead to that issue.\r\n\r\nChanges in the code (other than the test):\r\n\r\n- Replace a few NPD (null pointer deference) flows and division by zero with an assertion, so that it doesn't fail the test. (since we set the server to use `exit` rather than `abort` on assertion).\r\n- Fix quite a lot of flows in rdb.c that could have lead to memory leaks in RESTORE command (since it now responds with an error rather than panic)\r\n- Add a DEBUG flag for SET-SKIP-CHECKSUM-VALIDATION so that the test don't need to bother with faking a valid checksum\r\n- Remove a pile of code in serverLogObjectDebugInfo which is actually unsafe to run in the crash report (see comments in the code)\r\n\r\ntest suite infra improvements:\r\n- be able to run valgrind checks before the process terminates\r\n- rotate log files when restarting servers\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-09-17T03:14:58Z",
        "closed_at": "2020-09-20T09:06:18Z",
        "merged_at": "2020-09-20T09:06:18Z",
        "body": "fixes build warning:\r\n\r\ndebug.c:1583:13: warning: unused function 'killThreads' [-Wunused-function]\r\nstatic void killThreads(void) {\r\n            ^\r\n\r\nthe killThreads and killMainThread function are only used in #if defined(HAVE_PROC_MAPS) block, therefore we can avoid the make warnings by enlarging the condition block. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2020-09-17T01:54:46Z",
        "closed_at": "2020-09-20T10:36:20Z",
        "merged_at": "2020-09-20T10:36:20Z",
        "body": "This PR adds SwapDb module event, currently there is no notification mechanism for letting user know a swapdb has been called, this PR provides mechanism for adding swapdb as a new module event type.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2020-09-16T14:57:19Z",
        "closed_at": "2020-09-16T17:21:04Z",
        "merged_at": "2020-09-16T17:21:04Z",
        "body": "The fix in error handling of rdbGenericLoadStringObject is an actual bugfix",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 3,
        "changed_files": 5,
        "created_at": "2020-09-15T05:09:28Z",
        "closed_at": "2020-09-16T11:15:03Z",
        "merged_at": "2020-09-16T11:15:03Z",
        "body": "If one thread got SIGSEGV, function sigsegvHandler() would be triggered,\r\nit would call bioKillThreads(). But call pthread_cancel() to cancel itself\r\nwould make it block.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-09-15T01:59:34Z",
        "closed_at": "2020-09-15T05:58:21Z",
        "merged_at": "2020-09-15T05:58:21Z",
        "body": "This commit makes stream object returning \"stream\" as encoding type in OBJECT ENCODING subcommand and DEBUG OBJECT command.\r\n\r\nBefore:\r\n127.0.0.1:6379> object encoding tt\r\n\"unknown\"\r\n\r\nAfter:\r\n127.0.0.1:6379> object encoding tt\r\n\"stream\"",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 151,
        "deletions": 58,
        "changed_files": 5,
        "created_at": "2020-09-14T13:42:02Z",
        "closed_at": "2020-09-24T05:59:16Z",
        "merged_at": "2020-09-24T05:59:15Z",
        "body": "Syntax: \r\n```\r\nZINTER/ZUNION numkeys key [key ...] [WEIGHTS weight [weight ...]]\r\n[AGGREGATE SUM|MIN|MAX] [WITHSCORES]\r\n```\r\n\r\nsee #7624",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-09-13T07:44:49Z",
        "closed_at": "2020-09-13T10:51:22Z",
        "merged_at": "2020-09-13T10:51:22Z",
        "body": "These tests started failing every day on http 404 (not being able to\r\ninstall valgrind)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2020-09-13T07:05:11Z",
        "closed_at": "2020-09-13T10:50:24Z",
        "merged_at": "2020-09-13T10:50:24Z",
        "body": "This test was nearly always failing on MacOS github actions.\r\nThis is because of bugs in the test that caused it to nearly always run\r\nall 3 attempts and just look at the last one as the pass/fail creteria.\r\n\r\ni.e. the test was nearly always running all 3 attempts and still sometimes\r\nsucceed. this is because the break condition was different than the test\r\ncompletion condition.\r\n\r\nThe reason the test succeeded is because the break condition tested the\r\nresults of all 3 tests (PSETEX/PEXPIRE/PEXPIREAT), but the success check\r\nat the end was only testing the result of PSETEX.\r\n\r\nThe reason the PEXPIREAT test nearly always failed is because it was\r\ngetting the current time wrong: getting the current second and loosing\r\nthe sub-section time, so the only chance for it to succeed is if it run\r\nright when a certain second started.\r\n\r\nBecause i now get the time from redis, adding another round trip, i\r\nadded another 100ms to the PEXPIRE test to make it less fragile, and\r\nalso added many more attempts.\r\n\r\nAdding many more attempts before failure to account for slow platforms,\r\ngithub actions and valgrind",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-09-11T18:38:23Z",
        "closed_at": "2020-09-21T13:57:16Z",
        "merged_at": "2020-09-21T13:57:16Z",
        "body": "`--enable-cc-silence` was already removed from `configure.ac` file in the newer version of jemalloc library. It is time to update Makefile.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-09-11T18:06:20Z",
        "closed_at": "2020-09-21T14:04:35Z",
        "merged_at": "2020-09-21T14:04:35Z",
        "body": "Change `val` to `unsigned char` before being tested.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 5,
        "created_at": "2020-09-11T00:53:43Z",
        "closed_at": "2020-09-29T14:10:56Z",
        "merged_at": "2020-09-29T14:10:56Z",
        "body": "Fix warnings mentioned in issue https://github.com/redis/redis/issues/7784.\r\nThe pull request is for lua module.",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 18,
        "changed_files": 8,
        "created_at": "2020-09-10T11:41:14Z",
        "closed_at": "2020-09-20T10:43:29Z",
        "merged_at": "2020-09-20T10:43:29Z",
        "body": "Also if the fork child is a module fork child, reduce the log level\r\nof sigKillChildHandler since some modules create a lot of forks and\r\nif floods the log.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-09-10T10:07:08Z",
        "closed_at": "2020-09-10T14:02:28Z",
        "merged_at": "2020-09-10T14:02:28Z",
        "body": "BTW\r\n\r\n> volatile access does not establish inter-thread synchronization.\r\n\r\nhttps://en.cppreference.com/w/c/atomic/memory_order",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-09-10T06:01:52Z",
        "closed_at": "2020-09-10T07:09:14Z",
        "merged_at": "2020-09-10T07:09:14Z",
        "body": "Fixes #7776",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-09-10T05:18:43Z",
        "closed_at": "2020-09-10T07:22:17Z",
        "merged_at": "2020-09-10T07:22:17Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 907,
        "deletions": 175,
        "changed_files": 36,
        "created_at": "2020-09-09T21:06:38Z",
        "closed_at": "2020-09-10T11:09:01Z",
        "merged_at": "2020-09-10T11:09:01Z",
        "body": "Upgrade urgency HIGH: Anyone who's using Redis 6.0.7 with Sentinel or\r\nCONFIG REWRITE command is affected and should upgrade ASAP, see #7760.\r\n\r\nBug fixes:\r\n\r\n* CONFIG REWRITE after setting oom-score-adj-values either via CONFIG SET or\r\n  loading it from a config file, will generate a corrupt config file that will\r\n  cause Redis to fail to start\r\n* Fix issue with redis-cli --pipe on MacOS\r\n* Fix RESP3 response for HKEYS/HVALS on non-existing key\r\n* Various small bug fixes\r\n\r\nNew features / Changes:\r\n\r\n* Remove THP warning when set to madvise\r\n* Allow EXEC with read commands on readonly replica in cluster\r\n* Add masters/replicas options to redis-cli --cluster call command\r\n\r\nModule API:\r\n\r\n* Add RedisModule_ThreadSafeContextTryLock",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 440,
        "deletions": 420,
        "changed_files": 80,
        "created_at": "2020-09-09T15:39:16Z",
        "closed_at": "2020-09-10T10:43:39Z",
        "merged_at": "2020-09-10T10:43:39Z",
        "body": "Many duplicate and conflicting PRs accumulated over the years.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-09-09T08:07:08Z",
        "closed_at": "2020-09-09T09:30:44Z",
        "merged_at": "2020-09-09T09:30:44Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2020-09-09T07:32:17Z",
        "closed_at": "2020-09-09T12:12:58Z",
        "merged_at": "2020-09-09T12:12:58Z",
        "body": "Save parameters should either be default or whatever specified in the\r\nconfig file. This fixes an issue introduced in #7092 which causes\r\nconfiguration file settings to be applied on top of the defaults.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 59,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2020-09-09T05:34:30Z",
        "closed_at": "2020-09-09T06:35:43Z",
        "merged_at": "2020-09-09T06:35:43Z",
        "body": "Per https://github.com/redis/redis/issues/7683 adds the ability to do MULTI/EXEC read only operations on a READONLY replica.\r\n\r\nMinor:\r\n* Added tests for the new behavior\r\n* Minor refactor that Adds Rn function that return the n-th Redis test instance",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2020-09-09T03:31:23Z",
        "closed_at": "2020-09-09T14:13:36Z",
        "merged_at": "2020-09-09T14:13:36Z",
        "body": "Add aclfile load tests.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 6,
        "changed_files": 6,
        "created_at": "2020-09-08T18:00:58Z",
        "closed_at": "2020-09-09T12:43:11Z",
        "merged_at": "2020-09-09T12:43:11Z",
        "body": "This is a catch-all test to confirm that that rewrite produces a valid\r\noutput for all parameters.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-09-08T17:55:57Z",
        "closed_at": "2020-09-09T16:29:00Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 82,
        "deletions": 8,
        "changed_files": 6,
        "created_at": "2020-09-08T14:34:24Z",
        "closed_at": "2020-09-17T15:20:11Z",
        "merged_at": "2020-09-17T15:20:11Z",
        "body": "Remove tmp rdb file in background thread to avoid unnecessary time cost. Maybe removing tmp aof file is similar.",
        "comments": 17
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2020-09-08T04:48:27Z",
        "closed_at": "2021-01-28T10:09:12Z",
        "merged_at": null,
        "body": "When we deploy redis in kubernetes, each pod corresponds to a redis instance or sentinel instance, using the dns address as the pod's fixed external address, we found:\r\n\r\n1. When sentinel is configured with \"sentinel announce-ip sentinel-0-svc\", sentinel-0-svc is this sentinel`s dns address, sentinel will always report the \"+sentinel-address-switch\" log when it is started.\r\n2. When slave is configured with \"replicaof redis-0-svc\", redis-0-svc is the master redis instance`s dns address, sentinel will report the \u201c+slave\u201d log first, followed by the \u201c+fix-slave-config\u201c log\uff0cthis will cause the slave to re-initiate replication to the master.\r\n3. When redis slave is configured with \"replica-announce-ip redis-0-svc\" (redis-0-svc is the dns address of the redis instance) and a failover occurs, sentinel will list the slave instance twice.\r\n\r\nSo, in these scenarios,\r\n1. When storing redis slave instances in sentinel, ip:port is used as the key uniformly (modify this in  line 1198), even if the redis slave uses the dns address to configure replica-announce-ip.\r\n2. It should first convert the dns address (or hostname) to the ip address when search an instance (modify this in  line 1397 and line 2372).\r\n\r\nThis PR is related to #7393 ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-09-08T03:17:51Z",
        "closed_at": "2020-09-08T13:28:59Z",
        "merged_at": "2020-09-08T13:28:59Z",
        "body": "### 1) cur_test: when restart_server, \"no such variable\" error occurs \r\n```\r\n\u279c  redis git:(unstable) \u2717 ./runtest --single integration/rdb    \r\nCleanup: may take some time... OK\r\nStarting test server at port 11111\r\n[ready]: 67092\r\nTesting integration/rdb\r\n[ready]: 67093\r\n[ready]: 67091\r\n[ready]: 67094\r\n[ready]: 67095\r\n[ready]: 67097\r\n[ready]: 67099\r\n[ready]: 67104\r\n[ready]: 67096\r\n[ready]: 67100\r\n[ready]: 67098\r\n[ready]: 67105\r\n[ready]: 67106\r\n[ok]: RDB encoding loading test\r\n[ok]: Check for memory leaks (pid 67109)\r\n[ok]: Server started empty with non-existing RDB file\r\n[ready]: 67101\r\n[ready]: 67102\r\n[ready]: 67103\r\n[ok]: Check for memory leaks (pid 67133)\r\n[ok]: Server started empty with empty RDB file\r\n[ok]: Check for memory leaks (pid 67153)\r\n[ok]: Test RDB stream encoding\r\n[ok]: Check for memory leaks (pid 67173)\r\n[ok]: Server should not start if RDB file can't be open\r\n[ok]: Server should not start if RDB is corrupted\r\n[ok]: Test FLUSHALL aborts bgsave\r\n[ok]: bgsave resets the change counter\r\n[ok]: Check for memory leaks (pid 67207)\r\n[ok]: Check for memory leaks (pid 67231)\r\n[ok]: Check for memory leaks (pid 67282)\r\n[ok]: client freed during loading\r\n[exception]: Executing test client: can't unset \"::cur_test\": no such variable.\r\ncan't unset \"::cur_test\": no such variable\r\n    while executing\r\n\"unset ::cur_test\"\r\n    (procedure \"test\" line 92)\r\n    invoked from within\r\n\"test {client freed during loading} {\r\n    start_server [list overrides [list key-load-delay 10 rdbcompression no]] {\r\n        # create a big rdb that wi...\"\r\n    (file \"tests/integration/rdb.tcl\" line 151)\r\n    invoked from within\r\n\"source $path\"\r\n    (procedure \"execute_test_file\" line 4)\r\n    invoked from within\r\n\"execute_test_file $data\"\r\n    (procedure \"test_client_main\" line 10)\r\n    invoked from within\r\n\"test_client_main $::test_server_port \"\r\nKilling still running Redis server 67193\r\nKilling still running Redis server 67200\r\n```\r\n\r\n#### The calling process is as follows\uff1a\r\n```\r\ntest {client freed during loading}\r\n      SET ::cur_test\r\n      restart_server\r\n        kill_server\r\n          test \"Check for memory leaks (pid $pid)\"\r\n          SET ::cur_test\r\n          UNSET ::cur_test\r\n      UNSET ::cur_test // This global variable has been unset, so \"no such variable\" error occurs \r\n```\r\n#### fixd way:\r\nI think unset can be cancelled, because every test will actively set at the beginning of the run.\r\n\r\n@oranagra Do you have any suggestions?\r\n\r\n\r\n### 2) ps --ppid\r\n`ps --ppid` not available on macOS platform, can be replaced with `pgrep -P pid`.",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 29,
        "changed_files": 1,
        "created_at": "2020-09-07T08:25:55Z",
        "closed_at": "2020-09-07T15:06:25Z",
        "merged_at": "2020-09-07T15:06:25Z",
        "body": "This test was failing from time to time see discussion at the bottom of #7635\r\nThis was probably due to timing, the DEBUG SLEEP executed by redis-cli\r\ndidn't sleep for enough time.\r\n\r\nThis commit changes:\r\n1) use SET-ACTIVE-EXPIRE instead of DEBUG SLEEP\r\n2) reduce many `after` sleeps with retry loops to speed up the test.\r\n3) add many comment explaining the different steps of the test and\r\n   it's purpose.\r\n4) config appendonly before populating the volatile keys, so that they'll\r\n   be part of the AOF command stream rather than the preamble RDB portion.\r\n\r\nother complications: recently kill_instance switched from SIGKILL to\r\nSIGTERM, and this would sometimes fail since there was an AOFRW running\r\nin the background. now we wait for it to end before attempting the kill.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-09-07T06:36:25Z",
        "closed_at": "2020-09-07T13:26:12Z",
        "merged_at": "2020-09-07T13:26:12Z",
        "body": "2b998de46 added a file for stderr to keep valgrind log but i forgot to\r\nadd a similar thing when valgrind isn't being used.\r\nthe result is that `glob */err.txt` fails.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 42,
        "changed_files": 1,
        "created_at": "2020-09-06T15:28:40Z",
        "closed_at": "2021-06-10T13:01:52Z",
        "merged_at": null,
        "body": "@oranagra maybe we do a full sweep on src? ",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-09-06T13:52:29Z",
        "closed_at": "2020-09-09T16:23:04Z",
        "merged_at": null,
        "body": "Specifically:\r\n\r\n* RM_AutoMemory completed instead of pointing to docs\r\n* Updated link to custom type doc\r\n* A bunch of trailing spaces autoremoved by IDE <- can be reverted if too much for the blame log\r\n\r\nIn lieu of #3715",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2020-09-03T08:49:02Z",
        "closed_at": "2020-09-03T15:15:49Z",
        "merged_at": "2020-09-03T15:15:48Z",
        "body": "Fix issues with `writeConn()` which resulted with corruption of the stream by leaving an extra byte in the buffer. The trigger for this is partial writes or write errors which were not experienced on Linux but reported on macOS.\r\n\r\nFixes #7744 ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-09-03T06:57:23Z",
        "closed_at": "2020-09-03T13:02:55Z",
        "merged_at": null,
        "body": "Fix #4284 `Conditional jump or move depends on uninitialised value(s)`",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-09-02T11:38:52Z",
        "closed_at": "2020-09-02T14:18:11Z",
        "merged_at": "2020-09-02T14:18:10Z",
        "body": "When redis isn't configured to have a log file, having these prints\r\nbefore damonization puts them in the calling process stdout rather than\r\n/dev/null",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 103,
        "deletions": 49,
        "changed_files": 7,
        "created_at": "2020-09-02T10:53:48Z",
        "closed_at": "2020-09-06T13:43:57Z",
        "merged_at": "2020-09-06T13:43:57Z",
        "body": "Starting redis 6.0 and the changes we made to the diskless master to be\r\nsuitable for TLS, I made the master avoid reaping (wait3) the pid of the\r\nchild until we know all replicas are done reading their rdb.\r\n\r\nI did that in order to avoid a state where the rdb_child_pid is -1 but\r\nwe don't yet want to start another fork (still busy serving that data to\r\nreplicas).\r\n\r\nIt turns out that the solution used so far was problematic in case the\r\nfork child was being killed (e.g. by the kernel OOM killer), in that\r\ncase there's a chance that we currently disabled the read event on the\r\nrdb pipe, since we're waiting for a replica to become writable again.\r\nand in that scenario the master would have never realized the child\r\nexited, and the replica will remain hung too.\r\nNote that there's no mechanism to detect a hung replica while it's in\r\nrdb transfer state.\r\n\r\nThe solution here is to add another pipe which is used by the parent to\r\ntell the child it is safe to exit. this mean that when the child exits,\r\nfor whatever reason, it is safe to reap it.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 122,
        "deletions": 1,
        "changed_files": 7,
        "created_at": "2020-09-01T17:03:55Z",
        "closed_at": "2020-09-09T13:01:17Z",
        "merged_at": "2020-09-09T13:01:17Z",
        "body": "",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-09-01T11:26:55Z",
        "closed_at": "2020-09-09T16:23:07Z",
        "merged_at": null,
        "body": "there is no 4 in comments",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 17,
        "changed_files": 1,
        "created_at": "2020-09-01T10:36:48Z",
        "closed_at": "2020-09-09T16:23:11Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-08-31T19:58:40Z",
        "closed_at": "2020-09-01T07:02:15Z",
        "merged_at": "2020-09-01T07:02:15Z",
        "body": "All user-supplied variables that affect the build should be explicitly\r\npersisted.\r\n\r\nFixes #7254",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 207,
        "deletions": 82,
        "changed_files": 7,
        "created_at": "2020-08-31T08:26:54Z",
        "closed_at": "2020-09-06T06:59:21Z",
        "merged_at": "2020-09-06T06:59:20Z",
        "body": "would be easier to review one commit at a time.\r\nnote to self: **merge with rebase**",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 139,
        "deletions": 30,
        "changed_files": 10,
        "created_at": "2020-08-30T12:30:08Z",
        "closed_at": "2020-09-03T05:47:30Z",
        "merged_at": "2020-09-03T05:47:30Z",
        "body": "During long running scripts or loading RDB/AOF, we may need to do some\r\ndefragging. Since processEventsWhileBlocked is called periodically at\r\nunknown intervals, and many cron jobs either depend on run_with_period\r\n(including active defrag), or rely on being called at server.hz rate\r\n(i.e. active defrag knows ho much time to run by looking at server.hz),\r\nthe whileBlockedCron may have to run a loop triggering the cron jobs in it\r\n(currently only active defrag) several times.\r\n\r\nOther changes:\r\n- Adding a test for defrag during aof loading.\r\n- Changing key-load-delay config to take negative values for fractions\r\n  of a microsecond sleep",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 83,
        "deletions": 23,
        "changed_files": 3,
        "created_at": "2020-08-30T10:43:40Z",
        "closed_at": "2020-09-06T08:11:50Z",
        "merged_at": "2020-09-06T08:11:50Z",
        "body": "- redirect valgrind reports to a dedicated file rather than console\r\n- try to avoid killing instances with SIGKILL so that we get the memory\r\n  leak report (killing with SIGTERM before resorting to SIGKILL)\r\n- search for valgrind reports when done, print them and fail the tests\r\n- add --dont-clean option to keep the logs on exit\r\n- fix exit error code when crash is found (would have exited with 0)\r\n\r\nchanges that affect the normal redis test suite:\r\n- refactor check_valgrind_errors into two functions one to search and\r\n  one to report\r\n- move the search half into util.tcl to serve the cluster tests too\r\n- ignore \"address range perms\" valgrind warnings which seem non relevant.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 78,
        "changed_files": 2,
        "created_at": "2020-08-28T13:47:29Z",
        "closed_at": "2020-12-21T09:35:57Z",
        "merged_at": null,
        "body": "Only the slave side need to merge the REPLCONF ip-address, listening-port, capa request, the master side need no change, it can process ip-address, listening-port, capa in one REPLCONF request.\r\n\r\nThis change save two roundtrip, make the \"handshake\" of master and slave more faster.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1881,
        "deletions": 458,
        "changed_files": 63,
        "created_at": "2020-08-28T08:32:38Z",
        "closed_at": "2020-09-01T06:27:59Z",
        "merged_at": "2020-09-01T06:27:59Z",
        "body": "Bug fixes:\r\n\r\n* CONFIG SET could hung the client when arrives during RDB/ROF loading (When\r\n  processed after another command that was also rejected with -LOADING error)\r\n* LPOS command when RANK is greater than matches responded wiht broken protocol\r\n  (negative multi-bulk count)\r\n* UNLINK / Lazyfree for stream type key would have never do async freeing\r\n* PERSIST should invalidate WATCH (Like EXPIRE does)\r\n* EXEC with only read commands could have be rejected when OOM\r\n* TLS: relax verification on CONFIG SET (Don't error if some configs are set\r\n  and tls isn't enabled)\r\n* TLS: support cluster/replication without tls-port\r\n* Systemd startup after network is online\r\n* Redis-benchmark improvements\r\n* Various small bug fixes\r\n\r\nNew features:\r\n\r\n* Add oom-score-adj configuration option to control Linux OOM killer\r\n* Show IO threads statistics and status in INFO output\r\n* Add optional tls verification mode (see tls-auth-clients)\r\n\r\nModule API:\r\n\r\n* Add RedisModule_HoldString\r\n* Add loaded keyspace event\r\n* Fix RedisModuleEvent_LoadingProgress\r\n* Fix RedisModuleEvent_MasterLinkChange hook missing on successful psync\r\n* Fix missing RM_CLIENTINFO_FLAG_SSL\r\n* Refactor redismodule.h for use with -fno-common / extern\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11032,
        "deletions": 2546,
        "changed_files": 148,
        "created_at": "2020-08-27T08:16:52Z",
        "closed_at": "2020-08-27T09:54:44Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 23,
        "changed_files": 4,
        "created_at": "2020-08-27T06:08:32Z",
        "closed_at": "2020-08-27T09:54:01Z",
        "merged_at": "2020-08-27T09:54:01Z",
        "body": "65a3307bc (released in 6.0.6) has a side effect, when processCommand\r\nrejects a command with pre-made shared object error string, it trims the\r\nnewlines from the end of the string. if that string is later used with\r\naddReply, the newline will be missing, breaking the protocol, and\r\nleaving the client hung.\r\n\r\nIt seems that the only scenario which this happens is when replying with\r\n-LOADING to some command, and later using that reply from the CONFIG\r\nSET command (still during loading). this will result in hung client.\r\n\r\nRefactoring the code in order to avoid trimming these newlines from\r\nshared string objects, and do the newline trimming only in other cases\r\nwhere it's needed.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-26T03:45:48Z",
        "closed_at": "2020-08-27T05:09:40Z",
        "merged_at": "2020-08-27T05:09:40Z",
        "body": "plus minor other fixes to list.tcl",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 207,
        "deletions": 120,
        "changed_files": 13,
        "created_at": "2020-08-25T08:59:07Z",
        "closed_at": "2020-09-17T13:01:47Z",
        "merged_at": "2020-09-17T13:01:46Z",
        "body": "Try to solve #7509 \r\n\r\nFirstly, I try to explain how `_Atomic` can guarantee our program execute safely. The default behavior of `_Atomic` operations provides for sequentially consistent ordering. For threaded IO in redis, we use inter-thread synchronization of `io_threads_pending[i]`, so other IO threads could see the updated values of `io_threads_list`  instead of history values when `io_threads_pending[i]` is fired.\r\n\r\nCurrently we  implement atomic that only supports `memory_order_relaxed` for redis in `atomicvar.h`, so I implement operations with sequentially-consistent ordering, just like  the default behavior of `_Atomic`.\r\n\r\n> More details for memory_order, we can see https://en.cppreference.com/w/c/atomic/memory_order\r\n\r\nCurrently, atomic variables we used in Redis are as follow\r\n```\r\nserver.unixtime\r\nserver.lruclock\r\nserver.next_client_id\r\nserver.stat_total_writes_processed\r\nserver.stat_total_reads_processed\r\nserver.stat_net_output_bytes\r\nserver.stat_net_input_bytes\r\nserver.client_max_querybuf_len\r\n```\r\n\r\nFor `server.unixtime`, `server.lruclock`, `server.next_client_id`, Salvatore started using atomic variable from commits\r\nhttps://github.com/redis/redis/commit/ece658713b659513e2c43a9498da286cafec17dd https://github.com/redis/redis/commit/1f598fc2bb6975417751486405303d98246bb7bc\r\nI just restored these operations back\r\n\r\nFor `stat_total_writes_processed`, `stat_total_reads_processed`, `stat_net_output_bytes`,`stat_net_input_bytes`, we still \r\n use redis atomic implemented before\uff0cthat means we use `memory_order_relaxed` access operations for them, it is ok because there is no happen-before relationship.\r\n\r\nFor `client_max_querybuf_len`, I don't find we write and read it simultaneously in redis, there is no data race, so i think it is safe to read in IO threads.\r\n\r\nOther work i did is that we can test Redis with Helgrind, Now there still are 3 errors in helgrind report when we run redis with a module, as flow. I think there is no risk. For 1, it means redis still holds 4 locks(io_threads_mutex) when redis exits. For 2, 3, that is because we may acquire two mutex `moduleGIL` and `io_threads_mutex` but they are unrelated actully.\r\n```\r\n1 Thread #1: Exiting thread still holds 4 locks\r\n\r\n2 Thread #1: lock order \"0x77DB48 before 0x767D00\" violated\r\nAddress 0x767d00 is 0 bytes inside data symbol \"moduleGIL\"\r\n\r\n3 Thread #1: lock order \"0x767D00 before 0x77DB48\" violated\r\nAddress 0x77db48 is 40 bytes inside data symbol \"io_threads_mutex\"\r\n```",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-24T16:17:46Z",
        "closed_at": "2020-08-24T19:59:57Z",
        "merged_at": "2020-08-24T19:59:57Z",
        "body": "`%d` is format specifiers of printf instead of sdscatfmt, for signed int, it should be `%i`, so we couldn't see right info of `io_threads_active` when execute INFO server command.\r\n\r\nbefore\r\n`io_threads_active:d`\r\nafter\r\n`io_threads_active`:1",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-24T09:42:26Z",
        "closed_at": "2020-08-24T10:54:34Z",
        "merged_at": "2020-08-24T10:54:34Z",
        "body": "There is a data race #7391 I think we don't solve it currently.\r\nIf two threads get `bug_report_start` simultaneously, both of them will execute the following instructions if `bug_report_start == 0`, because read and write of `bug_report_start` is not atomic. \r\n```\r\n if (bug_report_start == 0) {\r\n    serverLogRaw(LL_WARNING|LL_RAW,\r\n        \"\\n\\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\\n\");\r\n    bug_report_start = 1;\r\n}\r\n```\r\nwe can use CAS of `_Atomic` to implement it efficiently https://en.cppreference.com/w/c/atomic/atomic_compare_exchange. But I think we should not introduce complicated operations with atomic variables, and this data race only will happen when more than one threads panic simultaneously. So i use `pthread_mutex_t ` easily to fix it.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-24T08:41:24Z",
        "closed_at": "2020-09-25T00:43:35Z",
        "merged_at": null,
        "body": "Let the write command with multiple keys and the write command with single key have consistent behavior when a slot is unstable.\r\n\r\nNow the two are different. For example, we are migrating slot 12706 from node A to node B and we know the key k1 belongs to the slot 12706. When a client executes \"set k1 a\" on node B with ASKING flag and node B doesn't have k1, the client can execute the command successfully. But when the client executes \"mset k1 a a{k1} aa\" in the same situation, it will receive \"TRYGAGIN\".\r\n\r\nThis commit fix this, let the client receive the same \"TRYAGAIN\".",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-08-24T07:42:52Z",
        "closed_at": "2020-08-24T09:54:56Z",
        "merged_at": "2020-08-24T09:54:56Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 570,
        "deletions": 43,
        "changed_files": 20,
        "created_at": "2020-08-23T16:19:13Z",
        "closed_at": "2020-12-13T20:31:42Z",
        "merged_at": null,
        "body": "This introduces a new `tlsauth` module that makes it possible to use TLS client-side certificates together with ACL configuration. Being a module, it makes it possible to easily extend the naive configuration provided to match more complex real-world needs.\r\n\r\nThis also introduces a change to the modules source tree: current modules which are essentially examples with no real-world use are moved to an `examples/` directory, and the `modules/` main directory is reserved to usable modules. In the future we may consider adding additional modules that have real-world value but don't necessarily have to be part of core Redis.\r\n\r\nResolves #6899.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2020-08-23T10:55:25Z",
        "closed_at": "2020-08-27T06:19:25Z",
        "merged_at": "2020-08-27T06:19:25Z",
        "body": "If the server gets MULTI command followed by only read\r\ncommands, and right before it gets the EXEC it reaches OOM,\r\nthe client will get OOM response.\r\n\r\nSo, from now on, it will get OOM response only if there was\r\nat least one command that was tagged with `use-memory` flag",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-08-21T20:13:41Z",
        "closed_at": "2020-10-16T12:36:07Z",
        "merged_at": null,
        "body": "This PR resolves an issue observed on RedisGraph in which prematurely closing clients that are awaiting replies from Redis can cause a server crash on a broken pipe.\r\n\r\nI can consistently reproduce this issue locally, but the code to do so is unfortunately non-trivial. The reproduction steps include instantiating several clients, issuing RedisGraph commands which return data, then disconnecting the clients without waiting for responses.\r\n\r\nThis causes a crash with the stack trace:\r\n```\r\nThread 1 \"redis-server\" received signal SIGPIPE, Broken pipe.\r\n0x00007ffff76252c7 in __libc_write (fd=16, buf=0x555555a74bc8, nbytes=140) at ../sysdeps/unix/sysv/linux/write.c:27\r\n27      ../sysdeps/unix/sysv/linux/write.c: No such file or directory.\r\n\r\n#0  0x00007ffff76252c7 in __libc_write (fd=16, buf=0x555555a74bc8, nbytes=140) at ../sysdeps/unix/sysv/linux/write.c:27\r\n#1  0x000055555564dfef in connSocketWrite (conn=0x555555a1ec50, data=0x555555a74bc8, data_len=140) at connection.c:168\r\n#2  0x00005555555a2af7 in connWrite (conn=0x555555a1ec50, data=0x555555a74bc8, data_len=140) at connection.h:140\r\n#3  0x00005555555a630f in writeToClient (c=0x555555a74970, handler_installed=0) at networking.c:1311\r\n#4  0x00005555555a6780 in handleClientsWithPendingWrites () at networking.c:1426\r\n#5  0x00005555555ab64c in handleClientsWithPendingWritesUsingThreads () at networking.c:3063\r\n#6  0x0000555555590ef5 in beforeSleep (eventLoop=0x555555928230) at server.c:2202\r\n#7  0x000055555558a42a in aeProcessEvents (eventLoop=0x555555928230, flags=27) at ae.c:443\r\n#8  0x000055555558a748 in aeMain (eventLoop=0x555555928230) at ae.c:539\r\n#9  0x00005555555996fe in main (argc=2, argv=0x7fffffffd2a8) at server.c:5325\r\n```\r\n\r\nThis issue is reproduce on unstable and 6.0.6, this PR resolves it on both.\r\n",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2020-08-21T19:52:22Z",
        "closed_at": "2020-08-23T07:17:44Z",
        "merged_at": "2020-08-23T07:17:44Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-08-20T20:55:29Z",
        "closed_at": "2020-08-21T20:37:50Z",
        "merged_at": "2020-08-21T20:37:50Z",
        "body": "The following warnings were generated when I do make to build binary today..\r\n\r\nserver.c:2556:10: warning: unused variable 'buf' [-Wunused-variable]\r\n    char buf[64];\r\n         ^\r\nserver.c:2555:9: warning: unused variable 'val' [-Wunused-variable]\r\n    int val;\r\n        ^\r\nserver.c:2554:9: warning: unused variable 'fd' [-Wunused-variable]\r\n    int fd;\r\n        ^\r\n3 warnings generated.\r\n\r\nThis PR fixes these warnings ...",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 62,
        "deletions": 36,
        "changed_files": 6,
        "created_at": "2020-08-20T20:04:23Z",
        "closed_at": "2020-08-27T08:09:32Z",
        "merged_at": "2020-08-27T08:09:32Z",
        "body": "During a long AOF or RDB loading, the memory stats were not updated, and\r\nINFO would return stale data, specifically about fragmentation and RSS.\r\nIn the past some of these were sampled directly inside the INFO command,\r\nbut were moved to cron as an optimization.\r\n\r\nThis commit introduces a concept of loadingCron which should take\r\nsome of the responsibilities of serverCron.\r\nIt attempts to limit it's rate to approximately the server Hz, but may\r\nnot be very accurate.\r\n\r\nIn order to avoid too many system call, we use the cached ustime, and\r\nalso make sure to update it in both AOF loading and RDB loading inside\r\nprocessEventsWhileBlocked (it seems AOF loading was missing it).",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2020-08-20T15:56:01Z",
        "closed_at": "2020-08-23T13:03:31Z",
        "merged_at": "2020-08-23T13:03:31Z",
        "body": "For example,\r\nbefore the fix we get:\r\n```\r\ntelnet localhost 6379\r\nTrying 127.0.0.1...\r\nConnected to localhost.\r\nEscape character is '^]'.\r\nLPUSH l a\r\n:1\r\nLPOS l b RANK 5 COUNT 10\r\n*-4\r\n```\r\n\r\nand after the fix we get:\r\n```\r\ntelnet localhost 6379\r\nTrying 127.0.0.1...\r\nConnected to localhost.\r\nEscape character is '^]'.\r\nLPUSH l a\r\n:1\r\nLPOS l b RANK 5 COUNT 10\r\n*0\r\n```",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-08-20T07:19:17Z",
        "closed_at": "2020-08-20T20:31:07Z",
        "merged_at": "2020-08-20T20:31:07Z",
        "body": "In `module.c/processModuleLoadingProgressEvent`, there is two bugs.\r\n`fi` should be `RedisModuleLoadingProgressV1`, not `RedisModuleFlushInfoV1`.\r\n```\r\nRedisModuleFlushInfoV1 fi = {REDISMODULE_LOADING_PROGRESS_VERSION,\r\n                                     server.hz,\r\n                                     progress};\r\n```\r\n\r\nWhen calculating `progress`, we should use `server.loading_loaded_bytes`. Otherwise, `progress` will always be 1024.\r\n\r\n```\r\nif (server.loading_total_bytes)\r\n      progress = (server.loading_total_bytes<<10) / server.loading_total_bytes;\r\n```",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-19T19:47:54Z",
        "closed_at": "2020-08-20T02:13:33Z",
        "merged_at": "2020-08-20T02:13:33Z",
        "body": "Correcting the variable to clusterMsgModule.\r\n\r\nFixes -\r\nhttps://github.com/redis/redis/issues/7634",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-08-19T03:09:37Z",
        "closed_at": "2020-08-19T07:52:54Z",
        "merged_at": "2020-08-19T07:52:54Z",
        "body": "I found the meaning of `slave.repldboff` is different from disk-backed replication when use diskless replication. \r\nIt's easy to misunderstand for `slave->repldboff = nwritten;`, at first glance, I think it's an error,  I think we should add comments on it.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-18T21:33:13Z",
        "closed_at": "2020-08-20T02:07:44Z",
        "merged_at": "2020-08-20T02:07:44Z",
        "body": "hmset and hset use the same code path, but we print the wrong error out when using hset. ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-08-18T18:32:20Z",
        "closed_at": "2022-04-10T07:56:00Z",
        "merged_at": null,
        "body": "When redis-cli trying to connect with the Redis server for the very first time and if it finds the server isn't reachable which is running behind NAT, the commands gets blocked for 2 to 3 mins. This creates unnecessary delay when redis-cli is being used in shell script. Updated the redisConnect to redisConnectWithTimeout function and introduced a constant timeout of 1.5 seconds.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 7,
        "changed_files": 4,
        "created_at": "2020-08-18T10:18:01Z",
        "closed_at": "2020-08-20T05:59:03Z",
        "merged_at": "2020-08-20T05:59:03Z",
        "body": "After fork, the child process(redis-aof-rewrite) will get the fd opened\r\nby the parent process(redis), when redis killed by kill -9, it will not\r\ngraceful exit(call prepareForShutdown()), so redis-aof-rewrite thread may still\r\nalive, the fd(lock) will still be held by redis-aof-rewrite thread, and\r\nredis restart will fail to get lock, means fail to start.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-08-18T08:31:08Z",
        "closed_at": "2020-08-18T16:08:00Z",
        "merged_at": "2020-08-18T16:08:00Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-18T05:39:13Z",
        "closed_at": "2020-08-18T06:53:59Z",
        "merged_at": "2020-08-18T06:53:59Z",
        "body": "Since users often post just the crash log in github issues, the log\r\nprint that's above it is missing.\r\nNo reason not to include the size in the panic message itself.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-18T04:11:51Z",
        "closed_at": "2020-08-18T05:50:04Z",
        "merged_at": "2020-08-18T05:50:04Z",
        "body": "the REDISMODULE_NO_EXPIRE was predefined in redismodule.h ... we should using this rather than hardcoded -1 in order to avoid future problem if we decided to change its value.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-17T19:54:07Z",
        "closed_at": "2020-08-18T05:28:43Z",
        "merged_at": "2020-08-18T05:28:43Z",
        "body": "65a3307bc9 added rejectCommand which takes an robj reply and passes it\r\nthrough addReplyErrorSafe to addReplyErrorLength.\r\nThe robj contains newline at it's end, but addReplyErrorSafe converts it\r\nto spaces, and passes it to addReplyErrorLength which adds the protocol\r\nnewlines.\r\n\r\nThe result was that most error replies (like OOM) had extra two trailing\r\nspaces in them.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2020-08-17T05:15:38Z",
        "closed_at": "2020-08-18T09:58:35Z",
        "merged_at": null,
        "body": "No need to check lobj each time inside for loop suggested by #6003.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 723,
        "deletions": 627,
        "changed_files": 13,
        "created_at": "2020-08-15T22:06:54Z",
        "closed_at": "2020-08-16T05:54:37Z",
        "merged_at": "2020-08-16T05:54:37Z",
        "body": "This commit updates Hiredis to use totally unique symbol names for its bundled copy of the SDS library.\r\n\r\nSee #7609 for a good discussion of why we need to do this as well as all of the different options considered.\r\n\r\nI did my best to verify (manually and with some scripting) that Redis sentinel never directly interacts with Hiredis' sds strings, and that I didn't miss any functions to rename.\r\n\r\nTests seem good but let me know if you'd like any changes to the PR.\r\n\r\nI plan on [removing SDS from hiredis](https://github.com/redis/hiredis/issues/866) at which point we can clean up this fix.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-08-14T17:13:10Z",
        "closed_at": "2020-09-09T16:23:14Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 163,
        "deletions": 97,
        "changed_files": 7,
        "created_at": "2020-08-13T00:11:11Z",
        "closed_at": "2020-09-16T06:16:02Z",
        "merged_at": "2020-09-16T06:16:02Z",
        "body": "# Overview\r\nWhen Redis breaches the `maxmemory` threshold, `freeMemoryIfNeededAndSafe` is used to perform evictions until the memory situation is corrected.  This is a synchronous process, and can cause large periods of unresponsiveness as well as wild variation in command latencies.\r\n\r\n### Extreme use case\r\nConsider a large Redis instance with 268MM keys.  This requires a hash table that's 2GB in size.  Add another 1MM keys and the hash table will require expansion.  Redis will allocate a new 4GB block of memory for the hash table.  (Some time later, after rehashing is complete, the old 2GB table is released.)\r\n\r\nIf the instance was near it's `maxmemory` setting before the rehash, we could suddenly find ourselves 4GB over the limit.  At this point, Redis will stop everything while running the eviction algorithm until 4GB of memory has been freed.  Unresponsiveness in this case might reasonably last several minutes.\r\n\r\n# Solution\r\nUnderstand that when eviction is happening, Redis has already breached `maxmemory`.  Redis does NOT prevent us from breaching `maxmemory`.  At this point, the intent is to recover - however we don't need to recover instantly.\r\n\r\nThis update limits eviction processing to 500us.\r\n* This is enough time to evict a number of keys (definitely more than 1).\r\n* This is small enough to limit fluctuations in latencies due to eviction spikes.\r\n\r\nGiven that eviction processing is run before every command (read/write/other) Redis will quickly return to its nominal memory usage.  Also, this update creates a timer event which will continue to evict in-between client requests until normal memory usage is restored.\r\n\r\nGiven a large eviction event like mentioned above, the effects will be:\r\n* Memory usage will take a little longer to return to normal\r\n* Command processing will be minimally impacted\r\n* CPU will jump to 100% (like today), as all idle time will be used for evictions until memory usage has returned to normal\r\n* Redis will remain fully responsive\r\n\r\nFor more common, smaller, eviction events:\r\n* CPU will behave similarly to without the update (jumping to 100% for a short time)\r\n* Latencies will be smoothed out.  Rather than having 1 command with an extra 10ms latency, 1-20 commands would have an extra 500us latency (and the dead time between commands would be utilized).\r\n* Clients will see more predictable performance\r\n\r\n### Refactoring\r\nIncluded in this update is a slight refactoring.  The original function `freeMemoryIfNeeded` is wrapped by `freeMemoryIfNeededAndSafe` (awkward at best).  Although these routines are identified to \"free memory\", the only technique employed is to evict.  So this is a very general purpose name applied to a very specific function.\r\n\r\nThis update replaces the above 2 routines with `processEvictions` which is more in accordance with what the function actually does.  \"IfNeeded\" and \"AndSafe\" are not needed as it should be obvious that `processEvictions` won't/shouldn't evict if unnecessary or unsafe.\r\n\r\nThe new `processEvictions` function has 3 return codes - `EVICT_OK`, `EVICT_RUNNING`, & `EVICT_FAIL` (nothing left to evict).  The `processCommand` function will only reject (write) commands when `EVICT_FAIL` is returned.\r\n\r\nReferences updated.\r\n\r\n### Dependencies\r\nThis update is dependent on #7644.  The monotonic clock is used for timing the eviction loop.  Once that is approved and merged, This update will be rebased.\r\n\r\n# Caveat\r\nEviction processing can still take a long time in the case that large values (hash/set/etc) are present and `server.lazyfree_lazy_eviction` is false.  However it's generally unlikely that large hashes would be evicted.  In the case that a large hash _was_ evicted, performance would be similar to existing performance.",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-08-11T13:25:28Z",
        "closed_at": "2020-08-18T05:59:25Z",
        "merged_at": "2020-08-18T05:59:25Z",
        "body": "this is a follow up pr for https://github.com/redis/redis/pull/6773, the auth failed message should incude the case if user has been disabled, which is different than normal wrong user-pass. Therefore i think showing this information to user is necessary.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-08-11T08:42:24Z",
        "closed_at": "2020-08-11T10:35:20Z",
        "merged_at": "2020-08-11T10:35:20Z",
        "body": "Improve the output of runtest-cluster and runtest-moduleapi, make it more exactly.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 279,
        "deletions": 90,
        "changed_files": 8,
        "created_at": "2020-08-10T23:30:28Z",
        "closed_at": "2020-08-28T08:54:10Z",
        "merged_at": "2020-08-28T08:54:10Z",
        "body": "## Background\r\nRedis needs an internal API for using a monotonic clock.  Currently, most of Redis uses `gettimeofday`.  This is a problem because time of day can be altered due to system time changes.  The system time can be altered manually or by NTP.  There is actually code, included in `ae.c`, which checks to see if the clock has moved backwards.  However, most of Redis uses `gettimeofday` for timing, etc. - simply assuming that it is a monotonic clock.\r\n\r\n## This update provides the following\r\n1. **An interface for retrieving a monotonic clock.**   `getMonotonicUs` returns a `uint64_t` (aka `monotime`) with the number of micro-seconds from an arbitrary point.  No more messing with `tv_sec`/`tv_usec`.  Simple routines are provided for measuring elapsed milli-seconds or elapsed micro-seconds (the most common use case for a monotonic timer).  No worries about time moving backwards.\r\n2. **High-speed assembler implementation for x86 and ARM.**  The standard method for retrieving the monotonic clock is POSIX.1b (1993):  `clock_gettime(CLOCK_MONOTONIC, timespec*)`.  However, most modern processors provide a constant speed instruction clock which can be retrieved in a fraction of the time that it takes to call `clock_gettime`.  For x86, this is provided by the `RDTSC` instruction.  For ARM, this is provided by the `CNTVCT_EL0` instruction.  As a compile-time option, these high-speed timers can be chosen.  (Default is POSIX `clock_gettime`.)\r\n3. **Refactor of event loop timers.**  The timer processing in `ae.c` has been refactored to use the new monotonic clock interface.  This results in simpler/cleaner logic and improved performance.\r\n\r\n## Performance implications\r\n1. When using the processor's instruction clock, timer processing in the event loop is dramatically improved.  In a test program, overall timer event processing has been measured to run at nearly 6x performance.  (Test details below.)\r\n2. When using the POSIX `clock_gettime`, timer processing is improved about 18% due to streamlined logic in `ae.c`.\r\n\r\n## Test details\r\nSimple test program creating an event loop and 3 timers - each firing with 0ms delay.  Linux host, with processor: Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz.  The events each incremented a counter.  A `beforeSleep` proc checked elapsed time (using the monotonic clock) until 5 seconds had passed.  Compared average count of timers fired.\r\n\r\nNote: on my test machine, `clock_gettime` is measured to run about 5% slower than `gettimeofday`.  However it is expected that this varies on system-to-system and that they are nominally similar in terms of performance.\r\n\r\n## Future updates\r\nIntroduction of this API provides the opportunity to begin replacing `ustime` and `mstime` calls throughout the code.  With the huge performance difference provided by using the processor clock, this will provide benefits to any feature which relies on time-based code.\r\n\r\n## For those unfamiliar with the x86 TSC\r\nThe x86 maintains a counter which is based on a constant clock rate at the nominal clocking of the processor.  Even though individual cores are running at various and changing clock rates, they all share the same constant-rate clock.  On multi-CPU systems, all CPUs are sync'd to a common clock source.\r\n\r\nThough you can still find articles about older CPUs that did not support a constant-rate TSC, all of the reasonably modern x86 and AMD processors support constant-rate TSC.  On Linux, you can check for the `constant_tsc` flag in `/proc/cpuinfo`.\r\n\r\nNote also, though the TSC runs at a constant rate at the nominal speed of the processor, the processor itself doesn't actually know it's speed.  The OS will report the speed in `/proc/cpuinfo` (Linux).  On Windows, the `QueryPerformanceFrequency` function may be used.  The ticks-per-second needs to be used to convert the TSC to common units of time (milli-seconds/micro-seconds).\r\n\r\nThe TSC is not guaranteed to be wall-clock precise - it may suffer from clock-drift and is not corrected by NTP.  It's fine for timing short intervals, but (for example) system uptime should be measured with a different mechanism (like `gettimeofday`).\r\n\r\n",
        "comments": 37
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-08-10T15:10:21Z",
        "closed_at": "2020-08-11T05:11:48Z",
        "merged_at": "2020-08-11T05:11:48Z",
        "body": "Appears to be handled by server.stream_node_max_bytes in reality.\r\n\r\nAnd a couple of spaces autoremoved by my editor.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-08-10T02:20:28Z",
        "closed_at": "2020-08-11T09:30:34Z",
        "merged_at": "2020-08-11T09:30:34Z",
        "body": "The two lines allow systemd to start redis.service after the network is online. Only after the network is online that Redis could bind to IP address other than 127.0.0.1 during initial boot up process.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-08-08T12:53:53Z",
        "closed_at": "2020-08-09T03:08:01Z",
        "merged_at": "2020-08-09T03:08:01Z",
        "body": "When runtest-cluster, at first, we need to create a cluster use spawn_instance,\r\na port which is not used is choosen, however sometimes we can't run server on\r\nthe port. possibley due to a race with another process taking it first.\r\nsuch as redis/redis/runs/896537490. It may be due to the machine problem or\r\nIn order to reduce the probability of failure when start redis in\r\nruntest-cluster, we attemp to use another port when find server do not start up.\r\n\r\nCo-authored-by: yanhui13 <yanhui13@meituan.com>\r\nCo-authored-by: Oran Agra <oran@redislabs.com>",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 64,
        "changed_files": 9,
        "created_at": "2020-08-08T08:08:34Z",
        "closed_at": "2020-08-12T03:04:55Z",
        "merged_at": "2020-08-12T03:04:55Z",
        "body": "Saw a couple of recent PRs not use the helper, so replaced all the instances the codebase doesn't follow the convention outside of checks for multiple different types, hopefully nudge people int he right direction. \r\n\r\nI also refactored a couple of points related to the helper to move the check closer to the key lookup and reduce the nesting on some ifs, to hopefully improve readability. I didn't refactor places with open CRs, hopefully to avoid any merge conflicts when they aren't necessary. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 15,
        "changed_files": 5,
        "created_at": "2020-08-07T09:48:43Z",
        "closed_at": "2020-08-11T05:18:10Z",
        "merged_at": "2020-08-11T05:18:10Z",
        "body": "Hi, This PR avoid invalid signalKeyAsReady.\r\n\r\n[Edit] by @guybe7 advise\r\nAvoid redundant calls to signalKeyAsReady\r\nsignalKeyAsReady has some overhead (namely dictFind) so we should\r\nonly call it when a client is blocked on the relevant type (BLOCKED_*)\r\n\r\n[Edit]\r\nThe benefits of refactoring are as follows:\r\n  1) The judgment like `val->type == OBJ_LIST` can be cancelled in dbAdd\r\n  the code is clearer.\r\n  2) By judging `server.blocked_clients_by_type[btype]`, avoid the cost\r\n  of invalid calling `dictFind(db->blocking_keys)`.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-08-07T05:07:07Z",
        "closed_at": "2020-08-08T11:42:33Z",
        "merged_at": "2020-08-08T11:42:33Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-07T03:18:29Z",
        "closed_at": "2020-09-09T16:23:17Z",
        "merged_at": null,
        "body": "fix spelling in redis.conf",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-08-06T12:45:28Z",
        "closed_at": "2020-08-07T21:19:19Z",
        "merged_at": "2020-08-07T21:19:19Z",
        "body": "The else block would be executed when `newlen == 0` and in the case memmove won't be called, so there's no need to set `start`.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-06T07:48:50Z",
        "closed_at": "2020-08-11T11:31:12Z",
        "merged_at": "2020-08-11T11:31:12Z",
        "body": "I notice that `redis-benchmark` doesn't support `zset` type. But `zset` is basic data type of redis, so I add `zadd` and `zrem` tests.\r\n\r\nBTW, `redis-benchmark` also is useful to test performance  for  some key/value servers that support redis protocol. ",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-08-06T07:30:03Z",
        "closed_at": "2020-09-09T16:23:20Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 57,
        "deletions": 3,
        "changed_files": 5,
        "created_at": "2020-08-05T23:48:46Z",
        "closed_at": "2020-08-11T08:55:07Z",
        "merged_at": "2020-08-11T08:55:07Z",
        "body": "This is a rebased version of #3078 originally by shaharmor\r\nwith the following patches by TysonAndre made after rebasing\r\nto work with the updated C API:\r\n\r\n1. Add 2 more unit tests\r\n   (wrong argument count error message, integer over 64 bits)\r\n2. Use addReplyArrayLen instead of addReplyMultiBulkLen.\r\n3. Undo changes to src/help.h - for the ZMSCORE PR,\r\n   I heard those should instead be automatically\r\n   generated from the redis-doc repo if it gets updated\r\n\r\nMotivations:\r\n\r\n- Example use case: Client code to efficiently check if each element of a set\r\n  of 1000 items is a member of a set of 10 million items.\r\n  (Similar to reasons for working on #7593)\r\n- HMGET and ZMSCORE already exist. This may lead to developers deciding\r\n  to implement functionality that's best suited to a regular set with a\r\n  data type of sorted set or hash map instead, for the multi-get support.\r\n\r\nCurrently, multi commands or lua scripting to call sismember multiple times\r\nwould almost definitely be less efficient than a native smismember\r\nfor the following reasons:\r\n\r\n- Need to fetch the set from the string every time\r\n  instead of reusing the C pointer.\r\n- Using pipelining or multi-commands would result in more bytes sent\r\n  and received by the client for the repeated SISMEMBER KEY sections.\r\n- Need to specially encode the data and decode it from the client\r\n  for lua-based solutions.\r\n- Proposed solutions using Lua or SADD/SDIFF could trigger writes to\r\n  memory, which is undesirable on a redis replica server\r\n  or when commands get replicated to replicas.\r\n\r\nCo-Authored-By: Shahar Mor <shahar@peer5.com>\r\nCo-Authored-By: Tyson Andre <tysonandre775@hotmail.com>",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-05T16:18:05Z",
        "closed_at": "2020-08-05T18:14:46Z",
        "merged_at": null,
        "body": "...line arguments are passed to redis-server\r\n\r\nFixes #4567\r\n\r\n(best issue number ever)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-08-04T10:51:37Z",
        "closed_at": "2020-08-05T09:06:34Z",
        "merged_at": "2020-08-05T09:06:34Z",
        "body": "as discussed in #7599, adding a force option to the 'create-cluster create' command",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2020-08-04T10:40:37Z",
        "closed_at": "2020-08-05T15:30:44Z",
        "merged_at": "2020-08-05T15:30:44Z",
        "body": "the variable was introduced only in the 5.0 branch in #5879 bc6c1c40db. But therefore was never merged to unstable and therefore to the 6.0 branch. I suggest if the PR is accepted, it should also be applied to the 6.0 branch.\r\n\r\nsee also #7599",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 2544,
        "deletions": 559,
        "changed_files": 47,
        "created_at": "2020-08-03T22:35:18Z",
        "closed_at": "2020-08-09T08:19:04Z",
        "merged_at": "2020-08-09T08:19:04Z",
        "body": "Bullet points:\r\n\r\n* Bump hiredis to v1.0.0\r\n* Adds support for `RESP3` push messages in redis-cli (e.g. `SUBSCRIBE`/client tracking messages)\r\n* Adds an option `--print-push-msgs` to display the messages (maybe this should be the default for invalidations?)\r\n\r\nI'm happy to rework how invalidations are displayed but it seemed reasonable to use a special format (like redirections) for that when in normal mode, but output in csv/raw when requested.\r\n\r\nHere's how it looks:\r\n```bash\r\n127.0.0.1:6379> client tracking on\r\nOK\r\n127.0.0.1:6379> get k1\r\n\"v123\"\r\n127.0.0.1:6379> get k2\r\n\"v999\"\r\n127.0.0.1:6379> set k1 newvalue\r\n-> invalidate: 'k1'\r\nOK\r\n127.0.0.1:6379> set k2 newvalue\r\n-> invalidate: 'k2'\r\nOK\r\n```",
        "comments": 33
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-08-03T02:27:18Z",
        "closed_at": "2020-08-04T09:29:31Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-08-01T12:29:31Z",
        "closed_at": "2020-08-04T05:53:50Z",
        "merged_at": "2020-08-04T05:53:50Z",
        "body": "apparenlty on github actions sometimes 500ms is not enough",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2125,
        "deletions": 56,
        "changed_files": 10,
        "created_at": "2020-07-31T16:45:22Z",
        "closed_at": "2020-08-25T18:21:30Z",
        "merged_at": "2020-08-25T18:21:30Z",
        "body": "Fixes #3535 \r\n\r\nThis PR is the first step to enable a consistent full percentile analysis on query latency so that we can fully understand the performance and stability characteristics of the redis-server system we are measuring. It also improves the instantaneous reported metrics, and the csv output format. \r\nApart from extending the reported metrics it should also reduce the overhead of keeping all latency samples on an array. We are moving from keeping track of all data, to keeping track of a data sketch by making usage of an HDR histogram. We've also micro-benchmarked insertions to ensure that this will not impact the benchmark himself ( adding a datapoint takes 6 ns, [results here](https://github.com/HdrHistogram/HdrHistogram_c/pull/76) ).\r\n\r\n## 1) changes to benchmark summary \r\nCurrently `redis-benchmark` outputs variable precision latency distributions that make it hard to compare multiple runs, given that there is no ensurance that we can retrieve fixed quantile information (we iterate over latency numbers and not quantiles leading to q50, q95, q99,etc... sometimes not being presented ). \r\nHere is an example of the current summary output:\r\n### 1.1) Current latency report\r\n```\r\n====== SET ======\r\n  100000 requests completed in 1.06 seconds\r\n  50 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n  host configuration \"save\": \r\n  host configuration \"appendonly\": no\r\n  multi-thread: no\r\n\r\n0.00% <= 0.1 milliseconds\r\n0.00% <= 0.2 milliseconds\r\n92.27% <= 0.3 milliseconds\r\n99.05% <= 0.4 milliseconds\r\n99.77% <= 0.5 milliseconds\r\n99.96% <= 0.6 milliseconds\r\n100.00% <= 0.7 milliseconds\r\n100.00% <= 0.7 milliseconds\r\n93984.96 requests per second\r\n```\r\n### 1.2) Proposed latency report\r\n-  output a logarithmic percentile spectrum ( focusing at the high percentiles)\r\n- output one liner with minimum ( p0 ), p50, p95, p99, max (p100), and avg(just to be backwards compatible)\r\n- together with the latency histogram, output the cumulative count of samples so we can quickly relate both\r\n```\r\n====== SET ======                                                     \r\n  100000 requests completed in 0.75 seconds\r\n  50 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n  host configuration \"save\": 3600 1 300 100 60 10000\r\n  host configuration \"appendonly\": no\r\n  multi-thread: no\r\n\r\nLatency by percentile distribution:\r\n0.000% <= 0.119 milliseconds (cumulative count 1)\r\n50.000% <= 0.175 milliseconds (cumulative count 57365)\r\n75.000% <= 0.191 milliseconds (cumulative count 80250)\r\n87.500% <= 0.215 milliseconds (cumulative count 88194)\r\n93.750% <= 0.295 milliseconds (cumulative count 93964)\r\n96.875% <= 0.447 milliseconds (cumulative count 96879)\r\n98.438% <= 0.567 milliseconds (cumulative count 98465)\r\n99.219% <= 0.679 milliseconds (cumulative count 99222)\r\n99.609% <= 0.863 milliseconds (cumulative count 99617)\r\n99.805% <= 0.999 milliseconds (cumulative count 99808)\r\n99.902% <= 1.103 milliseconds (cumulative count 99910)\r\n99.951% <= 2.135 milliseconds (cumulative count 99952)\r\n99.976% <= 2.615 milliseconds (cumulative count 99976)\r\n99.988% <= 2.839 milliseconds (cumulative count 99988)\r\n99.994% <= 2.975 milliseconds (cumulative count 99994)\r\n99.997% <= 3.031 milliseconds (cumulative count 99997)\r\n99.998% <= 3.079 milliseconds (cumulative count 99999)\r\n99.999% <= 3.095 milliseconds (cumulative count 100000)\r\n100.000% <= 3.095 milliseconds (cumulative count 100000)\r\n\r\nCumulative distribution of latencies:\r\n0.000% <= 0.103 milliseconds (cumulative count 0)\r\n86.348% <= 0.207 milliseconds (cumulative count 86348)\r\n94.222% <= 0.303 milliseconds (cumulative count 94222)\r\n96.402% <= 0.407 milliseconds (cumulative count 96402)\r\n97.752% <= 0.503 milliseconds (cumulative count 97752)\r\n98.831% <= 0.607 milliseconds (cumulative count 98831)\r\n99.324% <= 0.703 milliseconds (cumulative count 99324)\r\n99.504% <= 0.807 milliseconds (cumulative count 99504)\r\n99.689% <= 0.903 milliseconds (cumulative count 99689)\r\n99.816% <= 1.007 milliseconds (cumulative count 99816)\r\n99.910% <= 1.103 milliseconds (cumulative count 99910)\r\n99.942% <= 1.207 milliseconds (cumulative count 99942)\r\n99.948% <= 1.303 milliseconds (cumulative count 99948)\r\n99.949% <= 1.407 milliseconds (cumulative count 99949)\r\n99.950% <= 1.503 milliseconds (cumulative count 99950)\r\n99.951% <= 2.103 milliseconds (cumulative count 99951)\r\n100.000% <= 3.103 milliseconds (cumulative count 100000)\r\n\r\nSummary:\r\n  throughput summary: 132802.12 requests per second\r\n  latency summary (msec):\r\n          avg       min       p50       p95       p99       max\r\n        0.195     0.112     0.175     0.343     0.631     3.095\r\n```\r\n\r\n## 2) changes to the reported instant metrics\r\nThe current output of while running a benchmark can be misleading given that it presents the overall average ops/sec. Right aside to the overall ops/sec we output the instantaneous ops/sec. In addition to it, we output also the average overall and instantaneous latency.\r\n\r\n### 2.1) current output while running benchmark\r\n```\r\nSET: 95375.08\r\n```\r\n\r\n### 2.2) Proposed output while running benchmark\r\n```\r\nSET: rps=154504.0 (overall: 152518.7) avg_msec=0.159 (overall: 0.159)\r\n```\r\n\r\n## 3) changes to the csv output\r\nfollowing the proposed changes to the overall latency report we've added the latency metrics ( and consequently a header given that we have now more than just ops/sec on the csv ).\r\n\r\n### 3.1) current csv output\r\n```\r\nredis-benchmark --csv\r\n\"PING_INLINE\",\"94161.95\"\r\n\"PING_BULK\",\"93632.96\"\r\n\"SET\",\"93720.71\"\r\n\"GET\",\"94339.62\"\r\n\"INCR\",\"94339.62\"\r\n\"LPUSH\",\"94073.38\"\r\n\"RPUSH\",\"94786.73\"\r\n\"LPOP\",\"94696.97\"\r\n\"RPOP\",\"94339.62\"\r\n\"SADD\",\"94517.96\"\r\n\"HSET\",\"94161.95\"\r\n\"SPOP\",\"94250.71\"\r\n\"LPUSH (needed to benchmark LRANGE)\",\"95328.88\"\r\n\"LRANGE_100 (first 100 elements)\",\"60753.34\"\r\n\"LRANGE_300 (first 300 elements)\",\"27314.94\"\r\n\"LRANGE_500 (first 450 elements)\",\"20820.32\"\r\n\"LRANGE_600 (first 600 elements)\",\"16716.82\"\r\n\"MSET (10 keys)\",\"104058.27\"\r\n```\r\n\r\n### 3.3) Proposed csv output\r\n```\r\nredis-benchmark --csv\r\n\"test\",\"rps\",\"avg_latency_ms\",\"min_latency_ms\",\"p50_latency_ms\",\"p95_latency_ms\",\"p99_latency_ms\",\"max_latency_ms\"\r\n\"PING_INLINE\",\"131752.31\",\"0.196\",\"0.112\",\"0.183\",\"0.279\",\"0.639\",\"1.863\"\r\n\"PING_BULK\",\"125313.29\",\"0.208\",\"0.080\",\"0.199\",\"0.295\",\"0.575\",\"5.367\"\r\n\"SET\",\"135501.36\",\"0.191\",\"0.104\",\"0.191\",\"0.247\",\"0.359\",\"1.655\"\r\n\"GET\",\"141242.94\",\"0.182\",\"0.112\",\"0.175\",\"0.231\",\"0.375\",\"1.207\"\r\n\"INCR\",\"129701.68\",\"0.199\",\"0.104\",\"0.183\",\"0.335\",\"0.423\",\"3.351\"\r\n\"LPUSH\",\"135135.14\",\"0.194\",\"0.112\",\"0.191\",\"0.247\",\"0.431\",\"2.695\"\r\n\"RPUSH\",\"136798.91\",\"0.190\",\"0.088\",\"0.183\",\"0.247\",\"0.367\",\"1.567\"\r\n\"LPOP\",\"159489.64\",\"0.171\",\"0.080\",\"0.159\",\"0.271\",\"0.551\",\"1.583\"\r\n\"RPOP\",\"139470.02\",\"0.186\",\"0.112\",\"0.183\",\"0.239\",\"0.359\",\"1.503\"\r\n\"SADD\",\"134770.89\",\"0.192\",\"0.104\",\"0.191\",\"0.231\",\"0.343\",\"2.311\"\r\n\"HSET\",\"138504.16\",\"0.188\",\"0.088\",\"0.183\",\"0.231\",\"0.327\",\"1.303\"\r\n\"SPOP\",\"142247.52\",\"0.183\",\"0.096\",\"0.175\",\"0.239\",\"0.383\",\"1.183\"\r\n\"LPUSH (needed to benchmark LRANGE)\",\"129032.27\",\"0.201\",\"0.096\",\"0.199\",\"0.247\",\"0.367\",\"1.871\"\r\n\"LRANGE_100 (first 100 elements)\",\"66844.91\",\"0.384\",\"0.248\",\"0.367\",\"0.487\",\"0.751\",\"5.255\"\r\n\"LRANGE_300 (first 300 elements)\",\"23218.02\",\"1.093\",\"0.320\",\"0.975\",\"1.807\",\"2.631\",\"8.103\"\r\n\"LRANGE_500 (first 450 elements)\",\"12818.87\",\"1.968\",\"0.520\",\"1.727\",\"3.511\",\"4.519\",\"13.839\"\r\n\"LRANGE_600 (first 600 elements)\",\"14015.42\",\"1.784\",\"0.304\",\"1.703\",\"2.671\",\"3.199\",\"14.167\"\r\n\"MSET (10 keys)\",\"121065.38\",\"0.339\",\"0.096\",\"0.327\",\"0.495\",\"0.631\",\"1.055\"\r\n```",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-07-31T11:25:36Z",
        "closed_at": "2020-08-04T06:29:57Z",
        "merged_at": null,
        "body": "This is a followup on my PR #5879 for Redis 6.0. \r\nWith Redis 6.0 redis-cli has an option to force cluster creation (without interaction) via the '--cluster-yes' option. Unfortunately there is still no '--force' option for the 'create-cluster create' script command.\r\nTherefore I created this pull request to add a simple '-f' option to the script command. A more versatile approach would be to generically pass-thorough command-line options to redis-cli, but I wanted to keep the pull request small (to begin with).",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 77,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2020-07-31T02:42:38Z",
        "closed_at": "2020-08-04T14:49:34Z",
        "merged_at": "2020-08-04T14:49:34Z",
        "body": "Syntax: `ZMSCORE KEY MEMBER [MEMBER ...]`\r\n\r\nThis is an extension of https://github.com/redis/redis/pull/2359\r\namended by Tyson Andre to work with the changed unstable API,\r\nadd more tests, and consistently return an array.\r\n\r\n- It seemed as if it would be more likely to get reviewed\r\n  after updating the implementation.\r\n\r\nCurrently, multi commands or lua scripting to call zscore multiple times\r\nwould almost definitely be less efficient than a native ZMSCORE\r\nfor the following reasons:\r\n\r\n- Need to fetch the set from the string every time instead of reusing the C\r\n  pointer.\r\n- Using pipelining or multi-commands would result in more bytes sent by\r\n  the client for the repeated `ZMSCORE KEY` sections.\r\n- Need to specially encode the data and decode it from the client\r\n  for lua-based solutions.\r\n- The fastest solution I've seen for large sets(thousands or millions)\r\n  involves lua and a variadic ZADD, then a ZINTERSECT, then a ZRANGE 0 -1,\r\n  then UNLINK of a temporary set (or lua). This is still inefficient.\r\n\r\nCo-authored-by: Tyson Andre <tysonandre775@hotmail.com>",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-07-30T08:01:03Z",
        "closed_at": "2020-08-07T20:01:15Z",
        "merged_at": "2020-08-07T20:01:15Z",
        "body": "@madolson @trevor211  I reopen a PR to fix the problem in https://github.com/redis/redis/pull/7571 ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2020-07-30T06:49:03Z",
        "closed_at": "2020-07-31T10:01:40Z",
        "merged_at": "2020-07-31T10:01:40Z",
        "body": "this seems like leftover from before 6eb51bf",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-07-30T06:12:40Z",
        "closed_at": "2020-08-12T07:23:55Z",
        "merged_at": "2020-08-12T07:23:55Z",
        "body": "fix spelling in rax.c\r\nFix hasActiveChildProcess function comment error in server.c",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 205,
        "deletions": 112,
        "changed_files": 5,
        "created_at": "2020-07-29T14:47:26Z",
        "closed_at": "2020-08-06T13:47:29Z",
        "merged_at": "2020-08-06T13:47:28Z",
        "body": "This makes it possible to add tests that generate assertions, and run\r\nthem with valgrind, making sure that there are no memory violations\r\nprior to the assertion.\r\n\r\nNew config options:\r\n- crash-log-enabled - can be disabled for cleaner core dumps\r\n- crash-memcheck-enabled - useful for faster termination after a crash\r\n- use-exit-on-panic - to be used by the test suite so that valgrind can\r\n  detect leaks and memory corruptions\r\n\r\nOther changes:\r\n- Crash log is printed even on system that dont HAVE_BACKTRACE, i.e. in\r\n  both SIGSEGV and assert / panic\r\n- Assertion and panic won't print registers and code around EIP (which\r\n  was useless), but will do fast memory test (which may still indicate\r\n  that the assertion was due to memory corrpution)\r\n\r\nI had to reshuffle code in order to re-use it, so i extracted come code\r\ninto function without actually doing any changes to the code:\r\n- logServerInfo\r\n- logModulesInfo\r\n- doFastMemoryTest (with the exception of it being conditional)\r\n- dumpCodeAroundEIP\r\n\r\nchanges to the crash report on segfault:\r\n- logRegisters is called right after the stack trace (before info) done\r\n  just in order to have more re-usable code\r\n- stack trace skips the first two items on the stack (the crash log and\r\n  signal handler functions)",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-07-29T13:45:56Z",
        "closed_at": "2020-07-31T10:14:29Z",
        "merged_at": "2020-07-31T10:14:29Z",
        "body": "besides, hooks test was time sensitive. when the replica managed to\r\nreconnect quickly after the client kill, the test would fail",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 50,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-07-29T12:14:30Z",
        "closed_at": "2020-07-30T10:25:11Z",
        "merged_at": "2020-07-30T10:25:11Z",
        "body": "",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-07-28T16:54:45Z",
        "closed_at": "2020-08-08T19:28:45Z",
        "merged_at": "2020-08-08T19:28:45Z",
        "body": "in Sentinel mode, the command help was missing, this pr add brief Sentinel subcommands description in help.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 83,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2020-07-28T09:33:57Z",
        "closed_at": "2020-08-09T03:11:48Z",
        "merged_at": "2020-08-09T03:11:48Z",
        "body": "PR follow the discussion on https://github.com/redis/redis/issues/7544\r\n\r\nAdded RedisModule_HoldString that either returns a shallow copy of the given String (by increasing the String ref count) or a new deep copy of String in case it's not possible to get a shallow copy.\r\n\r\nNotice that I still wonder if passing the ctx in this case and add the String to its auto memory handling is the correct approach. Maybe we should just not allow passing a ctx in this case and say that HoldString keeps the String handling in the module writer hands and he needs to free it (no auto memory in this case).\r\n\r\nLet me know what you think\r\n\r\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2020-07-27T12:25:54Z",
        "closed_at": "2020-07-28T08:32:48Z",
        "merged_at": "2020-07-28T08:32:48Z",
        "body": "The connection API may create an accepted connection object in an error\r\nstate, and callers are expected to check it before attempting to use it.\r\n\r\nThis is an improvement over #7569 provided by @mrpre\r\n\r\nCo-authored-by: mrpre <mrpre@163.com>",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-07-27T11:24:25Z",
        "closed_at": "2020-07-27T22:55:16Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2020-07-27T11:06:04Z",
        "closed_at": "2020-07-27T12:31:54Z",
        "merged_at": "2020-07-27T12:31:53Z",
        "body": "Initialize and configure OpenSSL even when tls-port is not used, because\r\nwe may still have tls-cluster or tls-replication.\r\n\r\nAlso, make sure to reconfigure OpenSSL when these parameters are changed\r\nas TLS could have been enabled for the first time.\r\n\r\nFixes #7565 ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 34,
        "changed_files": 3,
        "created_at": "2020-07-27T10:41:32Z",
        "closed_at": "2020-07-28T08:15:30Z",
        "merged_at": "2020-07-28T08:15:30Z",
        "body": "- the test now waits for specific set of log messages rather than wait for\r\n  timeout looking for just one message.\r\n- we don't wanna sample the current length of the log after an action, due\r\n  to a race, we need to start the search from the line number of the last\r\n  message we where waiting for.\r\n- when attempting to trigger a full sync, use multi-exec to avoid a race\r\n  where the replica manages to re-connect before we completed the set of\r\n  actions that should force a full sync.\r\n- fix verify_log_message which was broken and unused",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-07-27T07:47:22Z",
        "closed_at": "2020-07-30T01:05:28Z",
        "merged_at": "2020-07-30T01:05:27Z",
        "body": "When a slot's migration is over (after the destination node receive the command of \"CLUSTER SETSLOT <SLOT> NODE <NODE ID>\") , the max delay for other node to know the node who is responsible for the slot is server.cluster_node_timeout/2 (7.5s), which may lead to a client receive moved error for two times. \r\nSo, if broadcast a PONG message when slot's migration is over, the other node will know that as soon as possible.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 22,
        "changed_files": 1,
        "created_at": "2020-07-27T06:52:40Z",
        "closed_at": "2020-07-30T01:02:34Z",
        "merged_at": null,
        "body": "Update the pong_received of node B  in node A by ping/pong from node C will net the Ping/Pong between A and B reduce, and this will block the slots info sync between A and B by ping/pong.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2020-07-27T06:02:20Z",
        "closed_at": "2020-07-28T08:35:17Z",
        "merged_at": null,
        "body": "To fix #7565 \r\n\r\n(1). Redis miss the return value check of `SSL_new`.  \r\n\r\n(2). It' disgusting for redis to check whether `ssl` is null if `connCreateAcceptedxxx `  returns an allocated `conn` with null `ssl` item, so I just free the `conn` inside the the `connCreateAcceptedxxx ` . May be we should add such api like `connSuccess(xx)` to check the connection.  \r\n\r\n(3). Since (2), I think the TLS will not be the only protocol to be used in the future which means `connCreateAcceptedxxx` is likely to return null  and `acceptCommonHandler` should be protected.  \r\n\r\nmore detail:\r\n\r\n![image](https://user-images.githubusercontent.com/5129205/88508582-c072ec80-d011-11ea-8dd2-3bf5e56e9ac7.png)\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2020-07-27T05:44:50Z",
        "closed_at": "2020-07-27T17:36:50Z",
        "merged_at": null,
        "body": "While loading the data during diskless replication, the replica can receive a message on cluster bus stating that master\r\nof this node has changed. This message will be processed when rdbLoadRio yields to process file based events. When the\r\nmessage is processed, the connection with the current master is closed. This cleans up the ssl state related to FD used\r\nfor replicating data from the old master. So when the control came back to rdbLoadRio, it tries to perform a read on\r\nthe FD who's connection has been freed. This causes a crash.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-27T04:46:51Z",
        "closed_at": "2020-09-09T16:23:23Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-25T18:41:18Z",
        "closed_at": "2020-07-26T05:27:31Z",
        "merged_at": "2020-07-26T05:27:30Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-07-23T15:10:56Z",
        "closed_at": "2020-07-29T05:22:55Z",
        "merged_at": "2020-07-29T05:22:55Z",
        "body": "Before this commit, the XGROUP subfunction such as add/delete consumer group in stream object doesn't call SignalModifiedKey hook, although the stream object in keyspace is updated. This commit added the SignalModifiedKey hook in these related functions which changes the stream object in keyspace.\r\n",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2020-07-23T13:42:25Z",
        "closed_at": "2020-09-13T15:40:00Z",
        "merged_at": "2020-09-13T15:40:00Z",
        "body": "It is not enough to change `$::instances_count` to run sentinel tests on more instances than five because some tests assume that there are exactly five instances. These changes refactor such tests.  ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-07-23T06:10:44Z",
        "closed_at": "2020-07-23T10:06:25Z",
        "merged_at": "2020-07-23T10:06:25Z",
        "body": "on ci.redis.io the test fails a lot, reporting that bgsave didn't end.\r\nincreaseing the timeout we wait for that bgsave to get aborted.\r\nin addition to that, i also verify that it indeed got aborted by\r\nchecking that the save counter wasn't reset.\r\n\r\nadd another test to verify that a successful bgsave indeed resets the\r\nchange counter.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-07-22T13:06:22Z",
        "closed_at": "2020-07-23T09:37:44Z",
        "merged_at": "2020-07-23T09:37:44Z",
        "body": "this code is in use only if the master is disk-based, and the replica is\r\ndiskless. In this case we use a buffered reader, but we must avoid reading\r\npast the rdb file, into the command stream. which Luckly rdb.c doesn't\r\nreally attempt to do (it knows how much it should read).\r\n\r\nWhen rioConnRead detects that the extra buffering attempt reaches beyond\r\nthe read limit it should read less, but if the caller actually requested\r\nmore, then it should return with an error rather than a short read. the\r\nbug would have resulted in short read.\r\n\r\nin order to fix it, the code must consider the real requested size, and\r\nnot the extra buffering size.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2020-07-22T07:34:20Z",
        "closed_at": "2020-07-27T07:18:12Z",
        "merged_at": null,
        "body": "When runtest-cluster, at first, we need to create a cluster use spawn_instance, a port which is not used is choosen, however we can't run server on the port. such as https://github.com/redis/redis/runs/896537490. It may be due to the machine problem or another process take up the port at once. As a result, runtest-cluster failed. In order to reduce the probability of failure when start redis in runtest-cluster, we attemp to use another port when find server do not start up.",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-22T07:09:12Z",
        "closed_at": "2020-07-22T08:33:58Z",
        "merged_at": "2020-07-22T08:33:58Z",
        "body": "Fix typo in deps/README.md",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-21T15:30:51Z",
        "closed_at": "2020-10-27T06:50:15Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-07-21T09:27:32Z",
        "closed_at": "2020-07-21T11:07:07Z",
        "merged_at": "2020-07-21T11:07:07Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2020-07-21T07:21:22Z",
        "closed_at": "2020-11-23T11:14:49Z",
        "merged_at": null,
        "body": "The current sentinel's master->flag status is sdown for a long time,when not enough sentinels are available to vote for a failover leader. Such as many other sentinels are unlinkable. Master node will not take failover in time if it real shutdown.\r\n\r\ncase:\r\n5 sentinels, while 3 of them are unlinkable .  Right now, master node has a disaster.  2 of 5 sentinels could't vote-for-leader , so when check sentinel status, it always show \"+sdown\".\r\n\r\na possible solution:\r\n1\u3001we asume current sentinel's master->flags is odown,if \r\n\t\t* 1)other sentinel connected number is 0 or\r\n\t\t* 2) connected sentinels< master->quorum, and none of these connected sentinels agree the master is ok.\r\n2\u3001we asume the failover leader is sentinel.myid,if \r\n\t* 1)other sentinel connect number is 0 or\r\n\t* 2) connected sentinels< master->quorum,  and none of these connected sentinels agree the master is ok.\r\n\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-07-21T03:22:24Z",
        "closed_at": "2020-07-22T00:00:14Z",
        "merged_at": "2020-07-22T00:00:14Z",
        "body": "When loading from an rdb, redis aborts on any file error except for ENOENT (File not found). However, the function rdbLoad can fail for other reasons such as parsing issues. You can therefor run into the unlikely state of a previous non-terminal failure producing a a ENOENT error code and then rdbLoad failing because of a parsing issue which doesn't set errno. This results in a partially up server with partially loaded data. \r\n\r\nI did run into this somehow while testing with a corrupted RDB, but I don't remember this specific details, so it's not purely theoretical. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-21T01:09:32Z",
        "closed_at": "2020-07-21T06:41:45Z",
        "merged_at": "2020-07-21T06:41:45Z",
        "body": "Before that PR, processCommand() did not notice that cmd could be a module\r\ncommand in which case getkeys_proc member has a different meaning.\r\n\r\nThe outcome was that a module command which doesn't take any key names in its\r\narguments (similar to SLOWLOG) would be handled as if it might have key name arguments\r\n(similar to MEMORY), would consider cluster redirect but will end up with 0 keys\r\nafter an excessive call to getKeysFromCommand, and eventually do the right thing.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-07-20T08:24:53Z",
        "closed_at": "2020-07-20T12:33:07Z",
        "merged_at": "2020-07-20T12:33:07Z",
        "body": "Before this commit, processCommand() did not notice that cmd \r\ncould be a module command which is designed to use 0 number\r\nof keys, just like some non-module command, e.g. `flushall`, \r\n`info` and so on, and handled it for the purpose of cluster redirect it\r\n as if it does use any keys.\r\n\r\nThis commit fixed it by reusing the codes in addReplyCommand().",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-07-20T06:41:26Z",
        "closed_at": "2020-07-20T14:21:56Z",
        "merged_at": "2020-07-20T14:21:56Z",
        "body": "replica should handle -NOPERM error after in `REPL_STATE_RECEIVE_PONG` state, in case default user disable PING command.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1346,
        "deletions": 466,
        "changed_files": 65,
        "created_at": "2020-07-19T13:48:35Z",
        "closed_at": "2020-07-20T18:08:27Z",
        "merged_at": "2020-07-20T18:08:27Z",
        "body": "to be merged with a **rebase merge**.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 135,
        "deletions": 4,
        "changed_files": 8,
        "created_at": "2020-07-19T12:39:04Z",
        "closed_at": "2020-07-23T09:38:52Z",
        "merged_at": "2020-07-23T09:38:52Z",
        "body": "The new event will be fired after a key was loaded from RDB and added to the keyspace.\r\nThe new event will be triggered to modules which registered to REDISMODULE_NOTIFY_GENERIC event type.\r\n\r\nThis PR also introduce tests for the new notification inside a new test file called: keyspace_events.c\r\n\r\nnotice that basically there are no tests for keyspace notification API so I thought it make sense to\r\nadd a new test file for this and add the future keyspace notification tests in this file.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-07-19T12:34:04Z",
        "closed_at": "2020-07-20T10:57:55Z",
        "merged_at": "2020-07-20T10:57:55Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2020-07-18T18:16:15Z",
        "closed_at": "2020-07-20T06:22:25Z",
        "merged_at": "2020-07-20T06:22:25Z",
        "body": "Proposing updating the version of `actions/checkout` used in `workflows/ci.yml` to the latest - `v2`: https://github.com/actions/checkout#checkout-v2",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2020-07-17T05:39:01Z",
        "closed_at": "2022-05-19T15:16:39Z",
        "merged_at": null,
        "body": "* Changed link step to use compiler driver instead of linker so we could just use `-m32` / `-m64` instead of needing to know which emulation target to choose.  This also fixes 32-bit builds for [all?] non-i386 platforms.\r\n* Detect whether Redis was built 32 or 64-bit and select `-m32` / `-m64` accordingly\r\n* Made `32bit` target an alias for `all` since it was no longer needed",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 63,
        "deletions": 19,
        "changed_files": 4,
        "created_at": "2020-07-17T02:00:02Z",
        "closed_at": "2020-07-21T05:13:06Z",
        "merged_at": "2020-07-21T05:13:06Z",
        "body": "",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-07-16T22:14:05Z",
        "closed_at": "2020-07-19T14:01:02Z",
        "merged_at": null,
        "body": "If you use send a redis command with too many arguments (example: MSET) you will get the message:\r\n\r\nProtocol error: invalid multibulk length\r\n\r\nI had to google to figure out what that means (and apparently so do other people) so I made the error message better.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-07-16T18:32:38Z",
        "closed_at": "2020-07-17T12:20:55Z",
        "merged_at": "2020-07-17T12:20:55Z",
        "body": "Ref: https://github.com/redis/redis-io/issues/207",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-16T14:55:48Z",
        "closed_at": "2020-07-16T17:59:39Z",
        "merged_at": "2020-07-16T17:59:39Z",
        "body": "Specifically, the key passed to the module aof_rewrite callback is a stack allocated robj. When passing it to RedisModule_EmitAOF (with appropriate \"s\" fmt string) redis used to panic when trying to inc the ref count of the stack allocated robj. Now support such robjs by coying them to a new heap robj. This doesn't affect performance because using the alternative \"c\" or \"b\" format strings also copies the input to a new heap robj.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 193,
        "deletions": 21,
        "changed_files": 6,
        "created_at": "2020-07-15T16:28:23Z",
        "closed_at": "2020-09-24T09:02:41Z",
        "merged_at": "2020-09-24T09:02:41Z",
        "body": "XREADGROUP auto-creates the consumer inside the consumer group the\r\nfirst time it saw it.\r\nWhen XREADGROUP is being used with NOACK option, the message will not\r\nbe added into the client's PEL and XGROUP SETID would be propagated.\r\nWhen the replica gets the XGROUP SETID it will only update the last delivered\r\nid of the group, but will not create the consumer.\r\n\r\nSo, in this commit XGROUP CREATECONSUMER is being added.\r\nCommand pattern: XGROUP CREATECONSUMER <key> <group> <consumer>.\r\n\r\nWhen NOACK option is being used, createconsumer command would be\r\npropagated as well.\r\n\r\nIn case of AOFREWRITE, consumer with an empty PEL would be saved with\r\nXGROUP CREATECONSUMER whereas consumer with pending entries would be\r\nsaved with XCLAIM",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-07-15T01:48:22Z",
        "closed_at": "2020-11-25T21:37:54Z",
        "merged_at": "2020-11-25T21:37:54Z",
        "body": "Merge two aeDeleteFileEvent refs into one",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-14T10:15:12Z",
        "closed_at": "2020-07-14T15:04:08Z",
        "merged_at": "2020-07-14T15:04:08Z",
        "body": "this test when run with valgrind on github actions takes 160 seconds",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-07-14T08:42:01Z",
        "closed_at": "2020-07-14T17:22:01Z",
        "merged_at": "2020-07-14T17:22:00Z",
        "body": "in case the rdb child failed, crashed or terminated unexpectedly redis\r\nwould have marked the replica clients with repl_put_online_on_ack and\r\nthen kill them only after a minute when no ack was received.\r\n\r\nit would not stream anything to these connections, so the only effect of\r\nthis bug is a delay of 1 minute in the replicas attempt to re-connect.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-07-14T07:11:50Z",
        "closed_at": "2020-07-14T08:35:05Z",
        "merged_at": "2020-07-14T08:35:05Z",
        "body": "Before this commit, the output of \"./runtest-cluster --help\" is incorrect.\r\nAfter this commit, the format of the following 3 output is consistent:\r\n./runtest --help\r\n./runtest-cluster --help\r\n./runtest-sentinel --help",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-07-13T15:51:55Z",
        "closed_at": "2020-07-14T02:43:52Z",
        "merged_at": null,
        "body": "when compare user password, not to compare all chars at some times.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-07-13T12:08:42Z",
        "closed_at": "2020-07-13T13:09:09Z",
        "merged_at": "2020-07-13T13:09:09Z",
        "body": "in the majority of the cases (on this rarely used feature) we want to\r\nstop and be able to connect to the shard with redis-cli.\r\nsince these are two different processes interracting with the tty we\r\nneed to stop both, and we'll have to hit enter twice, but it's not that\r\nbad considering it is rarely used.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2020-07-13T11:08:37Z",
        "closed_at": "2020-07-13T13:40:04Z",
        "merged_at": "2020-07-13T13:40:04Z",
        "body": "interestingly the latency monitor test fails because valgrind is slow\r\nenough so that the time inside PEXPIRE command from the moment of\r\nthe first mstime() call to get the basetime until checkAlreadyExpired\r\ncalls mstime() again is more than 1ms, and that test was too sensitive.\r\n\r\nusing this opportunity to speed up the test (unrelated to the failure)\r\nthe fix is just the longer time passed to PEXPIRE.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-07-13T10:03:35Z",
        "closed_at": "2020-07-13T13:40:20Z",
        "merged_at": "2020-07-13T13:40:20Z",
        "body": "a silly leak i recently introduced.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2020-07-11T16:45:45Z",
        "closed_at": "2020-07-22T13:24:40Z",
        "merged_at": "2020-07-22T13:24:40Z",
        "body": "Process proposal for vulnerability review and disclosure, as mentioned in #7497.\r\n\r\nMy goals for this:\r\n* Lightweight for the core team who already have a lot of issues to slog through\r\n* Clear and fair for security researchers and contributors\r\n\r\nI added a reference in the README so people doing a quick search on the GitHub page will know where to look.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 5,
        "changed_files": 6,
        "created_at": "2020-07-10T11:11:43Z",
        "closed_at": "2020-07-28T07:45:21Z",
        "merged_at": "2020-07-28T07:45:21Z",
        "body": "In TLS there are three common scene\r\n1. Server doesn't need client certificate.\r\n2. Server requires client's certificate and verifies it and abort the connection if verify fail.  \r\n3. Server requests client's certificate not mandatorily. \r\n\r\nScene 3 can be used in access control in the future. Server can grants or denies some client operation based on whether client sends the certificate or not.   ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-10T07:54:34Z",
        "closed_at": "2020-07-10T13:02:19Z",
        "merged_at": "2020-07-10T13:02:19Z",
        "body": "Fix the typo and add building guide about tls",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 106,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2020-07-09T21:00:06Z",
        "closed_at": "2021-02-08T14:14:29Z",
        "merged_at": null,
        "body": "And minor edits to README",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-09T02:16:41Z",
        "closed_at": "2020-07-10T05:29:45Z",
        "merged_at": "2020-07-10T05:29:45Z",
        "body": "In defrag.c function `activeDefragSdsListAndDict`:\r\n```\r\n        sdsele = ln->value;\r\n        if ((newsds = activeDefragSds(sdsele))) {\r\n            /* When defragging an sds value, we need to update the dict key */\r\n            uint64_t hash = dictGetHash(d, sdsele);\r\n```\r\nIf `activeDefragSds` return not NULL\uff0csdsele is released, we can't use it to calculate hash.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 383,
        "deletions": 4,
        "changed_files": 9,
        "created_at": "2020-07-08T17:08:54Z",
        "closed_at": "2021-01-29T13:38:31Z",
        "merged_at": "2021-01-29T13:38:31Z",
        "body": "This PR enables tracking time of the background tasks and on replies, opening the door for properly tracking commands that rely on blocking/background work via the slowlog, latency history, and commandstats. \r\n\r\nSome notes:\r\n- The time spent blocked waiting for key changes, or blocked on synchronous replication is not accounted for. \r\n\r\n- **This PR does not affect latency tracking of commands that are non-blocking or do not have background work.** ( meaning that it all stays the same with exception to `BZPOPMIN`,`BZPOPMAX`,`BRPOP`,`BLPOP`, etc... and module's commands that rely on background threads ). \r\n\r\n-  Specifically for latency history command we've added a new event class named `command-unblocking` that will enable latency monitoring on commands that spawn background threads to do the work.\r\n\r\n- For blocking commands we're now considering the total time of a command as the time spent on call() + the time spent on replying when unblocked.\r\n\r\n- For Modules commands that rely on background threads we're now considering the total time of a command as the time spent on call (main thread) + the time spent on the background thread ( if marked within `RedisModule_MeasureTimeStart()` and `RedisModule_MeasureTimeEnd()` ) + the time spent on replying (main thread)\r\n\r\n\r\n## test\r\nTo test for this feature we've added a `unit/moduleapi/blockonbackground` test that relies on a module that blocks the client and sleeps on the background for a given time. \r\n- check blocked command that uses RedisModule_MeasureTimeStart() is tracking background time\r\n- check blocked command that uses RedisModule_MeasureTimeStart() is tracking background time even in timeout\r\n- check blocked command with multiple calls RedisModule_MeasureTimeStart()  is tracking the total background time\r\n- check blocked command without calling RedisModule_MeasureTimeStart() is not reporting background time\r\n\r\n## Samples\r\n### current behaviour ( not tracking time spent on background ):\r\n```\r\n127.0.0.1:6379> config set slowlog-log-slower-than 100000\r\nOK\r\n127.0.0.1:6379> slowlog len\r\n(integer) 0\r\n127.0.0.1:6379> block.debug 0 10000\r\n(integer) 343176910\r\n127.0.0.1:6379> slowlog len\r\n(integer) 0\r\n127.0.0.1:6379> block.debug 2000 10000\r\n(integer) 1207730440\r\n(2.00s)\r\n127.0.0.1:6379> slowlog len\r\n(integer) 0\r\n```\r\n#### commandstats example\r\n```\r\n127.0.0.1:6379> block.debug 2000 10000\r\n(integer) 1768881473\r\n(2.00s)\r\n127.0.0.1:6379> block.debug 4000 10000\r\n(integer) 630830182\r\n(4.00s)\r\n127.0.0.1:6379> info commandstats\r\n# Commandstats\r\ncmdstat_block.debug:calls=2,usec=226,usec_per_call=113.00\r\n```\r\n\r\n### new behaviour ( tracking time spend on background ):\r\n\r\n```\r\n127.0.0.1:6379> config set slowlog-log-slower-than 100000\r\nOK\r\n127.0.0.1:6379> slowlog len\r\n(integer) 0\r\n127.0.0.1:6379> block.debug 0 10000\r\n(integer) 1471490433\r\n127.0.0.1:6379> slowlog len\r\n(integer) 0\r\n127.0.0.1:6379> block.debug 2000 10000\r\n(integer) 1127835047\r\n(2.00s)\r\n127.0.0.1:6379> slowlog len\r\n(integer) 1\r\n127.0.0.1:6379> slowlog get 1\r\n1) 1) (integer) 0\r\n   2) (integer) 1593712320\r\n   3) (integer) 2053204\r\n   4) 1) \"block.debug\"\r\n      2) \"2000\"\r\n      3) \"10000\"\r\n   5) \"127.0.0.1:51818\"\r\n   6) \"\"\r\n```\r\n#### commandstats example\r\n```\r\n127.0.0.1:6379> block.debug 2000 10000\r\n(integer) 1052512224\r\n(2.00s)\r\n127.0.0.1:6379> block.debug 4000 10000\r\n(integer) 215961991\r\n(4.00s)\r\n127.0.0.1:6379> info commandstats\r\n# Commandstats\r\ncmdstat_block.debug:calls=2,usec=6099877,usec_per_call=3049938.50\r\n```\r\n#### latency monitor example\r\n```\r\n127.0.0.1:6379> config set latency-monitor-threshold 200\r\nOK\r\n127.0.0.1:6379> block.debug 500 1000\r\n(integer) 1887208743\r\n(0.50s)\r\n127.0.0.1:6379> latency history command-unblocking\r\n1) 1) (integer) 1611265472\r\n   2) (integer) 500\r\n```\r\n",
        "comments": 22
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 23,
        "changed_files": 2,
        "created_at": "2020-07-08T12:35:27Z",
        "closed_at": "2020-07-11T12:52:42Z",
        "merged_at": "2020-07-11T12:52:42Z",
        "body": "solves #7484 ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 56,
        "deletions": 40,
        "changed_files": 1,
        "created_at": "2020-07-08T11:25:10Z",
        "closed_at": "2020-07-10T13:37:12Z",
        "merged_at": "2020-07-10T13:37:12Z",
        "body": "fix benchmark in cluster mode fails to authenticate #6817 ",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-08T05:33:44Z",
        "closed_at": "2020-07-20T10:14:27Z",
        "merged_at": "2020-07-20T10:14:27Z",
        "body": "",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-08T05:23:46Z",
        "closed_at": "2020-07-15T09:37:45Z",
        "merged_at": "2020-07-15T09:37:45Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-07-08T05:18:31Z",
        "closed_at": "2020-07-09T00:11:44Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-07T13:25:17Z",
        "closed_at": "2020-07-07T23:09:08Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 68,
        "deletions": 26,
        "changed_files": 5,
        "created_at": "2020-07-07T12:42:08Z",
        "closed_at": "2020-07-12T10:55:27Z",
        "merged_at": "2020-07-12T10:55:27Z",
        "body": "* update daily CI to include cluster and sentinel tests\r\n* update daily CI to run when creating a new release\r\n* update release scripts to work on the new redis.io hosts",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 5,
        "created_at": "2020-07-07T12:38:41Z",
        "closed_at": "2020-07-10T05:25:26Z",
        "merged_at": "2020-07-10T05:25:26Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 3,
        "changed_files": 6,
        "created_at": "2020-07-07T05:21:32Z",
        "closed_at": "2020-07-15T09:38:23Z",
        "merged_at": "2020-07-15T09:38:23Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 16,
        "changed_files": 4,
        "created_at": "2020-07-05T15:29:00Z",
        "closed_at": "2020-07-10T07:02:37Z",
        "merged_at": "2020-07-10T07:02:37Z",
        "body": "Similarly to EXPIREAT with TTL in the past, which implicitly deletes the\r\nkey and return success, RESTORE should not store key that are already\r\nexpired into the db.\r\nWhen used together with REPLACE it should emit a DEL to keyspace\r\nnotification and replication stream.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 21,
        "changed_files": 1,
        "created_at": "2020-07-05T09:45:15Z",
        "closed_at": "2020-07-10T07:07:49Z",
        "merged_at": "2020-07-10T07:07:48Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2020-07-04T14:05:04Z",
        "closed_at": "2020-07-15T17:53:42Z",
        "merged_at": "2020-07-15T17:53:41Z",
        "body": "Fixes #7463 \r\nWhen a flush occurs, redis intends to send clients with tracking enabled an empty string to indicate that all keys are invalid. Before, the code contains a bug that sends a bulk string of length 1 when it means to use 0.\r\n\r\nRegardless, there is no safe bulk string to use for a special value for \"invalidate all\" because any bulk string potentially conflicts with a valid Redis key.\r\n\r\nInstead, send a RESP NULL to indicate flush.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 97,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2020-07-04T13:18:38Z",
        "closed_at": "2020-07-16T18:01:28Z",
        "merged_at": "2020-07-16T18:01:28Z",
        "body": "This adds three templates for submitting crash, bug and feature request issues in the repository.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-03T18:12:18Z",
        "closed_at": "2020-07-10T13:05:30Z",
        "merged_at": "2020-07-10T13:05:30Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-07-02T12:32:54Z",
        "closed_at": "2020-07-11T19:51:59Z",
        "merged_at": null,
        "body": "There are 32 bits. So the max value is `2^32-1`. But now it is `32^2-1`.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-07-02T11:25:43Z",
        "closed_at": "2020-07-10T07:32:22Z",
        "merged_at": "2020-07-10T07:32:22Z",
        "body": "This fixes #7437 and basically avoids asking for or verifying client side certificates if `tls-auth-clients` is disabled.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 166,
        "deletions": 52,
        "changed_files": 3,
        "created_at": "2020-07-02T09:13:14Z",
        "closed_at": "2020-07-10T07:25:56Z",
        "merged_at": "2020-07-10T07:25:56Z",
        "body": "This adds support for several `redis-cli` options that were not yet supported with TLS:\r\n\r\n* `redis-cli --rdb`\r\n* `redis-cli --pipe`\r\n* `redis-cli --replica`\r\n\r\nAs a bonus, this PR also reinstates `redis-cli` tests that were broken and thereby disabled for the last 10 years as well as additional coverage for the above functions.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-07-02T06:36:51Z",
        "closed_at": "2020-07-09T06:44:44Z",
        "merged_at": null,
        "body": "Fix some configuration in redisCommandTable\r\n",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-07-01T19:10:24Z",
        "closed_at": "2020-07-15T09:44:04Z",
        "merged_at": "2020-07-15T09:44:04Z",
        "body": "Export following environment variables before building on macOS on Apple silicon\r\n\r\nexport ARCH_FLAGS=\"-arch arm64\"\r\nexport SDK_NAME=macosx\r\nexport SDK_PATH=$(xcrun --show-sdk-path --sdk $SDK_NAME)\r\nexport CFLAGS=\"$ARCH_FLAGS -isysroot $SDK_PATH -I$SDK_PATH/usr/include\"\r\nexport CXXFLAGS=$CFLAGS\r\nexport LDFLAGS=\"$ARCH_FLAGS\"\r\nexport CC=\"$(xcrun -sdk $SDK_PATH --find clang) $CFLAGS\"\r\nexport CXX=\"$(xcrun -sdk $SDK_PATH --find clang++) $CXXFLAGS\"\r\nexport LD=\"$(xcrun -sdk $SDK_PATH --find ld) $LDFLAGS\"\r\n\r\nmake\r\nmake test\r\n..\r\nAll tests passed without errors!\r\n\r\nBacktrack logging assumes x86 and required updating.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-07-01T18:34:26Z",
        "closed_at": "2020-07-10T07:30:10Z",
        "merged_at": "2020-07-10T07:30:10Z",
        "body": "Fixes #7448.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 21,
        "changed_files": 1,
        "created_at": "2020-07-01T11:29:10Z",
        "closed_at": "2020-07-16T05:57:28Z",
        "merged_at": "2020-07-16T05:57:28Z",
        "body": "Hi,\r\nThis PR prevents multiple calls to streamParseStrictIDOrReply in `xdel` and `xack`, which simplifies the code and avoids redundant operations.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 14,
        "changed_files": 3,
        "created_at": "2020-06-30T14:35:52Z",
        "closed_at": "2020-07-29T05:46:45Z",
        "merged_at": "2020-07-29T05:46:45Z",
        "body": "Resolves  #7173",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-06-30T04:24:52Z",
        "closed_at": "2020-07-15T09:38:48Z",
        "merged_at": "2020-07-15T09:38:48Z",
        "body": "The error message for custer mode & non-cluster mode need switched",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-06-29T21:39:54Z",
        "closed_at": "2020-07-29T05:25:57Z",
        "merged_at": "2020-07-29T05:25:57Z",
        "body": "The Redis sentinel would crash with a segfault after a few minutes\r\nbecause it tried to read from a page without read permissions. Check up\r\nfront whether the sds is long enough to contain redis:slave or\r\nredis:master before memcmp() as is done everywhere else in\r\nsentinelRefreshInstanceInfo().\r\n\r\nBug report and commit message from Theo Buehler. Fix from Nam Nguyen.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2020-06-29T06:37:06Z",
        "closed_at": "2020-07-10T05:29:03Z",
        "merged_at": "2020-07-10T05:29:03Z",
        "body": "On some platforms strtold(\"+inf\") with valgrind returns a non-inf result\r\n\r\n[err]: INCRBYFLOAT does not allow NaN or Infinity in tests/unit/type/incr.tcl\r\nExpected 'ERR*would produce*' to equal or match '1189731495357231765085759.....'",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-06-28T10:34:39Z",
        "closed_at": "2020-08-02T10:59:52Z",
        "merged_at": "2020-08-02T10:59:52Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-06-27T14:53:25Z",
        "closed_at": "2021-09-07T11:33:49Z",
        "merged_at": "2021-09-07T11:33:49Z",
        "body": "Fork() in rdb will create a process but not thread. THREAD will mislead new-comers.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2020-06-27T14:08:13Z",
        "closed_at": "2020-09-29T12:52:14Z",
        "merged_at": "2020-09-29T12:52:13Z",
        "body": "",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2020-06-25T13:36:21Z",
        "closed_at": "2021-01-13T20:00:56Z",
        "merged_at": "2021-01-13T20:00:56Z",
        "body": "I found some if condition is always true, and try to remove these unnecessary checks.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-06-25T11:11:33Z",
        "closed_at": "2020-09-09T16:23:26Z",
        "merged_at": null,
        "body": "Some clarification and grammr fix of the config file.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-06-23T16:51:32Z",
        "closed_at": "2020-06-25T10:36:37Z",
        "merged_at": "2020-06-25T10:36:37Z",
        "body": "Changed \"2015\" to \"2020\"",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2020-06-23T14:50:09Z",
        "closed_at": "2022-01-10T06:00:04Z",
        "merged_at": "2022-01-10T06:00:04Z",
        "body": " readonly/readwrite only set client flags for slave in cluster mode, so it should be ok for setting ok-stale and ok-loading command flag",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-06-23T01:24:45Z",
        "closed_at": "2022-02-11T07:15:22Z",
        "merged_at": null,
        "body": "When processing CLUSTER SETSLOT <SLOT> NODE <NODE ID>, it won't check the role\r\nof the specified node, thus causing route inconsistency between the processing\r\nnode and others. Worse still, the slot ownership information on that node won't\r\nchange until another migrating of that slot would happen later on.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-06-22T03:14:01Z",
        "closed_at": "2020-06-22T09:06:51Z",
        "merged_at": "2020-06-22T09:06:51Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 56,
        "deletions": 16,
        "changed_files": 6,
        "created_at": "2020-06-21T18:33:20Z",
        "closed_at": "2020-07-10T08:33:48Z",
        "merged_at": "2020-07-10T08:33:48Z",
        "body": "This adds TLS session caching support and some configuration options to control it.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-06-20T11:01:01Z",
        "closed_at": "2020-06-21T02:07:11Z",
        "merged_at": null,
        "body": "Comments say \"equal or less\",but the codes say \"less\".",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 62,
        "deletions": 62,
        "changed_files": 27,
        "created_at": "2020-06-19T03:00:13Z",
        "closed_at": "2020-09-09T16:23:29Z",
        "merged_at": null,
        "body": "It took half an hour to fix the typos with [CLion Spellchecking\ufeff](https://www.jetbrains.com/help/clion/spellchecking.html).",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 39,
        "deletions": 63,
        "changed_files": 1,
        "created_at": "2020-06-18T02:09:23Z",
        "closed_at": "2020-07-03T03:09:43Z",
        "merged_at": null,
        "body": "Hi,\r\nThis PR reuses the code of `spopwithcount` for `spop`, `spop key` means `spopwithcount key 1`.\r\n\r\n`./runtest --single unit/type/set` is ok, but I\u2019m not sure if `spop` to `srem` will be affected for Replicate/AOF.\r\n\r\nPlease contact me with any suggestions.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2020-06-17T20:53:11Z",
        "closed_at": "2020-06-18T09:26:53Z",
        "merged_at": "2020-06-18T09:26:53Z",
        "body": "**Issue description:**\r\nWhen Redis is running in **Sentinel mode** - shutting down of the server behaves differently in case of:\r\n  1. `SHUTDOWN` command - saving of RDB is explicitly disabled via checking `(server.loading || server.sentinel_mode)`, which sets `SHUTDOWN_NOSAVE` flag\r\n  1. `SIGINT/SIGTERM signal` - although `SHUTDOWN_NOFLAGS` is being used in call to `prepareForShutdown()` - using improper configuration file that defines at least 1 `save ...` option triggers RDB saving, thus in turn could lead to overwriting of an existing RDB file with an empty one (as loading data is skipped in Sentinel mode).\r\n\r\n**Steps to reproduce:**\r\nThis can be easily reproduced by running e.g. `redis-server redis.conf --port 9876 --sentinel` (using default `redis.conf` with `save ...` options) and then stopping it via `ctrl+c` - RDB is saved although it must not.\r\n\r\n**Issue elimination:**\r\nThis change moves the check mentioned in regard to `SHUTDOWN` command to `prepareForShutdown()`, so Sentinel mode is checked consistently in both cases.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-06-17T14:17:47Z",
        "closed_at": "2020-07-22T07:25:53Z",
        "merged_at": null,
        "body": "In the function of rioConnRead, read_so_far is the size of redis get out from the connection buf, buffered is the size of connection buf left, so read_so_far+buffered is the total size of redis read from network. read_limit should compare to read_so_far+buffered instead of read_so_far-buffered.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2595,
        "deletions": 2595,
        "changed_files": 85,
        "created_at": "2020-06-16T17:08:47Z",
        "closed_at": "2020-06-16T20:38:34Z",
        "merged_at": null,
        "body": "This PR renames the following:\r\n- Masters to Primaries\r\n- Master to Primary\r\n- masters to primaries\r\n- master to primary\r\n- Slaves to Secondaries\r\n- Slave to Secondary\r\n- slaves to secondaries\r\n- slave to secondary\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 114,
        "deletions": 57,
        "changed_files": 4,
        "created_at": "2020-06-16T09:02:57Z",
        "closed_at": "2020-07-10T05:26:53Z",
        "merged_at": "2020-07-10T05:26:53Z",
        "body": "DEBUG REATART causes two issues:\r\n1. it uses execve which replaces the original process and valgrind doesn't\r\n   have a chance to check for errors, so leaks go unreported.\r\n2. valgrind report invalid calls to close() which we're unable to resolve.\r\n\r\nSo now the tests use restart_server mechanism in the tests, that terminates\r\nthe old server and starts a new one, new PID, but same stdout, stderr.\r\n\r\nsince the stderr can contain two or more valgrind report, it is not enough\r\nto just check for the absence of leaks, we also need to check for some known\r\nerrors, we do both, and fail if we either find an error, or can't find a\r\nreport saying there are no leaks.\r\n\r\nother changes:\r\n- when killing a server that was already terminated we check for leaks too.\r\n- adding DEBUG LEAK which was used to test it.\r\n- adding --trace-children to valgrind, although no longer needed.\r\n- since the stdout contains two or more runs, we need slightly different way\r\n  of checking if the new process is up (explicitly looking for the new PID)\r\n- move the code that handles --wait-server to happen earlier (before\r\n  watching the startup message in the log), and serve the restarted server too.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-06-16T04:39:14Z",
        "closed_at": "2021-09-15T20:02:50Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2020-06-16T02:51:40Z",
        "closed_at": "2020-06-18T09:29:07Z",
        "merged_at": "2020-06-18T09:29:07Z",
        "body": "fix comments in listpack.c",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-06-15T05:06:18Z",
        "closed_at": "2020-07-21T01:53:04Z",
        "merged_at": "2020-07-21T01:53:04Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-06-15T02:36:54Z",
        "closed_at": "2020-06-25T10:56:27Z",
        "merged_at": "2020-06-25T10:56:27Z",
        "body": "https://github.com/antirez/redis/issues/7395   \r\n I remove \"if (nodeIsMaster(myself))\" judgement before clusterSendFail in markNodeAsFailingIfNeeded, and solved the problem in the above issue.\r\nWhen a slave receive pfails about it's dead master from majority masters, the slave will mark  the dead master as fail, and next clusterCron the slave detect the flags of it's master is fail , so it start a slave failover. But if the slave can't send the fail message to the other node, all other masters may reject to vote as they don't receive the fail broadcast.\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2020-06-14T08:23:40Z",
        "closed_at": "2020-06-15T08:43:34Z",
        "merged_at": "2020-06-15T08:43:34Z",
        "body": "The scan key module API provides the scan callback with the current\r\nfield name and value (if it exists). Those arguments are RedisModuleString*\r\nwhich means it supposes to point to robj which is encoded as a string.\r\nUsing createStringObjectFromLongLong function might return robj that\r\npoints to an integer and so break a module that tries for example to\r\nuse RedisModule_StringPtrLen on the given field/value.\r\n\r\nThe PR introduces a fix that uses the createObject function and sdsfromlonglong function.\r\nUsing those function promise that the field and value pass to the to the\r\nscan callback will be Strings.\r\n\r\nThe PR also changes the Scan test module to use RedisModule_StringPtrLen\r\nto catch the issue. without this, the issue is hidden because\r\nRedisModule_ReplyWithString knows to handle integer encoding of the\r\ngiven robj (RedisModuleString).\r\n\r\nThe PR also introduces a new test to verify the issue is solved.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2020-06-12T06:20:57Z",
        "closed_at": "2021-01-28T16:20:10Z",
        "merged_at": null,
        "body": "When we deploy redis in kubernetes, each pod corresponds to a redis instance or sentinel instance, using the dns address as the pod's fixed external address, we found:\r\n1. When sentinel is configured with \"sentinel announce-ip sentinel-0-svc\", sentinel-0-svc is this sentinel`s dns address, sentinel will always report the \"+sentinel-address-switch\" log when it is started.\r\n2. When slave is configured with \"replicaof redis-0-svc\",  redis-0-svc is the master redis instance`s dns address, sentinel will report the \u201c+slave\u201d log first, followed by the \u201c+fix-slave-config\u201c log\uff0cthis will cause the slave to re-initiate replication to the master.\r\n\r\nSo, in these scenarios, should first convert the dns address (or hostname) to the ip address.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 204,
        "deletions": 91,
        "changed_files": 6,
        "created_at": "2020-06-11T18:11:15Z",
        "closed_at": "2020-06-23T11:12:52Z",
        "merged_at": "2020-06-23T11:12:52Z",
        "body": "In order to support the use of multi-exec in pipeline, it is important that\r\nMULTI and EXEC are never rejected and it is easy for the client to know if the\r\nconnection is still in multi state.\r\n\r\nIt was easy to make sure MULTI and DISCARD never fail (done by previous\r\ncommits) since these only change the client state and don't do any actual\r\nchange in the server, but EXEC is a different story.\r\n\r\nSince in the past, it was possible for clients to handle some EXEC errors and\r\nretry the EXEC, we now can't affort to return any error on EXEC other than\r\nEXECABORT, which now carries with it the real reason for the abort too.\r\n\r\nOther fixes in this commit:\r\n- Some checks that where performed at the time of queuing need to be re-\r\n  validated when EXEC runs, for instance if the transaction contains writes\r\n  commands, it needs to be aborted. there was one check that was already done\r\n  in execCommand (-READONLY), but other checks where missing: -OOM, -MISCONF,\r\n  -NOREPLICAS, -MASTERDOWN\r\n- When a command is rejected by processCommand it was rejected with addReply,\r\n  which was not recognized as an error in case the bad command came from the\r\n  master. this will enable to count or MONITOR these errors in the future.\r\n- make it easier for tests to create additional (non deferred) clients.\r\n- add tests for the fixes of this commit.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 1,
        "changed_files": 6,
        "created_at": "2020-06-10T12:35:24Z",
        "closed_at": "2020-10-27T13:04:19Z",
        "merged_at": "2020-10-27T13:04:18Z",
        "body": "Some discuss about THP side effect on Linux:\r\naccording to http://www.antirez.com/news/84, we can see that\r\nredis latency spikes are caused by linux kernel THP feature.\r\nI have tested on E3-2650 v3, and found that 2M huge page costs\r\nabout 0.25ms to fix COW page fault.\r\n\r\nAlways try to disable THP by prctl syscall, if syscall succeed,\r\nthere is no need to check kernel THP config any more.\r\n\r\nSigned-off-by: zhenwei pi <pizhenwei@bytedance.com>",
        "comments": 15
    },
    {
        "merged": false,
        "additions": 44,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2020-06-09T21:01:59Z",
        "closed_at": "2020-06-12T10:14:56Z",
        "merged_at": null,
        "body": "The `LRANK` command returns the index (position) of a given element\r\nwithin a list. Using the `direction` argument it is possible to specify\r\ngoing from head to tail (acending, 1) or from tail to head (decending,\r\n-1). Only the first found index is returend. The complexity is O(N).\r\n\r\nWhen using lists as a queue it can be of interest at what position a\r\ngiven element is, for instance to monitor a job processing through a\r\nwork queue. This came up within the Python `rq` project which is based\r\non Redis[0].\r\n\r\n[0]: https://github.com/rq/rq/issues/1197\r\n\r\nSigned-off-by: Paul Spooren <mail@aparcar.org>",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2020-06-09T11:59:50Z",
        "closed_at": "2020-12-27T14:37:28Z",
        "merged_at": "2020-12-27T14:37:28Z",
        "body": "As discussed in https://github.com/antirez/redis/issues/7364, it is good\r\nto have a HELLO command variant, which does not switch the current proto\r\nversion of a redis server.\r\n\r\nWhile `HELLO` will work, it introduced a certain difficulty on parsing\r\noptions of the command. We will need to offset the index of authentication\r\nand setname option by -1.\r\n\r\nSo 0 is marked a special version meaning non-switching. And we do not\r\nneed to change the code much.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2020-06-09T04:02:31Z",
        "closed_at": "2020-06-12T10:31:15Z",
        "merged_at": "2020-06-12T10:31:15Z",
        "body": "This PR fixes a server crash error for STRALGO LCS command, it happens when user specifies non string type object associated with KEYS option. Therefore we need to check whether the provided value of KEYS option is string type. reproduce:\r\n\r\n127.0.0.1:6379> hset foo1 bar 1 ttt rrr\r\n(integer) 0\r\n127.0.0.1:6379> hset bar1 bar 1 ttt rrr\r\n(integer) 0\r\n127.0.0.1:6379> STRALGO LCS keys foo1 bar1\r\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\r\nnot connected> \r\n\r\nCrash Log:\r\n\r\n\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n14070:M 08 Jun 2020 23:59:48.834 # ------------------------------------------------\r\n14070:M 08 Jun 2020 23:59:48.834 # !!! Software Failure. Press left mouse button to continue\r\n14070:M 08 Jun 2020 23:59:48.834 # Guru Meditation: Unknown encoding type #object.c:534\r\n14070:M 08 Jun 2020 23:59:48.834 # (forcing SIGSEGV in order to print the stack trace)\r\n14070:M 08 Jun 2020 23:59:48.834 # ------------------------------------------------\r\n14070:M 08 Jun 2020 23:59:48.834 # Redis 999.999.999 crashed by signal: 11\r\n14070:M 08 Jun 2020 23:59:48.834 # Crashed running the instruction at: 0x10f350a9e\r\n14070:M 08 Jun 2020 23:59:48.834 # Accessing address: 0xffffffffffffffff\r\n14070:M 08 Jun 2020 23:59:48.834 # Failed assertion: <no assertion failed> (<no file>:0)\r\n\r\n------ STACK TRACE ------\r\nEIP:\r\n0   redis-server                        0x000000010f350a9e _serverPanic + 334\r\n\r\nBacktrace:\r\n0   redis-server                        0x000000010f35354e logStackTrace + 110\r\n1   redis-server                        0x000000010f35390c sigsegvHandler + 236\r\n2   libsystem_platform.dylib            0x00007fff8e30cb3a _sigtramp + 26\r\n3   ???                                 0x0000000000000400 0x0 + 1024\r\n4   redis-server                        0x000000010f31e18a getDecodedObject + 202\r\n5   redis-server                        0x000000010f33523e stralgoLCS + 222\r\n6   redis-server                        0x000000010f305ffb call + 347\r\n7   redis-server                        0x000000010f306b78 processCommand + 1912\r\n8   redis-server                        0x000000010f319727 processInputBuffer + 407\r\n9   redis-server                        0x000000010f3aab00 connSocketEventHandler + 304\r\n10  redis-server                        0x000000010f2fd877 aeProcessEvents + 807\r\n11  redis-server                        0x000000010f2fdbbd aeMain + 29\r\n12  redis-server                        0x000000010f30a09e main + 1918\r\n13  libdyld.dylib                       0x00007fff8e0fd235 start + 1\r\n14  ???                                 0x0000000000000001 0x0 + 1\r\n\r\n------ INFO OUTPUT ------\r\n# Server\r\nredis_version:999.999.999\r\nredis_git_sha1:48b2915c\r\nredis_git_dirty:0\r\nredis_build_id:3ac4d049bacc7d05\r\nredis_mode:standalone\r\nos:Darwin 16.7.0 x86_64\r\narch_bits:64\r\nmultiplexing_api:kqueue\r\natomicvar_api:atomic-builtin\r\ngcc_version:4.2.1\r\nprocess_id:14070\r\nrun_id:d52ca7d9fddac41d9650e98ba90eb027cadd1a83\r\ntcp_port:6379\r\nuptime_in_seconds:42\r\nuptime_in_days:0\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:14616884\r\nexecutable:/Users/danieldai/Documents/hwware/redis/src/./redis-server\r\nconfig_file:\r\n\r\n# Clients\r\nconnected_clients:1\r\nclient_recent_max_input_buffer:2\r\nclient_recent_max_output_buffer:0\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\n\r\n# Memory\r\nused_memory:1063560\r\nused_memory_human:1.01M\r\nused_memory_rss:4255744\r\nused_memory_rss_human:4.06M\r\nused_memory_peak:1063560\r\nused_memory_peak_human:1.01M\r\nused_memory_peak_perc:100.16%\r\nused_memory_overhead:1016786\r\nused_memory_startup:999464\r\nused_memory_dataset:46774\r\nused_memory_dataset_perc:72.97%\r\nallocator_allocated:1216128\r\nallocator_active:1454080\r\nallocator_resident:4308992\r\ntotal_system_memory:17179869184\r\ntotal_system_memory_human:16.00G\r\nused_memory_lua:37888\r\nused_memory_lua_human:37.00K\r\nused_memory_scripts:0\r\nused_memory_scripts_human:0B\r\nnumber_of_cached_scripts:0\r\nmaxmemory:0\r\nmaxmemory_human:0B\r\nmaxmemory_policy:noeviction\r\nallocator_frag_ratio:1.20\r\nallocator_frag_bytes:237952\r\nallocator_rss_ratio:2.96\r\nallocator_rss_bytes:2854912\r\nrss_overhead_ratio:0.99\r\nrss_overhead_bytes:-53248\r\nmem_fragmentation_ratio:4.17\r\nmem_fragmentation_bytes:3234792\r\nmem_not_counted_for_evict:0\r\nmem_replication_backlog:0\r\nmem_clients_slaves:0\r\nmem_clients_normal:16986\r\nmem_aof_buffer:0\r\nmem_allocator:jemalloc-5.1.0\r\nactive_defrag_running:0\r\nlazyfree_pending_objects:0\r\n\r\n# Persistence\r\nloading:0\r\nrdb_changes_since_last_save:2\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1591675146\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:-1\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_last_cow_size:0\r\naof_enabled:0\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\n\r\n# Stats\r\ntotal_connections_received:1\r\ntotal_commands_processed:3\r\ninstantaneous_ops_per_sec:0\r\ntotal_net_input_bytes:189\r\ntotal_net_output_bytes:18543\r\ninstantaneous_input_kbps:0.00\r\ninstantaneous_output_kbps:0.00\r\nrejected_connections:0\r\nsync_full:0\r\nsync_partial_ok:0\r\nsync_partial_err:0\r\nexpired_keys:0\r\nexpired_stale_perc:0.00\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:0\r\nevicted_keys:0\r\nkeyspace_hits:2\r\nkeyspace_misses:0\r\npubsub_channels:0\r\npubsub_patterns:0\r\nlatest_fork_usec:0\r\nmigrate_cached_sockets:0\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:0\r\nactive_defrag_misses:0\r\nactive_defrag_key_hits:0\r\nactive_defrag_key_misses:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\n\r\n# Replication\r\nrole:master\r\nconnected_slaves:0\r\nmaster_replid:0e77fe777bae3dfa99c66541d625aae5789cc7d5\r\nmaster_replid2:0000000000000000000000000000000000000000\r\nmaster_repl_offset:0\r\nsecond_repl_offset:-1\r\nrepl_backlog_active:0\r\nrepl_backlog_size:1048576\r\nrepl_backlog_first_byte_offset:0\r\nrepl_backlog_histlen:0\r\n\r\n# CPU\r\nused_cpu_sys:0.026122\r\nused_cpu_user:0.027866\r\nused_cpu_sys_children:0.000000\r\nused_cpu_user_children:0.000000\r\n\r\n# Modules\r\n\r\n# Commandstats\r\ncmdstat_hset:calls=2,usec=56,usec_per_call=28.00\r\ncmdstat_command:calls=1,usec=1422,usec_per_call=1422.00\r\n\r\n# Cluster\r\ncluster_enabled:0\r\n\r\n# Keyspace\r\ndb0:keys=6,expires=0,avg_ttl=0\r\n\r\n------ CLIENT LIST OUTPUT ------\r\nid=4 addr=127.0.0.1:49906 fd=8 name= age=39 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=56 qbuf-free=32712 obl=0 oll=0 omem=0 events=r cmd=stralgo user=default\r\n\r\n------ CURRENT CLIENT INFO ------\r\nid=4 addr=127.0.0.1:49906 fd=8 name= age=39 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=56 qbuf-free=32712 obl=0 oll=0 omem=0 events=r cmd=stralgo user=default\r\nargv[0]: 'STRALGO'\r\nargv[1]: 'LCS'\r\nargv[2]: 'keys'\r\nargv[3]: 'foo1'\r\nargv[4]: 'bar1'\r\n\r\n------ REGISTERS ------\r\n14070:M 08 Jun 2020 23:59:48.835 # \r\nRAX:20bad09b37900028 RBX:000000010f456e94\r\nRCX:0000220000002303 RDX:0000000000012068\r\nRDI:00007fff97000f78 RSI:000000000011bd00\r\nRBP:00007fff50906640 RSP:00007fff50906450\r\nR8 :0000000000000040 R9 :00007fff97000f70\r\nR10:ffffffffffffffff R11:0000000000012068\r\nR12:0000000000000004 R13:000000010febfcc0\r\nR14:0000000000000216 R15:000000010f44dbfb\r\nRIP:000000010f350a9e EFL:0000000000010206\r\nCS :000000000000002b FS:0000000000000000  GS:0000000000000000\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff5090645f) -> 000000010f4028bf\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff5090645e) -> 000000010f81a490\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff5090645d) -> 000000010f811270\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff5090645c) -> 000000010f807b40\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff5090645b) -> 000000010f40340e\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff5090645a) -> 00007fff50906510\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff50906459) -> 000000010f81a4c8\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff50906458) -> 000000010f81a4b8\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff50906457) -> 000000010f81a4b0\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff50906456) -> 000000010f81a4a8\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff50906455) -> 000000010f301950\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff50906454) -> 0000000000000031\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff50906453) -> 000000000df09345\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff50906452) -> 000000010f81a470\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff50906451) -> 000000010f81a4a8\r\n14070:M 08 Jun 2020 23:59:48.835 # (00007fff50906450) -> 000000010f81a4b8\r\n\r\n------ MODULES INFO OUTPUT ------\r\n\r\n------ DUMPING CODE AROUND EIP ------\r\nSymbol: _serverPanic (base: 0x10f350950)\r\nModule: /Users/danieldai/Documents/hwware/redis/src/./redis-server (base 0x10f2f9000)\r\n$ xxd -r -p /tmp/dump.hex /tmp/dump.bin\r\n$ objdump --adjust-vma=0x10f350950 -D -b binary -m i386:x86-64 /tmp/dump.bin\r\n------\r\n14070:M 08 Jun 2020 23:59:48.835 # dump of function (hexdump of 462 bytes):\r\n554889e541574156534881ecd80100004889d34189f64989ff84c074380f298540feffff0f298d50feffff0f299560feffff0f299d70feffff0f29a580feffff0f29ad90feffff0f29b5a0feffff0f29bdb0feffff4c898d38feffff4c898530feffff48898d28feffff488b0547961100488b00488945e0488d8510feffff488945d0488d4510488945c8c745c430000000c745c018000000488dbdc0feffff4c8d4dc0be0001000031d2b9000100004989d8e868320f00488d1d99f0130083bb940d000000751b488d352f651000bf03040000e8f708fbffc783940d000001000000488d1d5a641000bf0300000031c04889dee8e70afbff488d3575641000bf0300000031c0e8d40afbff488d359c641000488d95c0feffffbf0300000031c04c89f94589f0e8b40afbff488d3597641000bf0300000031c0e8a10afbffbf0300000031c04889dee8920afbffc60425ffffffff78488b055b951100488b00483b45e0750e4881c4d80100005b415e415f5dc3e883310f000f1f8000000000554889e54157415641554154534881ecd8000000488b051d951100488b00488945d0c747100000000048c74708000000004889bd20ffffff48c70700000000488d1592ef13008b82c007000085c0\r\nFunction at 0x10f443c70 is je_witness_postfork_child\r\nFunction at 0x10f301320 is serverLogRaw\r\nFunction at 0x10f301530 is serverLog\r\nFunction at 0x10f443c4c is je_witness_postfork_child\r\n\r\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\r\n\r\n       Please report the crash by opening an issue on github:\r\n\r\n           http://github.com/antirez/redis/issues\r\n\r\n  Suspect RAM error? Use redis-server --test-memory to verify it.\r\n\r\nSegmentation fault: 11\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-06-08T06:52:31Z",
        "closed_at": "2020-06-08T08:43:52Z",
        "merged_at": "2020-06-08T08:43:52Z",
        "body": "the recent change in that loop (iteration rather than waiting for it to\r\nbe empty) was intended to avoid an endless loop in case some slave would\r\nrefuse to be freed.\r\n\r\nbut the lookup of the first client remained, which would have caused it\r\nto try the first one again and again instead of moving on.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-06-08T06:44:23Z",
        "closed_at": "2020-06-08T09:08:08Z",
        "merged_at": "2020-06-08T09:08:08Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-06-08T06:25:47Z",
        "closed_at": "2020-06-08T09:04:27Z",
        "merged_at": "2020-06-08T09:04:27Z",
        "body": "Much like MULTI/EXEC/DISCARD, the WATCH and UNWATCH are not actually\r\noperating on the database or server state, but instead operate on the\r\nclient state. the client may send them all in one long pipeline and check\r\nall the responses only at the end, so failing them may lead to a\r\nmismatch between the client state on the server and the one on the\r\nclient end, and execute the wrong commands (ones that were meant to be\r\ndiscarded)\r\n\r\nthe watched keys are not actually stored in the client struct, but they\r\nare in fact part of the client state. for instance, they're not cleared\r\nor moved in SWAPDB or FLUSHDB.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 11,
        "changed_files": 3,
        "created_at": "2020-06-07T15:50:06Z",
        "closed_at": "2020-07-10T05:28:23Z",
        "merged_at": "2020-07-10T05:28:23Z",
        "body": "tests were sensitive to additional log lines appearing in the log\r\ncausing the search to come empty handed.\r\n\r\ninstead of just looking for the n last log lines, capture the log lines\r\nbefore performing the action, and then search from that offset.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-06-07T14:22:23Z",
        "closed_at": "2020-07-12T10:52:02Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-06-07T05:40:02Z",
        "closed_at": "2020-06-08T10:54:15Z",
        "merged_at": "2020-06-08T10:54:15Z",
        "body": "HELLO should return the current proto version. But current code always return 3.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-06-04T09:08:14Z",
        "closed_at": "2021-03-10T11:23:42Z",
        "merged_at": null,
        "body": "",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-06-04T07:06:40Z",
        "closed_at": "2020-09-10T11:41:36Z",
        "merged_at": null,
        "body": "Modules are using this Module API to terminate forked process this event is not a warning and might happen often.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2020-06-03T09:55:56Z",
        "closed_at": "2020-06-08T09:02:01Z",
        "merged_at": "2020-06-08T09:02:01Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-06-02T11:10:49Z",
        "closed_at": "2020-06-11T18:12:26Z",
        "merged_at": null,
        "body": "We allow EXEC to run during a busy script so that it discards the\r\ntransaction state. this happens correctly if you actually tried to schedule\r\nany command while the script is busy.\r\nbut if all the commands were already scheduled, and just the EXEC\r\narrives during the script, we would have executed it and violate the\r\nscript atomicity.\r\n\r\nthis bug was added in ec007559ff703d27916f54ad0a41d154a88d9ac4\r\nwhich fixed another type of  violation (not clearing the transaction\r\nstate of the client connection).\r\n\r\nfact is that we must alwasy execute MULTI/EXEC/DISCARD in order to\r\nchange the client state, but we need to make sure not to actually\r\nexecute the command in that case.",
        "comments": 42
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-06-02T03:40:08Z",
        "closed_at": "2020-06-08T08:43:18Z",
        "merged_at": "2020-06-08T08:43:18Z",
        "body": "This PR is related to #7234, ping @antirez @oranagra \r\n\r\nIn `freeClientsInAsyncFreeQueue` we should skip protected client, since free protected client would add itself back to `server.clients_to_close`, and then redis will fall into infinite loop.\r\n\r\nThe reason is now we call `freeClientsInAsyncFreeQueue` when `ProcessingEventsWhileBlocked`.\r\n\r\nIt's easy to reproduce, process `DEBUG RELOAD` and another client process `CLIENT KILL TYPE NORMAL`.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-31T12:52:15Z",
        "closed_at": "2020-05-31T14:00:05Z",
        "merged_at": "2020-05-31T14:00:05Z",
        "body": "avoid -LOADING error",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2020-05-29T05:58:02Z",
        "closed_at": "2020-12-24T03:14:58Z",
        "merged_at": null,
        "body": "In trackingInvalidateKeysOnFlush function, since we already send caching invalidation message to all tracking clients to flush all client side caches, therefore there is no need to maintain previous record of tracking, no matter whether FLUSHALL was called or not ..",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-27T21:59:55Z",
        "closed_at": "2021-06-21T08:01:31Z",
        "merged_at": "2021-06-21T08:01:31Z",
        "body": "Fixes #6792.\r\nAdded support of REDIS_REPLY_SET in raw and csv output of `./redis-cli`\r\n\r\nTest:\r\n\r\nrun commands to test:\r\n  ./redis-cli -3 --csv COMMAND\r\n  ./redis-cli -3 --raw COMMAND\r\n\r\nNow they are returning results, commands were failing with: \"Unknown reply type: 10\" before the change. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-05-27T15:20:04Z",
        "closed_at": "2020-05-28T07:51:59Z",
        "merged_at": "2020-05-28T07:51:59Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-27T12:56:53Z",
        "closed_at": "2020-05-31T12:32:57Z",
        "merged_at": "2020-05-31T12:32:57Z",
        "body": "This impacts client verification for chained certificates (such as Lets\r\nEncrypt certificates). Client Verify requires the full chain in order to\r\nproperly verify the certificate.\r\n\r\nReferenced issue: https://github.com/antirez/redis/issues/7303",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-05-27T09:46:34Z",
        "closed_at": "2020-08-13T11:30:52Z",
        "merged_at": "2020-08-13T11:30:52Z",
        "body": "Clients asking for greater replication offset will be handled firstly by changing listAddNodeTail() to listAddNodeHead() in wait command,making last_offset and last_numreplicas useful in method processClientsWaitingReplicas.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-05-27T04:09:31Z",
        "closed_at": "2020-05-29T08:15:27Z",
        "merged_at": "2020-05-29T08:15:27Z",
        "body": "`clusterStartHandshake` will start handshake\r\nand eventually send CLUSTER MEET message, which is strictly prohibited\r\nin the REDIS CLUSTER SPEC.\r\nOnly system administrator can initiate CLUSTER MEET message.\r\nFurther, according to the SPEC, rather than IP/PORT pairs, only nodeid\r\ncan be trusted.\r\n\r\nFix https://github.com/antirez/redis/issues/7287",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 31,
        "changed_files": 5,
        "created_at": "2020-05-26T08:28:39Z",
        "closed_at": "2020-05-28T07:52:11Z",
        "merged_at": "2020-05-28T07:52:11Z",
        "body": "apparently when running tests in parallel (the default of --clients 16),\r\nthere's a chance for two tests to use the same port.\r\nspecifically, one test might shutdown a master and still have the\r\nreplica up, and then another test will re-use the port number of master\r\nfor another master, and then that replica will connect to the master of\r\nthe other test.\r\n\r\nthis can cause a master to count too many full syncs and fail a test if\r\nwe run the tests with --single integration/psync2 --loop --stop\r\n\r\nsee Probmem 2 in #7314 ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2020-05-25T15:17:41Z",
        "closed_at": "2020-08-08T11:36:42Z",
        "merged_at": "2020-08-08T11:36:42Z",
        "body": "Fix #7319",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-05-25T14:33:09Z",
        "closed_at": "2020-09-17T15:42:34Z",
        "merged_at": null,
        "body": "Fix #7318 #7317",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-05-25T03:33:03Z",
        "closed_at": "2020-05-25T09:37:26Z",
        "merged_at": "2020-05-25T09:37:26Z",
        "body": "After adjustMeaningfulReplOffset(), all the other related variable should be updated, including server.second_replid_offset.\r\n\r\nOr the old version redis like 5.0 may receive wrong data from replication stream, cause redis 5.0 can sync with redis 6.0, but doesn't know meaningful offset.\r\n\r\nHere is the original issue:\r\n\r\n> Hi all, I've read this thread, all the fixes makes sense and can work well for redis 6.0, but for old version redis 5.0, there is still a problem, it's about `server. second_replid_offset`.\r\n> \r\n> Redis 5.0 can sync with redis 6.0, but doesn't know meaningful offset, so the old version redis 5.0 may receive wrong data from replication stream, for example:\r\n> \r\n> 1. We have three instances, `R6A` `R6B` is redis 6, and `R6B` is an replica of `R6A`, and `R5` is redis 5 replica of `R6B`.\r\n> 2. Now replication offset is 126 in all the three instance, and `R6A` `R6B`'s meaningful offset is 60, but `R5` doesn't know the meaningful offset.\r\n> 3. Then we send `replicaof no one` to `R6B` promote it to be master, and `R6B` would store `replid2` and `second_replid_offset` with 60, and disconnect `R5`.\r\n> 4. After that before `R5` reconnect with `R6B`, we append some commands to `R6B` increasing the offset to 130, and `R5` try psync replid2 126 to `R6B`, it works but `R5` will receive wrong data.\r\n> \r\n> To fix it, I think after adjustMeaningfulReplOffset(), all the other related variable should be updated like `server.second_replid_offset`, see PR #7320\r\n> \r\n> Moreover, should we increase `RDB_VERSION`? Then redis 5 cannot sync with redis 6, but redis 6.0 doesn't change any data types.\r\n\r\n@QuChen88 's question:\r\n\r\n> @soloestoy I don't know what you mean specifically about \"receiving the wrong data\". Can you please be more specific as to what data with R5 receive from R6B? i.e. from which offset will the new master R6B send the data to R5? Or will the PSYNC request be rejected and force a full sync? It would be helpful to implement a test case that would reproduce this hypothetical scenario.\r\n> \r\n> I just walked through the code for the method shiftReplicationId(), it is setting `server. second_replid_offset` to be `server.master_repl_offset+1`. My understanding is that this will not introduce any regression for older redis versions that don't support meaningful offset feature to PSYNC with redis 6.0. Please correct me if I am wrong.\r\n\r\nAnd my reply:\r\n\r\n> @QuChen88 no full sync, psync can work in step 4, but offset 126 which `R5` expect to get from `R6B` is not correct, cause `R6B` has rollback offset to meaningful offset `60`, but backlog doesn't know that, so data from `126` to `130` in `R6B`'s backlog will be send to `R5`, but what `R5` want is data from `60` to `130`.\r\n\r\n",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 34,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2020-05-24T06:36:08Z",
        "closed_at": "2020-05-24T07:50:05Z",
        "merged_at": "2020-05-24T07:50:05Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2020-05-23T02:52:26Z",
        "closed_at": "2020-06-28T10:26:40Z",
        "merged_at": null,
        "body": "The function `anetTcpKeepAlive` is actually never used , and we have been using `anetKeepAlive` instead, so it can be removed.\r\n\r\nAnd in function `anetV6Only` the error message does not contain the  optname(`IPV6_V6ONLY`), so added it.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 142,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-05-22T11:52:17Z",
        "closed_at": "2020-12-27T11:14:40Z",
        "merged_at": "2020-12-27T11:14:40Z",
        "body": "Now we have a lot of options for client tracking, I think we need a way to show the tracking status.\r\n\r\nThis PR reply an txt, maybe map is better, not sure.",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2020-05-22T04:44:11Z",
        "closed_at": "2020-09-24T15:17:54Z",
        "merged_at": "2020-09-24T15:17:54Z",
        "body": "This PR fix https://github.com/antirez/redis/issues/7256, fclose should be called only once to avoid undefined behaviour..",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 60,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-05-22T04:19:15Z",
        "closed_at": "2020-05-22T10:23:08Z",
        "merged_at": "2020-05-22T10:23:08Z",
        "body": "Currently, when the replica disconnects from the master and performs a PSYNC, its sub-replicas will remain connected because the master ID remains the same. However, with the Redis 6.0 introduction of master meaningful offset mechanism, doing so would cause steady-state replication offset mismatch between master and chained replicas. \r\n\r\nIf the master has previously replicated down some non-meaningful PING commands to its replicas. When the replica disconnects, it requests the master from the last meaningful offset value, and the master would again send down the non-meaningful PINGs a second time. If the chained replicas remain connected, then it will receive the additional PINGs in the replication stream, and advance its own master_repl_offset field beyond the actual master's replication offset and remain that way forever. This will lead to it doing a full-sync un-necessarily next time when a real failover happens. ",
        "comments": 27
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-21T22:29:11Z",
        "closed_at": "2020-05-22T10:25:41Z",
        "merged_at": "2020-05-22T10:25:41Z",
        "body": "For TLS, we might not have read in a full frame, so there is data on the connection so epoll fires, but there isn't enough data to actually read from the socket. \r\n\r\nNot sure how common this is, we have some testing that randomly fragments TLS packets to catch some of this stuff. I don't think this can happen for TCP. ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-05-21T19:05:21Z",
        "closed_at": "2020-09-09T16:23:32Z",
        "merged_at": null,
        "body": "This PR fixes the typo. eviciton => eviction",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-21T05:57:59Z",
        "closed_at": "2020-05-21T07:26:13Z",
        "merged_at": "2020-05-21T07:26:13Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-20T20:47:54Z",
        "closed_at": "2020-09-29T05:49:36Z",
        "merged_at": "2020-09-29T05:49:36Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-05-20T12:16:13Z",
        "closed_at": "2020-05-21T13:49:37Z",
        "merged_at": "2020-05-21T13:49:37Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2020-05-19T13:43:46Z",
        "closed_at": "2022-03-08T19:51:37Z",
        "merged_at": null,
        "body": "In our specific environment we run many different Redis instances+clusters in a multi-tenant environment where we do not have control over which ports we get assigned but rather have to work with that we get. Usually this is a perfect usecase for the port remapping that's provided by Docker but sadly we can't rely on that in our case for a multitude of reasons. \r\n\r\nThis change adds a `cluster-bus-port` option that allows the user to override the actual port that the cluster bus port listens on. This allows us to not only announce a different cluster port but also actually listen on any port. \r\n\r\nAs for the announcement/propagation of the port within the Redis cluster, this is now the priority:\r\n- If `cluster-announce-bus-port` is set, then that port is announced. You can set both `cluster-bus-port` but still announce another port with `cluster-announce-bus-port`.\r\n- If `cluster-bus-port` is set, then that port is announced\r\n- In the default case, we both bind & announce the client port + 10000\r\n\r\nI've done a manual validation of this changeset with a config file that looks like this:\r\n```\r\nport 7000\r\ncluster-bus-port 6000\r\ncluster-enabled yes\r\ncluster-config-file nodes.conf\r\ncluster-node-timeout 5000\r\nappendonly yes\r\n```\r\n\r\nI've looked at adding automated testing for this but it appears to have a pretty significant effort given that the setup logic is in the base testing code. Please advise on how to proceed there.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-19T04:45:01Z",
        "closed_at": "2020-05-19T14:14:31Z",
        "merged_at": "2020-05-19T14:14:31Z",
        "body": "Similar to PR: https://github.com/antirez/redis/pull/7204, there is another problem in this part of code, if first CONFIG GET succeeds but second fails, we also need to free cfg->save, therefore freeRedisConfig function shoud be used during the failed clean up",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-05-19T04:40:03Z",
        "closed_at": "2021-09-16T17:59:36Z",
        "merged_at": null,
        "body": "Since test name and title was passed as string iterals, therefore string constant type should be used in both two places",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-05-18T17:01:06Z",
        "closed_at": "2020-05-19T14:19:29Z",
        "merged_at": "2020-05-19T14:19:29Z",
        "body": "in ACLSetUserCommandBit, when the command bit overflows, no operation\r\nis performed, so no need clear the USER_FLAG_ALLCOMMANDS flag.\r\n\r\nin ACLSetUser, when adding subcommand, we don't need to call\r\nACLGetCommandID ahead since subcommand may be empty.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-17T16:42:02Z",
        "closed_at": "2020-09-09T16:23:35Z",
        "merged_at": null,
        "body": "Just a quick typo fix!",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-16T05:05:56Z",
        "closed_at": "2020-07-29T07:59:00Z",
        "merged_at": null,
        "body": "When diskless replication is enabled, the config item \"repl-diskless-sync-delay\" may no longer have any effect under following situation:\r\n1.The master enables diskless replication and sets \"repl-diskless-sync-delay\";\r\n2.A client connects to this master and starts BGSAVE;\r\n3.BGSAVE starts;\r\n4.A slave with EOF capa connects to this master and asks for a full sync;\r\n5.BGSAVE ends and the signal handler starts a new BGSAVE for replication(in updateSlavesWaitingBgsave()),the new BGSAVE is socket target.\r\n6.repl-diskless-sync-delay expires now.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2020-05-15T13:13:24Z",
        "closed_at": "2020-05-15T15:44:06Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2020-05-14T18:16:27Z",
        "closed_at": "2020-05-18T08:31:48Z",
        "merged_at": "2020-05-18T08:31:48Z",
        "body": "Mistyping the removal of a password hash is not likely to cause issues, but since the validation already exists I think it's a better user experience to also validate it's a valid password hash. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-05-14T14:44:51Z",
        "closed_at": "2020-05-18T08:33:09Z",
        "merged_at": "2020-05-18T08:33:09Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-05-13T16:26:10Z",
        "closed_at": "2020-05-18T08:23:54Z",
        "merged_at": "2020-05-18T08:23:54Z",
        "body": "This make it so that all prompts for all redis-cli --cluster commands are automatically answered with a yes.\r\n\r\nIf it is deemed too dangerous to do so for cluster-fix mode, then the change can be more local (like my suggestion in #7246), however users will find a way around this, and just use the `yes | redis-cli...` trick.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2020-05-12T20:21:59Z",
        "closed_at": "2020-05-14T16:14:07Z",
        "merged_at": "2020-05-14T16:14:07Z",
        "body": "This platform supports CPU affinity (but not OpenBSD).",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 151,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2020-05-11T20:25:16Z",
        "closed_at": "2020-12-16T07:23:35Z",
        "merged_at": null,
        "body": "Make REQUIREPASS and MASTERAUTH directives handle binary safe strings. Previously neither server.requirepass or server.masterauth handles binary safe SDS strings. This leads to inconsistencies in client experience around password authentication. \r\n\r\ni.e. On master\r\n\r\n> config set requirepass \"ab\\x00cd\"\r\nOK\r\n> AUTH ab\r\nOK\r\n> AUTH \"ab\\x00cd\"\r\n(error) WRONGPASS invalid username-password pair\r\n\r\n\r\nAfter the fix. \r\n\r\n> config set requirepass \"ab\\x00cd\"\r\nOK\r\n> AUTH ab\r\n(error) WRONGPASS invalid username-password pair\r\n> AUTH \"ab\\x00cd\"\r\nOK\r\n\r\nSame goes for MASTERAUTH. ",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-11T19:26:25Z",
        "closed_at": "2020-07-13T14:16:07Z",
        "merged_at": "2020-07-13T14:16:07Z",
        "body": "The CLUSTER NODES command is intended to report only the master's config epoch on replicas in cluster mode. However, that is not what is implemented currently. Replicas are reporting their own individual epochs. This commit fixes it.\r\n\r\nSee documentation https://redis.io/commands/cluster-nodes\r\n\r\nTested and it works.\r\n\r\nBefore fix:\r\n\r\n127.0.0.1:6382> cluster nodes\r\n6cb6540c43dd83bb07ec1db191b90063b3374ac0 127.0.0.1:6379@16379 master - 0 1544216055000 1 connected 1-3\r\nec1d5360e7270bf833fa675fbeea3fadd91b179a 127.0.0.1:6380@16380 master - 0 1544216056277 2 connected 4-6\r\n129fd991448e44502cb4a62cb08a873c514cfd4f 127.0.0.1:6382@16382 myself,slave ec1d5360e7270bf833fa675fbeea3fadd91b179a 0 1544216053000 3 connected\r\nff9c9805d1e3c83cfa04df00b918e5bf1bc57b9b 127.0.0.1:6381@16381 slave 6cb6540c43dd83bb07ec1db191b90063b3374ac0 0 1544216055264 1 connected\r\n\r\nAfter fix:\r\n\r\n127.0.0.1:6382> cluster nodes 3f8969fac2c3dc652d02cca4905bb739c7ed98f4 127.0.0.1:6381@16381 slave 8492f9a7d36ab632610da9aee75491466658cd67 0 1544212190000 1 connected\r\n8492f9a7d36ab632610da9aee75491466658cd67 127.0.0.1:6379@16379 master - 0 1544212190160 1 connected 1-3\r\n9dfbd151548a44ecf8a5dd39c69e830bca5cbd6a 127.0.0.1:6380@16380 master - 0 1544212191172 0 connected 4-6\r\n666925840c9780bf852bfbde46ae36fbbc44e67c 127.0.0.1:6382@16382 myself,slave 9dfbd151548a44ecf8a5dd39c69e830bca5cbd6a 0 1544212189000 0 connected",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 133,
        "deletions": 58,
        "changed_files": 3,
        "created_at": "2020-05-11T08:34:30Z",
        "closed_at": "2020-05-14T09:28:42Z",
        "merged_at": "2020-05-14T09:28:42Z",
        "body": "This bug was introduced by a recent change in which readQueryFromClient\r\nis using freeClientAsync, and despite the fact that now\r\nfreeClientsInAsyncFreeQueue is in beforeSleep, that's not enough since\r\nit's not called during loading in processEventsWhileBlocked.\r\nfurthermore, afterSleep was called in that case but beforeSleep wasn't.\r\n\r\nThis bug also caused slowness sine the level-triggered mode of epoll\r\nkept signaling these connections as readable causing us to keep doing\r\nconnRead again and again for ll of these, which keep accumulating.\r\n\r\nnow both before and after sleep are called, but not all of their actions\r\nare performed during loading, some are only reserved for the main loop.\r\n\r\nfixes issue #7215",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-05-11T05:17:04Z",
        "closed_at": "2020-05-19T07:23:44Z",
        "merged_at": "2020-05-19T07:23:44Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 45,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-10T16:31:21Z",
        "closed_at": "2020-05-14T16:14:50Z",
        "merged_at": "2020-05-14T16:14:50Z",
        "body": "This is really required only for older OpenSSL versions.\r\n\r\nAlso, at the moment Redis does not use OpenSSL from multiple threads so\r\nthis will only be useful if modules end up doing that.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2020-05-10T16:30:18Z",
        "closed_at": "2020-05-14T16:15:18Z",
        "merged_at": "2020-05-14T16:15:17Z",
        "body": "Seems like on some systems choosing specific TLS v1/v1.1 versions no\r\nlonger works as expected. Test is reduced for v1.2 now which is still\r\ngood enough to test the mechansim, and matters most anyway.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-10T11:01:53Z",
        "closed_at": "2020-08-11T11:16:10Z",
        "merged_at": "2020-08-11T11:16:10Z",
        "body": "lazyfree.c:  void lazyfreeFreeSlotsMapFromBioThread(rax *rt);\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 41,
        "deletions": 39,
        "changed_files": 4,
        "created_at": "2020-05-08T15:51:21Z",
        "closed_at": "2020-05-18T15:44:51Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-08T04:12:32Z",
        "closed_at": "2020-10-27T06:54:08Z",
        "merged_at": null,
        "body": "During PUB/SUB heavy load, the cluster bus becomes overloaded. As a result PONG packets got delayed, which in turn causes spurious PFAIL states, then FAIL and then CLUSTER-DOWN state.\r\nThis PR mitigates this issue by treating pings as proof of liveness in addition to PONG replies by resetting `ping_sent` timer.",
        "comments": 22
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-07T15:17:19Z",
        "closed_at": "2021-04-26T14:04:53Z",
        "merged_at": null,
        "body": "There is no code to set supervised_mode when upstart_job environment is set.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-06T22:15:15Z",
        "closed_at": "2020-07-10T13:40:27Z",
        "merged_at": null,
        "body": "When we fetch the cluster configuration, we need to authenticate first if we are running against a password/ACL protected redis instance.\r\n\r\nKindof a follow up to https://github.com/antirez/redis/pull/6665",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-06T05:10:26Z",
        "closed_at": "2020-05-06T13:42:04Z",
        "merged_at": "2020-05-06T13:42:04Z",
        "body": "jemalloc-bg-thread config was introduced in https://github.com/antirez/redis/pull/6145, this pr add this configuration in redis.conf",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-06T03:36:40Z",
        "closed_at": "2020-05-06T13:40:26Z",
        "merged_at": "2020-05-06T13:40:26Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-05T15:29:24Z",
        "closed_at": "2020-05-08T08:53:21Z",
        "merged_at": "2020-05-08T08:53:21Z",
        "body": "I find redis-benchmark coredumps when I do some tests by changing parameters of `client-output-buffer-limit`. `reply` will be the last value if the second call 'redisGetReply' fails but the first succeeds, so we will free memory two times. https://github.com/antirez/redis/blob/unstable/src/redis-benchmark.c#L282\r\n\r\nThe backtrac is shown below.\r\n```\r\n./src/redis-benchmark -p 6381 -t get\r\nERROR: failed to fetch CONFIG from 127.0.0.1:6381\r\n*** Error in `./src/redis-benchmark': double free or corruption (fasttop): 0x00000000015513d0 ***\r\n======= Backtrace: =========\r\n/usr/local/gcc-8.2/lib/libc.so.6(+0x73940)[0x7f4c4adfb940]\r\n/usr/local/gcc-8.2/lib/libc.so.6(+0x78ed8)[0x7f4c4ae00ed8]\r\n/usr/local/gcc-8.2/lib/libc.so.6(+0x79fd3)[0x7f4c4ae01fd3]\r\n./src/redis-benchmark[0x4117a6]\r\n./src/redis-benchmark(main+0x2bd)[0x40e16d]\r\n/opt/compiler/gcc-8.2/lib/libc.so.6(__libc_start_main+0xee)[0x7f4c4ada9b8e]\r\n./src/redis-benchmark(_start+0x29)[0x40f2f9]\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 144,
        "deletions": 2,
        "changed_files": 7,
        "created_at": "2020-05-05T14:46:41Z",
        "closed_at": "2020-09-24T13:01:42Z",
        "merged_at": "2020-09-24T13:01:42Z",
        "body": "I config set small memory sizes for \"normal\" of `client-output-buffer-limit` to limit the memory clients used, but I actually found the memory kept increasing when users executed `smembers` command on a big key.\r\n\r\nI also could read some replies exactly when I send redis command by \u201ctelnet\u201d even if the client output buffer size is more than the part of `client-output-buffer-limit` I set.\r\n\r\nI consider we shouldn't write replies to the client output buffer if we want to close it asap, that will not use unnecessary memory. And we also should remove the client from the list of pending writes, so that we won't `write(2)` replies to the client.",
        "comments": 41
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-05-05T14:06:30Z",
        "closed_at": "2020-05-06T14:17:12Z",
        "merged_at": "2020-05-06T14:17:11Z",
        "body": "`stringLen` can never be `0` here because it was checked in the `while` condition.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-05-05T09:25:05Z",
        "closed_at": "2020-06-16T09:03:55Z",
        "merged_at": null,
        "body": "sometimes valgrind report is completely empty, not reporting anytying,\r\nand we need to ignore it.\r\n\r\nexample:\r\n*** [err]: Valgrind error: ==46468== Memcheck, a memory error detector\r\n==46468== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.\r\n==46468== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info\r\n==46468== Command: src/redis-server ./tests/tmp/redis.conf.5351.333\r\n==46468==",
        "comments": 30
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-05-04T15:10:10Z",
        "closed_at": "2020-05-05T08:28:36Z",
        "merged_at": "2020-05-05T08:28:36Z",
        "body": "I think that should do it. Didn't test too hard will do that ...",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-04T14:48:03Z",
        "closed_at": "2020-05-19T14:16:38Z",
        "merged_at": "2020-05-19T14:16:38Z",
        "body": "Instead of using the same character 'x' in redis-benchmark, use random characters.",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-05-04T09:37:35Z",
        "closed_at": "2020-05-05T08:26:02Z",
        "merged_at": "2020-05-05T08:26:02Z",
        "body": "",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-03T21:17:55Z",
        "closed_at": "2020-09-09T16:23:38Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2020-05-03T21:12:02Z",
        "closed_at": "2020-05-04T09:06:45Z",
        "merged_at": "2020-05-04T09:06:45Z",
        "body": "Number of tracking prefixes should also be considered to be a stats showing to user",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-03T17:57:52Z",
        "closed_at": "2020-05-04T08:14:25Z",
        "merged_at": "2020-05-04T08:14:25Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 20,
        "changed_files": 4,
        "created_at": "2020-05-03T13:51:49Z",
        "closed_at": "2020-05-04T08:50:22Z",
        "merged_at": "2020-05-04T08:50:22Z",
        "body": "Same goes for XGROUP DELCONSUMER (But in this case, it doesn't\r\nhave any visible effect)",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 106,
        "deletions": 18,
        "changed_files": 5,
        "created_at": "2020-05-03T06:32:38Z",
        "closed_at": "2020-05-04T08:11:25Z",
        "merged_at": "2020-05-04T08:11:25Z",
        "body": "* fix memlry leaks with diskless replica short read.\r\n* fix a few timing issues with valgrind runs\r\n* fix issue with valgrind and watchdog schedule signal\r\n\r\nabout the valgrind WD issue:\r\nthe stack trace test in logging.tcl, has issues with valgrind:\r\n==28808== Can't extend stack to 0x1ffeffdb38 during signal delivery for thread 1:\r\n==28808==   too small or bad protection modes\r\n\r\nit seems to be some valgrind bug with SA_ONSTACK.\r\nSA_ONSTACK seems unneeded since WD is not recursive (SA_NODEFER was removed),\r\nalso, not sure if it's even valid without a call to sigaltstack()",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-03T01:37:56Z",
        "closed_at": "2020-12-06T17:09:42Z",
        "merged_at": null,
        "body": "Corrected the null pointer check condition.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-05-02T22:26:15Z",
        "closed_at": "2020-09-09T16:23:41Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 180,
        "deletions": 1,
        "changed_files": 12,
        "created_at": "2020-05-02T12:35:20Z",
        "closed_at": "2020-05-04T08:56:21Z",
        "merged_at": "2020-05-04T08:56:20Z",
        "body": "Currently, there are several types of threads/child processes of a\r\nredis server. Sometimes we need deeply optimise the performance of\r\nredis, so we would like to isolate threads/processes.\r\n\r\nThere were some discussion about cpu affinity cases in the issue:\r\nhttps://github.com/antirez/redis/issues/2863\r\n\r\nSo implement cpu affinity setting by redis.conf in this patch, then\r\nwe can config server_cpulist/bio_cpulist/aof_rewrite_cpulist/\r\nbgsave_cpulist by cpu list.\r\n\r\nTest on linux/freebsd, both work fine.\r\n\r\nSigned-off-by: zhenwei pi <pizhenwei@bytedance.com>",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-05-02T11:31:40Z",
        "closed_at": "2020-05-04T08:19:17Z",
        "merged_at": null,
        "body": "Make all internal functions dict.c [ _funName] internal by declaring them static",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-02T04:56:36Z",
        "closed_at": "2020-05-04T08:20:32Z",
        "merged_at": "2020-05-04T08:20:31Z",
        "body": "When deffered reply is added the previous reply node cannot be used so\r\nall the extra space we allocated in it is wasted. in case someone uses\r\ndeffered replies in a loop, each time adding a small reply, each of\r\nthese reply nodes (the small string reply) would have consumed a 16k\r\nblock.\r\nnow when we add anther diferred reply node, we trim the unused portion\r\nof the previous reply block.\r\n\r\nsee #7123\r\n\r\ncherry picked from commit fb732f7a944a4d4c90bb7375cb6030e88211f5aa\r\nwith fix to handle a crash with LIBC allocator, which apparently can\r\nreturn the same pointer despite changing it's size.\r\ni.e. shrinking an allocation of 16k into 56 bytes without changing the\r\npointer.",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2020-05-02T04:01:05Z",
        "closed_at": "2020-05-05T16:22:24Z",
        "merged_at": "2020-05-05T16:22:24Z",
        "body": "Fix #7166 \r\n\r\nTested with ACL and old school password only.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-05-01T16:08:41Z",
        "closed_at": "2020-05-02T00:12:04Z",
        "merged_at": null,
        "body": "TIL what a uint64_t actually is.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-05-01T12:22:32Z",
        "closed_at": "2020-07-15T10:29:27Z",
        "merged_at": "2020-07-15T10:29:27Z",
        "body": "Systemd notify rework only notifies systemd on redis startup, not on sentinel startup, so systemd with notify=systemd never successfully starts.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-30T01:26:52Z",
        "closed_at": "2020-09-09T16:23:44Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-04-28T15:15:04Z",
        "closed_at": "2020-05-02T09:38:35Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2020-04-28T06:19:26Z",
        "closed_at": "2020-04-28T08:04:47Z",
        "merged_at": "2020-04-28T08:04:47Z",
        "body": "@antirez another race in the various psync2 tests. they can try to do DBSIZE or GET too early and get -LOADING error.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-04-27T03:24:11Z",
        "closed_at": "2021-11-26T20:45:16Z",
        "merged_at": null,
        "body": "In Sentinel and Cluster mode, currently fsync was used for sync the current sentinel/cluster state into disk, however when the state change very frequently or failover happens in short period, fsync could be a potential bottleneck, an example of this is https://github.com/antirez/redis/issues/6807, as a simple improvement, we can use fdatasync rather than fsync to reduce one disk write in linux environment, this was predefined in redis_fsync marco..",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 465,
        "deletions": 156,
        "changed_files": 7,
        "created_at": "2020-04-25T00:57:38Z",
        "closed_at": "2020-04-27T15:29:14Z",
        "merged_at": "2020-04-27T15:29:14Z",
        "body": "c4b301d42bbb:ElastiCacheRedis matolson$ src/redis-server test crc64\r\n[calcula]: e9c6d914c4b8d9ca == e9c6d914c4b8d9ca\r\n[64speed]: e9c6d914c4b8d9ca == e9c6d914c4b8d9ca\r\n[calcula]: c7794709e69683b3 == c7794709e69683b3\r\n[64speed]: c7794709e69683b3 == c7794709e69683b3\r\n\r\ncrcspeed is mostly unchanged from where it was taken from. The crc64 is a drop replacement for the existing one. \r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-24T15:47:08Z",
        "closed_at": "2020-09-09T16:23:47Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-04-24T14:48:00Z",
        "closed_at": "2020-04-27T14:45:48Z",
        "merged_at": "2020-04-27T14:45:48Z",
        "body": "When deffered reply is added the previous reply node cannot be used so\r\nall the extra space we allocated in it is wasted. in case someone uses\r\ndeffered replies in a loop, each time adding a small reply, each of\r\nthese reply nodes (the small string reply) would have consumed a 16k\r\nblock.\r\nnow when we add anther diferred reply node, we trim the unused portion\r\nof the previous reply block.\r\n\r\nsee #7123",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 215,
        "deletions": 95,
        "changed_files": 5,
        "created_at": "2020-04-23T22:08:43Z",
        "closed_at": "2020-04-27T13:53:07Z",
        "merged_at": null,
        "body": "Now both master and replicas keep track of the last replication offset\r\nthat contains meaningful data (ignoring the tailing pings), and both\r\ntrim that tail from the replication backlog, and the offset with which\r\nthey try to use for psync.\r\n\r\nthe implication is that if someone missed some pings, or even have\r\nexcessive pings that the promoted replica has, it'll still be able to\r\npsync (avoid full sync).\r\n\r\nthe downside (which was already committed) is that replicas running old\r\ncode may fail to psync, since the promoted replica trims pings form it's\r\nbacklog.\r\n\r\nThis commit adds a test that reproduces several cases of promotions and\r\ndemotions with stale and non-stale pings\r\n\r\nBackground:\r\nThe mearningful offset on the master was added recently to solve a problem were\r\nthe master is left all alone, injecting PINGs into it's backlog when no one is\r\nlistening and then gets demoted and tries to replicate from a replica that didn't\r\nhave any of the PINGs (or at least not the last ones).\r\n\r\nhowever, consider this case:\r\nmaster A has two replicas (B and C) replicating directly from it.\r\nthere's no traffic at all, and also no network issues, just many pings in the\r\ntail of the backlog. now B gets promoted, A becomes a replica of B, and C\r\nremains a replica of A. when A gets demoted, it trims the pings from its\r\nbacklog, and successfully replicate from B. however, C is still aware of\r\nthese PINGs, when it'll disconnect and re-connect to A, it'll ask for something\r\nthat's not in the backlog anymore (since A trimmed the tail of it's backlog),\r\nand be forced to do a full sync (something it didn't have to do before the\r\nmeaningful offset fix).\r\n\r\nBesides that, the psync2 test was always failing randomly here and there, it\r\nturns out the reason were PINGs. Investigating it shows the following scenario:\r\n\r\ncycle 1: redis #1 is master, and all the rest are direct replicas of #1\r\ncycle 2: redis #2 is promoted to master, #1 is a replica of #2 and #3 is replica of #1\r\nnow we see that when #1 is demoted it prints:\r\n17339:S 21 Apr 2020 11:16:38.523 * Using the meaningful offset 3929963 instead of 3929977 to exclude the final PINGs (14 bytes difference)\r\n17339:S 21 Apr 2020 11:16:39.391 * Trying a partial resynchronization (request e2b3f8817735fdfe5fa4626766daa938b61419e5:3929964).\r\n17339:S 21 Apr 2020 11:16:39.392 * Successful partial resynchronization with master.\r\nand when #3 connects to the demoted #2, #2 says:\r\n17339:S 21 Apr 2020 11:16:40.084 * Partial resynchronization not accepted: Requested offset for secondary ID was 3929978, but I can reply up to 3929964\r\n\r\nso the issue here is that the meaningful offset feature saved the day for the\r\ndemoted master (since it needs to sync from a replica that didn't get the last\r\nping), but it didn't help one of the other replicas which did get the last ping.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2020-04-22T14:04:41Z",
        "closed_at": "2020-04-23T14:12:08Z",
        "merged_at": "2020-04-23T14:12:08Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 226,
        "deletions": 34,
        "changed_files": 2,
        "created_at": "2020-04-22T13:07:55Z",
        "closed_at": "2020-04-28T14:31:01Z",
        "merged_at": "2020-04-28T14:31:01Z",
        "body": "Introducing XINFO STREAM <key> FULL",
        "comments": 18
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-22T08:34:43Z",
        "closed_at": "2020-04-23T14:15:34Z",
        "merged_at": null,
        "body": "On some architectures like powerpc or arm*, char is unsigned.\r\nThere is an if (c < 0) check a few lines below which is always\r\nfalse on these architectures.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-04-21T21:18:58Z",
        "closed_at": "2020-09-15T01:32:42Z",
        "merged_at": null,
        "body": "sdsnew() can return a NULL pointer",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2020-04-21T11:20:34Z",
        "closed_at": "2020-04-23T14:18:23Z",
        "merged_at": "2020-04-23T14:18:23Z",
        "body": "In cluster, when the distribution of slots is too decentralized, the command of \"cluster slots\" will cause the client breaked. The reason is that  the function of addReplyDeferredLen is called many times in a for-loop, when addReplyDeferredLen is called, a size of 16K memory will be allocated, but only a small part of memory is used. As a result, the memory which is allocated  is far greater than is actual used, the client's output buf reaches the hard limit and the connection of client to server is breaked. For example, in the test of tests/cluster/tests/15-cluster-slots.tcl, when put the result of cluster slots to a file, the size of file is about 400k, but the client's output buf is more than 40M.",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-20T23:41:07Z",
        "closed_at": "2020-04-23T14:29:01Z",
        "merged_at": "2020-04-23T14:29:01Z",
        "body": "If there's a panic before all threads have been started (say, if file descriptor 0 is closed at exec), the panic response will crash here again.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-20T23:40:20Z",
        "closed_at": "2020-04-23T14:30:28Z",
        "merged_at": "2020-04-23T14:30:28Z",
        "body": "If redis crashes early, before lua is set up (like, if File Descriptor 0 is closed before exec), it will crash again trying to print memory statistics.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 61,
        "deletions": 36,
        "changed_files": 2,
        "created_at": "2020-04-20T08:04:14Z",
        "closed_at": "2020-04-20T09:48:00Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-19T13:00:43Z",
        "closed_at": "2020-04-23T14:29:46Z",
        "merged_at": "2020-04-23T14:29:46Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-04-17T17:53:58Z",
        "closed_at": "2020-04-20T09:51:51Z",
        "merged_at": "2020-04-20T09:51:51Z",
        "body": "when trigger a always fail scripts, sentinel.running_scripts will increase ten times, however it\r\nonly decrease one times onretry the maximum. and it will't reset, when it become\r\nSENTINEL_SCRIPT_MAX_RUNNING, sentinel don't trigger scripts.\r\n\r\nThe  issue  recurrent is very easy.\r\nfor example\uff1a\r\n1. when script always returns an exit code of \"1\";\r\n2. sentinel will retry then script. \r\n3. After a while time, sentinel info will become:\r\n```\r\n# Sentinel\r\nsentinel_masters:1\r\nsentinel_tilt:0\r\nsentinel_running_scripts:9\r\nsentinel_scripts_queue_length:0\r\n```\r\n4. this means retry  ten times. but only decrease one times.\r\n5. if the script trigger two times and more..\r\n6. next normal request will't trigger.\r\n\r\n\r\nI think it's unreasonable. sentinel_running_scripts should decrease when every exce fail.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-17T12:48:45Z",
        "closed_at": "2020-09-07T18:17:58Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 119,
        "deletions": 41,
        "changed_files": 6,
        "created_at": "2020-04-17T12:29:07Z",
        "closed_at": "2020-04-24T09:31:20Z",
        "merged_at": null,
        "body": "Currently there are four datatypes that allow blocking commands:\r\nlist, zset, stream and module keys\r\n\r\nWhen a client is unblocked it may (and probably will) replicate\r\nsomething to replicas/AOF.\r\n\r\nThe code that unblocks list and zset keys calls directly to the\r\npropagate() API which always works.\r\nOn the other hand, stream and module keys use alsoPropagate which\r\nis useless outside of call() context.\r\nIn the case of module keys, moduleHandlePropagationAfterCommandCallback\r\nalready handles server.also_propagate.\r\nIn the case of streams, streamReplyWithRange may replicate\r\nXCLAIM/XSETID commands, and we must replicate server.also_propagate\r\nexplicitly.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2020-04-16T15:10:25Z",
        "closed_at": "2020-04-17T08:43:22Z",
        "merged_at": "2020-04-17T08:43:22Z",
        "body": "this test is time sensitive and it sometimes fail to pass below the\r\nlatency threshold, even on strong machines.\r\n\r\nthis test was the reson we're running just 2 parallel tests in the\r\ngithub actions CI, revering this.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-16T02:03:05Z",
        "closed_at": "2020-04-28T09:22:10Z",
        "merged_at": "2020-04-28T09:22:10Z",
        "body": "CLUSTER_MF_PAUSE_MULT was defined, but not used. Use it in correct place instead of hardcoded value.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2020-04-15T22:24:14Z",
        "closed_at": "2020-09-09T16:23:50Z",
        "merged_at": null,
        "body": "Some typos in server.c",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2020-04-15T20:58:44Z",
        "closed_at": "2021-09-21T17:49:54Z",
        "merged_at": null,
        "body": "This PR including two main changes:\r\n1. In clusterLoadConfig/ clusterSaveConfig adding missing fstat error check.\r\n2. using redis_fstat marco instead of fstat in both two places.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-04-14T17:00:18Z",
        "closed_at": "2020-05-22T11:55:47Z",
        "merged_at": "2020-05-22T11:55:47Z",
        "body": "I implement `sendfile` for redis, but now only support Linux and macOS since I only have these two operation systems. For performance, it may couldn't save time but can reduce CPU usage.\r\n\r\nIf we implement it on BSD systems, please notice that `sendfile` may return -1 and errno set is to EAGAIN even if some bytes have been sent successfully and the the len argument is set correctly when using a socket marked for non-blocking I/O just like on macOS.",
        "comments": 35
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-04-14T04:18:27Z",
        "closed_at": "2020-04-14T09:15:27Z",
        "merged_at": "2020-04-14T09:15:27Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-13T14:30:07Z",
        "closed_at": "2020-08-05T18:16:39Z",
        "merged_at": "2020-08-05T18:16:38Z",
        "body": "...when using any command line argument.\r\n\r\nFixes #5629\r\n\r\nProbably a breaking change because who knows how many users are currently relying on this behavior :/",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-13T12:44:05Z",
        "closed_at": "2020-09-09T16:23:53Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2020-04-13T02:11:30Z",
        "closed_at": "2020-04-20T09:54:26Z",
        "merged_at": "2020-04-20T09:54:26Z",
        "body": "Set thread name for each thread of redis-server, this helps us to\r\nmonitor the utilization and optimise the performance.\r\n\r\nAn exmaple like this:\r\n```\r\n # top -d 5 -p `pidof redis-server ` -H\r\n\r\n    PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\r\n3682671 root      20   0  227744   8248   3836 R 99.2  0.0   0:19.53 redis-server\r\n3682677 root      20   0  227744   8248   3836 S 26.4  0.0   0:04.15 io_thd_3\r\n3682675 root      20   0  227744   8248   3836 S 23.6  0.0   0:03.98 io_thd_1\r\n3682676 root      20   0  227744   8248   3836 S 23.6  0.0   0:03.97 io_thd_2\r\n3682672 root      20   0  227744   8248   3836 S  0.2  0.0   0:00.02 bio_close_file\r\n3682673 root      20   0  227744   8248   3836 S  0.2  0.0   0:00.02 bio_aof_fsync\r\n3682674 root      20   0  227744   8248   3836 S  0.0  0.0   0:00.00 bio_lazy_free\r\n3682678 root      20   0  227744   8248   3836 S  0.0  0.0   0:00.00 jemalloc_bg_thd\r\n3682682 root      20   0  227744   8248   3836 S  0.0  0.0   0:00.00 jemalloc_bg_thd\r\n3682683 root      20   0  227744   8248   3836 S  0.0  0.0   0:00.00 jemalloc_bg_thd\r\n3682684 root      20   0  227744   8248   3836 S  0.0  0.0   0:00.00 jemalloc_bg_thd\r\n3682685 root      20   0  227744   8248   3836 S  0.0  0.0   0:00.00 jemalloc_bg_thd\r\n3682687 root      20   0  227744   8248   3836 S  0.0  0.0   0:00.00 jemalloc_bg_thd\r\n```\r\n\r\nSigned-off-by: zhenwei pi <pizhenwei@bytedance.com>",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-04-12T07:10:59Z",
        "closed_at": "2020-04-16T15:37:45Z",
        "merged_at": "2020-04-16T15:37:45Z",
        "body": "While playing with ACLs I noticed that acllog-max-len wasn't in the redis.conf, but was a supported config. \r\n\r\nThis PR documents and adds the directive to the redis.conf file.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-11T14:17:03Z",
        "closed_at": "2020-04-14T09:40:52Z",
        "merged_at": "2020-04-14T09:40:52Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-10T19:46:54Z",
        "closed_at": "2021-05-18T07:27:33Z",
        "merged_at": null,
        "body": "Fixes #7081.\r\n\r\nHides `CONFIG SET requirepass/masterauth/masteruser`.\r\n\r\nThe next command that needs to be hidden should trigger a refactoring of the \"dangerous\" conditional logic to use some sort of stack struct with the hidden commands perhaps.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-04-09T09:51:37Z",
        "closed_at": "2020-04-15T08:23:24Z",
        "merged_at": "2020-04-15T08:23:24Z",
        "body": "geo_point {180, 85.05112878}  will decoded to  long = 180.00000268220901",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-08T10:22:48Z",
        "closed_at": "2020-04-10T08:15:26Z",
        "merged_at": "2020-04-10T08:15:26Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-04-06T12:52:52Z",
        "closed_at": "2020-04-06T15:32:45Z",
        "merged_at": "2020-04-06T15:32:45Z",
        "body": "fix comments about RESIZE DB opcode in rdb.c",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-04-06T11:38:33Z",
        "closed_at": "2020-04-06T15:34:15Z",
        "merged_at": "2020-04-06T15:34:15Z",
        "body": "Judge the log level in advance\uff0cavoid log string splicing.\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 13,
        "changed_files": 2,
        "created_at": "2020-04-06T06:44:58Z",
        "closed_at": "2020-04-06T09:47:36Z",
        "merged_at": "2020-04-06T09:47:36Z",
        "body": "after merging RC3 into our fork i found that the fix of #6974 broke our fork.\r\nthe reason is that we already had a different fix for these problems locally, and these two collide.\r\nlooking at the difference, i must admit that i don't like the +100 -100 fix and reference to specific line numbers in other files.\r\n\r\nnote that the big bulk of change in test_helper.tcl is just indentation.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-04-05T22:26:49Z",
        "closed_at": "2020-07-28T19:05:49Z",
        "merged_at": "2020-07-28T19:05:49Z",
        "body": "valsize was not modified during the for loop below instead of getting from c->argv[4], therefore there is no need to put inside the for loop.. Moreover, putting the check outside loop will also avoid memory leaking, decrRefCount(key) should be called in the original code if we put the check in for loop...",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-04-02T15:50:13Z",
        "closed_at": "2020-04-03T09:10:27Z",
        "merged_at": "2020-04-03T09:10:27Z",
        "body": "According to the definition, max value for `fill` should be `(1 << (QL_FILL_BITS-1))-1` and max for `compress` should be `(1 << QL_COMP_BITS)-1`.\r\n\r\n```c\r\n    int fill : QL_FILL_BITS; \r\n    unsigned int compress : QL_COMP_BITS; \r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-04-02T15:41:58Z",
        "closed_at": "2020-04-02T18:00:52Z",
        "merged_at": "2020-04-02T18:00:52Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-04-02T12:03:32Z",
        "closed_at": "2020-04-03T12:12:32Z",
        "merged_at": "2020-04-03T12:12:32Z",
        "body": "There is an inherent race between the deferring client and the\r\n\"main\" client of the test: While the deferring client issues a blocking\r\ncommand, we can't know for sure that by the time the \"main\" client\r\ntries to issue another command (Usually one that unblocks the deferring\r\nclient) the deferring client is even blocked...\r\nFor lack of a better choice this commit uses TCL's 'after' in order\r\nto give some time for the deferring client to issues its blocking\r\ncommand before the \"main\" client does its thing.\r\nThis problem probably exists in many other tests but this commit\r\ntries to fix blockonkeys.tcl",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-03-30T07:56:24Z",
        "closed_at": "2020-04-02T09:20:55Z",
        "merged_at": "2020-04-02T09:20:55Z",
        "body": "propagate_last_id is declared outside of the loop but used\r\nonly from within the loop. Once it's '1' it will never go\r\nback to '0' and will replicate XSETID even for IDs that\r\ndon't actually change the last_id.\r\nWhile not a serious bug (XSETID always used group->last_id\r\nso there's no risk), it does causes redundant traffic\r\nbetween master and its replicas",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-03-30T03:29:25Z",
        "closed_at": "2020-04-02T09:22:31Z",
        "merged_at": "2020-04-02T09:22:31Z",
        "body": "This PR including two more checks when enabling tracking:\r\n\r\n1. Check OPTIN OPTOUT mode should not both been provided.\r\n\r\n2. Check for do not let user directly switch optin/optout mode before disabling the tracking, if we did not do so, when the previous mode do CLIENT CACHING YES directly follows a switching mode command(client tracking on OPTIN/OPTOUT), the flag in previous mode will affect next, similar to PR:https://github.com/antirez/redis/pull/7019",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-03-29T14:51:52Z",
        "closed_at": "2020-04-02T09:21:21Z",
        "merged_at": "2020-04-02T09:21:20Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 9,
        "changed_files": 6,
        "created_at": "2020-03-29T10:19:28Z",
        "closed_at": "2020-03-31T15:00:57Z",
        "merged_at": "2020-03-31T15:00:57Z",
        "body": "Makse sure call() doesn't wrap replicated commands with\r\na redundant MULTI/EXEC\r\n\r\nOther, unrelated changes:\r\n1. Formatting compiler warning in INFO CLIENTS\r\n2. Use CLIENT_ID_AOF instead of UINT64_MAX",
        "comments": 32
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-03-28T16:18:02Z",
        "closed_at": "2020-03-29T14:17:52Z",
        "merged_at": "2020-03-29T14:17:52Z",
        "body": "minimal type is int.  I think would it is unsigned long.\r\nwhen d->ht[0].used > 4294967296.  minimal will overflow, become negative. so will not trigger rehash in dictExpand();\r\nbut in 64bit system\uff0cthe ULONG_MAX is 0x7fffffffffffffffL\u3002 I think it should trigger  rehash.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2020-03-27T08:21:25Z",
        "closed_at": "2020-03-27T15:21:15Z",
        "merged_at": null,
        "body": "When we use the meaningful offset, rollback the replication backlog.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2020-03-26T13:47:12Z",
        "closed_at": "2020-04-03T09:14:13Z",
        "merged_at": "2020-04-03T09:14:13Z",
        "body": "\u2026tion is being used",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-03-26T13:41:37Z",
        "closed_at": "2020-04-02T09:23:24Z",
        "merged_at": "2020-04-02T09:23:24Z",
        "body": "First, we must parse the IDs, so that we abort ASAP.\r\nThe return value of this command cannot be an error if\r\nthe client successfully acknowledged some messages,\r\nso it should be executed in a \"all or nothing\" fashion.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 73,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2020-03-23T18:47:27Z",
        "closed_at": "2020-03-25T11:44:27Z",
        "merged_at": "2020-03-25T11:44:27Z",
        "body": "Redis refusing to run MULTI or EXEC during script timeout may cause partial\r\ntransactions to run.\r\n\r\n1) if the client sends MULTI+commands+EXEC in pipeline without waiting for\r\nresponse, but these arrive to the shards partially while there's a busy script,\r\nand partially after it eventually finishes: we'll end up running only part of\r\nthe transaction (since multi was ignored, and exec would fail).\r\n\r\n2) similar to the above if EXEC arrives during busy script, it'll be ignored and\r\nthe client state remains in a transaction.\r\n\r\nthe 3rd test which i added for a case where MULTI and EXEC are ok, and\r\nonly the body arrives during busy script was already handled correctly\r\nsince processCommand calls flagTransaction",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-03-23T05:41:03Z",
        "closed_at": "2020-03-23T10:09:56Z",
        "merged_at": "2020-03-23T10:09:56Z",
        "body": "Before this PR, the following unexpected scenario was seen when we switch from OPTIN/OPTOUT mode:\r\n**Client 1:** \r\n127.0.0.1:6801> client tracking on redirect 8 optin\r\nOK\r\n127.0.0.1:6801> client caching yes\r\nOK\r\n127.0.0.1:6801> client tracking off\r\nOK\r\n127.0.0.1:6801> client tracking on redirect 8 optout\r\nOK\r\n127.0.0.1:6801> get foo\r\n\"bar\"\r\n\r\nThen we set foo value to a different one in another client:\r\n**Client 2:**\r\n127.0.0.1:6801> set foo barr\r\nOK\r\n127.0.0.1:6801> \r\n\r\nThe epected behaviour is in OPTOUT mode, we should get invalidate message as default when the tracking key changes, however this doesn't happen in the redirect channnel..no invalidate message was sent....\r\n**Invalidate Channnel Client**\r\n127.0.0.1:6801> client id\r\n(integer) 8\r\n127.0.0.1:6801> subscribe __redis__:invalidate\r\nReading messages... (press Ctrl-C to quit)\r\n1) \"subscribe\"\r\n2) \"__redis__:invalidate\"\r\n3) (integer) 1\r\n\r\n\r\n**Reason:**\r\nWhen we disable the tracking we should clear CLIENT_TRACKING_CACHING flags in as well, inn order to clear the incluence from previous caching status...\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 58,
        "deletions": 40,
        "changed_files": 5,
        "created_at": "2020-03-22T12:57:03Z",
        "closed_at": "2020-03-23T10:11:00Z",
        "merged_at": "2020-03-23T10:10:59Z",
        "body": "This fixes a potential double free/corruption issue taking place when accept is rejected. It also clears up some ambiguity around how `connAccept()` callers should handler errors and connection cleanup, and removes some misleading accept error log messages.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-03-20T06:49:53Z",
        "closed_at": "2020-03-20T15:33:10Z",
        "merged_at": "2020-03-20T15:33:10Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-03-18T13:50:56Z",
        "closed_at": "2020-04-02T09:23:53Z",
        "merged_at": "2020-04-02T09:23:53Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-03-18T13:44:05Z",
        "closed_at": "2020-03-23T10:14:46Z",
        "merged_at": "2020-03-23T10:14:46Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2020-03-18T13:05:48Z",
        "closed_at": "2020-03-23T10:15:43Z",
        "merged_at": "2020-03-23T10:15:43Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 96,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2020-03-18T08:26:06Z",
        "closed_at": "2020-03-18T10:04:08Z",
        "merged_at": "2020-03-18T10:04:08Z",
        "body": "Before this commit, when upgrading a replica, expired keys will not\r\nbe loaded, thus causing replica having less keys in db. To this point,\r\nmaster and replica's keys is logically consistent. However, before\r\nthe keys in master and replica are physically consistent, that is,\r\nthey have the same dbsize, if master got a problem and the replica\r\ngot promoted and becomes new master of that partition, and master\r\nupdates a key which does not exist on master, but physically exists\r\non the old master(new replica), the old master would refuse to update\r\nthe key, thus causing master and replica data inconsistent.\r\n\r\nHow could this happen?\r\nThat's all because of the wrong judgement of roles while starting up\r\nthe server. We can not use server.masterhost to judge if the server\r\nis master or replica, since it fails in cluster mode.\r\n\r\nWhen we start the server, we load rdb and do want to load expired keys,\r\nand do not want to have the ability to active expire keys, if it is\r\na replica.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-03-18T05:41:12Z",
        "closed_at": "2020-03-18T10:06:23Z",
        "merged_at": "2020-03-18T10:06:23Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-03-16T15:20:55Z",
        "closed_at": "2020-03-18T10:06:50Z",
        "merged_at": "2020-03-18T10:06:50Z",
        "body": "Redis Cluster Proxy (https://github.com/RedisLabs/redis-cluster-proxy) is a proxy for Redis Clusters.\r\nIt has an internal command named `PROXY` that can be used to perform various proxy-related tasks,\r\nvia different subcommands.\r\n\r\nOne of these is `PROXY INFO` that works in a similar fashion to the classic `INFO` command in Redis but returning proxy-related info.\r\n\r\nThis pull request allows formatting the output from`PROXY INFO` as a raw string, in the same way of the output from Redis `INFO` command.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-03-16T03:24:53Z",
        "closed_at": "2020-03-16T09:48:23Z",
        "merged_at": "2020-03-16T09:48:23Z",
        "body": "Move `handleClientsWithPendingReadsUsingThreads` from `afterSleep` to `beforeSleep`, because the `afterSleep` is processed after the next system call `epoll_wait` and that may delay the pending reads clients processed, I think the right place to handle the clients is after the current event loop ASAP. This can get 10% performance improvement. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-03-15T15:42:31Z",
        "closed_at": "2020-03-16T09:53:55Z",
        "merged_at": "2020-03-16T09:53:55Z",
        "body": "```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n100338:M 15 Mar 2020 23:10:06.572 # Redis 999.999.999 crashed by signal: 11\r\n100338:M 15 Mar 2020 23:10:06.572 # Crashed running the instruction at: 0x444810\r\n100338:M 15 Mar 2020 23:10:06.572 # Accessing address: 0xffffffffffffffff\r\n100338:M 15 Mar 2020 23:10:06.572 # Failed assertion: <no assertion failed> (<no file>:0)\r\n\r\n------ STACK TRACE ------\r\nEIP:\r\n./redis-server *:6379(processInputBuffer+0x10)[0x444810]\r\n\r\nBacktrace:\r\n./redis-server *:6379(logStackTrace+0x29)[0x4724a9]\r\n./redis-server *:6379(sigsegvHandler+0xa6)[0x472b46]\r\n/lib64/libpthread.so.0(+0xf620)[0x7fae3d11e620]\r\n./redis-server *:6379(processInputBuffer+0x10)[0x444810]\r\n./redis-server *:6379(handleClientsWithPendingReadsUsingThreads+0x16d)[0x44534d]\r\n./redis-server *:6379(aeProcessEvents+0x3d5)[0x42d805]\r\n./redis-server *:6379(aeMain+0x2b)[0x42d9ab]\r\n./redis-server *:6379(main+0x4c4)[0x42a474]\r\n/lib64/libc.so.6(__libc_start_main+0xf5)[0x7fae3cd63445]\r\n./redis-server *:6379[0x42a6fd]\r\n```\r\n\r\nRedis crashes if we send `CLIENT KILL TYPE NORMAL` in io-threads-do-reads mode and io threads are active.\r\n\r\nThe reason is the way we iterator the `server.clients_pending_read` list:\r\n\r\n```c\r\n    /* Run the list of clients again to process the new buffers. */\r\n    listRewind(server.clients_pending_read,&li);\r\n    while((ln = listNext(&li))) {\r\n        client *c = listNodeValue(ln);\r\n        c->flags &= ~CLIENT_PENDING_READ;\r\n        if (c->flags & CLIENT_PENDING_COMMAND) {\r\n            c->flags &= ~ CLIENT_PENDING_COMMAND;\r\n            processCommandAndResetClient(c);\r\n        }\r\n        processInputBufferAndReplicate(c);\r\n    }\r\n```\r\n\r\nAfter `listNext` the iterator has already get the next pointer, but during `processCommand`, some client and the current client may be freed, just like `CLIENT KILL` or write error in `flushSlavesOutputBuffers`.\r\n\r\nAfter PR #6989, list `server.clients_pending_read` never grow during we loop it, so we can just check the list length to end the loop.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-03-12T12:47:55Z",
        "closed_at": "2020-03-13T11:26:19Z",
        "merged_at": "2020-03-13T11:26:18Z",
        "body": "When I view the latency.c of code, I found a spelling mistake\uff0cby the way to increase ignore.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-03-12T03:18:15Z",
        "closed_at": "2020-03-13T11:32:04Z",
        "merged_at": "2020-03-13T11:32:04Z",
        "body": "Remove duplicate obj files in Makefile.\r\nI also checked all the Makefiles and it was ok.\r\n\r\n@antirez Please review during your free time.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-03-11T16:48:26Z",
        "closed_at": "2020-03-13T11:35:01Z",
        "merged_at": "2020-03-13T11:35:00Z",
        "body": "... and removes an extra trailing space (ht vscode).\r\n\r\nPlease consider backporting for completeness.\r\n\r\nCredit: https://stackoverflow.com/questions/60633797/how-to-get-notification-from-redis-if-a-key-changes-as-part-of-restore-operati\r\n\r\nDocs PR: https://github.com/antirez/redis-doc/pull/1266",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-03-11T16:38:20Z",
        "closed_at": "2020-03-12T14:55:24Z",
        "merged_at": "2020-03-12T14:55:24Z",
        "body": "This is a backport 5.0 fix for redis compilation issues on Raspberry Pi, original issue https://github.com/antirez/redis/issues/6275.\r\n\r\nThis change was already applied to `unstable` branch: https://github.com/antirez/redis/commit/f5d48537f1aa76e5ce4f14953517bd25c9ad4673.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2020-03-11T13:27:46Z",
        "closed_at": "2020-03-13T11:27:27Z",
        "merged_at": "2020-03-13T11:27:27Z",
        "body": "### Steps to reproduce:\r\n\r\n1. Run tcl test with external server\r\n\r\n```\r\n\u279c  redis git:(unstable) \u2717 ./runtest --host $external_ip --port 6379 --single unit/type/string\r\nCleanup: may take some time... OK\r\nStarting test server at port 11111\r\n[ready]: 64884\r\nTesting unit/printver\r\n[ready]: 64880\r\nTesting unit/dump\r\n[ready]: 64883\r\nTesting unit/auth\r\n[ready]: 64881\r\nTesting unit/protocol\r\n[exception]: Executing test client: couldn't open socket: connection refused.\r\ncouldn't open socket: connection refused\r\n    while executing\r\n\"socket $server $port\"\r\n    (procedure \"redis\" line 7)\r\n    invoked from within\r\n\"redis $::host $::port 0 $::tls\"\r\n    (procedure \"start_server\" line 9)\r\n    invoked from within\r\n\"start_server {tags {\"dump\"}} {\r\n    test {DUMP / RESTORE are able to serialize / unserialize a simple key} {\r\n        r set foo bar\r\n        set encoded ...\"\r\n    (file \"tests/unit/dump.tcl\" line 1)\r\n    invoked from within\r\n\"source $path\"\r\n    (procedure \"execute_tests\" line 4)\r\n    invoked from within\r\n\"execute_tests $data\"\r\n    (procedure \"test_client_main\" line 10)\r\n    invoked from within\r\n\"test_client_main $::test_server_port \"\r\n```\r\nSimilar to this question\uff0c[using external redis server for testing tcl scripts](https://stackoverflow.com/questions/10390008/using-external-redis-server-for-testing-tcl-scripts).\r\n\r\n### Reason\r\n\r\n1. In `test_server_main`,`::port` will be reset via the `--port` option\r\n```\r\n    set start_port [expr {$::port+100}]\r\n    for {set j 0} {$j < $::numclients} {incr j} {\r\n        set start_port [find_available_port $start_port]  // Calculate new start_port\r\n        set p [exec $tclsh [info script] {*}$::argv \\\r\n            --client $port --port $start_port &]      // --port will reset ::port\r\n        lappend ::clients_pids $p\r\n        incr start_port 10\r\n    }\r\n```\r\n\r\n2. When connecting to Redis in `start_server`,  the `::port` error, increased by 100\r\n```\r\n    if {$::external} {\r\n        if {[llength $::servers] == 0} {\r\n            set srv {}\r\n            dict set srv \"host\" $::host \r\n            dict set srv \"port\" $::port  // the port is increased by 100\r\n            set client [redis $::host $::port 0 $::tls]\r\n            dict set srv \"client\" $client\r\n            $client select 9\r\n\r\n            # append the server to the stack\r\n            lappend ::servers $srv\r\n        }\r\n        uplevel 1 $code\r\n        return\r\n    }\r\n```\r\n\r\n### Fix\r\nsee the PR.\r\n\r\n### Result\r\n```\r\n\u279c  redis git:(fix-tcl-test-host-option) \u2717 ./runtest --host $ip --port 6379 --single unit/type/string\r\nCleanup: may take some time... OK\r\nStarting test server at port 11111\r\n[ready]: 65696\r\nTesting unit/type/string\r\n[ok]: SET and GET an item\r\n[ok]: SET and GET an empty item\r\n...\r\n```\r\n\r\n\r\n### Notice\r\n\r\n1. This fix only supports the case where `:: numclients` is 1. Of course, if you use an external server, setting`::numclients` too large doesn't make sense, because there is only one server.\r\n2. Some test cases fail, for example `overrides {requirepass foobar}` needs to pass parameters, but the external server has already started at this time.\r\n\r\n\r\n@antirez Please review in your free time, thank you.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 313,
        "deletions": 307,
        "changed_files": 28,
        "created_at": "2020-03-09T05:47:17Z",
        "closed_at": "2020-03-10T15:48:51Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2020-03-08T15:18:52Z",
        "closed_at": "2022-07-24T03:52:30Z",
        "merged_at": null,
        "body": "Add a generic flag to be used by modules.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-03-07T11:41:11Z",
        "closed_at": "2020-03-13T11:37:09Z",
        "merged_at": "2020-03-13T11:37:08Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 27,
        "changed_files": 1,
        "created_at": "2020-03-07T10:44:21Z",
        "closed_at": "2020-04-02T09:26:09Z",
        "merged_at": "2020-04-02T09:26:09Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2020-03-04T14:13:48Z",
        "closed_at": "2020-03-23T10:23:22Z",
        "merged_at": "2020-03-23T10:23:21Z",
        "body": "@antirez Hello\r\n\r\nWe encountered some massive read by using the `BITFIELD` command of many users, which is much similar to `GEORADIUS` case. \r\n\r\nIt can't be scattered to read-only slaves, please refer to #4084, I searched the previous `Issues` and `PR`, but no relevant discussion found. Hereby I submitted the `PR` accordingly.\r\n\r\nBy introducing a new command `BITFIELD_RO`, the performance increases dramatically in read-write separated Redis. \r\n\r\nThe `BITFIELD get xxx` is going to be executed in slaves by special proxies, especially in the scenario of more reading and less writing.\r\n\r\nPR is listed above, please advise!\r\n\r\nI will check whether other APIs have mixed read and write permissions, please let us know if there's the further job.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-03-04T02:03:54Z",
        "closed_at": "2020-03-10T15:47:07Z",
        "merged_at": "2020-03-10T15:47:07Z",
        "body": "When using TLS with a Redis.conf file the line for TLS reading tls-cert-file redis.crt tls-key-file redis.key is interpreted as one complete directive. I am separating this on two separate lines to improve usability so users do not get the below error. @yossigo  what are your thoughts on this?\r\n\r\n```\r\nubuntu@ip-172-31-29-250:~/redis-6.0-rc1$ ./src/redis-server redis.conf \r\n\r\n*** FATAL CONFIG FILE ERROR ***\r\nReading the configuration file, at line 145\r\n>>> 'tls-cert-file redis.crt tls-key-file redis.key'\r\nwrong number of arguments\r\nubuntu@ip-172-31-29-250:~/redis-6.0-rc1$ vi redis.conf \r\nubuntu@ip-172-31-29-250:~/redis-6.0-rc1$ ./src/redis-server redis.conf \r\n23085:C 04 Mar 2020 01:58:12.631 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n23085:C 04 Mar 2020 01:58:12.631 # Redis version=5.9.101, bits=64, commit=00000000, modified=0, pid=23085, just started\r\n23085:C 04 Mar 2020 01:58:12.631 # Configuration loaded\r\n```\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-03-02T10:16:55Z",
        "closed_at": "2020-03-03T12:07:41Z",
        "merged_at": null,
        "body": "### Steps to reproduce:\r\n1. In order to reproduce easily, `sleep` is added to the `rpoplpush` to simulate the situation of being expired.\r\n\r\n```\r\nvoid rpoplpushCommand(client *c) {\r\n    robj *sobj, *value;\r\n    if ((sobj = lookupKeyWriteOrReply(c,c->argv[1],shared.nullbulk)) == NULL ||\r\n        checkType(c,sobj,OBJ_LIST)) return;\r\n\r\n    if (listTypeLength(sobj) == 0) {\r\n        /* This may only happen after loading very old RDB files. Recent\r\n         * versions of Redis delete keys of empty lists. */\r\n        addReply(c,shared.nullbulk);\r\n    } else {\r\n        sleep(20); // sleep some time, for key be expired\r\n\r\n        robj *dobj = lookupKeyWrite(c->db,c->argv[2]);  // will expireIfNeeded\r\n        robj *touchedkey = c->argv[1];\r\n\r\n        if (dobj && checkType(c,dobj,OBJ_LIST)) return;\r\n        value = listTypePop(sobj,LIST_TAIL);\r\n        /* We saved touched key, and protect it, since rpoplpushHandlePush\r\n         * may change the client command argument vector (it does not\r\n         * currently). */\r\n        incrRefCount(touchedkey);\r\n        rpoplpushHandlePush(c,c->argv[2],dobj,value);\r\n\r\n        /* listTypePop returns an object with its refcount incremented */\r\n        decrRefCount(value);\r\n}\r\n```\r\n\r\n2. use redis-cli\r\n```\r\n127.0.0.1:6379> lpush list 1 2 3 4 5\r\n(integer) 5\r\n127.0.0.1:6379> expire list 10\r\n(integer) 1\r\n127.0.0.1:6379> RPOPLPUSH list list\r\n// wait some time, server will coredump\r\n```\r\n\r\n3. server stack path\r\n```\r\n    frame #4: 0x000000010206b65f redis-server`getDecodedObject(o=0x0000000000000000) at object.c:510\r\n    frame #5: 0x00000001021087cf redis-server`listTypePush(subject=0x00000001063167e0, value=0x0000000000000000, where=0) at t_list.c:44\r\n    frame #6: 0x00000001021175af redis-server`rpoplpushHandlePush(c=0x000000010997e180, dstkey=0x0000000105fcadb8, dstobj=0x00000001063167e0, value=0x0000000000000000) at t_list.c:559\r\n    frame #7: 0x00000001021180b1 redis-server`rpoplpushCommand(c=0x000000010997e180) at t_list.c:586\r\n    frame #8: 0x0000000101f922e2 redis-server`call(c=0x000000010997e180, flags=15) at server.c:2439\r\n    frame #9: 0x0000000101f9cec5 redis-server`processCommand(c=0x000000010997e180) at server.c:2733\r\n    frame #10: 0x0000000102047641 redis-server`processInputBuffer(c=0x000000010997e180) at networking.c:1470\r\n    frame #11: 0x0000000102048c66 redis-server`processInputBufferAndReplicate(c=0x000000010997e180) at networking.c:1505\r\n    frame #12: 0x000000010202a324 redis-server`readQueryFromClient(el=0x0000000105fcc0f0, fd=9, privdata=0x000000010997e180, mask=1) at networking.c:1587\r\n    frame #13: 0x0000000101f3b621 redis-server`aeProcessEvents(eventLoop=0x0000000105fcc0f0, flags=11) at ae.c:443\r\n    frame #14: 0x0000000101f407df redis-server`aeMain(eventLoop=0x0000000105fcc0f0) at ae.c:501\r\n    frame #15: 0x0000000101fb23bd redis-server`main(argc=5, argv=0x00007ffeedcf0368) at server.c:4200\r\n```\r\n\r\n@antirez PING",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-03-02T05:11:40Z",
        "closed_at": "2020-09-09T16:23:56Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-29T10:40:02Z",
        "closed_at": "2020-03-02T15:53:17Z",
        "merged_at": "2020-03-02T15:53:17Z",
        "body": "To avoid compiler warnings in ` acl.c`\r\n```\r\nacl.c: In function \u2018aclCommand\u2019:\r\nacl.c:1834:13: warning: \u2018reasonstr\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n             addReplyBulkCString(c,reasonstr);\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\nWe set `reasonstr` is \"unknown\" defaultly.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-29T10:00:24Z",
        "closed_at": "2020-08-08T19:03:57Z",
        "merged_at": "2020-08-08T19:03:57Z",
        "body": "Thank you @djmgit for discovering this problem in issue https://github.com/antirez/redis/issues/6857\r\nI consider that we just print `strerror` info because `fopen `will set `errno` if fails. And that includes all error cases.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2020-02-28T11:36:03Z",
        "closed_at": "2020-04-28T09:21:07Z",
        "merged_at": "2020-04-28T09:21:07Z",
        "body": "Allows for setting the binaries' path if used outside the upstream repo.\r\n\r\nAlso documents `call` in usage clause (TODO: port to\r\n`redis-cli --cluster call` or just deprecate it?).",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-02-27T08:57:22Z",
        "closed_at": "2020-09-09T16:23:59Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2020-02-27T06:40:16Z",
        "closed_at": "2020-02-27T08:52:41Z",
        "merged_at": "2020-02-27T08:52:41Z",
        "body": "it seems that running two clients at a time is ok too, resuces action\r\ntime from 20 minutes to 10. we'll use this for now, and if one day it\r\nwon't be enough we'll have to run just the sensitive tests one by one\r\nseparately from the others.\r\n\r\nthis commit also fixes an issue with the defrag test that appears to be\r\nvery rare.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-02-25T12:11:11Z",
        "closed_at": "2020-04-03T09:12:59Z",
        "merged_at": "2020-04-03T09:12:59Z",
        "body": "Example: Client uses a pipe to send the following to a\r\nstale replica:\r\n\r\nMULTI\r\n.. do something ...\r\nDISCARD\r\n\r\nThe replica will reply the MUTLI with -MASTERDOWN and\r\nexecute the rest of the commands... A client using a\r\npipe might not be aware that MULTI failed until it's\r\ntoo late.\r\n\r\nI can't think of a reason why MULTI/EXEC/DISCARD should\r\nnot be executed on stale replicas...\r\n\r\nAlso, enable MULTI/EXEC/DISCARD during loading",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 13,
        "changed_files": 2,
        "created_at": "2020-02-25T11:06:17Z",
        "closed_at": "2020-02-25T18:20:53Z",
        "merged_at": "2020-02-25T18:20:52Z",
        "body": "seems that github actions are slow, using just one client to reduce\r\nfalse positives.\r\n    \r\nalso adding verbose, testing only on latest ubuntu, and building on\r\nolder one.\r\n    \r\nwhen doing that, i can reduce the test threshold back to something saner\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-25T08:13:22Z",
        "closed_at": "2020-02-27T09:02:26Z",
        "merged_at": "2020-02-27T09:02:26Z",
        "body": "read buffer size is set to 4096, while the write buffer is set to PROTO_IOBUF_LEN(16384).\r\nSo the read buffer size can also be modified to 16384 , this will take less time for large data sync.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 382,
        "deletions": 99,
        "changed_files": 11,
        "created_at": "2020-02-24T18:38:46Z",
        "closed_at": "2020-10-08T05:33:18Z",
        "merged_at": "2020-10-08T05:33:18Z",
        "body": "Adds lpoprpush, rpoprpush and lpoplpush, and the equivalent blocking commands: blpoprpush, brpoprpush and blpoplpush.",
        "comments": 57
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-23T14:54:40Z",
        "closed_at": "2020-04-02T14:32:42Z",
        "merged_at": "2020-04-02T14:32:42Z",
        "body": "should (can) be merged after https://github.com/antirez/redis/pull/6926",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-02-23T14:49:09Z",
        "closed_at": "2020-02-27T08:58:05Z",
        "merged_at": "2020-02-27T08:58:05Z",
        "body": "in some cases we were trying to kill the fork before it got created",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-23T14:25:12Z",
        "closed_at": "2020-03-13T11:49:40Z",
        "merged_at": null,
        "body": "127.0.0.1:6500> config get save\r\n\r\n\"save\"\r\n\"10 5\"\r\n127.0.0.1:6500> dbsize\r\n(integer) 1\r\n127.0.0.1:6500> select 6\r\nOK\r\n127.0.0.1:6500[6]> dbsize\r\n(integer) 5\r\n127.0.0.1:6500[6]> lastsave\r\n(integer) 1582467466\r\n127.0.0.1:6500[6]> swapdb 0 6\r\nOK\r\n127.0.0.1:6500[6]> lastsave\r\n(integer) 1582467466\r\n127.0.0.1:6500[6]> lastsave\r\n(integer) 1582467466\r\nSave conndition is 10s at last 5 changes in DB, Although the size of db0 and db6 have 1 and 5 keys, after swapping db the rdb save was not triggered.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-23T14:13:47Z",
        "closed_at": "2020-03-02T16:07:42Z",
        "merged_at": "2020-03-02T16:07:42Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2020-02-23T14:07:31Z",
        "closed_at": "2020-02-27T08:53:53Z",
        "merged_at": "2020-02-27T08:53:53Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-02-23T13:45:23Z",
        "closed_at": "2020-02-27T08:59:52Z",
        "merged_at": "2020-02-27T08:59:52Z",
        "body": "Use built-in alsoPropagate mechanism that wraps commands\r\nin MULTI/EXEC before sending them to replica/AOF",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2020-02-23T10:49:45Z",
        "closed_at": "2020-02-24T10:53:33Z",
        "merged_at": "2020-02-24T10:53:33Z",
        "body": "I saw that the new defag test for list was failing in CI recently, so i\r\nreduce it's threshold from 12 to 60.\r\n\r\nbesides that, i add / improve the latency test for that other two defrag\r\ntests (add a sensitive latency and digest / save checks)\r\n\r\nand fix bad usage of debug populate (can't overrides existing keys).\r\nthis was the original intention, which creates higher fragmentation.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-02-22T21:53:34Z",
        "closed_at": "2020-02-27T09:01:49Z",
        "merged_at": "2020-02-27T09:01:49Z",
        "body": "Correct the function names in the commented instructions.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-22T07:05:27Z",
        "closed_at": "2020-03-13T11:46:27Z",
        "merged_at": "2020-03-13T11:46:27Z",
        "body": "I think use \"used->usedby\" instead of \"module->using\" maybe is right?",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-20T12:33:42Z",
        "closed_at": "2020-02-20T16:15:17Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-20T09:35:51Z",
        "closed_at": "2020-03-13T13:38:16Z",
        "merged_at": null,
        "body": "When aof is off,the bgrewriteaof command should be forbidden. \r\nIf aof is off and a person A execute the bgrewrite command, after that anther person B decides to modify the appendonly to yes in redis.conf file and restart the server, we will get mistake. In this case, we expect the db is empty, but we may get a A-bgrewriteaof edition. The server will load the appendonly.aof created by A.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-02-19T17:11:41Z",
        "closed_at": "2020-02-20T11:02:06Z",
        "merged_at": "2020-02-20T11:02:06Z",
        "body": "fss should be rss",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2020-02-19T02:59:32Z",
        "closed_at": "2020-02-20T11:54:15Z",
        "merged_at": "2020-02-20T11:54:15Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 350,
        "deletions": 34,
        "changed_files": 4,
        "created_at": "2020-02-18T16:04:38Z",
        "closed_at": "2020-02-20T12:00:40Z",
        "merged_at": "2020-02-20T12:00:40Z",
        "body": "When active defrag kicks in and finds a big list, it will create a bookmark to\r\na node so that it is able to resume iteration from that node later.\r\n\r\nThe quicklist manages that bookmark, and updates it in case that node is deleted.\r\n\r\nThis will increase memory usage only on lists of over 1000 (see\r\nactive-defrag-max-scan-fields) quicklist nodes (1000 ziplists, not 1000 items)\r\nby 16 bytes.\r\n\r\nIn 32 bit build, this change reduces the maximum effective config of\r\nlist-compress-depth and list-max-ziplist-size (from 32767 to 8191)",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-18T04:42:28Z",
        "closed_at": "2020-02-20T16:42:25Z",
        "merged_at": "2020-02-20T16:42:25Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 265,
        "deletions": 241,
        "changed_files": 1,
        "created_at": "2020-02-18T03:46:59Z",
        "closed_at": "2020-08-14T11:45:35Z",
        "merged_at": "2020-08-14T11:45:35Z",
        "body": "In order to keep the redismodule.h self-contained but still usable with\r\ngcc v10 and later, annotate each API function tentative definition with\r\nthe __common__ attribute.  This avoids the 'multiple definition' errors\r\nmodules will otherwise see for all API functions at link time.\r\n\r\nFurther details at https://gcc.gnu.org/gcc-10/porting_to.html",
        "comments": 23
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 17,
        "changed_files": 2,
        "created_at": "2020-02-16T14:04:19Z",
        "closed_at": "2020-02-20T12:03:32Z",
        "merged_at": "2020-02-20T12:03:32Z",
        "body": "The specific bugs:\r\n1. If you call flushall, it deletes the tracking table so all subsequent calls fail. I fixed it by initializing a new one. Easy repro is just run flushall twice after tracking a client. \r\n2. You can do client tracking on bcast bcast to enable bcast, but issuing that again fails, so I made that consistent by setting bcast = 1. It's a minor thing.\r\n3. Some cases memory might get leaked in client command if you construct a command with errors. \r\n\r\nUpdated documentation to remove some references to slots and replace with keys, and updated the client help. \r\n\r\nThere is also a second commit which is something I initially thought you could do while reading the API, which is to redirect to multiple clients, so I also added throwing a message so no one also tries that. (I'm not sure why some one would)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-02-16T13:43:48Z",
        "closed_at": "2020-02-28T09:22:54Z",
        "merged_at": "2020-02-28T09:22:54Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-16T11:36:30Z",
        "closed_at": "2020-09-09T16:24:02Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 340,
        "deletions": 79,
        "changed_files": 11,
        "created_at": "2020-02-14T22:00:00Z",
        "closed_at": "2022-02-08T11:14:43Z",
        "merged_at": "2022-02-08T11:14:43Z",
        "body": "**The problem/use-case that the feature addresses**\r\n\r\nThis is an enhancement for INFO command, previously INFO only support one argument for different info section , if user want to get more categories information, either perform INFO all/default or calling INFO for multiple times.\r\n\r\n**Description of the feature**\r\n\r\nThe goal of adding this feature is to let the user retrieve multiple categories via the INFO command, and still avoid emitting the same section twice.\r\n\r\nA use case for this is like Redis Sentinel, which periodically calling INFO command to refresh info from monitored Master/Slaves, only Server and Replication part categories are used for parsing information.\r\nIf the INFO command can return just enough categories that client side needs, it can save a lot of time for client side parsing it as well as network bandwidth.\r\n\r\n**Implementation**\r\nTo share code between redis, sentinel, and other users of INFO (DEBUG and modules), we have a new `genInfoSectionDict` function that returns a dict and some boolean flags (e.g. `all`) to the caller (built from user input).\r\nSentinel is later purging unwanted sections from that, and then it is forwarded to the info `genRedisInfoString`.\r\n\r\n**Usage Examples**\r\nINFO Server Replication   \r\nINFO CPU Memory\r\nINFO default commandstats",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-14T01:55:37Z",
        "closed_at": "2020-02-14T14:35:18Z",
        "merged_at": null,
        "body": "Corresponding to PR https://github.com/antirez/redis/pull/6679, update help message including new KEEPTTL flag.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 202,
        "deletions": 47,
        "changed_files": 1,
        "created_at": "2020-02-13T06:00:12Z",
        "closed_at": "2020-08-28T14:22:36Z",
        "merged_at": "2020-08-28T14:22:36Z",
        "body": "Dear sir:\r\nCurrent implementation of __ziplistCascadeUpdate is of O(n^2) time complexity. I try to optimize it with an algorithm of O(n) time complexity.\r\nHere is the basic idea: \r\n1. loop ziplist to find how many extra bytes need to update the whole list\r\n2. resize ziplist once and do update job from back to front\r\n\r\nThis PR has passed make test.",
        "comments": 28
    },
    {
        "merged": false,
        "additions": 228,
        "deletions": 228,
        "changed_files": 1,
        "created_at": "2020-02-13T00:20:26Z",
        "closed_at": "2020-02-13T07:02:39Z",
        "merged_at": null,
        "body": "Modules using a copy of redismodule.h from the Redis repo fail to compile with latest version (>=10) of gcc - link errors result due to duplicate definitions of API functions.  Resolved by adding the 'extern' annotation to forward declarations of these functions.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-12T19:14:10Z",
        "closed_at": "2021-12-21T13:16:33Z",
        "merged_at": null,
        "body": "Fix for issue #6883",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-02-12T08:54:32Z",
        "closed_at": "2020-02-12T13:04:16Z",
        "merged_at": "2020-02-12T13:04:16Z",
        "body": "fix #6880 in `redis-cli -h`:\r\n\r\n-user --> --user\r\n-pass --> --pass\r\n\r\nIn #6880 The last one is not a bug.\r\n\r\nSigned-off-by: lifubang <lifubang@acmcoder.com>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-02-10T10:16:51Z",
        "closed_at": "2020-02-27T09:26:56Z",
        "merged_at": "2020-02-27T09:26:56Z",
        "body": "RediSearch using fork process to do garbage collection and looks like those log messages blow the log file. After consulting with @oranagra we believe that log level verbos is good enough here.\r\n\r\n@antirez what do you think?",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-02-10T07:42:03Z",
        "closed_at": "2020-02-10T09:45:55Z",
        "merged_at": "2020-02-10T09:45:55Z",
        "body": "Hi,\r\n\r\nWe discovered that the CVE-2015-8080 vulnerability revisited in the latest version of Redis (5.0.7).\r\n - Initial issue number: **#2855**\r\n - Initial fix commit: **3a47c8cfb85af1b69cccf30eaaa690e4a23ab20a** (Dec. 2015)\r\n\r\nThe vulnerability is from the Lua source code that you already patched in Dec. 2015. \r\nHowever, as a result of the Lua update in May 2018 (commit: 1eb08bcd4634ae42ec45e8284923ac048beaa4c3), the vulnerability patch was removed during the update process.\r\n\r\nAs a result, we successfully reproduce the vulnerability in the latest stable version of Redis using simple PoC provided in issue #2855.\r\n\r\nPlease reflect it after confirmation.\r\nThank you.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-02-09T06:05:59Z",
        "closed_at": "2020-03-28T18:30:09Z",
        "merged_at": null,
        "body": "# Summary\r\n\r\nProposing updating the version of `actions/checkout` used in the GitHub Actions workflow to the latest - `v2`. Details: https://github.com/actions/checkout#checkout-v2",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 78,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2020-02-08T15:00:17Z",
        "closed_at": "2020-02-20T12:08:03Z",
        "merged_at": null,
        "body": "This PR tries to implement a command for generating key type count.\r\nIt will generate a summary showing which keytype contains how many keys.\r\nThis can help us gain some insight regarding how are the present keys\r\ndistributed among the known keytypes.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-02-07T14:33:37Z",
        "closed_at": "2020-02-07T21:04:07Z",
        "merged_at": null,
        "body": "This enables us to run `make test` after a `make clean`, but also\r\nmakes sure that any redis-cli change will be used in a test run.\r\nSame for `make test-sentinel`",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-06T13:07:16Z",
        "closed_at": "2020-02-20T12:08:32Z",
        "merged_at": "2020-02-20T12:08:32Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-06T13:07:04Z",
        "closed_at": "2020-02-13T16:02:20Z",
        "merged_at": "2020-02-13T16:02:20Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-06T12:54:30Z",
        "closed_at": "2020-02-28T10:05:32Z",
        "merged_at": "2020-02-28T10:05:32Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 21,
        "changed_files": 4,
        "created_at": "2020-02-06T12:52:29Z",
        "closed_at": "2020-04-02T14:46:46Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-06T11:11:40Z",
        "closed_at": "2022-03-02T12:20:36Z",
        "merged_at": null,
        "body": "This PR addresses the feature mentioned in issue #6857\r\nIt checks whether the given config file is present or not\r\nand also checks if it has got read permission, and logs accordingly.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-02-06T09:45:05Z",
        "closed_at": "2020-04-03T11:46:28Z",
        "merged_at": "2020-04-03T11:46:28Z",
        "body": "",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-06T08:22:18Z",
        "closed_at": "2020-02-06T09:23:12Z",
        "merged_at": "2020-02-06T09:23:12Z",
        "body": "althouh in theory, users can do BGREWRITEAOF even if aof is disabled, i\r\nsuppose it is more common that the scheduled flag is set by either\r\nstartAppendOnly, of a failed initial AOFRW fork (AOF_WAIT_REWRITE)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-02-06T08:16:01Z",
        "closed_at": "2020-02-06T09:27:06Z",
        "merged_at": "2020-02-06T09:27:06Z",
        "body": "replicationUnsetMaster can be called from other places, not just\r\nreplicaofCOmmand, and all of these need to restart AOF",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-06T08:07:45Z",
        "closed_at": "2020-02-06T09:29:46Z",
        "merged_at": "2020-02-06T09:29:46Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-02-06T07:42:49Z",
        "closed_at": "2020-02-06T09:30:40Z",
        "merged_at": "2020-02-06T09:30:40Z",
        "body": "this function possibly iterates on the module list",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-02-06T07:39:31Z",
        "closed_at": "2020-02-06T09:31:30Z",
        "merged_at": "2020-02-06T09:31:30Z",
        "body": "using panic rather than exit means you get s stack trace of the code\r\npath that experianced the error, and possibly other info.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2020-02-06T07:34:39Z",
        "closed_at": "2020-02-06T09:32:35Z",
        "merged_at": "2020-02-06T09:32:35Z",
        "body": "currently there's no bug since the flags these functions handle are\r\nalways lower than 32bit, but still better fix the type to prevent future\r\nbugs.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-02-06T07:25:20Z",
        "closed_at": "2020-02-06T09:33:16Z",
        "merged_at": "2020-02-06T09:33:16Z",
        "body": "the warning condition was if usage > limit (saying it'll cause eviction\r\nor oom), but in fact the eviction and oom depends on used minus slave\r\nbuffers.\r\n\r\nother than fixing the condition, i add info about the current usage and\r\nlimit, which may be useful when looking at the log.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-06T07:18:42Z",
        "closed_at": "2020-02-06T09:33:41Z",
        "merged_at": "2020-02-06T09:33:41Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2020-02-06T07:06:55Z",
        "closed_at": "2020-02-06T09:34:27Z",
        "merged_at": "2020-02-06T09:34:27Z",
        "body": "SELECT, and HELLO are commands that may be executed by the client\r\nas soon as it connects, there's no reason to block them, preventing the\r\nclient from doing the rest of his sequence (which might just be INFO or\r\nCONFIG, etc).\r\n\r\nMONITOR, DEBUG, SLOWLOG, TIME, LASTSAVE are all non-data accessing\r\ncommands, which there's no reason to block.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 31,
        "changed_files": 2,
        "created_at": "2020-02-05T19:23:45Z",
        "closed_at": "2020-02-06T09:35:22Z",
        "merged_at": "2020-02-06T09:35:22Z",
        "body": "Hi @antirez here are some minor updates to the TLS related documentation. I'll also approach the redis.io documentation sooner than later.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-05T17:48:53Z",
        "closed_at": "2020-02-06T09:35:43Z",
        "merged_at": "2020-02-06T09:35:43Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2020-02-05T16:27:54Z",
        "closed_at": "2020-02-28T16:47:53Z",
        "merged_at": "2020-02-28T16:47:53Z",
        "body": "The callback approach we took is very efficient, the module can do any\r\nfiltering of keys without building any list and cloning strings, it can\r\nalso read data from the key's value. but if the user tries to re-open\r\nthe key, or any other key, this can cause dict re-hashing (dictFind does\r\nthat), and that's very bad to do from inside dictScan.\r\n\r\nthis commit protects the dict from doing any rehashing during scan, but\r\nalso warns the user not to attempt any writes or command calls from\r\nwithin the callback, for fear of unexpected side effects and crashes.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-05T16:16:10Z",
        "closed_at": "2020-02-06T09:38:37Z",
        "merged_at": "2020-02-06T09:38:37Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2020-02-05T16:09:53Z",
        "closed_at": "2020-03-02T15:48:06Z",
        "merged_at": "2020-03-02T15:48:06Z",
        "body": "now that we may use it more often (ACL), these excessive calls to malloc\r\nand free can become an overhead.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-02-05T15:41:15Z",
        "closed_at": "2020-02-06T09:26:10Z",
        "merged_at": null,
        "body": "Unlike loading users from lines embedded in the config file.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 78,
        "deletions": 5,
        "changed_files": 5,
        "created_at": "2020-02-05T15:22:32Z",
        "closed_at": "2020-03-10T15:51:37Z",
        "merged_at": "2020-03-10T15:51:37Z",
        "body": "Although we have a more safer way, which is `REDISCLI_AUTH` env variable, to pass a password in redis-cli.\r\nBut for my company's security policy, we shouldn't save any password in the maintain computer, so I suggest to add `askpass` mode. With this mode, user can type password but don't echo it in stdin.\r\nI think there are many other database client tool that support this mode, such as mysql.\r\n\r\n```\r\nroot@DESKTOP-UVUP1SF:/opt/redis# ./src/redis-cli -p 12345 --askpass\r\nAuth password:\r\n127.0.0.1:12345>\r\n```\r\n\r\nSigned-off-by: lifubang <lifubang@acmcoder.com>",
        "comments": 20
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 97,
        "changed_files": 1,
        "created_at": "2020-02-05T09:44:18Z",
        "closed_at": "2020-02-05T12:41:16Z",
        "merged_at": "2020-02-05T12:41:16Z",
        "body": "We noticed that the error replies for the generic mechanism for enums\r\nare very verbose for config file parsing, but not for config set\r\ncommand.\r\n\r\ninstead of replicating this code, i did a small refactoring to share\r\ncode between CONFIG SET and config file parsing.\r\n\r\nand also renamed the enum group functions to be consistent with the\r\nnaming of other types.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2020-02-05T07:43:32Z",
        "closed_at": "2020-02-05T12:33:19Z",
        "merged_at": "2020-02-05T12:33:18Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2020-02-04T13:59:00Z",
        "closed_at": "2020-02-28T10:06:03Z",
        "merged_at": "2020-02-28T10:06:03Z",
        "body": "",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2020-02-04T09:39:16Z",
        "closed_at": "2020-02-06T10:22:21Z",
        "merged_at": "2020-02-06T10:22:21Z",
        "body": "redis-cli doesn't check ssl args, so if we don't pass a value to it, it will cause redis-cli aborted.\r\n\r\n```\r\nroot@DESKTOP-UVUP1SF:/opt/redis# /opt/redis/src/redis-cli --tls  --cert /opt/myredis/redis.crt   --key /opt/myredis/redis.key   --cacert\r\nzmalloc: Out of memory trying to allocate 18446744073709551608 bytes\r\nAborted (core dumped)\r\n```\r\n\r\nI think we should add `lastarg` check for these args and change ssl args's comments in `redis-cli --help`.\r\n\r\nSigned-off-by: lifubang <lifubang@acmcoder.com>",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 28,
        "changed_files": 3,
        "created_at": "2020-02-03T12:02:07Z",
        "closed_at": "2020-02-06T12:10:01Z",
        "merged_at": "2020-02-06T12:10:01Z",
        "body": "1. Call emptyDb even in case of diskless-load: We want modules\r\n   to get the same FLUSHDB event as disk-based replication.\r\n2. Do not fire any module events when flushing the backups array.\r\n3. Delete redundant call to signalFlushedDb (Called from emptyDb).",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 7,
        "changed_files": 5,
        "created_at": "2020-02-03T09:58:47Z",
        "closed_at": "2020-02-06T09:43:06Z",
        "merged_at": "2020-02-06T09:43:05Z",
        "body": "Because \"keymiss\" is \"special\" compared to the rest of\r\nthe notifications (Trying not to break existing apps\r\nusing the 'A' format for notifications)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-02-03T04:01:10Z",
        "closed_at": "2020-09-09T16:24:06Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-02-02T09:26:23Z",
        "closed_at": "2020-02-04T09:20:56Z",
        "merged_at": "2020-02-04T09:20:56Z",
        "body": "The optimal value given 2^64 elements and p=0.25 is:\r\n`log base[1/p] 2^64 = 32`\r\n\r\nUsing `ZSKIPLIST_MAXLEVEL` = 64 instead of 32, we are:\r\n\r\n- allocating 512 bytes per sorted set that are _never_ used. This is on the header node.\r\n- a similar waste on the stack on every function where we have `zskiplistNode *update[ZSKIPLIST_MAXLEVEL]`\r\n- the chance a node would get an unnecessarily high level because we are not capping where we should\r\n\r\nThe probability that a particular element reaches a level at least `k` is `p^k`, and the probability that any of the `n` elements reaches a level at least `k` is `n\u00b7p^k`.\r\n\r\nIt follows that if we use N = 2^64 as an upper bound on the number of elements in the skip list, the probability that **any of these** 2^64 elements reaches `level 33` is 1/4, `level 34` is 1/16, and so forth. \r\n\r\nIt is in these cases, level > 32, that we want to cap the level in `int zslRandomLevel(void)`. As of now, even if we were able to create a zset of 2^64 elements, the probability of the cap `return (level<ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL` kicking in is 1/(2^64)\r\n\r\nTo illustrate, this is an actual level distribution I got on a sorted set of 2^20 = 1,048,576 elements:\r\n\r\nLevel | Elements\r\n-- | --\r\n1 | 786874\r\n2 | 196554\r\n3 | 48861\r\n4 | 12271\r\n5 | 2983\r\n6 | 761\r\n7 | 196\r\n8 | 54\r\n9 | 19\r\n10 | 1\r\n11 | 2\r\n\r\nNote we don't get even close to the 20th level, and these two elements in the 11th level are the ones we should have capped to the 10th level if we were targeting a max capacity of 2^20 elements.\r\n\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-01-30T23:08:51Z",
        "closed_at": "2020-07-21T06:22:50Z",
        "merged_at": null,
        "body": "There are a few cases in RDB load where we return on error but do not free the object we're creating.  ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-01-30T21:20:26Z",
        "closed_at": "2021-07-15T22:42:41Z",
        "merged_at": null,
        "body": "crash occurs when negative value is passed for value size:\r\n\r\n./redis-cli debug populate 1 prefix -1\r\n\r\nmemory leak occurs when a non-numeric arg is passed\r\n\r\nreproduce leak with:\r\n\r\n./redis-cli -r 10000 debug populate 10 prefix badarg > /dev/null",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2020-01-30T13:46:13Z",
        "closed_at": "2020-04-02T14:20:49Z",
        "merged_at": "2020-04-02T14:20:49Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 11,
        "changed_files": 5,
        "created_at": "2020-01-30T12:47:07Z",
        "closed_at": "2020-01-30T16:51:05Z",
        "merged_at": "2020-01-30T16:51:05Z",
        "body": "This bug affected RM_StringToLongDouble and HINCRBYFLOAT.\r\nI added tests for both cases.\r\n\r\nMain changes:\r\n1. Fixed string2ld to fail if string contains \\0 in the middle\r\n2. Use string2ld in getLongDoubleFromObject - No point of\r\n   having duplicated code here\r\n\r\nThe two changes above broke RM_SaveLongDouble/RM_LoadLongDouble\r\nbecause the long double string was saved with length+1 (An innocent\r\nmistake, but it's actually a bug - The length passed to\r\nRM_SaveLongDouble should not include the last \\0).",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-01-29T19:45:29Z",
        "closed_at": "2020-02-03T15:43:57Z",
        "merged_at": "2020-02-03T15:43:57Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-01-23T12:57:35Z",
        "closed_at": "2020-03-02T16:43:37Z",
        "merged_at": "2020-03-02T16:43:37Z",
        "body": "LRU_CYCLE_PERIOD is defined,but not used.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-01-22T08:18:22Z",
        "closed_at": "2021-07-20T12:20:52Z",
        "merged_at": null,
        "body": "On `SET` if we specify the expire time in milliseconds this will never happen due to the overflow guard on `getLongLongFromObjectOrReply->getLongLongFromObject->string2ll`\r\nSee example bellow\r\n```\r\n127.0.0.1:6379> SET a b PX 9223372036854775806\r\nOK\r\n127.0.0.1:6379> SET a b PX 9223372036854775807\r\n(error) ERR value is not an integer or out of range\r\n127.0.0.1:6379> SET a b EX 9223372036854775807\r\nOK\r\n```\r\nThe issue here is that we don't check for overflow when we internally convert from seconds to milliseconds.\r\nfixes #6800 ",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 150,
        "deletions": 47,
        "changed_files": 4,
        "created_at": "2020-01-21T09:53:38Z",
        "closed_at": "2020-04-02T09:17:30Z",
        "merged_at": "2020-04-02T09:17:30Z",
        "body": "By using a \"circular BRPOPLPUSH\"-like scenario it was\r\npossible the get the same client on db->blocking_keys\r\ntwice (See comment in serveClientsBlockedOnKeyByModule)\r\n\r\nOther changes:\r\n1. Added two commands to blockonkeys.c test module (To\r\n   reproduce the case described above)\r\n2. Simplify blockonkeys.c in order to make testing easier",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2020-01-20T13:30:08Z",
        "closed_at": "2020-04-06T09:59:02Z",
        "merged_at": "2020-04-06T09:59:02Z",
        "body": "This patch intends to solve issue #6565 . The proposal was described at https://github.com/antirez/redis/issues/6565#issuecomment-576249167 .",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-01-20T03:52:15Z",
        "closed_at": "2021-06-28T13:10:15Z",
        "merged_at": null,
        "body": "inside func sdsnewlen, check malloc result after memset it seems weird.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 19,
        "changed_files": 1,
        "created_at": "2020-01-17T08:22:51Z",
        "closed_at": "2020-09-09T16:24:09Z",
        "merged_at": null,
        "body": "Just fixing a few typos in the comments :-)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-01-17T04:00:31Z",
        "closed_at": "2020-02-27T16:39:34Z",
        "merged_at": "2020-02-27T16:39:34Z",
        "body": "if aof-rewrite child killed by SIGUSR1, redis-server should NOT flag `sever.aof_lastbgrewrite_status`\r\nas `C_ERR`, but current impl will flag `server.aof_lastbgrewrite_status` as `C_ERR`. \r\n\r\nThis PR fix the impl  aof-child whitelist SIGUSR1 feature.\r\n\r\nsee issue #6696 ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 34762,
        "deletions": 8021,
        "changed_files": 317,
        "created_at": "2020-01-16T22:59:04Z",
        "closed_at": "2021-01-17T16:26:38Z",
        "merged_at": null,
        "body": "This should eliminate dataloss and reduce the \"no master\" outage period of planned failovers.\r\n\r\nShould solve this issue: https://github.com/antirez/redis/pull/6062\r\n\r\nfailoverto is issued to a master redis instance, it then tries to coordinate\r\nfailover with a targeted replica by pausing its clients and waiting until the\r\nreplica has caught up, then it uses a new psync arguement to cause that replica\r\nto become a master\r\n\r\nSample help document for FAILOVERTO command:\r\n\r\nFAILOVERTO host port [TIMEOUT] [FORCE]\r\n Available since 6.0.0\r\n\r\nThis command, that can only be sent to a Redis master node, forces the master to start a manual coordinated failover. The target replica is specified by host and port and must be an already connected replica. Special host and port arguments \"REPLICAOF ANY ONE\" allowing failover to any connected replica. Unless the FORCE flag is set failover will only occur to a replica that has received all replication data from the master before TIMEOUT milliseconds.\r\n\r\nDuring the failover process, for TIMEOUT milliseconds, the master will not process any commands from clients.\r\n\r\nThe command works in the following way:\r\n\r\nThe master replies OK to the caller and stops processing commands from clients for TIMEOUT milliseconds.\r\nThe master waits for the replication offset of the requested replica (or any replica if ANY ONE) to match its offset, to ensure the replica processed all replication data before it continues.\r\nThe master starts a failover by sending a PSYNC request to the target replica with the \"FAILOVER\" argument.\r\nThe target replica receives the PSYNC FAILOVER command and promotes itself to a master.\r\nThe target replica (now master) accepts the PSYNC request from the (old) master since their offsets are equal.\r\nFailover is complete and the old master resumes processing queries from clients.\r\nTIMEOUT option: bounds the amount of time to pause requests\r\nTimeout is specified in milliseconds. There is a default timeout of 5000 milliseconds if not specified by the caller.\r\n\r\nFORCE option: manual failover when data loss is acceptable\r\nThe FORCE option starts a failover after TIMEOUT seconds even if the replica's replication offset is not equal the master's offset at that time. This option cannot be used with the \"ANY ONE\" special arguments, to FORCE failover a target replica must be specified. It is possible for this option to fail to failover if the target replica does not accept the PSYNC FAILOVER command within 5000 milliseconds after TIMEOUT.\r\n\r\nImplementation details and notes\r\nFAILOVERTO does not execute a failover synchronously, it only schedules a manual failover, bypassing the failure detection stage. To check if the failover actually happened, ROLE or other means should be used to check that the roles of the nodes changed some time after the command was sent.\r\n\r\nIf the master starts a failover by sending a PSYNC FAILOVER command to a replica but the PSYNC command is not accepted before the timeout the master will re-promote itself to a master. This avoids the situation where the cluster does not have a master if the target replica does not understand the PSYNC FAILOVER command or its configuration has been changed to disallow this type of failover.\r\n\r\nReturn value\r\nSimple string reply: OK if the command was accepted and a manual failover is going to be attempted. An error if the operation cannot be executed, for example if we are talking with a node which is not a master or if the requested host and port is not valid for failover.",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-01-16T22:37:40Z",
        "closed_at": "2020-02-27T09:12:08Z",
        "merged_at": "2020-02-27T09:12:08Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-01-16T09:28:11Z",
        "closed_at": "2020-07-10T08:41:49Z",
        "merged_at": "2020-07-10T08:41:49Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-01-14T18:54:23Z",
        "closed_at": "2020-03-02T16:50:31Z",
        "merged_at": null,
        "body": "* `POST` and `HOST:` added to @connection\r\n* `TIME` and `LOLWUT` added to @connection in lieu of better a category\r\n* `MEMORY` is @dangerous",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-01-14T08:13:39Z",
        "closed_at": "2020-02-27T16:45:03Z",
        "merged_at": "2020-02-27T16:45:03Z",
        "body": "`strncpy` doesn't null terminate if source string's length is >= destination. So the overflow check has to be *after* the `strncpy` call (and it was null terminating the wrong buffer i.e. `enumerr` instead of `loadbuf`).\r\n\r\nFixes a couple of typoes as well.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-01-14T07:40:39Z",
        "closed_at": "2020-08-11T18:10:58Z",
        "merged_at": "2020-08-11T18:10:58Z",
        "body": "Fixes a couple of typoes in README.md",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-01-13T12:36:20Z",
        "closed_at": "2020-01-15T16:53:51Z",
        "merged_at": "2020-01-15T16:53:51Z",
        "body": "Adding useful details for an error message when client submitting command in context, that only limited scope of commands.\r\n\r\nAn opened issue: https://github.com/antirez/redis/issues/6770\r\n\r\nError message\r\n`ERR only (P)SUBSCRIBE / (P)UNSUBSCRIBE / PING / QUIT allowed in this context`\r\nwill become\r\n`ERR 'get' command submitted, but only (P)SUBSCRIBE / (P)UNSUBSCRIBE / PING / QUIT allowed in this context`",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-01-13T05:55:12Z",
        "closed_at": "2020-09-09T16:23:00Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2020-01-13T05:48:38Z",
        "closed_at": "2020-07-29T06:24:16Z",
        "merged_at": null,
        "body": "Put detailed error message for auth command for the following three error scenarios:\r\n**Disabled User:**\r\nacl setuser foo\r\nOK\r\n127.0.0.1:6379> auth foo 11\r\n(error) DISABLEDUSER The user name specified was disabled\r\n\r\n**Wrong Password:**\r\n127.0.0.1:6379> ACL SETUSER alice on >p1pp0 ~cached:* +get\r\nOK\r\n127.0.0.1:6379> auth alice 333\r\n(error) WRONGPASS The password specified for auth is not correct\r\n\r\n**Wrong User:**\r\n127.0.0.1:6379> auth ee 555\r\n(error) WRONGUSER The user name specified for auth does not exist\r\n\r\npassed make test:\r\n  189 seconds - integration/replication\r\n  223 seconds - integration/replication-psync\r\n\r\n\\o/ All tests passed without errors!\r\n\r\nCleanup: may take some time... OK",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 298,
        "deletions": 59,
        "changed_files": 21,
        "created_at": "2020-01-13T04:03:11Z",
        "closed_at": "2020-01-14T03:33:27Z",
        "merged_at": null,
        "body": " in handleClientsWithPendingWritesUsingThreads() method ,start io threads after setting io_threads_pending count  avoid io thread  empty loop .",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2020-01-10T08:50:00Z",
        "closed_at": "2020-01-10T11:15:32Z",
        "merged_at": null,
        "body": "this compare expression can be take out from loop.  may be it's not need to be take out. some c compiler will optimize it. it's up to you @antirez ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2020-01-08T20:38:26Z",
        "closed_at": "2020-06-03T16:50:23Z",
        "merged_at": null,
        "body": "As the title says, just a small amount of extra context provided to some verbose logging messages.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2020-01-08T02:10:55Z",
        "closed_at": "2020-01-08T12:06:07Z",
        "merged_at": "2020-01-08T12:06:07Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-01-07T21:05:12Z",
        "closed_at": "2020-01-10T11:18:20Z",
        "merged_at": "2020-01-10T11:18:20Z",
        "body": "Redis returns all keys if I use `KEYS` to pattern-match using a pattern that starts with *, followed by a null character.\r\n\r\nSteps to reproduce:\r\n\r\n1. On an empty database, do:\r\nSET notZeroTerm 1\r\nSET \"zeroTerm\\x00\" 1\r\nSET \"zeroMid\\x00End\" 1\r\n\r\n2. Do `KEYS \"*\\x00\"`\r\n**Expected:** \r\n\"zeroTerm\\x00\"\r\n**Actual:**\r\n\"zeroMid\\x00End\"\r\n\"notZeroTerm\"\r\n\"zeroTerm\\x00\"\r\n\r\n3. Do `KEYS \"*\\x00*\"` to match keys that include a null character\r\n**Expected:**\r\n\"zeroMid\\x00End\"\r\n\"zeroTerm\\x00\"\r\n**Actual:**\r\n\"zeroMid\\x00End\"\r\n\"notZeroTerm\"\r\n\"zeroTerm\\x00\"\r\n\r\nThe fix is a one-line correction in `db.c`\r\n\r\nThis bug is not present when using `SCAN`",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2020-01-07T13:00:25Z",
        "closed_at": "2020-01-08T02:16:25Z",
        "merged_at": null,
        "body": "Reduce the number of select rounds.\r\nOn success select()  return the number of file descriptors contained in the three returned descriptor sets ( readfds,  writefds, exceptfds) which may be zero if the timeout expires before anything interesting happens.  On error, -1 is returned, and errno is set to indicate the error; the file descriptor sets are unmodified, and timeout becomes undefined.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-01-07T02:56:36Z",
        "closed_at": "2020-01-08T11:30:49Z",
        "merged_at": "2020-01-08T11:30:49Z",
        "body": "When sentinel performs failover timeout\uff0c`promoted_slave` have a chance to be its own replica.\r\n\r\n**Sentinel_Log:**\r\n```c\r\n34109:X 06 Jan 20:56:19.716 # +sdown master simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:19.771 # +odown master simulation-test-4 0.0.0.225 6404 #quorum 2/2\r\n34109:X 06 Jan 20:56:19.771 # +new-epoch 41\r\n34109:X 06 Jan 20:56:19.771 # +try-failover master simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:19.772 # +vote-for-leader aed5fbc7cc7b8c80b31906e5c032bf0f7d50be9e 41\r\n34109:X 06 Jan 20:56:19.774 # f3e878776ad53ed8fc53aa61c0839878912b09a9 voted for aed5fbc7cc7b8c80b31906e5c032bf0f7d50be9e 41\r\n34109:X 06 Jan 20:56:19.774 # 3d8a949910a43d0d01ae4a7624b44b1f7f030721 voted for aed5fbc7cc7b8c80b31906e5c032bf0f7d50be9e 41\r\n34109:X 06 Jan 20:56:19.848 # +elected-leader master simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:19.849 # +failover-state-select-slave master simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:19.920 # +selected-slave slave 0.0.0.227:6404 0.0.0.227 6404 @ simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:19.920 * +failover-state-send-slaveof-noone slave 0.0.0.227:6404 0.0.0.227 6404 @ simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:19.976 * +failover-state-wait-promotion slave 0.0.0.227:6404 0.0.0.227 6404 @ simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:20.645 # +promoted-slave slave 0.0.0.227:6404 0.0.0.227 6404 @ simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:20.645 # +failover-state-reconf-slaves master simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:20.726 * +slave-reconf-sent slave 0.0.0.223:6404 0.0.0.223 6404 @ simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:20.885 # -odown master simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:56:21.670 * +slave-reconf-inprog slave 0.0.0.223:6404 0.0.0.223 6404 @ simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:57:45.653 # +failover-end-for-timeout master simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:57:45.653 # +failover-end master simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:57:45.653 * +slave-reconf-sent-be slave 0.0.0.223:6404 0.0.0.223 6404 @ simulation-test-4 0.0.0.225 6404\r\n/* here */\r\n34109:X 06 Jan 20:57:45.653 * +slave-reconf-sent-be slave 0.0.0.227:6404 0.0.0.227 6404 @ simulation-test-4 0.0.0.225 6404\r\n34109:X 06 Jan 20:57:45.738 # +switch-master simulation-test-4 0.0.0.225 6404 0.0.0.227 6404\r\n```\r\n\r\n**Redis_Log:**\r\n```c\r\n19096:S 06 Jan 20:56:18.899 * MASTER <-> SLAVE sync started\r\n19096:S 06 Jan 20:56:18.900 # Error condition on socket for SYNC: Connection refused\r\n19096:S 06 Jan 20:56:19.902 * Connecting to MASTER 0.0.0.225:6404\r\n19096:S 06 Jan 20:56:19.902 * MASTER <-> SLAVE sync started\r\n19096:S 06 Jan 20:56:19.902 # Error condition on socket for SYNC: Connection refused\r\n19096:M 06 Jan 20:56:19.976 * Discarding previously cached master state.\r\n19096:M 06 Jan 20:56:19.976 * MASTER MODE enabled (user request from 'id=630 addr=0.0.0.227:55179 fd=12 name=sentinel-aed5fbc7-cmd age=5107 idle=0 flags=x db=0 sub=0 psub=0 multi=3 qbuf=0 qbuf-free=32768 obl=36 oll=0 omem=0 events=r cmd=exec')\r\n19096:M 06 Jan 20:56:19.977 # CONFIG REWRITE executed with success.\r\n19096:M 06 Jan 20:56:21.234 * Slave 0.0.0.223:6404 asks for synchronization\r\n19096:M 06 Jan 20:56:21.234 * Full resync requested by slave 0.0.0.223:6404\r\n19096:M 06 Jan 20:56:21.234 * Starting BGSAVE for SYNC with target: disk\r\n19096:M 06 Jan 20:56:21.446 * Background saving started by pid 24064\r\n24064:C 06 Jan 20:56:48.833 * DB saved on disk\r\n24064:C 06 Jan 20:56:49.022 * RDB: 6 MB of memory used by copy-on-write\r\n19096:M 06 Jan 20:56:49.382 * Background saving terminated with success\r\n19096:M 06 Jan 20:56:50.127 * Synchronization with slave 0.0.0.223:6404 succeeded\r\n19096:S 06 Jan 20:57:45.654 # Connection with slave 0.0.0.223:6404 lost.\r\n/* here */\r\n19096:S 06 Jan 20:57:45.654 * SLAVE OF 0.0.0.227:6404 enabled (user request from 'id=630 addr=0.0.0.227:55179 fd=12 name=sentinel-aed5fbc7-cmd age=5193 idle=0 flags=x db=0 sub=0 psub=0 multi=3 qbuf=0 qbuf-free=32768 obl=36 oll=0 omem=0 events=r cmd=exec')\r\n19096:S 06 Jan 20:57:45.655 # CONFIG REWRITE executed with success.\r\n19096:S 06 Jan 20:57:46.143 * Connecting to MASTER 0.0.0.227:6404\r\n19096:S 06 Jan 20:57:46.143 * MASTER <-> SLAVE sync started\r\n19096:S 06 Jan 20:57:46.143 * Non blocking connect for SYNC fired the event.\r\n19096:S 06 Jan 20:57:46.143 * Master replied to PING, replication can continue...\r\n19096:S 06 Jan 20:57:46.143 * Partial resynchronization not possible (no cached master)\r\n19096:S 06 Jan 20:57:46.143 * Master does not support PSYNC or is in error state (reply: -ERR Can't SYNC while not connected with my master)\r\n19096:S 06 Jan 20:57:46.143 * Retrying with SYNC...\r\n19096:S 06 Jan 20:57:46.143 # MASTER aborted replication with an error: ERR Can't SYNC while not connected with my master\r\n19096:S 06 Jan 20:57:47.145 * Connecting to MASTER 0.0.0.227:6404\r\n19096:S 06 Jan 20:57:47.145 * MASTER <-> SLAVE sync started\r\n19096:S 06 Jan 20:57:47.145 * Non blocking connect for SYNC fired the event.\r\n19096:S 06 Jan 20:57:47.145 * Master replied to PING, replication can continue...\r\n19096:S 06 Jan 20:57:47.145 * Partial resynchronization not possible (no cached master)\r\n19096:S 06 Jan 20:57:47.145 * Master does not support PSYNC or is in error state (reply: -ERR Can't SYNC while not connected with my master)\r\n19096:S 06 Jan 20:57:47.145 * Retrying with SYNC...\r\n19096:S 06 Jan 20:57:47.145 # MASTER aborted replication with an error: ERR Can't SYNC while not connected with my master\r\n```\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2020-01-06T15:00:05Z",
        "closed_at": "2021-06-20T08:34:04Z",
        "merged_at": "2021-06-20T08:34:04Z",
        "body": "chdir to \"dir\" before fopen logfile in loadServerConfigFromString(), this avoid to create empty log file in the dir which redis-server startups.\r\nthis also avoids creating lots of files in case someone specified several `logfile` settings which override each other.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2020-01-06T12:11:51Z",
        "closed_at": "2020-01-13T12:23:50Z",
        "merged_at": "2020-01-13T12:23:50Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2020-01-04T16:37:33Z",
        "closed_at": "2020-01-08T11:50:14Z",
        "merged_at": "2020-01-08T11:50:14Z",
        "body": "Instead of 512, use 128 as defined in networking.c",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2020-01-04T16:31:38Z",
        "closed_at": "2021-08-02T05:05:03Z",
        "merged_at": null,
        "body": "",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2020-01-04T03:41:02Z",
        "closed_at": "2020-09-09T16:29:17Z",
        "merged_at": null,
        "body": "the comment of function listCreate in adlist.c mention that use function AlFreeList to free the list, but function AlFreeList do not exist in the whole repo.\r\nso maybe these comment should be delete.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2020-01-02T15:50:47Z",
        "closed_at": "2020-12-13T12:40:55Z",
        "merged_at": "2020-12-13T12:40:55Z",
        "body": "",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2020-01-01T15:36:43Z",
        "closed_at": "2020-01-09T16:10:02Z",
        "merged_at": "2020-01-09T16:10:02Z",
        "body": "This change has the compiler generate dependency files automatically which allows you to modify a header and not do a make clean.  This makes changing header files during development a bit easier, I frequently forget to do a make clean and end up with borked builds.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2020-01-01T07:18:37Z",
        "closed_at": "2020-02-29T10:53:43Z",
        "merged_at": null,
        "body": "I also submitted in **hiredis**  https://github.com/redis/hiredis/pull/746",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 9,
        "changed_files": 3,
        "created_at": "2019-12-31T10:45:35Z",
        "closed_at": "2020-02-19T17:06:20Z",
        "merged_at": null,
        "body": "According to the command results such as INFO, MEMORY, the term \"rss overhead\" is used. But the term \"rss extra\" is used in the code. It may be a bit confusing.\r\nSo I unified how to represent between rss overhead and rss extra.\r\n\r\n\r\n```\r\n-    float rss_extra;\r\n-    size_t rss_extra_bytes;\r\n+    float rss_overhead;\r\n+    size_t rss_overhead_bytes;\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-31T10:25:06Z",
        "closed_at": "2020-01-03T23:43:46Z",
        "merged_at": null,
        "body": "db.c seems not to use atomicvar.h. So I removed including atomicvar.h.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-12-31T10:20:25Z",
        "closed_at": "2020-01-09T16:05:55Z",
        "merged_at": "2020-01-09T16:05:55Z",
        "body": "Funcion adjustOpenFilesLimit() has an implicit parameter, which is server.maxclients.\r\nThis function aims to ajust maximum file descriptor number according to server.maxclients\r\nby best effort, which is \"bestlimit\" could be lower than \"maxfiles\" but greater than \"oldlimit\".\r\nWhen we try to increase \"maxclients\" using CONFIG SET command, we could increase maximum\r\nfile descriptor number to a bigger value without calling aeResizeSetSize the same time.\r\nWhen later more and more clients connect to server, the allocated fd could be bigger and bigger,\r\nand eventually exceeds events size of aeEventLoop.events. When new nodes joins the cluster,\r\nnew link is created, together with new fd, but when calling aeCreateFileEvent, we did not\r\ncheck the return value. In this case, we have a non-null \"link\" but the associated fd is not\r\nregistered.\r\n\r\nSo when we dynamically set \"maxclients\" we could reach an inconsistency between maximum file\r\ndescriptor number of the process and server.maxclients. And later could cause cluster link and link\r\nfd inconsistency.\r\n\r\nWhile setting \"maxclients\" dynamically, we consider it as failed when resulting \"maxclients\" is not\r\nthe same as expected. We try to restore back the maximum file descriptor number when we failed to set\r\n\"maxclients\" to the specified value, so that server.maxclients could act as a guard as before.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-12-31T09:37:39Z",
        "closed_at": "2020-02-19T17:09:06Z",
        "merged_at": null,
        "body": "Add description geodist,geohash,geopos other than geoadd,georadius,georadiusbymember to geo.c\r\n\r\n```\r\n  *   - geoadd - add coordinates for value to geoset\r\n  *   - georadius - search radius by coordinates in geoset\r\n  *   - georadiusbymember - search radius based on geoset member position\r\n+ *   - geodist - calculate distance between two geoset members\r\n+ *   - geohash - show geoset member position with geohash format\r\n+ *   - geopos - show geoset member position with coordinates\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-12-31T08:47:56Z",
        "closed_at": "2020-02-19T17:11:04Z",
        "merged_at": null,
        "body": "fss should be rss",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-29T13:07:17Z",
        "closed_at": "2020-07-21T03:46:56Z",
        "merged_at": null,
        "body": "max length should be 2^32-1, not  32^2-1",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-27T11:49:44Z",
        "closed_at": "2020-09-09T16:24:13Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2019-12-26T13:10:35Z",
        "closed_at": "2019-12-29T13:51:41Z",
        "merged_at": "2019-12-29T13:51:41Z",
        "body": "- make lua-replicate-commands mutable (it never was, but i don't see why)\r\n- make tcp-backlog immutable (fix a recent refactory mistake)\r\n- increase the max limit of a few configs to match what they were before\r\nthe recent refactory",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2019-12-26T10:08:50Z",
        "closed_at": "2019-12-29T13:53:07Z",
        "merged_at": "2019-12-29T13:53:07Z",
        "body": "This commit solves several edge cases that are related to\r\nexhausting the streamID limits: We should correctly calculate\r\nthe succeeding streamID instead of blindly incrementing 'seq'\r\nThis affects both XREAD and XADD.\r\n\r\nOther (unrelated) changes:\r\nReply with a better error message when trying to add an entry\r\nto a stream that has exhausted last_id",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 10,
        "changed_files": 3,
        "created_at": "2019-12-26T06:54:23Z",
        "closed_at": "2020-01-09T16:32:15Z",
        "merged_at": "2020-01-09T16:32:15Z",
        "body": "This commit solves the following bug:\r\n```\r\n127.0.0.1:6379> XGROUP CREATE x grp $ MKSTREAM\r\nOK\r\n127.0.0.1:6379> XADD x 666 f v\r\n\"666-0\"\r\n127.0.0.1:6379> XREADGROUP GROUP grp Alice BLOCK 0 STREAMS x >\r\n1) 1) \"x\"\r\n   2) 1) 1) \"666-0\"\r\n         2) 1) \"f\"\r\n            2) \"v\"\r\n127.0.0.1:6379> XADD x 667 f v\r\n\"667-0\"\r\n127.0.0.1:6379> XDEL x 667\r\n(integer) 1\r\n127.0.0.1:6379> XREADGROUP GROUP grp Alice BLOCK 0 STREAMS x >\r\n1) 1) \"x\"\r\n   2) (empty array)\r\n```\r\nThe root cause is that we use s->last_id in streamCompareID\r\nwhile we should use the last *valid* ID",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-26T03:55:47Z",
        "closed_at": "2020-08-04T15:00:59Z",
        "merged_at": "2020-08-04T15:00:58Z",
        "body": "It's trivial, but may help if you gonna use it to write cluster related tcl tests.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-26T01:20:15Z",
        "closed_at": "2020-10-18T21:27:27Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 51,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2019-12-24T11:49:38Z",
        "closed_at": "2020-01-29T11:06:14Z",
        "merged_at": "2020-01-29T11:06:14Z",
        "body": "If a blocked module client times-out (or disconnects, unblocked\r\nby CLIENT command, etc.) we need to call moduleUnblockClient\r\nin order to free memory allocated by the module sub-system\r\nand blocked-client private data\r\n\r\nOther changes:\r\nMade blockedonkeys.tcl tests a bit more aggressive in order\r\nto smoke-out potential memory leaks",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-12-23T17:52:02Z",
        "closed_at": "2020-01-08T12:09:49Z",
        "merged_at": "2020-01-08T12:09:49Z",
        "body": "This exposes to modules the fact that the current client has watched keys which are dirty, so a `MULTI/EXEC` sequence is going to fail.\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-12-23T15:48:01Z",
        "closed_at": "2020-09-24T09:45:31Z",
        "merged_at": "2020-09-24T09:45:31Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2019-12-23T08:17:49Z",
        "closed_at": "2020-04-02T17:00:20Z",
        "merged_at": "2020-04-02T17:00:20Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-21T19:47:09Z",
        "closed_at": "2020-02-12T13:04:54Z",
        "merged_at": "2020-02-12T13:04:54Z",
        "body": "This helps avoiding multiple definition of this variable, its also\r\ndefined globally in sds.c\r\n\r\n\r\nSigned-off-by: Khem Raj <raj.khem@gmail.com>",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-21T13:33:55Z",
        "closed_at": "2020-02-27T16:58:50Z",
        "merged_at": "2020-02-27T16:58:50Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-12-20T10:23:43Z",
        "closed_at": "2020-03-06T12:15:11Z",
        "merged_at": "2020-03-06T12:15:11Z",
        "body": "An oversight on my part leads to a situation where a ``redis-server`` replica won't correctly tell its supervising systemd instance that it's in ready state when *Partial Resynchronsiation* finishes successfully. This is a rather common scenario, but my initial suite of experiments/tests apparently failed to cover it. I apologize for any inconvenice this may have caused.\r\n\r\nI'd be very thankful for another pair of eyes going over the possible states in ``replication.c:syncWithMaster()`` to maybe spot similar instances of this kind of problem - I'm not certain that the fallback path that's using ''SYNC'' is correctly covered yet, for example.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-19T14:50:19Z",
        "closed_at": "2020-09-01T20:50:48Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-19T00:12:33Z",
        "closed_at": "2019-12-20T19:17:24Z",
        "merged_at": null,
        "body": "https://github.com/antirez/redis/issues/6229",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 22,
        "changed_files": 9,
        "created_at": "2019-12-18T07:48:05Z",
        "closed_at": "2019-12-18T10:52:09Z",
        "merged_at": "2019-12-18T10:52:09Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-18T07:36:03Z",
        "closed_at": "2020-09-09T16:24:16Z",
        "merged_at": null,
        "body": "returend->returned in sort.c",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-12-18T06:59:26Z",
        "closed_at": "2020-02-27T17:09:49Z",
        "merged_at": "2020-02-27T17:09:49Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-16T14:29:56Z",
        "closed_at": "2020-09-09T16:24:19Z",
        "merged_at": null,
        "body": "\"elmenent\" ->  \"element\"",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-16T07:52:58Z",
        "closed_at": "2020-09-09T16:24:22Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2019-12-16T01:39:54Z",
        "closed_at": "2020-09-09T16:24:25Z",
        "merged_at": null,
        "body": "\"Pricision\" should be \"Precision\".",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-12-13T11:37:59Z",
        "closed_at": "2020-07-10T13:40:36Z",
        "merged_at": null,
        "body": "Using the `-a` option on **redis-benchmark** when it's launched in cluster mode (`--cluster` option), should allow redis-benchmark to authenticate to Redis. Anyway, the authentication itself is currently performed only inside benchmark's queries.\r\nSo, with password-protected cluster, fetching the initial cluster configuration would fail, since the `-a` option is actually ignored during cluster configuration fetching.\r\nThis PR will let  **redis-benchmark** to use the password specified in the `-a` option also during cluster configuration fetching, letting it work also with password-protected clusters.\r\n\r\nSee https://github.com/antirez/redis/pull/5889#issuecomment-565272574",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-12-12T16:01:27Z",
        "closed_at": "2019-12-16T10:18:09Z",
        "merged_at": "2019-12-16T10:18:09Z",
        "body": "since the refactory of config.c, it was initialized from config_hz in initServer\r\nbut apparently that's too late since the config file loading creates objects\r\nwhich call LRU_CLOCK",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-12-11T18:40:19Z",
        "closed_at": "2020-07-30T00:54:38Z",
        "merged_at": "2020-07-30T00:54:38Z",
        "body": "Use higher-level API to funnel all generic propagation through single function call.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-10T09:18:01Z",
        "closed_at": "2020-04-02T14:32:20Z",
        "merged_at": "2020-04-02T14:32:20Z",
        "body": "the `in_dict_field` was missing, this doesn't really have an effect, but sometimes compilers warn about it.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-09T08:06:21Z",
        "closed_at": "2019-12-09T09:41:15Z",
        "merged_at": "2019-12-09T09:41:15Z",
        "body": "the code in:\r\n        c->flags &= ~(CLIENT_TRACKING|CLIENT_TRACKING_BROKEN_REDIR);\r\nwill do sign extension and turn on all the high 31 bits\r\nno damage so far since we don't have any yet",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 130,
        "deletions": 19,
        "changed_files": 5,
        "created_at": "2019-12-07T18:22:23Z",
        "closed_at": "2020-12-21T00:55:20Z",
        "merged_at": null,
        "body": "this PR pushes forward the observability on overall error statistics and command statistics within redis-server. \r\nIt extends `INFO COMMANDSTATS` to have failed-calls in addition to calls ( so we can keep track of errors that happen from the command itself, broken by command ) and adds a new section to `INFO`, named `ERRORSTATS` that enables keeping track of all errors that happened even outside `processCommand`.\r\nThis PR also fixes `RM_ReplyWithError` so that it can be correctly identified as an error reply. \r\n\r\n## info commandstats \r\nRight now all errors are aggregate as [simple error](https://gist.github.com/antirez/2bc68a9e9e45395e297d288453d5d54c#simple-types)\r\n```\r\n$ redis-cli \r\n127.0.0.1:6379> set a\r\n(error) ERR wrong number of arguments for 'set' command\r\n127.0.0.1:6379> LINDEX a\r\n(error) ERR wrong number of arguments for 'lindex' command\r\n127.0.0.1:6379> LINDEX x\r\n(error) ERR wrong number of arguments for 'lindex' command\r\n127.0.0.1:6379> LSET a\r\n(error) ERR wrong number of arguments for 'lset' command\r\n127.0.0.1:6379> info commandstats\r\n# Commandstats\r\ncmdstat_command:calls=1,failed_calls=0,usec=1532,usec_per_call=1532.00\r\ncmdstat_lset:calls=0,failed_calls=1,usec=0,usec_per_call=0.00\r\ncmdstat_lindex:calls=0,failed_calls=2,usec=0,usec_per_call=0.00\r\ncmdstat_set:calls=0,failed_calls=1,usec=0,usec_per_call=0.00\r\n```\r\n## info errorstats \r\nRight now all errors are aggregate as [simple error](https://gist.github.com/antirez/2bc68a9e9e45395e297d288453d5d54c#simple-types). In the future we can extend to different types of errors ( but it will imply changing `addReplyErrorLength` )\r\n```\r\n$ redis-cli \r\n127.0.0.1:6379> set a\r\n(error) ERR wrong number of arguments for 'set' command\r\n127.0.0.1:6379> LINDEX a\r\n(error) ERR wrong number of arguments for 'lindex' command\r\n127.0.0.1:6379> LINDEX x\r\n(error) ERR wrong number of arguments for 'lindex' command\r\n127.0.0.1:6379> LSET a\r\n(error) ERR wrong number of arguments for 'lset' command\r\n127.0.0.1:6379> info errorstats\r\n# Errorstats\r\nerrorstat_all_errors:calls=4\r\n```\r\n\r\n\r\n\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-06T19:58:54Z",
        "closed_at": "2019-12-16T10:25:54Z",
        "merged_at": "2019-12-16T10:25:54Z",
        "body": "Noticed this when reading the redis config.\r\n\r\nThe doc lists five behaviors, which at one point in time was correct. 3 years ago, additional behaviors were added.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-05T13:47:44Z",
        "closed_at": "2019-12-05T15:51:43Z",
        "merged_at": "2019-12-05T15:51:43Z",
        "body": "The memcpy from the key to the id relies on the fact that this key\r\n*should* be 8 bytes long as it was entered as such a few lines up the\r\ncode.\r\n\r\nBUT if someone will change the code to the point this is no longer true,\r\ncurrent code can trash the stack which makes debugging very hard\r\nwhile this fix will result in some garbage id, or even page fault.\r\nBoth are preferable to stack mangaling.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2019-12-04T15:32:53Z",
        "closed_at": "2020-03-26T10:12:45Z",
        "merged_at": "2020-03-26T10:12:45Z",
        "body": "the AOF will be loaded successfully, but the stream will be missing,\r\ni.e inconsistencies with the original db.\r\n\r\nadd a test to reproduce.",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 18118,
        "deletions": 38,
        "changed_files": 76,
        "created_at": "2019-12-04T08:38:28Z",
        "closed_at": "2020-08-11T12:16:28Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-12-03T05:04:17Z",
        "closed_at": "2020-08-11T13:03:10Z",
        "merged_at": null,
        "body": "Support HDEL command in redis-benchmark",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-12-02T05:36:00Z",
        "closed_at": "2019-12-02T07:51:05Z",
        "merged_at": "2019-12-02T07:51:05Z",
        "body": "Added the missed macro definition in slowlog.h to make it consistent with other header files. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-12-01T14:11:01Z",
        "closed_at": "2019-12-02T07:50:21Z",
        "merged_at": "2019-12-02T07:50:21Z",
        "body": "it seems that commit b087dd1db60ed23d9e59304deb0b1599437f6e23 accidentially changed\r\ngen_write_load to not use deferred client, which causes them to be slower and not\r\ngenerate high load which they should, making some tests less effecitive",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 614,
        "deletions": 753,
        "changed_files": 5,
        "created_at": "2019-11-26T15:00:22Z",
        "closed_at": "2019-12-02T07:59:37Z",
        "merged_at": "2019-12-02T07:59:37Z",
        "body": "- add capability for each config to have a callback to check if value is valid and return error string\r\n  will enable converting many of the remaining custom configs into generic ones (reducing the x4 repetition for set,get,config,rewrite)\r\n- add capability for each config to  to run some update code after config is changed (only for CONFIG SET)\r\n  will also enable converting many of the remaining custom configs into generic ones\r\n- add capability to move default values from server.h and server.c to config.c\r\n  will reduce many excess lines in server.h and server.c (plus, no need to rebuild the entire code base when a default change 8-))\r\n\r\nother behavior changes:\r\n- fix bug in bool config get (always returning 'yes')\r\n- fix a bug in modifying jemalloc-bg-thread at runtime (didn't call set_jemalloc_bg_thread, due to bad merge conflict resolution (my fault))\r\n- side effect when a failed attempt to enable activedefrag at runtime, we now respond with -ERR and not with -DISABLED",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2019-11-26T02:51:03Z",
        "closed_at": "2020-04-29T09:17:04Z",
        "merged_at": null,
        "body": "If client gets blocked again in `processUnblockedClients`, redis will not send\r\n`REPLCONF GETACK *` to slaves untill next eventloop, so the client will be\r\nblocked for 100ms by default(10hz) if no other file event fired.\r\n\r\nmove server.get_ack_from_slaves sinppet after `processUnblockedClients`, so\r\nthat both the first WAIT command that puts client in blocked context and the\r\nfollowing WAIT command processed in processUnblockedClients would trigger\r\nredis-sever to send `REPLCONF GETACK *`, so that the eventloop would get\r\n`REPLCONG ACK <reploffset>` from slaves and unblocked ASAP.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2019-11-25T07:50:42Z",
        "closed_at": "2019-12-16T10:36:05Z",
        "merged_at": null,
        "body": "There's an anomalous behavior with SET command.\r\n`set foo bar ex 10 ex 100`\r\n`set foo bar px 10000 px 100000`\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-11-24T14:33:39Z",
        "closed_at": "2019-12-05T13:44:03Z",
        "merged_at": null,
        "body": "While just replacing ri.key_len with sizeof(id) would have solved the issue. Refactoring and tieing the size to the actual key length as appears in the client struck is better/safer.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 444,
        "deletions": 821,
        "changed_files": 5,
        "created_at": "2019-11-23T20:53:31Z",
        "closed_at": "2020-08-08T12:25:51Z",
        "merged_at": null,
        "body": "* Allow including redismodule.h in several source files, by declaring API function vars weak [i.e., __attribute__ ((weak))].\r\n* Avoid double-mentions of APIs by using X-macro acrobatics\r\n* Allow for API extensions via REDISMODULE_XAPI_EXTENSIONS macro\r\n* Removed explicit REGISTER_API-s from module.c\r\n* Support inclusion from C++ sources\r\n\r\nSee also #6330",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 39,
        "deletions": 25,
        "changed_files": 6,
        "created_at": "2019-11-22T08:54:06Z",
        "closed_at": "2019-12-19T08:24:53Z",
        "merged_at": "2019-12-19T08:24:52Z",
        "body": "Hi @antirez @madolson , now I open this PR to discuss the codes I mentioned in  #6152 , except the original problem, here are other three changes:\r\n\r\n1. propagate `EXEC` directly in lua script\r\n\r\n    I think we don't need to use also propagate in lua, cause `MULTI` is already propagated.\r\n\r\n2. flag module client as `CLIENT_MULTI` if needed\r\n\r\n    In case of nested MULTI/EXEC, lua script has the same problem, see details in #5780\r\n\r\n3. propagte `BRPOPLPUSH` as `RPOPLPUSH` when unblock\r\n\r\n    After the expire problem is fixed, I think it's OK to do that now\r\n\r\nBTW, if this PR is OK, before merge it we should merge #5780 at first to avoid nested MULIT/EXEC when `SPOP` with count is called in lua script.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-11-21T03:43:39Z",
        "closed_at": "2019-11-25T09:04:29Z",
        "merged_at": "2019-11-25T09:04:29Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 524,
        "deletions": 648,
        "changed_files": 1,
        "created_at": "2019-11-20T17:35:55Z",
        "closed_at": "2019-11-22T16:42:43Z",
        "merged_at": "2019-11-22T16:42:43Z",
        "body": "This is a follow to a previous PR that made the boolean types more generic. \r\n\r\nThe current approach will give you warnings if you use the wrong types, which I see as a big benefit.\r\n\r\nI think you could make the case that configs really only need two functions, get and set, since load is just a fancy get and rewrite is just a load, get, set, but this does allow them to be more generic. This just leaves it a little more generic though, and isn't a huge cost. \r\n\r\nThe result of this PR, is that to add a config you will just need to add it to server.h, initialize it in server.c, and then add it to block config at the bottom for using regular integers, enums, strings, or bools. \r\n\r\nThe last thing to call out is a couple of weird values which may seem like they can be refactored, but don't naturally fit into the system:\r\nrepl-backlog-ttl: is a time_t type\r\nclient-query-buffer-limit: Is an atomic type\r\nsyslog-enabled: Doesn't have a get",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 660,
        "deletions": 542,
        "changed_files": 11,
        "created_at": "2019-11-20T12:20:02Z",
        "closed_at": "2019-11-23T20:46:28Z",
        "merged_at": null,
        "body": "* Introduced REDISMODULE_INIT_SYMBOLS macro, to allow including redismodule.h in several source files (this is a breaking change)\r\n* Avoid double-mentions of APIs by using X-macro\r\n* Support inclusion from C++ sources\r\n\r\nSee also #6330",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-11-20T02:46:17Z",
        "closed_at": "2019-11-20T09:06:34Z",
        "merged_at": "2019-11-20T09:06:34Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-11-19T19:25:52Z",
        "closed_at": "2019-11-19T22:01:58Z",
        "merged_at": null,
        "body": "Wanted to add the `MKSTREAM` option to the `XGROUP CREATE` subcommand. I added the option to the docs previously in https://github.com/antirez/redis-doc/pull/1211\r\n\r\nThis PR adds it officially to `redis-cli` help syntax.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-11-19T16:14:34Z",
        "closed_at": "2020-09-09T16:28:57Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2019-11-19T13:02:28Z",
        "closed_at": "2019-11-25T16:54:22Z",
        "merged_at": "2019-11-25T16:54:22Z",
        "body": "there were two lssues, one is taht BGREWRITEAOF failed since the initial one was still in progress\r\nthe solution for this one is to enable appendonly from the server startup so there's no initial aofrw.\r\n\r\nthe other problem was 0 loading progress events, theory is that on some\r\nplatforms a sleep of 1 will cause a much greater delay due to the context\r\nswitch, but on other platform it doesn't. in theory a sleep of 100 micro\r\nfor 1k keys whould take 100ms, and with hz of 500 we should be gettering\r\n50 events (one every 2ms). in practise it doesn't work like that, so trying\r\nto find a sleep that would be long enough but still not cause the test to take\r\ntoo long.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2019-11-19T06:36:13Z",
        "closed_at": "2019-12-01T23:04:11Z",
        "merged_at": null,
        "body": "Hi @antirez , could you take a look? Thank you! ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-11-19T05:05:54Z",
        "closed_at": "2019-11-25T09:02:44Z",
        "merged_at": "2019-11-25T09:02:43Z",
        "body": "https://github.com/antirez/redis/issues/6594",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 27,
        "changed_files": 1,
        "created_at": "2019-11-18T12:47:57Z",
        "closed_at": "2019-11-18T17:12:32Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2019-11-17T13:50:56Z",
        "closed_at": "2021-06-08T14:20:21Z",
        "merged_at": null,
        "body": "Remove wasteful useless code",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2019-11-15T10:40:16Z",
        "closed_at": "2020-12-08T10:39:39Z",
        "merged_at": null,
        "body": "fix spt_copyenv spt_clearenv memory leak",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 397,
        "deletions": 9,
        "changed_files": 7,
        "created_at": "2019-11-15T06:46:42Z",
        "closed_at": "2021-10-21T06:37:46Z",
        "merged_at": null,
        "body": "Hello. I do research on Redis, and I recently developed a new persistence method that improves AOF-PREAMBLE.\r\nThe proposed method was designed to reduce memory usage and improve the performance of Redis. This persistence method is called LESS, uses a mixture of AOF and RDB. It seems to similar to AOF-PREAMBLE but detailed behavior is different from AOF-PREAMBLE.\r\n\r\nI worked on the Redis 4 version of the module. So I asked for a PR from 4.0. \r\n\r\nAs the paper on the LESS module has been published, I will give you a link to the paper for your understanding. Please review it.\r\n\r\nhttps://ieeexplore.ieee.org/document/8679377\r\n\r\n*I'm not used to open-source activities. Please tell me if the method of PR is wrong.",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-11-15T00:15:50Z",
        "closed_at": "2020-09-09T16:24:29Z",
        "merged_at": null,
        "body": "A very small fix to documentation.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-11-14T10:37:57Z",
        "closed_at": "2021-05-05T15:46:29Z",
        "merged_at": null,
        "body": "From my perspective, I feel DECR, INCRBY, DECRBY commands are very much used by users in redis community. So adding support for those commands in redis benchmark.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-11-14T07:48:54Z",
        "closed_at": "2019-11-19T09:54:00Z",
        "merged_at": "2019-11-19T09:54:00Z",
        "body": "trimming talk about RESP protocol from API docs (should be independent to that anyway)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-11-14T07:09:38Z",
        "closed_at": "2019-11-19T10:02:59Z",
        "merged_at": "2019-11-19T10:02:59Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-11-13T11:17:52Z",
        "closed_at": "2019-11-19T09:50:45Z",
        "merged_at": "2019-11-19T09:50:45Z",
        "body": "Calling XADD with 0-0 or 0 would result in creating an\r\nempty key and storing it in the database.\r\nEven worse, because XADD will reply with error the action\r\nwill not be replicated, creating a master-replica\r\ninconsistency",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-11-13T09:19:04Z",
        "closed_at": "2020-08-11T12:24:27Z",
        "merged_at": null,
        "body": "fix coredump in rpoplpushCommand when src is the same with dest and t\u2026",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-11-13T02:15:01Z",
        "closed_at": "2019-11-19T10:42:46Z",
        "merged_at": "2019-11-19T10:42:46Z",
        "body": "Update listGetFree keep format consistent",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-11-12T13:10:24Z",
        "closed_at": "2020-08-18T17:22:44Z",
        "merged_at": null,
        "body": "update __sync atomicSet",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-11-11T13:18:05Z",
        "closed_at": "2020-09-09T16:24:32Z",
        "merged_at": null,
        "body": "fix comment typo in redis-cli.c\r\n(`predecence` -> `precedence`)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-11-11T13:06:18Z",
        "closed_at": "2019-11-25T08:45:30Z",
        "merged_at": "2019-11-25T08:45:30Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2019-11-11T03:33:32Z",
        "closed_at": "2021-06-29T03:15:40Z",
        "merged_at": null,
        "body": "I found that some code for checking the validity of the range(start <= end < len) in the function sdsrange is redundant and not very explicit or readable, so I simplified these code. The code has passed the unit test for sds.c.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2019-11-11T02:12:17Z",
        "closed_at": "2019-11-11T03:15:57Z",
        "merged_at": null,
        "body": "I found that some code for checking the validity of the range(start <= end < len) in the function sdsrange is redundant and not very explicit or readable, so I simplified these code. The code has passed the unit test for sds.c.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 27,
        "changed_files": 4,
        "created_at": "2019-11-10T08:01:00Z",
        "closed_at": "2019-11-20T09:08:09Z",
        "merged_at": "2019-11-20T09:08:09Z",
        "body": "Reduce default minimum effort, so that when fragmentation is just detected,\r\nthe impact on the latency will be minor.\r\n\r\nReduce the default maximum effort, mainly to prevent a case were a sudden\r\nmassive deletions, won't trigger an aggressive defrag that will cause latency.\r\n\r\nWhen activedefrag is disabled mid-run, reset the 'running' info field, and\r\nclear the scan cursor, so that when it'll be re-enabled, a new fresh scan will\r\nstart.\r\n\r\nClearing the 'running' variable is important since lowering the defragger\r\ntunables mid-scan won't help, the defragger only considers new threshold when\r\na new scan starts, and during a scan it can only become more aggressive,\r\n(when more severe fragmentation is detected), it'll never go less aggressive.\r\nSo by temporarily disabling activedefrag, one can lower th the tunables.\r\n\r\nRemoving the experimantal warning.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-11-10T07:25:27Z",
        "closed_at": "2019-11-19T10:49:44Z",
        "merged_at": "2019-11-19T10:49:44Z",
        "body": "recently added more reads into that function, if a later read fails, i must\r\neither free what's already allocated, or return the pointer so that the free\r\ncallback will release it.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 141,
        "deletions": 44,
        "changed_files": 8,
        "created_at": "2019-11-10T07:08:40Z",
        "closed_at": "2019-11-19T10:58:08Z",
        "merged_at": "2019-11-19T10:58:08Z",
        "body": "- the API name was odd, separated to two apis one for LRU and one for LFU\r\n- the LRU idle time was in 1 second resolution, which might be ok for RDB\r\n  and RESTORE, but i think modules may need higher resolution\r\n- adding tests for LFU and for handling maxmemory policy mismatch",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-11-08T11:22:00Z",
        "closed_at": "2019-11-19T10:14:05Z",
        "merged_at": "2019-11-19T10:14:05Z",
        "body": "Expand for #6537 , the same problem also happens in `handleClientsBlockedOnKeys()`, here are the codes:\r\n\r\n1. When a key is signaled as ready and handle it, at first we lookup the key and hold the pointer.\r\n```c\r\nvoid handleClientsBlockedOnKeys(void) {\r\n    while(listLength(server.ready_keys) != 0) {\r\n        list *l;\r\n        l = server.ready_keys;\r\n        server.ready_keys = listCreate();\r\n\r\n        while(listLength(l) != 0) {\r\n            listNode *ln = listFirst(l);\r\n            readyList *rl = ln->value;\r\n...\r\n            robj *o = lookupKeyWrite(rl->db,rl->key);\r\n```\r\n\r\n2. If the object is a list, we step into `serveClientsBlockedOnListKey`\r\n```c\r\n            if (o != NULL) {\r\n                if (o->type == OBJ_LIST)\r\n                    serveClientsBlockedOnListKey(o,rl);\r\n``` \r\n\r\n3. And then we step into `serveClientBlockedOnList`\r\n```c\r\nint serveClientBlockedOnList(client *receiver, robj *key, robj *dstkey, redisDb *db, robj *value, int where)\r\n{\r\n    if (dstkey == NULL) {\r\n...\r\n    } else {\r\n        /* BRPOPLPUSH */\r\n        robj *dstobj =\r\n            lookupKeyWrite(receiver->db,dstkey);\r\n```\r\n\r\n4. Now the list maybe expired and deleted if src and dest are same, then go back to `serveClientsBlockedOnListKey` and process the reset thing, but object o is already freed, and we access the freed memory then crash.\r\n```c\r\n    if (listTypeLength(o) == 0) {\r\n        dbDelete(rl->db,rl->key);\r\n        notifyKeyspaceEvent(NOTIFY_GENERIC,\"del\",rl->key,rl->db->id);\r\n    }\r\n```\r\n\r\nTo fix it, I think we can ready keys just like `call()` function.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2019-11-07T07:52:11Z",
        "closed_at": "2019-11-19T10:53:11Z",
        "merged_at": "2019-11-19T10:53:11Z",
        "body": "When a redis instance becomes a slave, sentinel also kills pubsub\r\nclients.\r\n\r\nCloses #6545",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2019-11-06T10:24:12Z",
        "closed_at": "2019-11-21T09:03:51Z",
        "merged_at": "2019-11-21T09:03:51Z",
        "body": "Fixes GitHub issue #6492\r\nAdded stream support in RM_KeyType and RM_ValueLength.\r\nAlso moduleDelKeyIfEmpty was updated, even though it has\r\nno effect now (It will be relevant when stream type direct\r\nAPI will be coded - i.e. RM_StreamAdd)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-11-05T13:54:30Z",
        "closed_at": "2020-04-02T14:26:57Z",
        "merged_at": "2020-04-02T14:26:57Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 95,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2019-11-04T17:52:36Z",
        "closed_at": "2019-11-19T11:15:54Z",
        "merged_at": "2019-11-19T11:15:54Z",
        "body": "Implemented APIs:\r\n\r\n- RedisModule_CreateStringFromLongDouble\r\n- RedisModule_StringToLongDouble\r\n- RedisModule_ReplyWithLongDouble\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2019-11-04T13:27:00Z",
        "closed_at": "2020-08-11T07:55:35Z",
        "merged_at": null,
        "body": "Recently I met a case can lead to redis crash. In detail, it's about expire time and multi keys command on same key.\r\n\r\nFor example, `RPOPLPUSH` can pop from list A and push to list B, even A and B is the same list, like `RPOPLPUSH xxx xxx` and here is the problem. If the list is with expire time, `RPOPLPUSH` may access freed memory.\r\n\r\nSay list `xxx` with expire time `E`.\r\n\r\n1. Get the source object from `xxx` and hold it in `sobj` before time `E`.\r\n```c\r\nvoid rpoplpushCommand(client *c) {\r\n    robj *sobj, *value;\r\n    if ((sobj = lookupKeyWriteOrReply(c,c->argv[1],shared.null[c->resp]))\r\n        == NULL || checkType(c,sobj,OBJ_LIST)) return;\r\n```\r\n\r\n2. Get the dest object from `xxx` after time `E`.\r\n```c\r\n        robj *dobj = lookupKeyWrite(c->db,c->argv[2]);\r\n        robj *touchedkey = c->argv[1];\r\n```\r\n\r\n   Now list `xxx` reach the expire time and has been deleted.\r\n\r\n3. Pop value from `sobj`, but it's already freed, redis crashed.\r\n```c\r\n        value = listTypePop(sobj,LIST_TAIL);\r\n```\r\n\r\nThe root cause I think is `expireIfNeeded()` function in different `lookupKey*()` use different time even in one command. So to fix it I introduce a new arg `server. cmd_start_mstime` means the start time of a command just like `lua_time_start`.\r\n\r\nNotice:\r\n\r\n1. All commands in transaction just use the `exec`'s start time, and it's reasonable I think.\r\n2. Not sure the effects about modules.",
        "comments": 21
    },
    {
        "merged": true,
        "additions": 349,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2019-11-04T08:53:21Z",
        "closed_at": "2019-11-04T09:54:00Z",
        "merged_at": "2019-11-04T09:54:00Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 69,
        "deletions": 20,
        "changed_files": 4,
        "created_at": "2019-11-03T15:40:38Z",
        "closed_at": "2019-11-04T10:02:38Z",
        "merged_at": "2019-11-04T10:02:38Z",
        "body": "This PR extracts the code behind flushallCommand so that it can be used by the module api.\r\n\r\n@antirez in addition to that, i considered to expose an RM_KeyExists and RM_KeyTouch, as faster alternatives to opening and closing a key.\r\nThese two can also be exposed as one RM_KeyExists api with a 'touch' argument.\r\nIn theory the non-touch code path can use `dbExists`, and skip the expiration attempt, but i'm not entirely sure any of this is useful / desired. please let me know what you think.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 86,
        "deletions": 29,
        "changed_files": 8,
        "created_at": "2019-11-03T14:46:43Z",
        "closed_at": "2019-11-04T16:33:36Z",
        "merged_at": "2019-11-04T16:33:36Z",
        "body": "looks like each platform implements long double differently (different bit count)\r\nso we can't save them as binary, and we also want to avoid creating a new RDB\r\nformat version, so we save these are hex strings using \"%La\".\r\n\r\nThis commit includes a change in the arguments of ld2string to support this.\r\nas well as tests for coverage and short reads.\r\n\r\ncoded by @guybe7",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 297,
        "deletions": 34,
        "changed_files": 10,
        "created_at": "2019-11-03T13:09:36Z",
        "closed_at": "2019-11-21T09:07:23Z",
        "merged_at": "2019-11-21T09:07:23Z",
        "body": "- Add RM_GetServerInfo and friends\r\n- Add auto memory for new opaque struct\r\n- Add tests for new APIs\r\n\r\nother minor fixes:\r\n- add const in various char pointers\r\n- requested_section in modulesCollectInfo was actually not sds but char*\r\n- extract new string2d out of getDoubleFromObject for code reuse\r\n\r\nAdd module API for",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2019-10-31T23:06:14Z",
        "closed_at": "2019-11-19T04:59:31Z",
        "merged_at": null,
        "body": "",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-10-31T18:26:30Z",
        "closed_at": "2020-02-07T19:43:46Z",
        "merged_at": null,
        "body": "Adding the ability to hash passwords stored in the Redis.conf file to the documentation.\r\n\r\nI think this will be useful to clarify functionality for security-conscious users.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-10-31T13:47:00Z",
        "closed_at": "2019-12-12T08:33:49Z",
        "merged_at": "2019-12-12T08:33:49Z",
        "body": "Addresses #6521 ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-10-30T20:18:56Z",
        "closed_at": "2021-06-29T06:18:39Z",
        "merged_at": null,
        "body": "Hello!\r\n\r\nIf invalid value is present for maxmemory option in config file, it is silently ignored and maxmemory is set to  `0` (infinite). Anyway invalid value can't be set via `CONFIG SET` command.\r\n\r\nWe had some issues with oom killed redis thanks to this since our config was generated by script and by mistake it was using decimal number for maxmemory value. Instead of warning or fail on start we got actually started redis with infinite memory limit and it got killed later by oom.\r\n\r\nI have not found any tests for similar `check and fail` so I have not added test for this. Feel free to point me to some place where this test should be added if it is needed.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-10-30T15:26:12Z",
        "closed_at": "2019-10-31T13:30:05Z",
        "merged_at": null,
        "body": "This may need extra work on the completion stats involved. Just needed this for work and thought it was an interesting and simple change that could be useful elsewhere.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-10-30T03:26:54Z",
        "closed_at": "2020-01-09T21:30:22Z",
        "merged_at": null,
        "body": "Hi @antirez , could you take a look my first commit for resolving some typos in the comments? I know it is a tiny fix, :P",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-10-29T22:37:56Z",
        "closed_at": "2019-12-12T08:41:20Z",
        "merged_at": "2019-12-12T08:41:20Z",
        "body": "Adding no-slowlog to acl command to prevent acl passwords from showing in slowlog to close #6515\r\n\r\n@antirez @itamarhaber  - let me know if you agree w/ this. ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 737,
        "deletions": 94,
        "changed_files": 13,
        "created_at": "2019-10-29T16:06:16Z",
        "closed_at": "2019-11-04T09:37:17Z",
        "merged_at": "2019-11-04T09:37:16Z",
        "body": "* replication hooks: role change, master link status, replica online/offline\r\n* persistence hooks: saving, loading, loading progress\r\n* misc hooks: cron loop, shutdown, module loaded/unloaded\r\n* change the way hooks test work, and add tests for all of the above\r\n\r\nstartLoading() now gets flag indicating what is loaded.\r\nstopLoading() now gets an indication of success or failure.\r\nadding startSaving() and stopSaving() with similar args and role.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2019-10-29T15:41:19Z",
        "closed_at": "2019-11-19T10:34:12Z",
        "merged_at": "2019-11-19T10:34:11Z",
        "body": "sometimes we have several assertions with the same condition in the same test\r\nat different stages, and when these fail (the ones that print the condition\r\ntext) you don't know which one it was. other assertions didn't print the\r\ncondition text (variable names), just the expected and unexpected values.\r\n\r\nSo now, all assertions print context line, and conditin text.\r\n\r\nbesides, one of the major differences between 'assert' and 'assert_equal',\r\nis that the later is able to print the value that doesn't match the expected.\r\nif there is a rare non-reproducible failure, it is helpful to know what was\r\nthe value the test encountered and how far it was from the threshold.\r\n\r\nSo now, adding assert_lessthan and assert_range that can be used in some places.\r\nwere we used just 'assert { a > b }' so far.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 431,
        "deletions": 12,
        "changed_files": 6,
        "created_at": "2019-10-29T11:32:52Z",
        "closed_at": "2019-11-19T10:08:16Z",
        "merged_at": "2019-11-19T10:08:15Z",
        "body": "@antirez this PR added the scan capability for module api (see #6071 for original issue).\r\nThe implementation expose the following new functions:\r\n1. RedisModule_CursorCreate - allow to create a new cursor object for\r\nkeys scanning\r\n2. RedisModule_CursorRestart - restart an existing cursor to restart the\r\nscan\r\n3. RedisModule_CursorDestroy - destroy an existing cursor\r\n4. RedisModule_Scan - scan keys\r\n\r\nAs discussed, the Cursor object is an opaque struct and the implementation can be change in the future without changing the api.\r\n\r\nThe RedisModule_Scan function gets a cursor, a callback and void*\r\n(used as user private data).\r\nThe callback will be called for each key in the database, it will proving the key\r\nname (as RedisModuleString) and the value (as RedisModuleKey).\r\n\r\nA usage example:\r\n```\r\nCursor* c = RedisModule_CursorCreate();\r\nwhile(RedisModule_Scan(ctx, c, callback, privateData));\r\nRedisModule_CursorDestroy(c);\r\n```\r\n\r\nIt is also possible to use this api from another thread such that the GIL only have to be acquired during the actual call to RM_Scan:\r\n```\r\n Cursor* c = RedisModule_CursorCreate();\r\n RedisModule_ThreadSafeCtxLock(ctx);\r\n while(RedisModule_Scan(ctx, c, callback, privateData)){\r\n     RedisModule_ThreadSafeCtxUnlock(ctx);\r\n     // do some background job\r\n     RedisModule_ThreadSafeCtxLock(ctx);\r\n }\r\n RedisModule_CursorDestroy(c);\r\n```\r\n\r\nThe PR also adds tests to the new api. Those tests will be run when running the runtest-moduleapi script.\r\n\r\nAfter consulting with @oranagra, we think that the Keys api and ScanByType api is not needed. A module writer can use the scan api to get all the keys and he can also filter by type (as he gets the keys values in the callback). Let us know if you think otherwise.",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-10-28T12:29:49Z",
        "closed_at": "2019-10-29T09:07:45Z",
        "merged_at": "2019-10-29T09:07:45Z",
        "body": "Need to add calls to REDISMODULE_API_FUNC...",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-10-28T09:25:02Z",
        "closed_at": "2019-11-04T09:18:30Z",
        "merged_at": null,
        "body": "To remove compile warning for lolwut6.c\r\njust adding default value as 0.\r\nI found lwGetPixel returns 0, when x,y is outside of valid position.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2019-10-28T06:52:30Z",
        "closed_at": "2019-10-28T08:56:56Z",
        "merged_at": "2019-10-28T08:56:56Z",
        "body": "\u2026Array and ReplyWithEmptyString to redis module API",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2019-10-27T08:48:52Z",
        "closed_at": "2019-10-28T06:52:56Z",
        "merged_at": null,
        "body": "Extend Redis module API with `ReplyWithEmptyArray`",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-10-25T17:40:43Z",
        "closed_at": "2019-10-28T08:57:20Z",
        "merged_at": "2019-10-28T08:57:20Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-10-25T13:38:12Z",
        "closed_at": "2019-10-28T08:57:44Z",
        "merged_at": "2019-10-28T08:57:44Z",
        "body": "fix comment typo in redis-cli.c\r\n(Bracket is missing.)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2019-10-24T11:26:57Z",
        "closed_at": "2019-10-28T08:59:26Z",
        "merged_at": "2019-10-28T08:59:26Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-10-24T08:14:04Z",
        "closed_at": "2019-12-18T16:06:06Z",
        "merged_at": "2019-12-18T16:06:06Z",
        "body": "This is useful to tell redis and modules to try to avoid doing things that may\r\nincrement the replication offset, and should be used when draining a master\r\nand waiting for replicas to be in perfect sync before a failover.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 28,
        "changed_files": 4,
        "created_at": "2019-10-24T06:57:00Z",
        "closed_at": "2019-10-28T09:05:52Z",
        "merged_at": "2019-10-28T09:05:52Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 94,
        "deletions": 4,
        "changed_files": 7,
        "created_at": "2019-10-24T06:42:54Z",
        "closed_at": "2019-10-29T09:07:06Z",
        "merged_at": "2019-10-29T09:07:06Z",
        "body": "This API is mainly useful in case you wanna forward a module command to another\r\none using RM_Call.\r\n\r\nAdding a test for coverage for RM_Call and RM_CallArgv in a new \"misc\" unit\r\nto be used for various short simple tests",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 24,
        "changed_files": 1,
        "created_at": "2019-10-23T17:15:19Z",
        "closed_at": "2019-10-28T09:00:41Z",
        "merged_at": "2019-10-28T09:00:41Z",
        "body": "As requested by @oranagra, this removes some boilerplate per module and adds a clean target.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-10-23T11:07:57Z",
        "closed_at": "2020-09-02T13:23:50Z",
        "merged_at": "2020-09-02T13:23:50Z",
        "body": "@hengku @itamarhaber Please have a look\r\n\r\nRelated to https://github.com/antirez/redis/issues/6474\r\n\r\nOutput Log:\r\n**Default Option:**\r\n\r\n```\r\nsrc/redis-cli --cluster call 127.0.0.1:6001 set key1 value1\r\n>>> Calling set key1 value1\r\n127.0.0.1:6001: MOVED 9189 127.0.0.1:6002\r\n\r\n127.0.0.1:7001: MOVED 9189 127.0.0.1:6002\r\n\r\n127.0.0.1:6003: MOVED 9189 127.0.0.1:6002\r\n\r\n127.0.0.1:6002: OK\r\n127.0.0.1:7002: MOVED 9189 127.0.0.1:6002\r\n\r\n127.0.0.1:7003: MOVED 9189 127.0.0.1:6002\r\n```\r\n\r\n**Master Option:**\r\n\r\n```\r\nsrc/redis-cli --cluster call 127.0.0.1:6001 master set key2 value1\r\n>>> Calling set key2 value1\r\n127.0.0.1:6001: OK\r\n127.0.0.1:6003: MOVED 4998 127.0.0.1:6001\r\n\r\n127.0.0.1:6002: MOVED 4998 127.0.0.1:6001\r\n```\r\n\r\n**Slave Option:**\r\n\r\n```\r\nsrc/redis-cli --cluster call 127.0.0.1:6001 slave set key2 value1\r\n>>> Calling set key2 value1\r\n127.0.0.1:7001: MOVED 4998 127.0.0.1:6001\r\n\r\n127.0.0.1:7002: MOVED 4998 127.0.0.1:6001\r\n\r\n127.0.0.1:7003: MOVED 4998 127.0.0.1:6001\r\n```\r\n\r\n\r\n",
        "comments": 16
    },
    {
        "merged": false,
        "additions": 509,
        "deletions": 18,
        "changed_files": 12,
        "created_at": "2019-10-23T10:44:32Z",
        "closed_at": "2019-11-04T08:53:49Z",
        "merged_at": null,
        "body": "Added key-blocking mechanism to allow modules\r\nto implement BLPOP-like commands",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2019-10-23T09:16:26Z",
        "closed_at": "2019-10-29T09:40:23Z",
        "merged_at": "2019-10-29T09:40:23Z",
        "body": "This commit also fixes an uninitialized module struct member (that luckily never got released)",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 57,
        "deletions": 6,
        "changed_files": 5,
        "created_at": "2019-10-23T09:10:25Z",
        "closed_at": "2019-11-04T10:05:09Z",
        "merged_at": "2019-11-04T10:05:09Z",
        "body": "Some commands would want to open a key without touching it's LRU/LFU\r\nsimilarly to the OBJECT or DEBUG command do.\r\n\r\nOther commands may want to implement logic similar to what RESTORE\r\ndoes (and in the future MIGRATE) and get/set the LRU or LFU.",
        "comments": 16
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-10-22T10:22:59Z",
        "closed_at": "2020-01-10T04:29:31Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-10-22T09:35:37Z",
        "closed_at": "2019-10-28T11:16:05Z",
        "merged_at": "2019-10-28T11:16:05Z",
        "body": "Also, add an API for getting server.notify_keyspace_events\r\n\r\nOther (unrelated) changes:\r\n1. Fix trivial compiler warning in lolwut6.c\r\n2. Add RM_GetKeynameFromModuleKey",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-10-21T18:26:46Z",
        "closed_at": "2019-10-25T15:10:23Z",
        "merged_at": null,
        "body": "Based on the same logic described in the comment in flushAppendOnlyFile:\r\n\r\n>  We also use an additional event name to save all samples which is useful for graphing / monitoring purposes.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 112,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2019-10-21T08:07:16Z",
        "closed_at": "2019-12-02T10:56:58Z",
        "merged_at": null,
        "body": "License is missing in few test files",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-10-20T10:23:12Z",
        "closed_at": "2020-08-08T19:11:14Z",
        "merged_at": "2020-08-08T19:11:14Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-10-20T07:09:10Z",
        "closed_at": "2019-10-29T09:25:35Z",
        "merged_at": "2019-10-29T09:25:35Z",
        "body": "",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2019-10-19T07:07:51Z",
        "closed_at": "2020-09-07T21:10:14Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-10-18T08:36:03Z",
        "closed_at": "2021-07-20T11:59:15Z",
        "merged_at": null,
        "body": "Fixes\r\n```\r\nconnection.c:344:30: error: use of undeclared identifier 'SOL_SOCKET'\r\n    if (getsockopt(conn->fd, SOL_SOCKET, SO_ERROR, &sockerr, &errlen) == -1)\r\n```\r\n\r\nHopefully this isn't a duplicate.  I did attempt to find one.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2019-10-17T10:18:01Z",
        "closed_at": "2020-01-14T04:37:11Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-10-16T20:32:45Z",
        "closed_at": "2020-01-10T11:16:14Z",
        "merged_at": "2020-01-10T11:16:14Z",
        "body": "@yossigo @antirez - minor update to the redis.conf file to call the directive tls-prefer-server-ciphers. (It is missing the \"s\" at the end of ciphers.\r\n\r\nThe directive tls-prefer-server-cipher is actually tls-prefer-server-ciphers in config.c. This results in a failed directive call shown below. This pull request adds the \"s\" in ciphers so that the directive is able to be properly called in config.c\r\n\r\nubuntu@ip-172-31-16-31:~/redis$ src/redis-server ./redis.conf \r\n\r\n*** FATAL CONFIG FILE ERROR ***\r\nReading the configuration file, at line 200\r\n>>> 'tls-prefer-server-cipher yes'\r\nBad directive or wrong number of arguments",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-10-16T07:01:42Z",
        "closed_at": "2019-10-29T05:24:47Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-10-16T06:19:25Z",
        "closed_at": "2019-10-29T05:24:33Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2019-10-16T05:13:19Z",
        "closed_at": "2020-09-09T16:24:35Z",
        "merged_at": null,
        "body": "allocatoin->allocation\r\nTeturns -> Returns\r\nchagnes -> changes\r\nas oart of the main -> as part of the main",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 29,
        "changed_files": 1,
        "created_at": "2019-10-13T06:03:33Z",
        "closed_at": "2020-09-09T16:24:38Z",
        "merged_at": null,
        "body": "Redis and its documentation are great -- just wanted to submit a few corrections in the spirit of Hacktoberfest. Thanks for all your work on this project. I use it all the time and it works beautifully.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 51,
        "deletions": 28,
        "changed_files": 4,
        "created_at": "2019-10-12T09:06:12Z",
        "closed_at": "2021-09-10T08:44:26Z",
        "merged_at": null,
        "body": "Hi @antirez and @oranagra , recently I read about the `diskless-load` feature, it's interesting and can help us to save the full-sync time. But there is a little problem about cluster, we forgot the `slots_to_keys` info I think. And that can lead to some wrong result like `CLUSTER COUNTKEYSINSLOT` and `CLUSTER GETKEYSINSLOT` command. Here is a way to fix it, please check.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-10-11T16:36:57Z",
        "closed_at": "2019-12-13T11:27:20Z",
        "merged_at": null,
        "body": "Elements count in multi-bulk replies is read into a `long long` variable inside `processAggregateItem`.\r\nIn file deps/hiredis/read.c, line 423:\r\n\r\n`long long elements;`\r\n\r\n...\r\n\r\n`string2ll(p, len, &elements)`\r\n\r\nAnyway, later (line 482), its value is copied inside the `elements` member of a struct `redisReadTask`:\r\n\r\n`cur->elements = elements;`\r\n\r\nSince the `elements` member was declared as an `int`, an integer overflow could happen.\r\n\r\nDeclaring the element as `size_t` is safe since even if size_t was a 32bit, there's a check at line 442 that prevents writing values bigger than it can accept.\r\n\r\n`        \r\n       if (elements < -1 || (LLONG_MAX > SIZE_MAX && elements > SIZE_MAX)) {\r\n            __redisReaderSetError(r,REDIS_ERR_PROTOCOL,\r\n                    \"Multi-bulk length out of range\");\r\n            return REDIS_ERR;\r\n        }\r\n`\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 30,
        "changed_files": 1,
        "created_at": "2019-10-11T04:25:05Z",
        "closed_at": "2020-08-11T12:27:19Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-10-10T07:48:53Z",
        "closed_at": "2019-10-10T12:49:36Z",
        "merged_at": "2019-10-10T12:49:36Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-10-10T07:16:47Z",
        "closed_at": "2019-10-10T12:51:56Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2019-10-10T06:45:31Z",
        "closed_at": "2022-03-15T09:58:17Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-10-09T18:17:00Z",
        "closed_at": "2019-10-10T09:18:47Z",
        "merged_at": "2019-10-10T09:18:47Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-10-09T16:56:42Z",
        "closed_at": "2019-10-10T12:55:04Z",
        "merged_at": "2019-10-10T12:55:04Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2019-10-09T14:01:58Z",
        "closed_at": "2019-10-10T22:15:07Z",
        "merged_at": null,
        "body": "This PR exposes the module's internal method `replyWithStatus` to the user, altering its name to  `RedisModule_ReplyWithRESP`.\r\n\r\nThe purpose of it is to make it broader ( now it is only used for simple strings and errors ). With `RedisModule_ReplyWithRESP`, within a module, we will be able to test replying with different versions of RESP and the new types added on RESP 3. \r\nThe change its is not a breaking one, it is a simple addition to the experimental modules API. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-10-07T18:03:34Z",
        "closed_at": "2019-10-08T15:40:48Z",
        "merged_at": "2019-10-08T15:40:48Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-10-07T14:49:29Z",
        "closed_at": "2019-10-10T12:56:43Z",
        "merged_at": "2019-10-10T12:56:43Z",
        "body": "FIx simple typos salves to slaves in replication.c",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 22,
        "changed_files": 5,
        "created_at": "2019-10-06T11:57:30Z",
        "closed_at": "2019-10-08T15:10:58Z",
        "merged_at": "2019-10-08T15:10:58Z",
        "body": "cluster.c - stack buffer memory alignment\r\n    The pointer 'buf' is cast to a more strictly aligned pointer type\r\nevict.c - lazyfree_lazy_eviction, lazyfree_lazy_eviction always called\r\ndefrag.c - bug in dead code\r\nserver.c - casting was missing parenthesis\r\nrax.c - indentation / newline suggested an 'else if' was intended",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-10-04T21:03:41Z",
        "closed_at": "2022-08-30T18:10:05Z",
        "merged_at": null,
        "body": null,
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-10-03T21:35:10Z",
        "closed_at": "2020-08-18T17:32:41Z",
        "merged_at": null,
        "body": "The error message defined by ENODEV also appears for the use case where a password is attempted to be removed from a user that does not exist. This PR suggests minor changes to the error message to suggest that it may not just be that the password does not exist but also that the username does not exist.\r\n\r\nExample Below:\r\n\r\n```\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* +@all\"\r\n2) \"user kyle off #e66637277baf5cbf0199da7785b76afe9ce2adcdbb45cbdd14a438a428402517 #8b75b5b7eb1db071c9f2c7e649a5e6d07afd94f9b765f1f77a6fd4dd3a6c21a5 -@all\"\r\n127.0.0.1:6379> acl setuser IDoNotExist <fakepassword\r\n(error) ERR Error in ACL SETUSER modifier '<fakepassword': The password you are trying to remove from the user does not exist\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2019-10-02T05:43:30Z",
        "closed_at": "2019-10-04T09:58:06Z",
        "merged_at": "2019-10-04T09:58:06Z",
        "body": "since the slowlog and other means that can help you detect the bad script\r\nare only exposed after the script is done. it might be a good idea to at least\r\nprint the script name (sha) to the log when it timeouts.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2019-09-26T16:36:43Z",
        "closed_at": "2019-09-30T15:46:10Z",
        "merged_at": "2019-09-30T15:46:10Z",
        "body": "The changes to the cluster script involve the addition of the following:\r\n\r\n1. The CLUSTER_HOST variable:  The host variable has been added instead of using static localhost.  This allows the user to easily change the value of the hosts being created. This is useful when the cluster is being tested on a local environment alongside a local container setup. Failing to change this causes issues wherein the container's localhost doesn't map to the machine's localhost, resulting in a RedisClusterException with the message \"Can't communicate with any node in the cluster\".\r\nIssue example:\r\nhttps://stackoverflow.com/questions/35599977/redisclusterexception-with-phpredis-when-connecting-to-redis-cluster-which-is-no\r\n\r\n2. The PROTECTED_MODE variable: The variable will help users toggle protected mode for the clusters easily during setup.\r\n\r\n3. Additions to the README file.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-09-25T09:22:46Z",
        "closed_at": "2019-10-01T08:45:10Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 88,
        "deletions": 8,
        "changed_files": 3,
        "created_at": "2019-09-24T22:36:42Z",
        "closed_at": "2019-10-08T05:21:22Z",
        "merged_at": null,
        "body": "What is the motivation here? \r\n\r\nConfig rewrite is currently broken since it prints out the hashed password instead of the real password, so this fixes that. The original point is that now you can actually pass in the hashed password, so you don't need to store it in plaintext on disk, and of lesser importance in transit. Also actually added some tests. ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-09-23T23:36:12Z",
        "closed_at": "2019-09-25T16:03:40Z",
        "merged_at": "2019-09-25T16:03:40Z",
        "body": "I'm working on an instructional presentation presenting the nuts-and-bolts of how Redis works, and I've come across what I think is a mistake in the keying/seeding of SipHash.\r\n\r\nI've dug into this to the best of my ability, but I'm not a cryptographer, and there may be something I've missed as to Redis' approach.\r\n\r\nSipHash expects a 128-bit key, and Redis indeed generates 128-bits (16 random bytes), but restricts them to ASCII hex characters 0-9a-f, effectively giving us only 4 bits-per-byte of key material, and 64 bits overall.  Considering Redis uses SipHash 1-2, I think using the full key space would be beneficial.\r\n\r\nThanks",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2019-09-22T06:08:13Z",
        "closed_at": "2019-09-25T16:07:26Z",
        "merged_at": "2019-09-25T16:07:26Z",
        "body": "discard command should not fail during OOM, otherwise client MULTI state\r\nwill not be cleared.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-09-20T08:12:40Z",
        "closed_at": "2019-09-20T09:19:57Z",
        "merged_at": "2019-09-20T09:19:57Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-09-20T00:39:53Z",
        "closed_at": "2019-09-20T09:15:21Z",
        "merged_at": "2019-09-20T09:15:21Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-09-20T00:16:31Z",
        "closed_at": "2019-09-20T09:14:39Z",
        "merged_at": "2019-09-20T09:14:39Z",
        "body": "Fix bad handling of unexpected option while loading config \"lua-replicate-commands\".",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-09-16T21:18:37Z",
        "closed_at": "2019-09-20T09:24:08Z",
        "merged_at": "2019-09-20T09:24:08Z",
        "body": "- `HHL` -> `HLL`",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-09-15T20:47:41Z",
        "closed_at": "2019-09-30T08:57:06Z",
        "merged_at": "2019-09-30T08:57:06Z",
        "body": "# Summary Report \r\nThis PR improves performance of RM_ReplyWithSimpleString and RM_ReplyWithError by making usage addReplyProto instead of addReplySds leading to fewer memory allocations/deallocations. This PR does no change the `module.h` API ( only the auxiliar method replyWithStatus was removed ).\r\n\r\nUsing callgrind and running 1M commands that only return \"+OK\\r\\n\" we can get the average cyles per call of RM_ReplyWithSimpleString:\r\n- using addReplySds 38.38 Cycles per Call \r\n- using addReplyProto 13.85 Cycles per Call\r\n\r\n\r\nFocusing on the memory usage, if we compare both callgrind graph outputs we see that, as an example, zmalloc was called 3 times more for implementation of RM_ReplyWithSimpleString with addReplySds implementation vs the implementation with addReplyProto.\r\n\r\n----------------------\r\n\r\nRM_ReplyWithSimpleString and RM_ReplyWithError are used among all Redis Modules making this change improve the overall performance of all modules commands. \r\nFocusing on RM_ReplyWithSimpleString, even the simplest command that only returns the RESP OK Code ( \"+OK\\r\\n\" ) via calling RM_ReplyWithSimpleString(ctx,\"OK\"), due to the current implementation of RM_ReplyWithSimpleString we're constantly allocating and deallocating dynamic strings leading to penalties in terms of performance potentially to all commands competing for memory.\r\n\r\n## Average cycles per call of RM_ReplyWithSimpleString with addReplySds vs addReplyProto\r\nUsing callgrind and running 1M commands that only return \"+OK\\r\\n\" we can get the average cyles per call of RM_ReplyWithSimpleString:\r\n- using addReplySds 38.38 Cycles per Call \r\n- using addReplyProto 13.85 Cycles per Call\r\n\r\n## RM_ReplyWithSimpleString call graph with addReplySds vs addReplyProto\r\n\r\nBellow we show the two callgraphs from the implementation of RM_ReplyWithSimpleString(ctx,\"OK\") using addReplySds vs  RM_ReplyWithSimpleString(ctx,\"OK\") using addReplyProto.  \r\n\r\n ### RM_ReplyWithSimpleString call graph with addReplySds\r\n\r\n![image](https://user-images.githubusercontent.com/5832149/64927286-afa5f780-d800-11e9-9839-e725b48a4199.png)\r\n\r\n### RM_ReplyWithSimpleString call graph with addReplyProto\r\n![image](https://user-images.githubusercontent.com/5832149/64927332-673b0980-d801-11e9-9a4f-f9a5df6d8c34.png)\r\n\r\n\r\n## Allocator/Deallocator #calls\r\n### zmalloc\r\nIf we compare both callgrind graph outputs we see that, as an example, zmalloc was called ~3M times for the RM_ReplyWithSimpleString with addReplySds implementation vs 1M times for the RM_ReplyWithSimpleString with addReplyProto implementation.\r\n\r\n### zfree\r\nFollowing the same logic as above, zfree was called ~2M times for the RM_ReplyWithSimpleString with  addReplySds implementation vs ~0 times for the RM_ReplyWithSimpleString with addReplyProto implementation\r\n\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-09-15T13:06:09Z",
        "closed_at": "2019-12-02T08:02:09Z",
        "merged_at": "2019-12-02T08:02:09Z",
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2019-09-13T14:52:07Z",
        "closed_at": "2019-09-13T22:32:36Z",
        "merged_at": null,
        "body": "Hi there @antirez, this PR fixes #6382. I've excluded the example that tests it to keep the code changes short, but it can be seen at:\r\n- https://github.com/filipecosta90/redis/blob/fix-threaded-argv/src/modules/testmodule.c#L320.  \r\n\r\nBug example: https://github.com/filipecosta90/redis/tree/crash-threaded-argv\r\nFix: https://github.com/filipecosta90/redis/tree/fix-threaded-argv\r\n\r\n----\r\n\r\nOn a quick summary if any thread tries to access the command arguments from within the ThreadSafeContext they will crash redis. ( crash report and more details on #6382 ).\r\n\r\nThe solution lies in filling argv, argc, flags and db in the moment of when creating the non connected client via calling `RedisModuleBlockedClient`. \r\nAdditionally, and specifically for RM_WrongArity, we should also call moduleGetReplyClient within RM_WrongArity to make sure we're accessing the correct client ( blocked or non blocked ). This PR also includes it. \r\n\r\n\r\n\r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 83,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2019-09-12T14:51:41Z",
        "closed_at": "2019-10-04T15:48:29Z",
        "merged_at": null,
        "body": "This PR focus on reusing redis shared common object `shared.ok` to create a RedisModule_ReplyWithOK method, with the main goal to reduce memory allocation/deallocation during runtime, when compared to using RedisModule_ReplyWithSimpleString(ctx, \"OK\").\r\nEven tough \"OK\" is a simple and short string \"+OK\\r\\n\" ( 5 Bytes on RESP ), constantly issuing zmalloc/zfree has penalties in terms of performance. The more objects and memory we have to manage the more RedisModule_ReplyWithOK will perfom vs RedisModule_ReplyWithSimpleString. \r\nThis is a small code change wich implies:\r\n\r\n- [add] added RM_ReplyWithOK method that makes use of redis shared common objects to reduce memory allocation during runtime.\r\n- [add] added tests for RM_ReplyWithOK vs ReplyWithSimpleStringOK to the testsuit of Redis modules subsystem.\r\n\r\n-------\r\n## Benchmarks using memtier and output of commandstats, call history analysis using callgrind\r\n### Quick note\r\nThe benchmark values and call history analysis using callgrind presented bellow used Redis version=5.0.5, bits=64, commit=c696aebd, and were retrived on a local machine ( Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz ) with OS Darwin 18.7.0 x86_64. Considering that both the client and DB Server were on the same physical machine it is advised to run the same tests on a production graded DB machine with separate clients issuing the load. Despite that, all benchmark values ( even if local ) and call history among functions revelead steady state runs and results ( It all points towards performance improvements =) ).\r\n\r\nAcross tests, Redis OSS v5.0.5 was started always with:\r\n```\r\nredis-server --protected-mode no --save \"\" --appendonly no  --loadmodule src/modules/testmodule.so\r\n```\r\nwhen doing the call history analysis, callgrind was started in the following manner:\r\n```\r\nvalgrind  --tool=callgrind --dump-instr=yes --simulate-cache=no --collect-jumps=yes --collect-atstart=yes --instr-atstart=yes redis-server --protected-mode no --save \"\" --appendonly no  --loadmodule src/modules/testmodule.so\r\n```\r\n## 1)Benchmarks\r\nTo test the meaningfullness of this PR we've added 4 commands to the testsuit of Redis modules subsystem: \r\n - 1)Two that only reply with with \"+OK\\r\\n\", with the purpose of isolating and testing only RedisModule_ReplyWithOK. \r\n-  2)Two that set a new key to redis and reply with \"+OK\\r\\n\", with the purpose of testing the positive impact of RedisModule_ReplyWithOK to commands that modify the dataset. \r\n\r\n### 1.1) Simple commands that only reply with \"+OK\\r\\n\"\r\n- `TEST.REPLY.WITH.SS.OK` - which does nothing more than replying with RedisModule_ReplyWithSimpleString(ctx, \"OK\").\r\n- `TEST.REPLY.WITH.OK.OK`- which does nothing more than replying with RedisModule_ReplyWithOK(ctx).\r\n\r\nmemtier used commands:\r\n```\r\nmemtier_benchmark -n 20000 -c 50 -t 1 --command=\"TEST.REPLY.WITH.SS.OK\" --hide-histogram\r\nmemtier_benchmark -n 20000 -c 50 -t 1 --command=\"TEST.REPLY.WITH.OK.OK\" --hide-histogram\r\n```\r\n\r\n### 1.1.1)Memtier output for simple tests\r\nType | Ops/sec | Latency | KB/sec | # Connections | Total Commands\r\n-- | -- | -- | -- | -- | --\r\nRedisModule_ReplyWithSimpleString(ctx, \"OK\") | 111560.36 | 0.447 | 4030.99 | 50 | 1M\r\nRedisModule_ReplyWithOK(ctx) | 112980.73 | 0.441 | 4082.31 | 50 | 1M\r\n\r\n\r\n### 1.1.2)Redis command statistics output \r\n```\r\n# Commandstats\r\ncmdstat_test.reply.with.ok.ok:calls=1000000,usec=165097,usec_per_call=0.17\r\ncmdstat_test.reply.with.ss.ok:calls=1000000,usec=492430,usec_per_call=0.49\r\n```\r\n### 1.1.3)simple commands conclusions\r\nFrom the client prespective no major variation was observed, *however*, as seen from the Redis command statistics, **on average the command that returned the response via RedisModule_ReplyWithOK took  0.17 us, vs 0.49 us for RedisModule_ReplyWithSimpleString(ctx, \"OK\")**.\r\n\r\n----\r\n\r\n### 1.2)Testing the positive impact of RedisModule_ReplyWithOK to commands that modify the dataset \r\n- `TEST.SETKEY.REPLY.WITH.SS.OK` - which tests setting a Module Key and Replying with RedisModule_ReplyWithSimpleString(ctx, \"OK\").\r\n- `TEST.SETKEY.REPLY.WITH.OK.OK`- which tests setting a Module Key and Replying with RedisModule_ReplyWithOK(ctx).\r\n\r\nmemtier used commands:\r\n```\r\n##############\r\n# 50 Connections\r\n##############\r\n\r\n##############\r\n# RedisModule_ReplyWithSimpleString\r\n##############\r\nmemtier_benchmark -n 100000 -c 50 -t 1  --command=\"TEST.SET.REPLY.WITH.SS.OK __key__ __data__\" --command-key-patter=P --data-size=128 --hide-histogram\r\nmemtier_benchmark -n 40000 -c 50 -t 1  --command=\"TEST.SET.REPLY.WITH.SS.OK __key__ __data__\" --command-key-patter=P --data-size=1024 --hide-histogram\r\nmemtier_benchmark -n 40000 -c 50 -t 1  --command=\"TEST.SET.REPLY.WITH.SS.OK __key__ __data__\" --command-key-patter=P --data-size=8192 --hide-histogram\r\n\r\n##############\r\n# RedisModule_ReplyWithOK\r\n##############\r\nmemtier_benchmark -n 100000 -c 50 -t 1  --command=\"TEST.SET.REPLY.WITH.OK.OK __key__ __data__\" --command-key-patter=P --data-size=128 --hide-histogram\r\nmemtier_benchmark -n 100000 -c 50 -t 1  --command=\"TEST.SET.REPLY.WITH.OK.OK __key__ __data__\" --command-key-patter=P --data-size=1024 --hide-histogram\r\nmemtier_benchmark -n 100000 -c 50 -t 1  --command=\"TEST.SET.REPLY.WITH.OK.OK __key__ __data__\" --command-key-patter=P --data-size=8192 --hide-histogram\r\n\r\n##############\r\n# 200 Connections\r\n##############\r\n\r\n##############\r\n# RedisModule_ReplyWithSimpleString\r\n##############\r\nmemtier_benchmark -n 100000 -c 200 -t 1  --command=\"TEST.SET.REPLY.WITH.SS.OK __key__ __data__\" --command-key-patter=P --data-size=128 --hide-histogram\r\nmemtier_benchmark -n 100000 -c 200 -t 1  --command=\"TEST.SET.REPLY.WITH.SS.OK __key__ __data__\" --command-key-patter=P --data-size=1024 --hide-histogram\r\nmemtier_benchmark -n 100000 -c 200 -t 1  --command=\"TEST.SET.REPLY.WITH.SS.OK __key__ __data__\" --command-key-patter=P --data-size=8192 --hide-histogram\r\n\r\n##############\r\n# RedisModule_ReplyWithOK\r\n##############\r\nmemtier_benchmark -n 100000 -c 200 -t 1  --command=\"TEST.SET.REPLY.WITH.OK.OK __key__ __data__\" --command-key-patter=P --data-size=128 --hide-histogram\r\nmemtier_benchmark -n 100000 -c 200 -t 1  --command=\"TEST.SET.REPLY.WITH.OK.OK __key__ __data__\" --command-key-patter=P --data-size=1024 --hide-histogram\r\nmemtier_benchmark -n 100000 -c 200 -t 1  --command=\"TEST.SET.REPLY.WITH.OK.OK __key__ __data__\" --command-key-patter=P --data-size=8192 --hide-histogram\r\n\r\n```\r\n\r\n### 1.2.1)Memtier output for tests that incorporate commands that modify the dataset\r\n\r\nReply Type | Data size | ops/sec | Latency | KB/sec | # Connections | Total Commands\r\n-- | -- | -- | -- | -- | -- | --\r\nRedisModule_ReplyWithSimpleString(ctx, \"OK\") | 128 | 104870.66 | 1.906 | 20368.65 | 200 | 5M\r\nRedisModule_ReplyWithSimpleString(ctx, \"OK\") | 1024 | 98749.05 | 2.024 | 105681.52 | 200 | 5M\r\nRedisModule_ReplyWithSimpleString(ctx, \"OK\") | 8192 | 67341.87 | 2.968 | 543462.56 | 200 | 5M\r\nRedisModule_ReplyWithOK(ctx) | 128 | 103915.41 | 1.924 | 20183.11 | 200 | 5M\r\nRedisModule_ReplyWithOK(ctx) | 1024 | 96303.62 | 2.076 | 103064.41 | 200 | 5M\r\nRedisModule_ReplyWithOK(ctx) | 8192 | 71607.21 | 2.807 | 577884.72 | 200 | 5M\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/5832149/64791347-7f93f580-d56f-11e9-80d8-9756619960bf.png)\r\n\r\n\r\n### 1.2.2)Redis command statistics output \r\n```\r\n# Commandstats\r\n(200clients/128B) cmdstat_test.set.reply.with.ss.ok:calls=5000000,usec=11518708,usec_per_call=2.30\r\n(200clients/1024B) cmdstat_test.set.reply.with.ss.ok:calls=5000000,usec=14593663,usec_per_call=2.92\r\n(200clients/8192B) cmdstat_test.set.reply.with.ss.ok:calls=5000000,usec=33258066,usec_per_call=6.65\r\n(200clients/128B) cmdstat_test.set.reply.with.ok.ok:calls=5000000,usec=9615639,usec_per_call=1.92\r\n(200clients/1024B) cmdstat_test.set.reply.with.ok.ok:calls=5000000,usec=12931561,usec_per_call=2.59\r\n(200clients/8192B) cmdstat_test.set.reply.with.ok.ok:calls=5000000,usec=30069395,usec_per_call=6.01\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/5832149/64793093-68a2d280-d572-11e9-84b7-95a8b71519ab.png)\r\n\r\n\r\n### 1.2.3)simple commands conclusions\r\nFrom the client, we can observe performance improvements on RedisModule_ReplyWithOK for the largest data size and the largest number of clients.\r\nFrom the Redis command statistics, ** for all key sizes and client variations RedisModule_ReplyWithOK(ctx) performed significantly better than RedisModule_ReplyWithSimpleString(ctx, \"OK\")**.\r\n\r\n--------------\r\n## 2)Call history analysis using callgrind\r\nWhen doing the call history analysis, callgrind was started in the following manner:\r\n```\r\nvalgrind  --tool=callgrind --dump-instr=yes --simulate-cache=no --collect-jumps=yes --collect-atstart=yes --instr-atstart=yes redis-server --protected-mode no --save \"\" --appendonly no  --loadmodule src/modules/testmodule.so\r\n```\r\nThe output of the two benchmarks ( 1M commands each ) is sent as attachment. \r\n[callgrind.out.RedisModule_ReplyWithSimpleStringOKvsRedisModule_ReplyWithOK.zip](https://github.com/antirez/redis/files/3606028/callgrind.out.RedisModule_ReplyWithSimpleStringOKvsRedisModule_ReplyWithOK.zip)\r\n\r\n### Call History summary report \r\nAs seen on the following callgraph sections ( 2.1 and 2.2 ) RedisModule_ReplyWithSimpleString(ctx, \"OK\") represents 32.61% of the total Cycles spent on the command method, vs 8.38% of RedisModule_ReplyWithOK(ctx). \r\n\r\nYou can observe that RedisModule_ReplyWithSimpleString(ctx, \"OK\") makes more memory management function calls ( zmalloc zrealloc zfree ) spending 48.42 + 9.65 + 16.58 = 74.55% of the total cycles spent on the command ( Using the Redis command statistics output \r\n from RedisModule_ReplyWithOK(ctx) it represents 74.55% * 11518708 us = 8,587,196.814 us ). Compared to the command which issues RedisModule_ReplyWithOK(ctx) that spends 52.51 + 13.10 + 6.14 = 71.75% * 9615639 us = 6,899,220.9825 us it represents a difference of 20% less time spend on zmalloc zrealloc zfree. \r\n\r\n\r\n\r\n### 2.1)Call history analysis using callgrind for RedisModule_ReplyWithSimpleString(ctx, \"OK\")\r\n\r\n![image](https://user-images.githubusercontent.com/5832149/64793802-83297b80-d573-11e9-8c84-2de67a525939.png)\r\n\r\n\r\n\r\n### 2.2)Call history analysis using callgrind for RedisModule_ReplyWithOK(ctx)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/5832149/64793783-7dcc3100-d573-11e9-8b9d-f7c5a6efef14.png)\r\n\r\n\r\n\r\n\r\n\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-09-06T04:13:02Z",
        "closed_at": "2019-09-20T09:33:28Z",
        "merged_at": "2019-09-20T09:33:28Z",
        "body": "when I read the source code of redis\uff0cthe comment of function rdbLoadIntegerObject in the file of  `rdb.c` have a wrong place.  I can't find the the `rdbGenerincLoadStringObject` function in the whole project , it should be rdbGenericLoadStringObject. This may be a trivial point.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-09-05T15:28:17Z",
        "closed_at": "2019-09-20T09:33:58Z",
        "merged_at": "2019-09-20T09:33:58Z",
        "body": "Changes were originally made inside the api-ref file [inside the documentation](https://github.com/antirez/redis-doc/pull/1162 ) but I was referred to make the changes here instead. ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 111726,
        "deletions": 9906,
        "changed_files": 624,
        "created_at": "2019-09-05T00:59:15Z",
        "closed_at": "2020-08-11T12:30:48Z",
        "merged_at": null,
        "body": "aa",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-09-04T12:25:17Z",
        "closed_at": "2019-09-25T16:18:25Z",
        "merged_at": null,
        "body": "The aarch64 architecture is support normal memory unaligned access,\r\n\r\nso add the UNALIGNED_LE_CPU to the aarch64.\r\n\r\nAnd use the redis-benchmark tested on my arm64 server , everything runs well.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2019-09-02T02:09:21Z",
        "closed_at": "2019-09-25T16:14:25Z",
        "merged_at": "2019-09-25T16:14:25Z",
        "body": "hi,antirez\r\n\r\nThe src/debug.c do not support aarch64 dump utcontext, so I add the aarch64 uc_mcontext debug dump info.\r\n\r\nThe content comes from the definition of the sigcontext and tested on my aarch64 server.\r\n\r\nsigcontext defined at the linux kernel code:   arch/arm64/include/uapi/asm/sigcontext.h\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2019-08-31T23:04:53Z",
        "closed_at": "2020-09-09T16:24:41Z",
        "merged_at": null,
        "body": "fix typos",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-08-29T09:41:01Z",
        "closed_at": "2019-09-03T17:57:19Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-08-29T07:14:27Z",
        "closed_at": "2019-11-21T08:58:12Z",
        "merged_at": "2019-11-21T08:58:12Z",
        "body": "Hey @antirez.\r\n\r\nI added 2 small functions to the redismodule.h api.\r\n\r\nThe first allows to get the allocated memory for a given pointer. I thought it will be a good idea that modules could create their own allocators and keep stats about the amount of memory used by them, so if we see high memory usage on a database that has multiple modules, we can easily see for each module, how many memory it consume.\r\n\r\nThe second return the amount of memory currently used (in percentage). It allows modules that have some GC mechanism (like redisearch) to make a better decision on when to start collecting garbage.\r\n\r\nLet me know what you think.\r\nThanks.",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-08-27T03:03:10Z",
        "closed_at": "2020-09-09T16:24:44Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-08-27T02:02:59Z",
        "closed_at": "2019-09-02T09:42:01Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2019-08-23T13:52:46Z",
        "closed_at": "2021-01-15T12:04:05Z",
        "merged_at": null,
        "body": "when we just run \"make test\" or \"make test-sentinel\" before \"make\"\r\nIt will fail by lack of depednecy.\r\n\r\nthis pr only add dependency for make target for test and test-sentinel.\r\n\r\nafter that it is ok. just run \"make test\" or \"make test-sentinel\" before make.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2019-08-23T12:12:30Z",
        "closed_at": "2020-03-12T13:28:36Z",
        "merged_at": null,
        "body": "As we use sentinel in some hostile environment, \r\nsentinels may send \"replica of\" to change a master to be it's own replica by mistake.\r\nThis pull request can avoid this and record in logfile.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 175,
        "deletions": 0,
        "changed_files": 7,
        "created_at": "2019-08-21T13:06:13Z",
        "closed_at": "2020-02-24T11:30:11Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 360,
        "deletions": 332,
        "changed_files": 1,
        "created_at": "2019-08-20T15:55:18Z",
        "closed_at": "2020-08-08T12:25:03Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-08-20T13:28:29Z",
        "closed_at": "2020-09-09T16:24:47Z",
        "merged_at": null,
        "body": "In the comments the prop is referenced as replica-validity-factor,\r\nbut it is really named cluster-replica-validity-factor.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-08-20T12:02:37Z",
        "closed_at": "2020-09-09T16:24:50Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-08-20T11:54:01Z",
        "closed_at": "2022-07-12T16:25:00Z",
        "merged_at": "2022-07-12T16:25:00Z",
        "body": "long lendiff = (enclen+backlen_size)-replaced_len\r\ndst+replaced_len+lendiff\r\n= det+replaced_len+(enclen+backlen_size)-replaced_len\r\n= dest+enclen+backlen_size\r\n\r\nafter this improvement, LP_BEFORE case and LP_REPLACE case is the same",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2019-08-20T02:07:15Z",
        "closed_at": "2019-09-02T09:49:46Z",
        "merged_at": "2019-09-02T09:49:46Z",
        "body": "Fix to https://github.com/antirez/redis/issues/4493 at 2.8 version",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-08-18T14:12:17Z",
        "closed_at": "2021-04-30T00:50:26Z",
        "merged_at": null,
        "body": "It turns out main function call `exit` after memtest, but memtest also called `exit`. \r\n\r\n```c\r\nmemtest(atoi(argv[2]),50);\r\nexit(0);\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 84,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2019-08-18T03:12:39Z",
        "closed_at": "2021-12-24T12:49:23Z",
        "merged_at": null,
        "body": "The following tests all passed\r\n\r\n                   The End\r\n\r\nExecution time of different units:\r\n  1 seconds - unit/type/incr\r\n  1 seconds - unit/printver\r\n  1 seconds - unit/auth\r\n  2 seconds - unit/keyspace\r\n  2 seconds - unit/protocol\r\n  1 seconds - unit/quit\r\n  2 seconds - unit/multi\r\n  4 seconds - unit/type/stream-cgroups\r\n  12 seconds - unit/type/hash\r\n  14 seconds - unit/type/list\r\n  13 seconds - unit/other\r\n  14 seconds - unit/scan\r\n  14 seconds - unit/expire\r\n  16 seconds - unit/type/string\r\n  16 seconds - unit/type/set\r\n  2 seconds - integration/convert-zipmap-hash-on-load\r\n  1 seconds - integration/logging\r\n  4 seconds - integration/rdb\r\n  8 seconds - integration/aof\r\n  1 seconds - unit/pubsub\r\n  26 seconds - unit/type/zset\r\n  3 seconds - unit/slowlog\r\n  27 seconds - unit/sort\r\n  0 seconds - unit/introspection\r\n  27 seconds - integration/block-repl\r\n  2 seconds - unit/limits\r\n  7 seconds - unit/introspection-2\r\n  12 seconds - unit/scripting\r\n  3 seconds - unit/bitfield\r\n  23 seconds - integration/psync2-reg\r\n  10 seconds - unit/bitops\r\n  36 seconds - integration/replication-2\r\n  51 seconds - unit/dump\r\n  51 seconds - unit/type/list-2\r\n  3 seconds - unit/lazyfree\r\n  35 seconds - integration/psync2\r\n  7 seconds - unit/wait\r\n  23 seconds - unit/pendingquerybuf\r\n  87 seconds - unit/type/stream\r\n  74 seconds - integration/replication-4\r\n  88 seconds - unit/aofrw\r\n  82 seconds - integration/replication-3\r\n  66 seconds - unit/geo\r\n  116 seconds - unit/type/list-3\r\n  73 seconds - unit/hyperloglog\r\n  114 seconds - integration/replication-psync\r\n  107 seconds - unit/maxmemory\r\n  134 seconds - integration/replication\r\n  108 seconds - unit/memefficiency\r\n  164 seconds - unit/obuf-limits\r\n\r\n\\o/ All tests passed without errors!\r\n\r\nCleanup: may take some time... OK\r\nmake[1]: Leaving directory '/home/wubo/redis/src'\r\n\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-08-14T02:14:00Z",
        "closed_at": "2020-07-11T19:51:45Z",
        "merged_at": "2020-07-11T19:51:45Z",
        "body": "1. fix description about ZIP_BIG_PREVLEN, this is similar to antirez#4705\r\n2. fix description about ziplist entry encoding field, the max length should be 2^32 - 1 when encoding is 5 bytes",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2019-08-13T14:20:32Z",
        "closed_at": "2020-09-09T16:24:54Z",
        "merged_at": null,
        "body": "fix typos",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-08-13T08:45:28Z",
        "closed_at": "2019-08-14T12:55:46Z",
        "merged_at": null,
        "body": "We update aof_fsync_offset when aof rewriting is finished.\r\nI think there is carelessness that `server.aof_current_size` is assigned again.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-08-10T20:05:39Z",
        "closed_at": "2022-06-23T15:37:43Z",
        "merged_at": "2022-06-23T15:37:42Z",
        "body": "I have added an example about how to pass configuration using `stdin`. I am thinking about expanding this example a little and make it shows how to add more than one config line or the current example is enough ?\r\n\r\nSigned-off-by: Mostafa Hussein <mostafa.hussein91@gmail.com>",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-08-08T11:54:12Z",
        "closed_at": "2019-10-04T09:56:08Z",
        "merged_at": "2019-10-04T09:56:08Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-08-07T10:07:17Z",
        "closed_at": "2019-08-22T18:02:17Z",
        "merged_at": "2019-08-22T18:02:17Z",
        "body": "@antirez this needs to be cherry picked to 5.0 too. (currently broken)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-08-05T20:39:13Z",
        "closed_at": "2019-09-30T15:43:59Z",
        "merged_at": "2019-09-30T15:43:59Z",
        "body": "In function clusterManagerMigrateKeysInReply, each key from a command's result is copied to the new command, but the function used to allocate memory is `sdsnew`. This command assumes the keys are NULL-terminated. \r\n\r\nBy changing it to `sdsnewlen` and passing the real key length, the issue is fixed.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 12,
        "changed_files": 11,
        "created_at": "2019-08-03T03:20:22Z",
        "closed_at": "2020-09-09T16:24:57Z",
        "merged_at": null,
        "body": "Fix misspellings of \"function\" in four places in the code.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-08-02T09:22:12Z",
        "closed_at": "2019-08-31T12:39:34Z",
        "merged_at": "2019-08-31T12:39:34Z",
        "body": "Now we use `clients_pending_write` list as write handler at first, then not all clients need install a write file event, so we can just check `replstate` and `repl_put_online_on_ack`.\r\n\r\n@antirez please check.",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 43,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2019-08-02T00:41:08Z",
        "closed_at": "2020-08-18T18:04:58Z",
        "merged_at": null,
        "body": "This adds the ability for `CLIENT SETNAME` and `CLIENT GETNAME` to target arbitrary connections in addition to the current connection. Specifically, it adds syntaxes of the form:\r\n```\r\nCLIENT SETNAME myname ID 5\r\nCLIENT GETNAME 5\r\n```\r\n... which respectively set and get the name of the connection with ID 5.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 9,
        "changed_files": 5,
        "created_at": "2019-08-01T07:02:20Z",
        "closed_at": "2019-08-05T11:45:33Z",
        "merged_at": null,
        "body": "file dict.c:\r\nremove unnecessary return value, it seems that this return value is useless\r\nfile lauxlib.c ltablib.c:\r\ncode format\r\nfile ldo.c:\r\nCompile warnings, remove unused variables\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-07-31T12:26:02Z",
        "closed_at": "2021-02-04T09:04:54Z",
        "merged_at": null,
        "body": "",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-07-29T22:13:52Z",
        "closed_at": "2019-07-30T08:24:29Z",
        "merged_at": "2019-07-30T08:24:28Z",
        "body": "The bounds checking was insufficient after the search loop causing us to corrupt data.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 44,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2019-07-29T07:14:24Z",
        "closed_at": "2021-02-16T14:17:39Z",
        "merged_at": "2021-02-16T14:17:39Z",
        "body": "See issue #6279.\r\n\r\n> EDITED for final solution and benchmark numbers.\r\n\r\n## Solution\r\n\r\n- Pre-allocate `min(stream-node-max-bytes, 4096)` bytes of memory for new listpack.\r\n- When current listpack reached `stream-node-max-entries` limit, reallocate the listpack to exact memory it needs.\r\n\r\n## Result\r\n```\r\n$ ./redis-server-new --save no\r\n$ redis-benchmark -P 100 -n 1000000 xadd test '*' a 1 b 2\r\n====== xadd test * a 1 b 2 ======\r\n  1000000 requests completed in 1.48 seconds\r\n  50 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n\r\n0.14% <= 1 milliseconds\r\n0.25% <= 3 milliseconds\r\n0.29% <= 4 milliseconds\r\n0.36% <= 5 milliseconds\r\n0.93% <= 6 milliseconds\r\n59.04% <= 7 milliseconds\r\n93.64% <= 8 milliseconds\r\n98.86% <= 9 milliseconds\r\n99.81% <= 10 milliseconds\r\n100.00% <= 10 milliseconds\r\n675675.69 requests per second\r\n```\r\n\r\n```\r\n$ ./redis-server-old --save no\r\n$ redis-benchmark -P 100 -n 1000000 xadd test '*' a 1 b 2\r\n====== xadd test * a 1 b 2 ======\r\n  1000000 requests completed in 1.68 seconds\r\n  50 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n\r\n0.01% <= 1 milliseconds\r\n0.04% <= 3 milliseconds\r\n0.05% <= 4 milliseconds\r\n0.33% <= 5 milliseconds\r\n0.35% <= 7 milliseconds\r\n62.48% <= 8 milliseconds\r\n93.05% <= 9 milliseconds\r\n98.55% <= 10 milliseconds\r\n99.32% <= 11 milliseconds\r\n99.98% <= 12 milliseconds\r\n100.00% <= 12 milliseconds\r\n595592.62 requests per second\r\n```",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-07-28T13:35:37Z",
        "closed_at": "2019-07-30T16:24:39Z",
        "merged_at": "2019-07-30T16:24:39Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 105,
        "deletions": 38,
        "changed_files": 5,
        "created_at": "2019-07-24T13:26:05Z",
        "closed_at": "2020-08-06T13:53:07Z",
        "merged_at": "2020-08-06T13:53:07Z",
        "body": "Diskless master has some inherent latencies.\r\n1) fork starts with delay from cron rather than immediately\r\n2) replica is put online only after an ACK. but the ACK\r\n   was sent only once a second.\r\n3) but even if it would arrive immediately, it will not\r\n   register in case cron didn't yet detect that the fork is done.\r\n\r\nBesides that, when a replica disconnects, it doesn't immediately\r\nattempts to re-connect, it waits for replication cron (one per second).\r\nin case it was already online, it may be important to try to re-connect\r\nas soon as possible, so that the backlog at the master doesn't vanish.\r\n\r\nIn case it disconnected during rdb transfer, one can argue that it's\r\nnot very important to re-connect immediately, but this is needed for the\r\n\"diskless loading short read\" test to be able to run 100 iterations in 5\r\nseconds, rather than 3 (waiting for replication cron re-connection)\r\n\r\nchanges in this commit:\r\n1) sync command starts a fork immediately if no sync_delay is configured\r\n2) replica sends REPLCONF ACK when done reading the rdb (rather than on 1s cron)\r\n3) when replconf ACK arrives, check check if the fork child terminated already\r\n4) when a replica unexpectedly disconnets, it immediately tries to\r\n   re-connect rather than waiting 1s\r\n5) when when a child exits, if there is another replica waiting, we spawn a new\r\n   one right away, instead of waiting for 1s replicationCron.",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 365,
        "deletions": 6,
        "changed_files": 9,
        "created_at": "2019-07-24T09:59:54Z",
        "closed_at": "2019-10-01T16:02:33Z",
        "merged_at": "2019-10-01T16:02:33Z",
        "body": "this implements #6012",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-07-24T08:23:08Z",
        "closed_at": "2019-07-30T09:41:49Z",
        "merged_at": "2019-07-30T09:41:49Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 29,
        "changed_files": 2,
        "created_at": "2019-07-23T06:05:00Z",
        "closed_at": "2019-07-31T09:12:47Z",
        "merged_at": "2019-07-31T09:12:47Z",
        "body": "This may be a little controversial of a PR. \r\n\r\nWe think that commands that are used to authenticate should not be propagated to the slowlog or monitor, as some users may unintentionally allow them without being aware that the password information is propagated there. Although these are listed as admin commands, I think people are much more likely to be given permission to execute them compared to something like the ACL or CONFIG command. ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2019-07-23T05:55:11Z",
        "closed_at": "2019-07-23T15:05:11Z",
        "merged_at": "2019-07-23T15:05:11Z",
        "body": "Noticed that in some places we create a redis object to stick on a clients output, when there are light weight functions available.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 431,
        "deletions": 18,
        "changed_files": 9,
        "created_at": "2019-07-22T18:23:58Z",
        "closed_at": "2019-07-30T09:50:45Z",
        "merged_at": "2019-07-30T09:50:45Z",
        "body": "Other changes:\r\n* fix memory leak in error handling of rdb loading of type OBJ_MODULE",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-07-22T09:51:48Z",
        "closed_at": "2019-07-31T09:54:12Z",
        "merged_at": "2019-07-31T09:54:12Z",
        "body": "this bug will let the usememory of slave bigger than master after fullsync.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2019-07-22T08:16:14Z",
        "closed_at": "2020-11-04T11:38:47Z",
        "merged_at": "2020-11-04T11:38:47Z",
        "body": "This will allow to use: `RedisModule_CreateStringPrintf(ctx, \"%s %c %s\", \"string1\", 0, \"string2\");`",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-07-19T05:45:27Z",
        "closed_at": "2019-12-12T08:27:48Z",
        "merged_at": "2019-12-12T08:27:48Z",
        "body": "This would fix issue #6220.  A simple check if the execute bit is set before loading a module.  This is a mitigation for unsecured Redis servers that are unlucky enough to be found by an attacker.\r\n\r\nIn the #6220 attack an RDB file is streamed from a remote server and placed on the disk of the victim.  Redis is then instructed to load this file as a module.  Since it doesn't check execute permissions it will load the file.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 53,
        "deletions": 32,
        "changed_files": 3,
        "created_at": "2019-07-18T08:47:52Z",
        "closed_at": "2019-07-18T14:41:38Z",
        "merged_at": "2019-07-18T14:41:38Z",
        "body": "Hi @antirez , I think Client side caching also needs tracking `flushdb`, send a special hash -1 to clients to tell them all keys are invalidate.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-07-17T12:11:12Z",
        "closed_at": "2019-07-25T16:48:47Z",
        "merged_at": "2019-07-25T16:48:47Z",
        "body": "for instance detached thread safe contexts, or various callbacks that don't\r\nprovide a context.\r\n\r\nthis one replaces #4188 (based on the comments there)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 373,
        "deletions": 91,
        "changed_files": 16,
        "created_at": "2019-07-17T11:18:57Z",
        "closed_at": "2019-09-27T09:24:45Z",
        "merged_at": "2019-09-27T09:24:45Z",
        "body": "* create module API for forking child processes.\r\n* refactor duplicate code around creating and tracking forks by AOF and RDB.\r\n* child processes listen to SIGUSR1 and dies exitFromChild in order to\r\n  eliminate a valgrind warning of unhandled signal.\r\n* note that BGSAVE error reply has changed.\r\n\r\nvalgrind error is:\r\n  Process terminating with default action of signal 10 (SIGUSR1)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-07-16T15:38:29Z",
        "closed_at": "2020-09-09T16:25:00Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-07-16T13:36:10Z",
        "closed_at": "2019-07-18T12:58:02Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2019-07-16T12:55:05Z",
        "closed_at": "2020-04-06T15:27:39Z",
        "merged_at": "2020-04-06T15:27:39Z",
        "body": "Hi @antirez , there are some scenarios that users wanna use lazyfree especially `UNLINK`, but it's difficult to change their project codes, so I think maybe we can expand the effect of `lazyfree-lazy-server-del` to `DEL` command.\r\n\r\nBTW, it's unnecessary to add a new configuration IMHO.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 250,
        "deletions": 68,
        "changed_files": 6,
        "created_at": "2019-07-16T08:05:05Z",
        "closed_at": "2019-07-21T05:59:50Z",
        "merged_at": null,
        "body": "now that replica can read rdb directly from the socket, it should avoid exiting\r\non short read and instead try to re-sync.\r\n\r\nthis commit tries to have minimal effects on non-diskless rdb reading.\r\nand includes a test that tries to trigger this scenario on various read cases.",
        "comments": 21
    },
    {
        "merged": true,
        "additions": 3452,
        "deletions": 785,
        "changed_files": 56,
        "created_at": "2019-07-15T07:00:54Z",
        "closed_at": "2019-10-16T15:42:35Z",
        "merged_at": "2019-10-16T15:42:35Z",
        "body": "Hi @antirez, @madolson. I've made some progress with the idea of abstracting connection handling in a way that should make it possible to do TLS transparently.  This is actually a bit more than a preliminary POC, as I believe the interface is complete and it passes all tests, but there are still some small improvements and gaps in the API.\r\n\r\nYou can find more info on why and how things are done as they are in `connection.c`. \r\n\r\nInitially I did this on top of the SSL branch, but replication was not working well and as the SSL branch contains non-trivial changes (which were required for lack of connection abstraction), I moved to unstable.\r\n\r\nIf we can agree on this approach I think the way forward should be pretty simple and 99% of the work will be around the connections module rather than Redis which is otherwise unchanged.\r\n\r\nThings left to do:\r\n* Some cleanups/fixes to the API around better error handling/reporting, allow handlers to request connection closing, etc.\r\n* Handle AE_BARRIER properly, currently not done.\r\n* Rebase to latest unstable (just realized now it was not up to date, pre-diskless).",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 147,
        "deletions": 9,
        "changed_files": 6,
        "created_at": "2019-07-14T14:37:14Z",
        "closed_at": "2019-09-26T09:52:43Z",
        "merged_at": "2019-09-26T09:52:42Z",
        "body": "This is especially needed in diskless loading, were a short read could have\r\ncaused redis to exit. now the module can handle the error and return to the\r\ncaller gracefully.\r\n\r\nthis fixes #5326",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-07-12T10:03:42Z",
        "closed_at": "2019-07-16T13:35:22Z",
        "merged_at": null,
        "body": "In the unix os, read and write system call can sometimes return a negative value if it was interrupted by a signal when they are manipulate a socket. In this case, just call the function again. It has been mentioned many times in the Stevens's book apue and unp1.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2019-07-11T17:09:39Z",
        "closed_at": "2020-08-06T12:00:22Z",
        "merged_at": null,
        "body": "Hashes have hmget, maybe just make hget varargs too? Probably a very similar change.",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 76078,
        "deletions": 2236,
        "changed_files": 380,
        "created_at": "2019-07-10T07:50:37Z",
        "closed_at": "2019-08-02T16:54:22Z",
        "merged_at": null,
        "body": "\u8bd5\u8bd5redis\u53ef\u89c6\u5316",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-07-09T13:42:58Z",
        "closed_at": "2019-07-10T07:35:42Z",
        "merged_at": null,
        "body": "I find this bug when I use the make tool to build the project.It prompts the use of undeclared identifier 'out_remainingBufferedData'.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-07-09T05:02:30Z",
        "closed_at": "2019-07-10T09:59:53Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-07-05T08:09:11Z",
        "closed_at": "2019-07-08T01:52:01Z",
        "merged_at": null,
        "body": "seems the 167 line of `src/Makefile` contains typo, and it make the compilation fail.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-07-04T07:04:04Z",
        "closed_at": "2019-07-10T17:43:24Z",
        "merged_at": "2019-07-10T17:43:24Z",
        "body": "these had severe impact for small zsets, for instance ones with just one\r\nelement that is longer than 64 (causing it not to be ziplist encoded)\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-07-02T13:35:23Z",
        "closed_at": "2019-07-12T11:08:46Z",
        "merged_at": "2019-07-12T11:08:46Z",
        "body": "Tiny edit, the correct double representation for -infinity is `,-inf\\r\\n`, not `-inf\\r\\n` - noticed that the `,` was missing from the negative infinity.\r\n\r\nSee https://github.com/antirez/RESP3/blob/aabbd6b14a9e8a75b8960e04f06720dc5cb94f44/spec.md#L257-L288",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 647,
        "deletions": 251,
        "changed_files": 17,
        "created_at": "2019-07-01T14:14:39Z",
        "closed_at": "2019-07-08T16:10:17Z",
        "merged_at": "2019-07-08T16:10:17Z",
        "body": "The implementation of the diskless replication was currently diskless only on the master side.\r\nThe slave side was still storing the received rdb file to the disk before loading it back in and parsing it.\r\n\r\nThis commit adds two modes to load rdb directly from socket:\r\n1) when-empty\r\n2) using \"swapdb\"\r\nthe third mode of using diskless slave by flushdb is risky and currently not included.\r\n\r\nother changes:\r\n--------------\r\ndistinguish between aof configuration and state so that we can re-enable aof only when sync eventually\r\nsucceeds (and not when exiting from readSyncBulkPayload after a failed attempt)\r\nalso a CONFIG GET and INFO during rdb loading would have lied\r\n\r\nWhen loading rdb from the network, don't kill the server on short read (that can be a network error)\r\n\r\nFix rdb check when performed on preamble AOF\r\n\r\ntests:\r\nrun replication tests for diskless slave too\r\nmake replication test a bit more aggressive\r\nAdd test for diskless load swapdb",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-06-26T14:20:26Z",
        "closed_at": "2020-02-27T16:40:52Z",
        "merged_at": "2020-02-27T16:40:52Z",
        "body": "Freeing time events when we delete event loop to avoid avoid valgrind warning. \r\nAnd I think that should be fixed, because it also can be used in other projects such as `redis-cluster-proxy`",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-06-25T15:37:00Z",
        "closed_at": "2021-06-28T13:18:09Z",
        "merged_at": null,
        "body": "when os memory is not enough, malloc may fail, cause sh set to null,\r\nif init param is null, memset will get null pointer,\r\nthat will trigger segment fault exception.\r\n\r\njust make the null pointer check earlier, will fix the issue.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-06-25T10:36:43Z",
        "closed_at": "2019-07-16T16:33:56Z",
        "merged_at": "2019-07-16T16:33:56Z",
        "body": "fix readme.md\uff0cRedis data types should add `t_stream.c`.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 906,
        "deletions": 196,
        "changed_files": 42,
        "created_at": "2019-06-24T15:14:17Z",
        "closed_at": "2020-08-11T12:41:31Z",
        "merged_at": null,
        "body": "Update on Jun 24",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2019-06-20T00:40:24Z",
        "closed_at": "2021-11-22T15:23:14Z",
        "merged_at": null,
        "body": "please check:\r\n1. set *ptr='\\0' once is enough in syncReadLine, will enhance the performance.\r\n2. change 'failver' to 'failover' ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-06-16T08:50:26Z",
        "closed_at": "2020-09-02T18:08:11Z",
        "merged_at": null,
        "body": "Fix docs",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-06-14T14:56:18Z",
        "closed_at": "2021-06-29T04:14:21Z",
        "merged_at": null,
        "body": "file: sds.c  function: sdsnewline\u3002\r\nDid not check NULL after s_malloc (sh = s_malloc(hdrlen+initlen+1))",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 89,
        "deletions": 265,
        "changed_files": 1,
        "created_at": "2019-06-13T19:29:55Z",
        "closed_at": "2019-07-07T15:13:12Z",
        "merged_at": "2019-07-07T15:13:12Z",
        "body": "The motivation here is that adding configs requires duplicating a lot of the same values across multiple locations. It's easy to miss or partially update some of them, see https://github.com/antirez/redis/pull/6153 for an example. This change tries to streamline that code path so no values are duplicated if there is no special handling. This should also make reviewing PRs with new configs easier.\r\n\r\nA natural follow up if this looks good is numeric and enums. The numeric types are the most numerous and least consistent, so it will reduce a lot of code to clean that up. \r\n\r\nAs a note, cluster-enabled was left as is since we talked about making this dynamic at redis conf. ",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-06-12T13:12:01Z",
        "closed_at": "2019-07-07T15:01:30Z",
        "merged_at": "2019-07-07T15:01:30Z",
        "body": "This PR extends `REDISMODULE_CTX_FLAGS` with an additional flag: `REDISMODULE_CTX_FLAGS_LOADING`\r\nindicating that Redis is currently loading, either from RDB or from AOF.\r\n\r\nReason why, In RedisGraph for example, commands such as `GRAPH.QUERY` are executed on dedicated threads using blocked clients,\r\nin the situation where Redis loads from AOF it will reply commands to modules (RedisGraph) which in turn will spawn new threads and block fake clients\r\nthis causes Redis to trigger an assertion.\r\n\r\nTo solve this modules can check for the `REDISMODULE_CTX_FLAGS_LOADING` flag and execute commands on Redis main thread.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2019-06-12T00:39:31Z",
        "closed_at": "2021-06-16T09:19:17Z",
        "merged_at": null,
        "body": "in this line:\r\n#define CLUSTER_NODE_NOFAILOVER 512 /* Slave will not try to failver. */",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-06-11T08:50:07Z",
        "closed_at": "2020-08-11T05:32:57Z",
        "merged_at": null,
        "body": "rewriteConfigNumericalOption(state,\"io-threads\",server.dbnum,CONFIG_DEFAULT_IO_THREADS_NUM);",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 13,
        "changed_files": 7,
        "created_at": "2019-06-11T01:03:54Z",
        "closed_at": "2019-12-19T08:28:30Z",
        "merged_at": null,
        "body": "Two locations in Redis where commands that are replicated as multiple commands are not done so atomically. This can lead to a state on a replica, or AOF in some cases, that was never seen on the master if the output is not fully flushed.\r\n\r\nThe SPOP was converted to a transactions of SREM commands incase you have a large set and are dumping a lot of items because you might overwhelm the querybuf if you send it as a single SREM command. This could be optimized more.\r\n\r\nFor the blocking brpoplpush, I don't entirely know why it's not propagated as just an rpoplpush, so I just stuck with the original implementation, but that may be something that is a simpler fix. ",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-06-10T00:47:19Z",
        "closed_at": "2019-06-12T00:27:25Z",
        "merged_at": null,
        "body": "Nice to meet u,I'm senior developer, I like research algorithms,  and very like redis, I want to contribute to enhance redis. \r\nCan I do it?",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 135,
        "deletions": 17,
        "changed_files": 9,
        "created_at": "2019-06-02T13:49:19Z",
        "closed_at": "2019-10-10T12:49:00Z",
        "merged_at": "2019-10-10T12:49:00Z",
        "body": "jemalloc 5 doesn't immediately release memory back to the OS, instead there's a decaying mechanism, which doesn't work when there's no traffic (no allocations).\r\nthis is most evident if there's no traffic after flushdb, the RSS will remain high.\r\n\r\n1) enable jemalloc background purging\r\n2) explicitly purge in flushdb\r\n3) bugfix in jemalloc in which a bg thread could have blocked our serverCron\r\n\r\np.s. i did many tests to check for regression (which is how i found 3 above).\r\nwhen we use a non-async flushdb, it already blocks redis for so long, so purging (which is what would happen with jemalloc 4), doesn't matter.\r\n\r\nwith flushdb async, after populating a large db, and running traffic, i couldn't observe any performance regression.\r\nand the most i've seen jemalloc thread use is some 50% CPU, while redis and the lazy free thread, each ate 99%.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 69,
        "deletions": 12,
        "changed_files": 7,
        "created_at": "2019-05-31T23:17:29Z",
        "closed_at": "2019-12-17T08:15:21Z",
        "merged_at": "2019-12-17T08:15:21Z",
        "body": "This is to address this issue we opened about allowing clients to continue to read from redis cluster mode during some edge cases.\r\n\r\nhttps://github.com/antirez/redis/issues/5890",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-31T02:16:36Z",
        "closed_at": "2020-09-09T16:25:03Z",
        "merged_at": null,
        "body": "fix cna't to can't",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-29T06:39:45Z",
        "closed_at": "2019-11-19T10:40:35Z",
        "merged_at": "2019-11-19T10:40:34Z",
        "body": "Signed-off-by: Yuan Zhou <yuan.zhou@intel.com>",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2019-05-28T16:06:53Z",
        "closed_at": "2020-09-09T16:25:07Z",
        "merged_at": null,
        "body": "s/familar/familiar/\r\ns/compatiblity/compatibility/\r\ns/ ot / to /\r\ns/itsef/itself/\r\n\r\nPS: Are you interested in typo fixes also for source code/tests? (I limited the fixes to docs + conf files in this PR)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-28T12:32:49Z",
        "closed_at": "2020-09-09T16:28:50Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-05-23T04:52:21Z",
        "closed_at": "2019-05-23T08:43:35Z",
        "merged_at": null,
        "body": "udpate loop condition of  \"case '[':\" in stringmatchlen to skip quick",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-22T18:32:16Z",
        "closed_at": "2021-03-14T16:09:44Z",
        "merged_at": null,
        "body": "small typo: custer->cluster",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 72,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2019-05-22T15:54:00Z",
        "closed_at": "2019-07-08T10:53:35Z",
        "merged_at": "2019-07-08T10:53:35Z",
        "body": "Extends the `SCAN` (but not `ZSCAN`, `HSCAN` etc.) API to\r\n\r\n```\r\nSCAN cursor [MATCH pattern] [COUNT count] [TYPE type]\r\n```\r\n\r\nThis allows scanning the DB for keys of a single type, which may be useful for finding streams, garbage collection, migrations etc.\r\n\r\nThe feature was suggested by @gkorland in #6107.\r\n\r\nThe argument `type` is the string you get when asking Redis for the `TYPE` of a key, so works with modules as well as the native types. A quirk to be aware of here is that the GEO commands are backed by ZSETs not their own type, so they're not distinguishable from other ZSETs. Similarly, Hyperloglogs, bitmaps and bit fields are backed by strings so are also not distinguishable.\r\n\r\nAdditionally, for sparse types that are rare in the DB, this has the same behaviour as `MATCH` in that it may return many empty results before giving something, even for large `COUNTs`.\r\n\r\nAlso adds tests to check basic functionality of this optional `TYPE` keyword, and also tested with a module (redisgraph). Checked quickly with valgrind, no issues.\r\n\r\nCopies name the type name canonicalisation code from `typeCommand`, perhaps this would be better factored out to prevent the two diverging and both needing to be edited to add new `OBJ_*` types, but this is a little fiddly/messy with C string, so I haven't addressed it.\r\n\r\nThe [redis-doc](https://github.com/antirez/redis-doc/blob/master/commands.json) repo will need to be updated with this new arg if accepted.\r\n\r\nMany thanks as always \ud83d\udc4d",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-05-21T13:26:06Z",
        "closed_at": "2020-01-10T12:06:41Z",
        "merged_at": "2020-01-10T12:06:41Z",
        "body": "We will rename `tmp-rdb-file` into `server.rdb_filename` when replica finish receiving rdb. It will cost much time if the `server.rdb_filename` file is very big, and more time more bigger. So we should deal it like rename `tmp-rewrite-aof`.\r\n\r\nThere are some tests.\r\n\r\nThe size of rdb file is 2.5G, no rename rdb asynchronously. It cost 0.71s when rename rdb.\r\n```\r\n2694:S 21 May 2019 18:18:18.418 * Trying a partial resynchronization (request bec6b0e986a30197968ef1c0ad8ef2c96e17fc83:1).\r\n2694:S 21 May 2019 18:18:18.419 * Full resync from master: d2a5fc716373104188aae130482eaa9bd0ac818e:17598\r\n2694:S 21 May 2019 18:18:18.419 * Discarding previously cached master state.\r\n2694:S 21 May 2019 18:18:18.484 * MASTER <-> REPLICA sync: receiving 182 bytes from master\r\n2694:S 21 May 2019 18:18:19.194 * Rename used time 0.710000s\r\n2694:S 21 May 2019 18:18:19.194 * MASTER <-> REPLICA sync: Flushing old data\r\n```\r\n\r\nThe size of rdb file is 50G, no rename rdb asynchronously. It cost 7.468s when rename rdb.\r\n```\r\n7798:S 21 May 2019 18:20:43.769 * Trying a partial resynchronization (request 1eea7ee888983824fd2014d66f85f64011b44591:1).\r\n7798:S 21 May 2019 18:20:43.769 * Full resync from master: d2a5fc716373104188aae130482eaa9bd0ac818e:17640\r\n7798:S 21 May 2019 18:20:43.769 * Discarding previously cached master state.\r\n7798:S 21 May 2019 18:20:43.870 * MASTER <-> REPLICA sync: receiving 182 bytes from master\r\n7798:S 21 May 2019 18:20:51.338 * Rename used time 7.468000s\r\n7798:S 21 May 2019 18:20:51.338 * MASTER <-> REPLICA sync: Flushing old data\r\n```\r\nRename rdb asynchronously. It almost cost no time even if the size of rdb file is more than 50G.\r\n``` \r\n27509:S 21 May 2019 18:25:08.391 * Trying a partial resynchronization (request 47baeccd7091b072d5c60707b29bcac6cb736164:1).\r\n27509:S 21 May 2019 18:25:08.391 * Full resync from master: d2a5fc716373104188aae130482eaa9bd0ac818e:17696\r\n27509:S 21 May 2019 18:25:08.391 * Discarding previously cached master state.\r\n27509:S 21 May 2019 18:25:08.484 * MASTER <-> REPLICA sync: receiving 182 bytes from master\r\n27509:S 21 May 2019 18:25:08.484 * Rename used time 0.000000s\r\n27509:S 21 May 2019 18:25:08.484 * MASTER <-> REPLICA sync: Flushing old data\r\n```",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-21T13:17:09Z",
        "closed_at": "2020-09-09T16:25:10Z",
        "merged_at": null,
        "body": "typo-fixes",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-21T08:47:57Z",
        "closed_at": "2021-06-28T13:19:18Z",
        "merged_at": null,
        "body": "So need not to judge",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-05-21T03:51:01Z",
        "closed_at": "2020-01-13T11:46:40Z",
        "merged_at": "2020-01-13T11:46:40Z",
        "body": "Hi @antirez , the Threaded IO feature has a good performance, but now it works in busy loop mode, that looks a bit wasteful, especially the main thread.\r\n\r\nAfter main thread distributes IO jobs to IO threads, it can only wait until all jobs done, do nothing but loop, here I think we can use the main thread's loop time to handle the IO jobs, just take the main thread as an IO thread at this time.\r\n\r\nI test this mode, in the write side can optimize 25% or more depends on the io-threads, but the read side doesn't optimize much, you know.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2019-05-17T10:09:04Z",
        "closed_at": "2020-08-18T18:13:42Z",
        "merged_at": null,
        "body": "Use the checkType function to finish the deal the error type, which can simplified this part.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-05-17T08:45:47Z",
        "closed_at": "2022-11-26T04:36:42Z",
        "merged_at": null,
        "body": "To avoid set a deleted time event as nearest",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-05-15T14:15:07Z",
        "closed_at": "2019-05-15T15:48:41Z",
        "merged_at": "2019-05-15T15:48:41Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-05-12T14:16:24Z",
        "closed_at": "2020-07-29T14:03:39Z",
        "merged_at": "2020-07-29T14:03:39Z",
        "body": "Followup on #6073, reading the code again it seems valid but not well documented so I've taken the liberty of updating the comment.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-05-11T12:25:33Z",
        "closed_at": "2019-09-02T09:45:41Z",
        "merged_at": null,
        "body": "**Problem:**\r\nWhen use old gcc version, compile operation goes wrong as following:\r\nIn file included from server.c:30:0:\r\nserver.h:1026:5: error: expected specifier-qualifier-list before \u2018_Atomic\u2019\r\n     _Atomic unsigned int lruclock; /* Clock for LRU eviction */\r\n\r\n**Environment**\r\ngcc --version:\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/lib64/gcc/x86_64-suse-linux/4.8/lto-wrapper\r\nTarget: x86_64-suse-linux\r\nConfigured with: ../configure --prefix=/usr --infodir=/usr/share/info --mandir=/usr/share/man --libdir=/usr/lib64 --libexecdir=/usr/lib64 --enable-languages=c,c++,objc,fortran,obj-c++,java,ada --enable-checking=release --with-gxx-include-dir=/usr/include/c++/4.8 --enable-ssp --disable-libssp --disable-plugin --with-bugurl=http://bugs.opensuse.org/ --with-pkgversion='SUSE Linux' --disable-libgcj --disable-libmudflap --with-slibdir=/lib64 --with-system-zlib --enable-__cxa_atexit --enable-libstdcxx-allocator=new --disable-libstdcxx-pch --enable-version-specific-runtime-libs --enable-linker-build-id --enable-linux-futex --program-suffix=-4.8 --without-system-libunwind --with-arch-32=i586 --with-tune=generic --build=x86_64-suse-linux --host=x86_64-suse-linux\r\nThread model: posix\r\ngcc version 4.8.5 (SUSE Linux)\r\n\r\n**Reason:**\r\nIn old gcc version like 4.7/8, there is a bug that miss the macro like [__STDC_NO_ATOMICS__\r\n[https://gcc.gnu.org/bugzilla/show_bug.cgi?id=53769](url)\r\n\r\nAnd in [https://github.com/antirez/redis/commit/dd5b105c73a02389987e457cebbeaa801ba16977](url), we have used the \"_Atomic\", so old gcc version, compile operation will go wrong.\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-10T08:28:06Z",
        "closed_at": "2019-05-10T10:22:19Z",
        "merged_at": "2019-05-10T10:22:19Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2019-05-08T11:37:27Z",
        "closed_at": "2019-05-10T10:23:43Z",
        "merged_at": "2019-05-10T10:23:43Z",
        "body": "#### 1. Implicit declaration of strcasecmp in hiredis/read.c\r\n\r\nThis code is in `deps/hiredis/read.c` but seemingly not in the [redis/hiredis](https://github.com/redis/hiredis) repo.\r\nIncluding `<strings.h>` clears the warning. See also [hiredis/#18](https://github.com/redis/hiredis/pull/18)\r\n\r\n```\r\nread.c: In function \u2018processLineItem\u2019:\r\nread.c:295:21: warning: implicit declaration of function \u2018strcasecmp\u2019; [-Wimplicit-function-declaration]\r\n                 if (strcasecmp(buf,\",inf\") == 0) {\r\n                     ^~~~~~~~~~\r\n```\r\n\r\n#### 2. Enlarge error buffer in redis-check-aof.c\r\n\r\nsnprintf into error (size 1024) has a format string allowing sizeof(__buff) (1024) plus the full expansion of `\"0x%16llx: \"` which adds a further 20 characters. `snprintf` would prevent an overflow but the compiler warns against this truncation; enalrging the buffer by 20 chars clears this warning.\r\n\r\nSee also #5940 and b78ac354f41e370a4dc21ac01981cb0ccd0a1b7d\r\n\r\n```\r\nredis-check-aof.c:37:36: warning: \u2018snprintf\u2019 output may be truncated before the last format character [-Wformat-truncation=]\r\n     snprintf(error, sizeof(error), \"0x%16llx: %s\", (long long)epos, __buf); \\\r\n                                    ^~~~~~~~~~~~~~\r\n```\r\n\r\n----\r\n\r\nAll tests pass bar the ongoing coincidental fail of Client output buffer hard limit is enforced in tests/unit/obuf-limits.tcl\r\n\r\n```\r\nLinux 5.0.5-200.fc29.x86_64 #1 SMP Wed Mar 27 20:58:04 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\ngcc (GCC) 8.3.1 20190223 (Red Hat 8.3.1-2)\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-05-08T08:32:05Z",
        "closed_at": "2019-05-10T10:24:00Z",
        "merged_at": "2019-05-10T10:24:00Z",
        "body": "Fix `uint64_t` convert to `unsigned int` result to incorrect hash value in `replaceSateliteDictKeyPtrAndOrDefragDictEntry`.\r\n\r\nThe result is sds pointer become invalid after active defrag and core dumped.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-08T04:55:05Z",
        "closed_at": "2019-05-10T10:25:19Z",
        "merged_at": "2019-05-10T10:25:19Z",
        "body": "Hi, @antirez\r\n\r\nIn the code, to get the size of ziplist, \"unsigned int bytes = ZIPLIST_HEADER_SIZE+1;\" is correct, \r\nbut why not make it more readable and easy to understand",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-07T06:44:50Z",
        "closed_at": "2019-05-10T10:28:13Z",
        "merged_at": "2019-05-10T10:28:13Z",
        "body": "When i reading this code i find the  comments may want to say pop from the HEAD or TAIL, i think here may be a mistype.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-07T05:39:18Z",
        "closed_at": "2019-05-10T10:37:08Z",
        "merged_at": "2019-05-10T10:37:08Z",
        "body": "Hi @antirez , I think `c11` should be included in `STD`, right?\r\n\r\nBTW, I tested the latest unstable branch with threaded-io, the performance promotes about twice, it's an amazing work!",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-05-06T11:34:34Z",
        "closed_at": "2019-05-10T10:27:48Z",
        "merged_at": "2019-05-10T10:27:48Z",
        "body": "sdsTest() defined in sds.c did not match the call in server.c, and `make CFLAGS=-DREDIS_TEST` will failed.\r\nRemove it from REDIS_TEST, since test-sds had defined in Makefile.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-06T04:36:22Z",
        "closed_at": "2019-05-06T10:42:05Z",
        "merged_at": null,
        "body": "Do not del twice when key expire",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-05-06T03:47:20Z",
        "closed_at": "2019-05-10T10:39:36Z",
        "merged_at": "2019-05-10T10:39:36Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-05-05T17:36:57Z",
        "closed_at": "2019-05-10T10:32:47Z",
        "merged_at": "2019-05-10T10:32:47Z",
        "body": "Currently `client->id` as returned by `RM_GetClientId()` is wrong for blocked clients.\r\n\r\nHi @antirez I came across unclear behavior in `RM_BlockClient()` which, if called from Lua or inside `MULTI` will allocate and return a blocked client but return an error.  As it is now it seems like a bug... I would expect the error to be immediate with no blocked client returned (NULL), is there some other (undocumented) intention to this behavior?",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 34,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2019-05-05T05:25:33Z",
        "closed_at": "2019-05-06T16:05:33Z",
        "merged_at": "2019-05-06T16:05:33Z",
        "body": "solving few replication related tests race conditions which fail on slow machines\r\n\r\nbugfix in slave buffers test: since the test is executed twice, each time with\r\na different commands count, the threshold for the delta can't be a constant.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 232248,
        "deletions": 24617,
        "changed_files": 904,
        "created_at": "2019-05-04T12:39:01Z",
        "closed_at": "2019-05-15T10:02:21Z",
        "merged_at": null,
        "body": "set local ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2019-05-03T04:24:41Z",
        "closed_at": "2019-05-05T00:04:04Z",
        "merged_at": null,
        "body": "Redis zset sort elements with the same value lexicographically.\r\nHowever, in some occasion, we need a fifo order. \r\n\r\nIf set the `zset-fifo-order` to `yes`,\r\n```\r\n127.0.0.1:6379> ZADD a 1 d 1 c 1 b\r\n(integer) 0\r\n127.0.0.1:6379> ZRANGE a 0 -1\r\n1) \"d\"\r\n2) \"c\"\r\n3) \"b\"\r\n```",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-04-30T08:46:48Z",
        "closed_at": "2022-09-15T02:18:16Z",
        "merged_at": null,
        "body": "repl-backlog-size  should never larger than maxmemory(maxmemory >0) ? The PR didn't resize the repl-backlog-size when maxmemory was shanked, it's also may exceed the maxmemory in this case.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-04-30T07:33:16Z",
        "closed_at": "2019-05-06T11:25:35Z",
        "merged_at": null,
        "body": "sdsjoin() passes return value of sdscat() directly to sdscat() as the first argument, \r\nSince sdscat() may return NULL, it should can be passed a NULL argument to.\r\n\r\nsdsjoinsds() has the same problem, change of sdscatlen() can fix it too.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 54,
        "deletions": 62,
        "changed_files": 19,
        "created_at": "2019-04-29T11:59:35Z",
        "closed_at": "2019-11-28T08:36:02Z",
        "merged_at": null,
        "body": "This (trivial) pull-request fixes a couple of typoes and spelling-mistakes in the main configuration file `redis.conf`, and several of the source files.\r\n\r\nIf this pull-request is useful, and not annoying, I will read through the rest of the code and look for more.  That seemed more reasonably than submitting one massive pull-request which might be too hard to review.\r\n\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-04-28T11:54:35Z",
        "closed_at": "2020-07-25T06:39:05Z",
        "merged_at": null,
        "body": "my last REDIS_GIT_DIRTY number was 30 , after I clean my repo, I still can not update my release.h;\r\n^_^",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-04-28T11:42:56Z",
        "closed_at": "2020-01-15T16:56:41Z",
        "merged_at": null,
        "body": "In **_aof.c:675_**\r\n\r\n`if (buf[0] != '$') goto fmterr;`\r\n\r\nIt will goto **_fmterr_** when the aof is bad file format, but the memory of argv isn't freed.  We should free fake client\u2018s argv firstly.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2019-04-28T09:48:08Z",
        "closed_at": "2019-05-10T16:06:41Z",
        "merged_at": "2019-05-10T16:06:41Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 151,
        "deletions": 51,
        "changed_files": 7,
        "created_at": "2019-04-27T16:38:25Z",
        "closed_at": "2019-12-19T07:54:23Z",
        "merged_at": "2019-12-19T07:54:23Z",
        "body": "This PR replaces #5963 (which contains a lot of discussion on the redis<->systemd interaction) and fixes #5826.\r\n\r\nI hope it's OK to take this easy way out of the mess I somehow managed to created in my old PR.",
        "comments": 35
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-04-26T11:56:15Z",
        "closed_at": "2019-05-02T07:48:03Z",
        "merged_at": "2019-05-02T07:48:03Z",
        "body": "\r\nbenchmark fails when server have requirepass configuration:\r\nvattezhang# ./redis-cli -a foobared set a b\r\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\r\nOK\r\nvattezhang# ./redis-benchmark -a foobared -c 1 -n 1 set a b\r\nERROR: NOAUTH Authentication required.\r\nERROR: failed to fetch CONFIG from 127.0.0.1:6379\r\n\r\nreason:\r\ngetRedisConfig(add in https://github.com/antirez/redis/commit/834809cbb34a3b9aa8ca2209ad09883a1a9239e2) function does not have auth action.\r\n\r\nafter fix result:\r\nvattezhang# ./redis-benchmark -a wrongpass -c 1 -n 1 set a b\r\nERROR: WRONGPASS invalid username-password pair\r\nERROR: failed to fetch CONFIG from 127.0.0.1:6379\r\nvattezhang# ./redis-benchmark -a foobared -c 1 -n 1 set a b\r\n====== set a b ======\r\n  1 requests completed in 0.00 seconds\r\n  1 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n  host configuration \"save\": 900 1 300 10 60 10000\r\n  host configuration \"appendonly\": no\r\n  multi-thread: no\r\n\r\n100.00% <= 0.0 milliseconds\r\ninf requests per second\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-04-26T10:01:01Z",
        "closed_at": "2019-05-23T06:39:13Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2019-04-25T08:26:45Z",
        "closed_at": "2019-04-26T15:02:17Z",
        "merged_at": "2019-04-26T15:02:17Z",
        "body": "As per the below issue redis cluster is pretty mush stable on production now\r\nhttps://github.com/antirez/redis/issues/5291\r\nHence removing the warning in redis.cong",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-04-23T12:13:53Z",
        "closed_at": "2019-04-26T15:17:56Z",
        "merged_at": "2019-04-26T15:17:56Z",
        "body": "redis-benchmark would core dump when the `-r` is the last arg\r\n\r\n\r\n```c\r\nCore was generated by `./redis-benchmark -p 7666 -a foobared -r 1000000'.\r\nProgram terminated with signal 11, Segmentation fault.\r\n#0  0x00007f1409bfad47 in ____strtoll_l_internal () from /lib64/libc.so.6\r\nMissing separate debuginfos, use: debuginfo-install glibc-2.17-157.el7.x86_64\r\n(gdb) bt\r\n#0  0x00007f1409bfad47 in ____strtoll_l_internal () from /lib64/libc.so.6\r\n#1  0x00000000004118b1 in parseOptions (argc=argc@entry=7, argv=argv@entry=0x7fffe938b378) at redis-benchmark.c:1297\r\n#2  0x000000000040c149 in main (argc=7, argv=0x7fffe938b378) at redis-benchmark.c:1495\r\n(gdb) f 1\r\n#1  0x00000000004118b1 in parseOptions (argc=argc@entry=7, argv=argv@entry=0x7fffe938b378) at redis-benchmark.c:1297\r\n1297\t            config.randomkeys_keyspacelen = atoi(argv[++i]);\r\n(gdb) p argv[i]\r\n$1 = 0x0\r\n```",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 406,
        "deletions": 56,
        "changed_files": 7,
        "created_at": "2019-04-23T10:01:40Z",
        "closed_at": "2019-04-26T15:19:18Z",
        "merged_at": null,
        "body": "ewfrtfyuikojp/;k'",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2019-04-20T10:05:30Z",
        "closed_at": "2020-08-11T07:56:19Z",
        "merged_at": null,
        "body": "Hi @antirez, recently I have a talk with @braveHeart1996 about PSYNC2 who opened an issue #6030 .\r\n\r\nAlthough the fix in PR #6031 is not right, but one interesting idea is that can we allow a master to load replication info?\r\n\r\nMaybe we can load the replication info as a secondary ID and offset, just like what `replicaof no one` does, give a chance to replicas to partial resynchronizations with the rebooted master.\r\n\r\nJust for discussion.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-04-18T03:54:55Z",
        "closed_at": "2020-11-04T13:37:55Z",
        "merged_at": null,
        "body": "1.Master node restart must be full asynchronous replication\r\n2.Master-slave switching, the slave node becomes the new master node, the original master node restarts, and only full-scale asynchronous replication can be performed.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-04-17T13:34:37Z",
        "closed_at": "2019-05-15T10:03:20Z",
        "merged_at": "2019-05-15T10:03:20Z",
        "body": "Master may send \"PING\" to slave when client pause will let the manual cluster failover timeout or the other slave do full sync with new master.",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 39,
        "deletions": 39,
        "changed_files": 2,
        "created_at": "2019-04-16T20:15:38Z",
        "closed_at": "2019-10-07T17:47:26Z",
        "merged_at": null,
        "body": "minor non-semantic grammatical changes",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2019-04-16T19:17:05Z",
        "closed_at": "2019-09-26T09:58:53Z",
        "merged_at": "2019-09-26T09:58:53Z",
        "body": "Fixes #6012.\r\n\r\nAs long as \"INFO is broken\", this should be adequate IMO. Once we rework\r\n`INFO`, perhaps into RESP3, this implementation should be revisited.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 36,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-04-16T15:01:30Z",
        "closed_at": "2020-08-30T13:35:48Z",
        "merged_at": null,
        "body": "Signed-off-by: Itamar Haber <itamar@redislabs.com>",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-04-16T14:17:09Z",
        "closed_at": "2019-07-05T18:25:19Z",
        "merged_at": "2019-07-05T18:25:18Z",
        "body": "Fixes #6021 \r\n\r\nSigned-off-by: Itamar Haber <itamar@redislabs.com>",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 41,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2019-04-14T20:27:32Z",
        "closed_at": "2021-12-28T14:11:51Z",
        "merged_at": null,
        "body": "hi there @antirez, this commit fixes the -r random keys option for redis-benchmark and adds xadd_1, xadd_5, xadd_10 redis streams benchmarks",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-04-11T17:05:12Z",
        "closed_at": "2019-04-26T15:25:21Z",
        "merged_at": "2019-04-26T15:25:21Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-04-09T11:21:30Z",
        "closed_at": "2020-08-11T06:11:27Z",
        "merged_at": "2020-08-11T06:11:27Z",
        "body": "Tends to issue #5999 - I've verified that the return values _ACL_OK_, _ACL_DENIED_CMD_ & _ACL_DENIED_KEY_ are actually being used by _ACLCheckCommandPerm_() and not by _ACLCheckUserCredentials_() from their definition in **acl.c**. Have changed the comment accordingly to avoid confusion.  Thanks!",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-04-09T11:14:24Z",
        "closed_at": "2020-08-03T10:02:56Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-04-09T01:26:48Z",
        "closed_at": "2019-04-26T15:31:42Z",
        "merged_at": "2019-04-26T15:31:42Z",
        "body": "Fix memory leak in bitfieldCommand if key type is not a string.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2019-04-06T01:52:10Z",
        "closed_at": "2019-09-09T09:46:38Z",
        "merged_at": null,
        "body": "1. fix hello command is invalid in non authenticated state. because we can also use hello designated  AUTH option\r\n```\r\nif (auth_required || DefaultUser->flags & USER_FLAG_DISABLED) {\r\n        /* AUTH and HELLO are valid even in non authenticated state. */\r\n        if (c->cmd->proc != authCommand && c->cmd->proc != helloCommand) {\r\n                // handle err\r\n        }         \r\n}\r\n```\r\n2. when calling ACLSetUser() to set user a password, we shound check if the length of password  is longer than CONFIG_AUTHPASS_MAX_LEN.\r\n```\r\n} else if (op[0] == '>') {\r\n        if (oplen-1 > CONFIG_AUTHPASS_MAX_LEN) {\r\n             // handle\r\n        }\r\n        // add password to list\r\n}\r\n```\r\n3. fix a typo\r\n```\r\nthree -> tree\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-04-04T07:21:19Z",
        "closed_at": "2021-12-06T12:18:32Z",
        "merged_at": null,
        "body": "when I use `redis-cli --intrinsic-latency 3600`, it exits immediately\u3002\r\nlike this: \r\n```\r\nMax latency so far: 1 microseconds.\r\n\r\n20 total runs (avg latency: -34748364.8000 microseconds / -34748364800.00 nanoseconds per run).\r\nWorst run took -0x longer than the average latency.\r\n```\r\n\r\nI try to print the value of variable 'run_time' , and find that it's a negative number\u3002\r\n```\r\nrun_time so far: -694967296 microseconds.\r\nstart so far: 1554346634769841 microseconds.\r\nend so far: 1554346634769844 microseconds.\r\nMax latency so far: 3 microseconds.\r\ntest_end so far: 1554345939802543 microseconds.\r\n\r\n1 total runs (avg latency: -694967296.0000 microseconds / -694967296000.00 nanoseconds per run).\r\nWorst run took -0x longer than the average latency.\r\n```\r\n\r\nI fix it by changing the code `run_time = (long long)config.intrinsic_latency_duration*1000000;`",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 51,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2019-04-03T18:26:02Z",
        "closed_at": "2019-05-10T16:08:16Z",
        "merged_at": null,
        "body": "Tail of the AOF may never be synced to disk (until shutdown)\r\n\r\nConsider the following scenario:\r\n1. server executes INCR x - it is fed to aof_buf, beforeSleep is called,\r\nflushAppendOnly, aof_buf is written to file and cleared, background\r\nfsync thread starts (and ends), server.aof_last_fsync is set.\r\n2. server executes INCR y (less than a second after INCR x) - it is fed\r\nto aof_buf, beforeSleep is called, flushAppendOnly, aof_buf is written\r\nto file and cleared, background fsync thread does not start because\r\nit's been less than a second since server.aof_last_fsync was set.\r\n3. on the next beforeSleep flushAppendOnly returns immediately\r\n because aof_buf is empty, without starting a sync and so on..\r\n\r\nIn order to fix the above we remember the last synced offset and\r\ncall fsync in flushAppendOnly if it's not as expected (different\r\nfrom aof_current_size) even if aof_buf is empty\r\n\r\nWe use atomic operation on aof_current_size and aof_fsync_offset\r\nbecause now they are read/written from 2 parallel threads (may\r\nnot be atomic in 32bit systems)",
        "comments": 20
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-04-03T09:55:03Z",
        "closed_at": "2021-12-16T07:36:00Z",
        "merged_at": null,
        "body": "Symptom\uff1a\r\nIn Production Environment,we found the used_memory of master(about 19G) is larger than the used_memory of slave (about 17G), but\r\nthe dbsize of them (about 68,000,000) is equal. After I run flushall there used_memory go down to equal.\r\nAnalysis\uff1a\r\nwhen the dbsize is almost reach to 67108864, slave ask for sync to master, at the same time clients send write command to master, the dbsize is just reach to 67108864, but now master's dict_can_resize = 0( because server.rdb_child_pid != -1), then expand is delayed, when rdb is finished, dbsize is larger than 67108864, then _dictNextPower of master become 67108864 * 4, but _dictNextPower of slave is 67108864 * 2.\r\nDifference = (67108864 * 4 - 67108864 * 2 ) * 8B * 2 (because expired_dict and dict)",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2019-04-03T04:07:55Z",
        "closed_at": "2020-01-01T15:34:46Z",
        "merged_at": null,
        "body": "These need to be compiled if you want the tests to pass since they now are dependent upon them.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-03-30T06:13:51Z",
        "closed_at": "2021-05-25T13:52:16Z",
        "merged_at": null,
        "body": "Check sh earlier will make code more safety and efficient, for filling a NULL pointer as the first parameter of memset function is dangerous.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-03-28T20:08:14Z",
        "closed_at": "2022-12-20T19:05:10Z",
        "merged_at": null,
        "body": "Currently in Redis Cluster mode, the \"slotToKeyUpdateKey\" function which is responsible for bookkeeping per-slot key counts is updating the counts before actual key insertion or deletion.     \r\n\r\nTherefore, if \"slotToKeyUpdateKey\" is called to insert an existing key or to delete a non-existing key, the per-slot key counts will not be accurate anymore. While there isn't a scenario where this could actually happen today, having the correctness of a function relies implicitly and entirely on the behaviors and decisions of its callers is not robust, hence the proposed change here.\r\n\r\nNan Yan\r\nAmazon",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-03-28T11:25:49Z",
        "closed_at": "2021-12-06T14:54:55Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-03-28T06:38:58Z",
        "closed_at": "2019-04-26T15:32:22Z",
        "merged_at": "2019-04-26T15:32:22Z",
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 132,
        "deletions": 2,
        "changed_files": 8,
        "created_at": "2019-03-27T00:05:55Z",
        "closed_at": "2020-02-21T16:24:05Z",
        "merged_at": null,
        "body": "This PR adds support for limiting full slave sync speed (via config option 'slave-sync-speed-bps', in bits per second). Limit is enforced on the slave side. Fixes issue #4815.\r\n\r\nSome details:\r\nIf config option is not specified, the behavior remains the same.\r\nOtherwise, the concept of budget is used to enforce the limit:\r\n- if budget is non-negative, reading is allowed\r\n- budget increases over time, proportionally to slave-sync-speed-bps and time passed, and it is capped at 50ms worth of slave-sync-speed-bps\r\n- budget decreases when data is read, proportionally to the size of data\r\n- while reading is not allowed, readable event handler is unregistered, instead timed event is registered and gets triggered every 10ms to update budget and re-register readable event handler if needed",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 199,
        "deletions": 93,
        "changed_files": 19,
        "created_at": "2019-03-24T15:52:42Z",
        "closed_at": "2019-05-28T19:09:42Z",
        "merged_at": null,
        "body": "I believe this fixes #5826 while also reducing implementation complexity by dynamically re-using `libsystemd.so` components.\r\n\r\nThe patch as implemented only changes behaviour if the configuration specifies redis to be supervised by systemd *explicitly*. If the server is supervised by systemd \"accidentally\" only (i.e. by `supervised auto` being set and the redis unit file specifying `Type=notify` for the service), the current behaviour of reporting readiness very early after startup is preserved. This should prevent users who aren't aware of the concequences of having a systemd `Type=notify` service in operation from having to deal with any changes.",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2019-03-24T12:23:23Z",
        "closed_at": "2019-04-10T16:41:37Z",
        "merged_at": "2019-04-10T16:41:37Z",
        "body": "when redis appends the blocked client reply list to the real client, it didn't\r\nbother to check if it is in fact the master client. so a slave executing that\r\nmodule command will send replies to the master, causing the master to send the\r\nslave error responses, which will mess up the replication offset\r\n(slave will advance it's replication offset, and the master does not)",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 64,
        "deletions": 29,
        "changed_files": 7,
        "created_at": "2019-03-24T10:18:52Z",
        "closed_at": "2019-04-11T14:56:24Z",
        "merged_at": "2019-04-11T14:56:24Z",
        "body": "Hi @antirez, this includes a small fix and cleans up the CommandFilter API tests to have a proper `runtest-moduleapi`, creating a pattern for additional Redis Module API tests in the future.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2019-03-22T17:14:21Z",
        "closed_at": "2019-09-12T11:06:28Z",
        "merged_at": null,
        "body": "You can set a password that is too big to be checked and therefor you can never log in with it. This just adds validation and returns an error if you try to set a password that is too big. \r\n\r\nAlso added stringify, which I think is a nice macro to have and removes the need for having duplicates for limits. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-03-22T01:34:04Z",
        "closed_at": "2020-12-17T19:58:59Z",
        "merged_at": "2020-12-17T19:58:59Z",
        "body": "In response to large client query buffer optimization introduced in https://github.com/antirez/redis/commit/1898e6ce7fad3fb1a5a6d469c281a23d0b000c7f. The calculation of the amount of remaining bytes we need to write to the query buffer was calculated wrong, as a result we are unnecessarily growing the client query buffer by sdslen(c->querybuf) always. This fix corrects that behavior. \r\n\r\nPlease note the previous behavior prior to the before-mentioned change was correctly calculating the remaining additional bytes, and this change makes that calculate to be consistent. ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-03-21T18:12:35Z",
        "closed_at": "2020-08-12T03:16:42Z",
        "merged_at": "2020-08-12T03:16:42Z",
        "body": "Fixes a GCC warning for printf, which expects a type char * and not void *",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-03-20T00:58:33Z",
        "closed_at": "2020-08-11T05:36:35Z",
        "merged_at": "2020-08-11T05:36:35Z",
        "body": "When a safe iterator is present, dictionary rehashing is supposed to be suspended.  However the ```dictRehashMilliseconds()``` function bypasses this check.  This probably doesn't affect any existing code as this function is only called on the main dictionary.  However for completeness/correctness this should be fixed.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 12,
        "changed_files": 5,
        "created_at": "2019-03-19T11:20:36Z",
        "closed_at": "2019-03-22T16:41:00Z",
        "merged_at": "2019-03-22T16:41:00Z",
        "body": "This PR adds the ability to be notified with keyspace notifications on keyspace misses, i.e. whenever a read command hits a non existing key. You are not notified on which command it was, just that there was a miss and the missing key. \r\n\r\nThe notification has the command \"miss\", and is published as a GENERIC notification. Both a key and keyspace notifications are published. It's worth discussing creating a special category flag for these notifications.\r\n\r\nI haven't found any TCL tests for keyspace notifications so I've tested it only by adding a test to the module API unit tests. \r\n\r\nOnce this is merged I'll update the documentation in redis-doc accordingly. ",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 527,
        "deletions": 9,
        "changed_files": 9,
        "created_at": "2019-03-19T09:08:30Z",
        "closed_at": "2019-03-22T16:43:50Z",
        "merged_at": "2019-03-22T16:43:50Z",
        "body": "This is an extension of the previous Command Filtering API experiment.\r\nSome highlights:\r\n\r\nThe command filtering callback intentionally does not expose a `RedisModuleCtx`, as it operates in a very narrow context that must not be accidentally abused by modules.  For example, we don't want `RM_Call()` in this context, etc.\r\n\r\nAll argument processing is done through helper functions, no internals are exposed (as was the case with the preliminary experiment code).\r\n\r\nTo keep it simple, the filters are traversed by order of registration.  Every command passes through all filters.\r\n\r\nThe filters are applied on all command paths, i.e. `ProcessCommand()`, Lua `redis.call()` and modules `RM_Call()`.\r\n\r\nI have taken an experimental approach into Module API testing by creating a `modules/` sample module that utilizes the new API commands and TCL tests to actually load and test it.  I think it should be fairly easy to extend this approach to quickly achieve good coverage of other Module API calls. If it makes sense, it would probably be a good idea to separate example modules from test-suite modules and always build the test-suite modules (e.g. to catch compile errors, even if `make test` is not run).",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-03-17T05:24:59Z",
        "closed_at": "2019-03-18T16:05:05Z",
        "merged_at": null,
        "body": "The error is used by sprintf with __buf, the size of __buf is 1024, and the size of error is also 1024.\r\nWhen the size of error message is near 1024, with the \"\"0x%16llx: %s\",  this will have stack overflow problem.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2019-03-16T07:27:27Z",
        "closed_at": "2020-09-09T16:25:13Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2019-03-16T05:20:55Z",
        "closed_at": "2019-04-06T01:17:38Z",
        "merged_at": null,
        "body": "The rax project also has this mistake.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 40,
        "changed_files": 1,
        "created_at": "2019-03-15T03:41:23Z",
        "closed_at": "2021-08-30T09:39:36Z",
        "merged_at": null,
        "body": "fewer lines with clearer code for src/redis-benchmark.c\r\nalready such usage in src/expire.c \"for (int j = 1; ***)\".",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2019-03-14T18:25:14Z",
        "closed_at": "2019-04-03T04:06:47Z",
        "merged_at": null,
        "body": "Not all cases of corrupt hyperloglog values are detected and can result in errors.  This beefs up the validation logic.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-03-14T14:00:36Z",
        "closed_at": "2020-07-29T12:24:02Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-03-14T06:58:36Z",
        "closed_at": "2022-02-14T07:36:36Z",
        "merged_at": null,
        "body": "Test:\r\n1,redis-cli config set appendonly yes\r\n2,dd ->txt.file //disk full\r\n3,redis-cli set key1 value1 //result: ok\r\n4,redis-cli set key2 value2 //result:err\r\n5,redis-cli config set appendonly no\r\n6,redis-cli config set appendonly yes\r\n7,redis-cli set key3 value3 //result:err\r\n8,rm txt.file  //disk free\r\n9,redis-cli set key4 value4 //result:err\r\n...error",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-03-13T17:40:21Z",
        "closed_at": "2019-10-16T09:13:12Z",
        "merged_at": "2019-10-16T09:13:12Z",
        "body": "Salvatore,\r\n\r\nWhen loading a module, `RedisModule_OnLoad` is called.  This update adds a corresponding `RedisModule_OnUnload` function.  This is necessary to release any memory allocated by the OnLoad function or persisted between commands or callbacks.  It also provides the ability for a module to decline to be unloaded (maybe because of a dependency or state issue).\r\n\r\nThe new `RedisModule_OnUnload` is **optional** for the module creator and is fully backward compatible with existing modules.\r\n\r\nI've produced a corresponding update to redis-doc.\r\n\r\nJim Brunner\r\nAmazon",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-03-13T06:23:24Z",
        "closed_at": "2019-03-13T10:52:08Z",
        "merged_at": "2019-03-13T10:52:08Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-03-12T14:10:24Z",
        "closed_at": "2019-03-13T11:18:09Z",
        "merged_at": "2019-03-13T11:18:09Z",
        "body": "The if condition in clusterManagerShowClusterInfo is:\r\nif (reply != NULL || reply->type == REDIS_REPLY_INTEGER)\r\n\r\nThe \"||\" will lead this judgement condition to the wrong path:\r\n1. if reply==NULL, the redis-cli will crash because of null pointer;\r\n2. if reply!=NULL, the \"reply->type == REDIS_REPLY_INTEGER\" will not be executed, and after that \"dbsize = reply->integer;\" will always be executed.\r\n\r\nI think \"&&\" should be here instead.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-03-12T12:48:46Z",
        "closed_at": "2019-03-12T16:03:02Z",
        "merged_at": "2019-03-12T16:03:02Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 652,
        "deletions": 32,
        "changed_files": 13,
        "created_at": "2019-03-11T19:25:57Z",
        "closed_at": "2019-12-17T08:58:27Z",
        "merged_at": "2019-12-17T08:58:27Z",
        "body": "What is the motivation for this change:\r\nWe think there is a usecase for support of different sort of authentication mechanisms that can be controlled via modules. Cloud providers will likely want to integrate this with whatever authentication mechanism they vend as a service, and other large enterprises may want to integrate with some feature so they can vend access for adhoc access or some other authentication service. \r\n\r\nThere are some minor refactoring I think would help, but tried to isolate this main change as much as possible for this PR so it was small and easier to review. (For instance I don't like the way unlink is being handled)\r\n\r\n```\r\n127.0.0.1:6379> module load src/modules/helloacl.so\r\nOK\r\n127.0.0.1:6379> acl setuser test allcommands on\r\nOK\r\n127.0.0.1:6379> HELLOACL.AUTHASYNC test\r\nOK\r\n127.0.0.1:6379> acl whoami\r\n\"test\"\r\n127.0.0.1:6379> HELLOACL.AUTHGLOBAL\r\nOK\r\n127.0.0.1:6379> HELLOACL.AUTHGLOBAL\r\n(error) Global user currently used\r\n127.0.0.1:6379> acl whoami\r\n\"global\"\r\n127.0.0.1:6379> HELLOACL.REVOKE\r\nOK\r\n127.0.0.1:6379> acl whoami\r\n\"default\"\r\n127.0.0.1:6379> HELLOACL.AUTHGLOBAL\r\nOK\r\n127.0.0.1:6379> HELLOACL.RESET\r\nOK\r\n127.0.0.1:6379> acl whoami\r\n\"default\"\r\n127.0.0.1:6379> HELLOACL.AUTHGLOBAL\r\nOK\r\n127.0.0.1:6379> module unload helloacl\r\nOK\r\n127.0.0.1:6379> \r\n127.0.0.1:6379> \r\n127.0.0.1:6379> ping\r\nPONG\r\n127.0.0.1:6379> \r\n```",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-03-11T08:14:28Z",
        "closed_at": "2019-03-12T16:17:20Z",
        "merged_at": "2019-03-12T16:17:20Z",
        "body": "Use-case, In RedisGraph the command `GRAPH.QUERY` handles both READ and WRITE queries as such the command is registered with the `write` flag.\r\n\r\nAs a result SLAVE instances can't handle the `GRAPH.QUERY` command unless we enable the `slave-read-only` configuration, in which case clients can send both read and **write** queries to a slave causing inconsistency.\r\n\r\nThis PR allows a SLAVE to inspect the source of an incoming request by checking a new flag: `REDISMODULE_CTX_FLAGS_REPLICATED`, this indicates that the command was sent as part of a replication by its MASTER, by doing so we're able to protect ourselves from reaching inconsistency while scaling out reads.\r\n\r\nA simpler approach would have been extending RedisGraph API by introducing a new command GRAPH.READ_QUERY but we prefer to keep our API to a minimum, also I believe others will benefit from this change.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 7122,
        "deletions": 1282,
        "changed_files": 86,
        "created_at": "2019-03-10T14:38:48Z",
        "closed_at": "2019-03-12T16:06:37Z",
        "merged_at": null,
        "body": "   sh = s_malloc(hdrlen+initlen+1);\r\n    if (init==SDS_NOINIT)\r\n        init = NULL;\r\n    else if (!init)\r\n        memset(sh, 0, hdrlen+initlen+1);\r\n    if (sh == NULL) return NULL;\r\n \r\n  If s_malloc return NULL and init != NULL,It wil cause memset memory overflow .\r\n So if(sh == NULL) return NULL should adjust position\r\n  \r\n   sh = s_malloc(hdrlen+initlen+1);\r\n   if (sh == NULL) return NULL;\r\n   if (init==SDS_NOINIT)\r\n        init = NULL;\r\n    else if (!init)\r\n        memset(sh, 0, hdrlen+initlen+1);",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-03-10T07:32:55Z",
        "closed_at": "2019-03-12T16:25:22Z",
        "merged_at": "2019-03-12T16:25:22Z",
        "body": "This commit will fix bufferoverflow or wrong semantic bug in the source file redis-cli.c #5908 . Can anyone help to check it? Thanks. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-03-09T16:06:04Z",
        "closed_at": "2019-03-10T08:45:56Z",
        "merged_at": "2019-03-10T08:45:56Z",
        "body": "Running the replicaof command when the client is already a replica puts us in a bad state.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-03-09T15:23:10Z",
        "closed_at": "2019-03-13T11:19:04Z",
        "merged_at": "2019-03-13T11:19:04Z",
        "body": "It will fail pretty quickly since there is no `-f` readlink flag there.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 50,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2019-03-08T17:11:23Z",
        "closed_at": "2019-03-13T10:55:47Z",
        "merged_at": "2019-03-13T10:55:47Z",
        "body": "The [`XCLAIM` docs](https://redis.io/commands/xclaim) state:\r\n\r\n> Moreover, as a side effect, **XCLAIM will increment the count of attempted deliveries of the message**. In this way messages that cannot be processed for some reason, for instance because the consumers crash attempting to process them, will start to have a larger counter and can be detected inside the system.\r\n\r\nThis PR makes the code match the documentation - with added semantics for `JUSTID` - whilst still allowing `RETRYCOUNT` to be specified manually. A [separate PR](antirez/redis-doc#1093) has been opened for clarifying documentation\r\n\r\nMy understanding of the way `streamPropagateXCLAIM()` works is that this change will safely propagate to replicas since retry count is pulled directly from the `streamNACK` struct.\r\n\r\nFixes #5194 ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2019-03-08T04:51:02Z",
        "closed_at": "2019-10-10T18:09:50Z",
        "merged_at": null,
        "body": "#5905  fix null pointer deference problem.\r\n@antirez ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-03-07T14:09:52Z",
        "closed_at": "2019-03-12T16:20:53Z",
        "merged_at": "2019-03-12T16:20:53Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-03-07T10:22:00Z",
        "closed_at": "2019-03-08T15:53:21Z",
        "merged_at": "2019-03-08T15:53:21Z",
        "body": "hashTypeTryObjectEncoding() is not used now",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 72,
        "deletions": 56,
        "changed_files": 1,
        "created_at": "2019-03-07T10:14:55Z",
        "closed_at": "2019-03-13T11:18:32Z",
        "merged_at": "2019-03-13T11:18:32Z",
        "body": "Fix issue #5891",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-03-07T09:39:43Z",
        "closed_at": "2019-03-12T16:21:17Z",
        "merged_at": "2019-03-12T16:21:17Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-03-07T02:28:21Z",
        "closed_at": "2019-03-13T11:26:44Z",
        "merged_at": "2019-03-13T11:26:44Z",
        "body": "On `make uninstall`, removes executables/symlink:\r\n\r\n- /usr/local/bin/redis-benchmark\r\n- /usr/local/bin/redis-check-aof\r\n- /usr/local/bin/redis-check-rdb\r\n- /usr/local/bin/redis-cli\r\n- /usr/local/bin/redis-sentinel\r\n- /usr/local/bin/redis-server\r\n\r\n(Only the src/ versions are removed in `make clean`.)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-03-05T20:57:04Z",
        "closed_at": "2019-03-09T15:58:03Z",
        "merged_at": null,
        "body": "Deferring clients prevents timing issues.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2019-03-04T11:43:44Z",
        "closed_at": "2019-03-13T10:47:50Z",
        "merged_at": "2019-03-13T10:47:49Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-03-03T21:12:17Z",
        "closed_at": "2019-03-13T11:27:36Z",
        "merged_at": "2019-03-13T11:27:36Z",
        "body": "Fixes #5892 \r\n\r\nShould be backported to v5 as well.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-03-01T07:32:34Z",
        "closed_at": "2019-03-13T11:22:24Z",
        "merged_at": "2019-03-13T11:22:24Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 167,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2019-02-28T16:13:03Z",
        "closed_at": "2019-03-01T15:51:03Z",
        "merged_at": "2019-03-01T15:51:03Z",
        "body": "Sometimes the cluster creation (using `redis-cli --cluster create`) cannot complete its task and gets stuck into an endless wait, since some nodes in the cluster could be unreachable during the \"join\" phase (when \"CLUSTER MEET\" command is invoked an all the nodes), ie. because their bus ports could be behind a firewall.\r\n\r\nThis PR introduces some checks that allow users to better understand what is happening, helping them to fix the problem.\r\nThese checks are performed every N seconds (the interval duration depends on the number of nodes).\r\nIf some node is found to be in handshaking or disconnected state after the \"waiting interval\", users are warned with an alert like the one below:\r\n\r\n```\r\nWarning: 1 node(s) may be unreachable\r\n - The port 17001 of node 127.0.0.1 may be unreachable from:\r\n   127.0.0.1:7000\r\n   localhost:7002\r\n   localhost:7003\r\n   127.0.0.1:7004\r\n   127.0.0.1:7005\r\n   localhost:7006\r\nCluster bus ports must be reachable by every node.\r\nRemember that cluster bus ports are different from standard instance port.\r\n```\r\n\r\nIf this kind of issues are detected, redis-cli keeps trying to join the cluster (in some cases the problem could automatically disappear after some time or the users could manually fix it).\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-02-28T10:26:13Z",
        "closed_at": "2019-03-12T16:25:58Z",
        "merged_at": "2019-03-12T16:25:58Z",
        "body": "I'd liked to use the redis-cli tool or the create-cluster script to setup a redis cluster without user interaction. This is not possible since redis-cli asks for confirmation from the user e.g before joining the cluster nodes.\r\nSince this makes it difficult to use the tool in test scripts I suggest to add an option to use the tool in a really non-interactive mode. I borrowed the idea from 'yum' that has a 'assume-yes' option.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2019-02-28T06:36:35Z",
        "closed_at": "2019-03-02T20:19:40Z",
        "merged_at": null,
        "body": "Unrecognized RDB AUX field: 'redis-ver'",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2019-02-27T13:39:30Z",
        "closed_at": "2019-02-27T14:45:04Z",
        "merged_at": "2019-02-27T14:45:04Z",
        "body": "As you change redis command table falgs in (https://github.com/antirez/redis/commit/9e4fb96ca12476b1c7468b143efca86b478bfb4a#diff-0235f01a49d01b35e981a41f59a9d2d6).\r\nI think we should also change sentinel command table falgs.\r\nAt the same time, maybe we should also add set flags operation in sentinel init process.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-02-27T09:55:26Z",
        "closed_at": "2019-03-01T16:29:20Z",
        "merged_at": "2019-03-01T16:29:20Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2019-02-26T22:22:48Z",
        "closed_at": "2019-03-01T15:55:10Z",
        "merged_at": "2019-03-01T15:55:10Z",
        "body": "You introduced a minor inaccuracy when integrating my last pull request. This fixes it.\r\n\r\n % ./src/redis-benchmark -c 100 -n 1000000 -P 5 set a 5\r\n====== set a 5 ======\r\n  1000000 requests completed in 4.09 seconds\r\n  100 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n\r\n0.00% <= 0.5 milliseconds\r\n0.01% <= 0.6 milliseconds\r\n0.17% <= 0.7 milliseconds\r\n1.38% <= 0.8 milliseconds\r\n4.24% <= 0.9 milliseconds\r\n8.66% <= 1.0 milliseconds\r\n14.89% <= 1.1 milliseconds\r\n20.72% <= 1.2 milliseconds\r\n26.50% <= 1.3 milliseconds\r\n32.85% <= 1.4 milliseconds\r\n39.99% <= 1.5 milliseconds\r\n47.55% <= 1.6 milliseconds\r\n56.15% <= 1.7 milliseconds\r\n64.85% <= 1.8 milliseconds\r\n72.33% <= 1.9 milliseconds\r\n78.13% <= 2.0 milliseconds\r\n82.75% <= 2.1 milliseconds <----- This guy being here makes the below one wrong\r\n82.75% <= 2 milliseconds\r\n99.81% <= 3 milliseconds\r\n99.94% <= 4 milliseconds\r\n99.95% <= 16 milliseconds\r\n99.96% <= 17 milliseconds\r\n99.98% <= 18 milliseconds\r\n100.00% <= 19 milliseconds\r\n100.00% <= 19 milliseconds\r\n244498.77 requests per second\r\nTo\r\n\r\n\r\nWith change\r\n% ./src/redis-benchmark --precision 1 -c 100 -n 1000000 -P 5 set a 5\r\n====== set a 5 ======\r\n  1000000 requests completed in 3.91 seconds\r\n  100 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n\r\n0.00% <= 0.5 milliseconds\r\n0.01% <= 0.6 milliseconds\r\n0.31% <= 0.7 milliseconds\r\n2.22% <= 0.8 milliseconds\r\n6.36% <= 0.9 milliseconds\r\n12.76% <= 1.0 milliseconds\r\n22.33% <= 1.1 milliseconds\r\n30.76% <= 1.2 milliseconds\r\n38.03% <= 1.3 milliseconds\r\n45.16% <= 1.4 milliseconds\r\n52.57% <= 1.5 milliseconds\r\n59.81% <= 1.6 milliseconds\r\n67.43% <= 1.7 milliseconds\r\n74.60% <= 1.8 milliseconds\r\n80.36% <= 1.9 milliseconds\r\n84.73% <= 2 milliseconds\r\n99.85% <= 3 milliseconds\r\n99.99% <= 4 milliseconds\r\n100.00% <= 4 milliseconds\r\n255754.47 requests per second",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-02-26T22:17:27Z",
        "closed_at": "2019-02-27T08:31:58Z",
        "merged_at": "2019-02-27T08:31:58Z",
        "body": "This is similar to the ACL DELUSER change. ACL SETUSER will partially apply changes when there is an error. This gives an ambiguous response back to user and may cause unintended behavior.\r\n\r\nThis implementation is a lot slower, (~220K RPS vs ~150K RPS) but I think it is a clean way to do it and is comparable to the AOF implementation. Another implementation would be to copy the user at the beginning and on error copy everything back if there is an error and delete it if there is a new user. It's faster (~190K RPS) but I generally think doing all validation then applying is cleaner.\r\n\r\n% redis-cli\r\n127.0.0.1:6379> acl list\r\n1) \"user bob2 off -@all\"\r\n2) \"user default on nopass ~* +@all\"\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* +@all\"\r\n127.0.0.1:6379> \r\n127.0.0.1:6379> \r\n127.0.0.1:6379> acl setuser test1\r\nOK\r\n127.0.0.1:6379> acl setuser test1 +get +set\r\nOK\r\n127.0.0.1:6379> acl setuser test2 +get +set\r\nOK\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* +@all\"\r\n2) \"user test1 off -@all +get +set\"\r\n3) \"user test2 off -@all +get +set\"\r\n127.0.0.1:6379> acl setuser test3 +get +wrong\r\n(error) ERR Error in ACL SETUSER modifier '+wrong': Unknown command or category name in ACL\r\n127.0.0.1:6379> acl setuser test4\r\nOK\r\n127.0.0.1:6379> acl setuser test4 +get +wrong\r\n(error) ERR Error in ACL SETUSER modifier '+wrong': Unknown command or category name in ACL\r\n127.0.0.1:6379> acl list\r\n1) \"user default on nopass ~* +@all\"\r\n2) \"user test1 off -@all +get +set\"\r\n3) \"user test2 off -@all +get +set\"\r\n4) \"user test4 off -@all\"\r\n127.0.0.1:6379>",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-02-26T02:37:46Z",
        "closed_at": "2019-03-01T15:57:14Z",
        "merged_at": "2019-03-01T15:57:14Z",
        "body": "When you execute a binary program, the name of the RDB file is not valid parameter.\r\nBecause the program wrote the name dump.rdb as fix code. \r\nI made some bugfix, let the name of input rdb file name be valid.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-02-23T09:01:59Z",
        "closed_at": "2020-06-04T13:23:08Z",
        "merged_at": null,
        "body": "Is it a typo?",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 17,
        "changed_files": 1,
        "created_at": "2019-02-21T21:42:38Z",
        "closed_at": "2019-02-22T09:25:18Z",
        "merged_at": "2019-02-22T09:25:18Z",
        "body": "The logic here is the same as the logic used for computing string length. This change just removes the duplicated logic. The stringObjectLen also has a check to make sure a string object is passed in which is missing here.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-02-21T21:39:55Z",
        "closed_at": "2019-02-22T09:27:37Z",
        "merged_at": "2019-02-22T09:27:37Z",
        "body": "If a client attempts to delete multiple user, and one of them is the default, it will delete all the users in the list before default but not after. I think this is inconsistent behavior and having any user being the default should cause no users to be deleted and an error get returned.\r\n\r\nAlso, documentation wasn't consistent with implementation.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-02-20T14:38:38Z",
        "closed_at": "2019-02-22T09:28:31Z",
        "merged_at": "2019-02-22T09:28:31Z",
        "body": "Fix issue #5849",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-02-18T14:53:04Z",
        "closed_at": "2019-04-26T10:45:30Z",
        "merged_at": null,
        "body": "Sometimes we forget to run benchmark with -a or with wrong requirepass, benchmark should exit and warn us.   \r\nWhen we run benchmark but forget to set the right requirepass, benchmark should return error.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-02-18T08:35:58Z",
        "closed_at": "2021-12-03T12:11:30Z",
        "merged_at": null,
        "body": "If the file is not synchronized, it might be truncated (or empty)\r\nif the system is restarted forcefully just after the replication\r\nhas been finished.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 45,
        "deletions": 5,
        "changed_files": 5,
        "created_at": "2019-02-15T23:29:05Z",
        "closed_at": "2021-12-15T09:07:11Z",
        "merged_at": null,
        "body": " - Add option for number keys looked at on each iteration and option to configure if the loop should keep trying\r\n\r\n - Update example configuration",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2019-02-13T19:59:56Z",
        "closed_at": "2019-02-14T12:01:27Z",
        "merged_at": "2019-02-14T12:01:27Z",
        "body": "Gives a slightly more useful result if you care about sub ms results, the default is the same behavior as before. \r\n\r\n./src/redis-benchmark -t set --precision 1 -n 500 -P 1\r\n====== SET ======\r\n  500 requests completed in 0.01 seconds\r\n  50 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n\r\n0.20% <= 0.2 milliseconds\r\n0.80% <= 0.3 milliseconds\r\n30.80% <= 0.4 milliseconds\r\n84.80% <= 0.5 milliseconds\r\n94.60% <= 0.6 milliseconds\r\n98.60% <= 0.7 milliseconds\r\n100.00% <= 0.7 milliseconds\r\n55555.56 requests per second\r\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-02-13T06:51:13Z",
        "closed_at": "2019-06-04T08:18:24Z",
        "merged_at": null,
        "body": "fix typo in comment: encoidng -> encoding",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-02-13T06:41:22Z",
        "closed_at": "2019-02-15T06:38:45Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2019-02-13T03:47:37Z",
        "closed_at": "2019-02-13T18:00:00Z",
        "merged_at": "2019-02-13T18:00:00Z",
        "body": "",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2019-02-12T13:23:22Z",
        "closed_at": "2019-03-14T11:41:32Z",
        "merged_at": "2019-03-14T11:41:32Z",
        "body": "In some cases processMultibulkBuffer uses sdsMakeRoomFor to\r\nexpand the querybuf, but later in some cases it uses that query\r\nbuffer as is for an argv element (see \"Optimization\"), which means\r\nthat the sds in argv may have a lot of wasted space, and then in case\r\nmodules keep that argv RedisString inside their data structure, this\r\nspace waste will remain for long (until restarted from rdb).",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 41,
        "deletions": 35,
        "changed_files": 3,
        "created_at": "2019-02-11T14:36:30Z",
        "closed_at": "2019-02-12T08:45:11Z",
        "merged_at": null,
        "body": "After command `ACL LOAD`, we should kill the old users clients just like what `deluser` does, or redis will crash.\r\n\r\nMoreover, the subcommand `ACL LOAD` is dangerous IMO, so maybe move it to `debug` command is better I think.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2019-01-31T03:54:18Z",
        "closed_at": "2019-01-31T07:20:55Z",
        "merged_at": "2019-01-31T07:20:55Z",
        "body": "",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2019-01-30T19:32:17Z",
        "closed_at": "2020-09-09T16:25:16Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2019-01-30T03:13:41Z",
        "closed_at": "2019-12-18T11:01:17Z",
        "merged_at": null,
        "body": "Fix #5256",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2019-01-28T16:06:18Z",
        "closed_at": "2019-03-14T10:35:02Z",
        "merged_at": "2019-03-14T10:35:01Z",
        "body": "The string representation of `long double` may take\r\nup to ~5000 chars (see PR #3745).\r\n\r\nBefore this fix HINCRBYFLOAT would never overflow (since\r\nthe string could not exceed 256 chars). Now it can.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-01-25T14:23:35Z",
        "closed_at": "2019-01-27T11:16:14Z",
        "merged_at": "2019-01-27T11:16:14Z",
        "body": "Please check #5807 ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2019-01-24T16:31:40Z",
        "closed_at": "2019-01-25T10:47:46Z",
        "merged_at": "2019-01-25T10:47:46Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-01-23T15:57:49Z",
        "closed_at": "2019-01-25T10:27:22Z",
        "merged_at": "2019-01-25T10:27:22Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-01-23T09:16:24Z",
        "closed_at": "2019-03-14T10:29:25Z",
        "merged_at": "2019-03-14T10:29:25Z",
        "body": "Instead of popping the entire zset. This is actually a bug caused by `while (--count)` which never ends if count=0 (server is not stuck in an infinite loop because it breaks the loop if all elements were popped)\r\n\r\nbefore this commit:\r\n```\r\n127.0.0.1:6379> ZADD z 1 m1 2 m2 3 m3\r\n(integer) 3\r\n127.0.0.1:6379> ZPOPMAX z 0\r\n1) \"m3\"\r\n2) \"3\"\r\n3) \"m2\"\r\n4) \"2\"\r\n5) \"m1\"\r\n6) \"1\"\r\n```\r\n\r\nafter:\r\n```\r\n127.0.0.1:6379> ZADD z 1 m1 2 m2 3 m3\r\n(integer) 3\r\n127.0.0.1:6379> ZPOPMAX z 0\r\n(nil)\r\n```",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2019-01-21T06:54:19Z",
        "closed_at": "2019-01-21T16:14:04Z",
        "merged_at": "2019-01-21T16:14:04Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-01-19T03:52:03Z",
        "closed_at": "2019-01-22T16:28:46Z",
        "merged_at": "2019-01-22T16:28:46Z",
        "body": "Both `sdscat` and `dictReplace` are free `otypes`. This issue can be detected by AddressSanitizer.\r\n\r\nBelow is a clang AddressSanitizer patch for Makefile. Reproduce crash by command `redis-cli --cluster create 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002`.\r\n\r\n--------------------------------------------------------------------------------\r\ndiff --git a/src/Makefile b/src/Makefile                                                                          \r\nindex adf32d55..a8909577 100644                                  \r\n--- a/src/Makefile                                                             \r\n+++ b/src/Makefile                            \r\n@@ -73,7 +73,9 @@ endif\r\n -include .make-settings\r\n\r\n FINAL_CFLAGS=$(STD) $(WARN) $(OPT) $(DEBUG) $(CFLAGS) $(REDIS_CFLAGS)\r\n+FINAL_CFLAGS+=-fsanitize=address\r\n FINAL_LDFLAGS=$(LDFLAGS) $(REDIS_LDFLAGS) $(DEBUG)\r\n+FINAL_LDFLAGS+=-fsanitize=address\r\n FINAL_LIBS=-lm\r\n DEBUG=-g -ggdb",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2019-01-18T15:58:04Z",
        "closed_at": "2020-08-25T12:58:51Z",
        "merged_at": "2020-08-25T12:58:51Z",
        "body": "Otherwise, it is treated as a single allocation and freed synchronously.\r\n\r\nSigned-off-by: Itamar Haber <itamar@redislabs.com>",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2019-01-17T15:08:52Z",
        "closed_at": "2019-01-25T01:11:26Z",
        "merged_at": null,
        "body": "see the #5790",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2019-01-16T11:22:27Z",
        "closed_at": "2019-03-13T11:34:30Z",
        "merged_at": "2019-03-13T11:34:30Z",
        "body": "Fix issue #5785, in case create group on a key is not stream.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-01-16T08:40:13Z",
        "closed_at": "2020-07-10T05:20:28Z",
        "merged_at": "2020-07-10T05:20:28Z",
        "body": "For example:\r\n\r\n    BITOP not targetkey sourcekey\r\n\r\nIf `targetkey` and `sourcekey` doesn't exist, `BITOP` has no effect,\r\nwe do not propagate it, thus can save aof and replica flow.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-01-15T12:15:06Z",
        "closed_at": "2021-12-03T19:34:19Z",
        "merged_at": null,
        "body": "Hi,i find a problem.\r\nThe ae.c file,aeSearchNearestTimer method comment i think is error.\r\nThe algorithmic complexity of the skiplist lookup operation is O(log(N)).\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2019-01-15T12:09:33Z",
        "closed_at": "2019-12-19T08:06:29Z",
        "merged_at": "2019-12-19T08:06:29Z",
        "body": "Hi @antirez and @yossigo , we have talked about this in #5323, I think we should discuss it deeply now.\r\n\r\nTo avoid nested `MULTI/EXEC` in PR #4467, we check the `lua_caller`'s flag, if we are in the MULTI context we flag the `lua_client` as CLIENT_MULTI.\r\n\r\nBut it's not enough, we shoud flag `lua_client` as `CLIENT_MULTI` after `redis.replicate_commands()` immediately, or the first write command after `redis.replicate_commands()` cannot know it's in an transaction.\r\n\r\nI know the missing `CLIENT_MULTI` doesn't have any effect now, but it's a real bug and we should fix it, in case someday we allow some dangerous command like `BLPOP`.\r\n\r\nMoreover, reflect `MULTI` state in every `luaRedisGenericCommand()` is not clear and not easy to understand, I think we can change `lua_client`'s flag in a more explicit way, like in `evalGenericCommand()`.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-01-15T07:30:38Z",
        "closed_at": "2019-07-15T21:41:57Z",
        "merged_at": "2019-07-15T21:41:57Z",
        "body": "Found while building one day.\r\n\r\nI would imagine they would occur the other direction, but after 3k iterations locally only the case where redis rounds up showed up. ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2587,
        "deletions": 96,
        "changed_files": 47,
        "created_at": "2019-01-15T06:06:55Z",
        "closed_at": "2019-10-16T23:26:02Z",
        "merged_at": null,
        "body": "This is a POC implementation that antirez mentioned in https://github.com/antirez/redis/pull/4855\r\n\r\nNot sure if this is the best way to place this here so that you can look at them together, I'm going to post my summary on the other thread. So I don't intend to update this.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2019-01-12T07:28:34Z",
        "closed_at": "2020-11-17T16:35:57Z",
        "merged_at": "2020-11-17T16:35:56Z",
        "body": "Hi @antirez , there is a place optimized.\r\nThe edge case, where the offset exceeds the length of the zset. This is an invalid offset. It should be better to return directly. see issue #5767 ",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-01-12T06:49:02Z",
        "closed_at": "2019-01-16T05:10:58Z",
        "merged_at": null,
        "body": "There are two issues in ACLInit to call ACLSetUser.\r\n1] using allkeys instead of \"+@all\"\r\n -> it is not +@all, in code, there is \"allkeys\", so I changed. but in RCP1, I think it is better to use +#all\r\n\r\n2] invalid reset code\r\n -> I think memset needs allowed_commands, not allowed_subcommand. so it can cause crash.\r\n\r\n@antirez \r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2019-01-12T05:08:43Z",
        "closed_at": "2019-03-12T16:06:30Z",
        "merged_at": null,
        "body": "I think the validity check on pointer sh should be followed by s_malloc.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-01-06T07:03:09Z",
        "closed_at": "2019-01-09T09:10:54Z",
        "merged_at": "2019-01-09T09:10:54Z",
        "body": "Update comment",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2019-01-04T11:55:04Z",
        "closed_at": "2020-09-09T16:25:19Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2019-01-03T12:48:18Z",
        "closed_at": "2019-01-09T09:12:10Z",
        "merged_at": "2019-01-09T09:12:10Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2019-01-03T11:11:16Z",
        "closed_at": "2020-12-17T12:49:20Z",
        "merged_at": "2020-12-17T12:49:20Z",
        "body": "replaces #3494 \r\n/cc @antirez @itamarhaber ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2019-01-02T19:29:31Z",
        "closed_at": "2019-01-22T16:29:38Z",
        "merged_at": "2019-01-22T16:29:38Z",
        "body": "This addresses two problems, one where infinite (negative) repeat count is broken for all types for Redis, and another problem specific to cluster mode (where redirection is needed).\r\n\r\nThis fix now allows for and works correctly with negative (i.e. -1) repeat values passed with `-r` argument to redis-cli as documented [here](https://redis.io/topics/rediscli#continuously-run-the-same-command).\r\n\r\nThis broken behaviour exists currently (e504583b7806d946da9c3627784d551a742be4d0), and redis-cli will just exit immediately with repeat `-r <= 0` as opposed to send commands indefinitely as it should with `-r < 0`\r\n\r\nThe repeat feature seems to have regressed as a in 95b988b6c69083fff3e00271653c2239d482ea0d (though that commit removed a would-be long integer wrap-around to error) I've commented to hopefully prevent this happening again...\r\n\r\nAdditionally, this fix prevents a `repeat * interval` seconds hang spent doing nothing at the start before issuing commands in cluster mode (`-c`), where the command needed to redirect to a slot on another node, as commands were failing and waiting to be reissued but the sequence was fully repeated before being reissued. For example,\r\n\r\n        redis-cli -c -r 10 -i 0.5 INCR test_key_not_on_6379\r\n\r\nWould wrongly hang and show nothing for 5 seconds (10 * 0.5) before showing\r\n\r\n        (integer) 1\r\n        (integer) 2\r\n        (integer) 3\r\n        (integer) 4\r\n        (integer) 5\r\n        (integer) 6\r\n        (integer) 7\r\n        (integer) 8\r\n        (integer) 9\r\n        (integer) 10\r\n\r\nat half second intervals as intended. Now it'll just hit one command needing a reissue before redirecting and running properly.\r\n\r\nI can't think of an easy way to test the (rare) case where a key changes slot and therefore node mid-repeat, but it should work just fine.\r\n\r\nAdditionally there's a small white-space fix I noticed near this part of the code.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-31T09:53:56Z",
        "closed_at": "2019-07-12T10:36:34Z",
        "merged_at": "2019-07-12T10:36:34Z",
        "body": "This prevents an integer overflow bug. Closes #5737.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-29T05:28:35Z",
        "closed_at": "2020-09-09T16:25:22Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-28T00:18:14Z",
        "closed_at": "2020-09-09T16:25:26Z",
        "merged_at": null,
        "body": "minor grammar fix",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-12-27T16:22:47Z",
        "closed_at": "2019-01-09T09:11:28Z",
        "merged_at": "2019-01-09T09:11:28Z",
        "body": "See issue #5687",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2018-12-27T14:10:50Z",
        "closed_at": "2021-12-29T07:30:21Z",
        "merged_at": null,
        "body": "when slot is moved, we shoud Set target slot first,\r\nOtherwise source and target will redirect to each other.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 75,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2018-12-26T12:34:13Z",
        "closed_at": "2020-07-10T01:19:35Z",
        "merged_at": null,
        "body": "When getting the configurations, as shown below, the readability of the output is really poor:\r\n```\r\n127.0.0.1:6379> config get ac*\r\n 1) \"active-defrag-threshold-lower\"\r\n 2) \"10\"\r\n 3) \"active-defrag-threshold-upper\"\r\n 4) \"100\"\r\n 5) \"active-defrag-ignore-bytes\"\r\n 6) \"104857600\"\r\n 7) \"active-defrag-cycle-min\"\r\n 8) \"5\"\r\n 9) \"active-defrag-cycle-max\"\r\n10) \"75\"\r\n11) \"active-defrag-max-scan-fields\"\r\n12) \"1000\"\r\n13) \"activerehashing\"\r\n14) \"yes\"\r\n15) \"activedefrag\"\r\n16) \"no\"\r\n```\r\nSo I formatted the output as follows to make it more readable:\r\n```\r\n127.0.0.1:6379> config get ac*\r\n1) active-defrag-threshold-lower: \"10\"\r\n2) active-defrag-threshold-upper: \"100\"\r\n3) active-defrag-ignore-bytes: \"104857600\"\r\n4) active-defrag-cycle-min: \"5\"\r\n5) active-defrag-cycle-max: \"75\"\r\n6) active-defrag-max-scan-fields: \"1000\"\r\n7) activerehashing: \"yes\"\r\n8) activedefrag: \"no\"\r\n```\r\n> Please note that I have two separate commits. I am looking forward to your reply.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-26T06:51:48Z",
        "closed_at": "2021-06-29T04:12:18Z",
        "merged_at": null,
        "body": "This PR has not been verified, but I think the code obviously has some problems, so I tried to fix it.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-26T05:50:35Z",
        "closed_at": "2021-08-24T19:54:22Z",
        "merged_at": "2021-08-24T19:54:21Z",
        "body": "When `decr_step` is greater than `oldlimit`, the final `bestlimit` may be invalid.\r\n\r\n    For example, oldlimit = 10, decr_step = 16.\r\n    Current bestlimit = 15 and setrlimit() failed. Since bestlimit  is less than decr_step , then exit the loop.\r\n    The final bestlimit is larger than oldlimit but is invalid.\r\n\r\nNote that this only matters if the system fd limit is below 16, so unlikely to have any actual effect.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-26T02:02:33Z",
        "closed_at": "2020-09-09T16:25:29Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 43,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2018-12-20T11:28:07Z",
        "closed_at": "2022-02-15T09:54:10Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-12-18T17:46:12Z",
        "closed_at": "2018-12-19T16:24:39Z",
        "merged_at": "2018-12-19T16:24:39Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2018-12-18T11:58:03Z",
        "closed_at": "2021-12-09T13:27:32Z",
        "merged_at": null,
        "body": "Avoid dangling pointer and save a little memory.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-18T09:34:04Z",
        "closed_at": "2020-09-09T16:25:32Z",
        "merged_at": null,
        "body": "Just fixed REDISMODULE_H spell bug~",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 126,
        "deletions": 112,
        "changed_files": 5,
        "created_at": "2018-12-18T02:27:09Z",
        "closed_at": "2022-03-17T06:56:51Z",
        "merged_at": null,
        "body": "Hi @antirez we have talked about how to simplify the replication stream to avoid copy data into two buffers in #5284, now I have a new idea wanna share.\r\n\r\nThe idea is very simple, just like MULIT/EXEC, replica queue the original master command and then propagate the original command to its replica after the command executed(for MULIT/EXEC we only propagate the whole transaction after EXEC command).\r\n\r\nJust an idea for reference.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-12-16T05:40:51Z",
        "closed_at": "2020-09-09T16:25:36Z",
        "merged_at": null,
        "body": "As titled",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-14T12:26:33Z",
        "closed_at": "2021-03-16T14:43:03Z",
        "merged_at": "2021-03-16T14:43:03Z",
        "body": "Should `run` be `build` ?",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-12-14T03:26:46Z",
        "closed_at": "2020-09-03T07:31:19Z",
        "merged_at": "2020-09-03T07:31:19Z",
        "body": "Fix some wrong comments in redis.conf.\r\n\r\n1, comment about always-show-logo is not consistent with code\r\n2, comment about cluster-replica-no-failover is wrong since we can only do manualy failover upon replicas",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-12-13T13:01:34Z",
        "closed_at": "2019-03-14T10:32:04Z",
        "merged_at": "2019-03-14T10:32:04Z",
        "body": "No point of calling `vsnprintf` if log line will be filtered ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-12-11T17:11:31Z",
        "closed_at": "2018-12-12T10:29:11Z",
        "merged_at": "2018-12-12T10:29:11Z",
        "body": "When loading data, we call processEventsWhileBlocked\r\nto process events and execute commands.\r\nBut if we are loading AOF it's dangerous, because\r\nprocessCommand would call freeMemoryIfNeeded to evict,\r\nand that will break data consistency, see issue #5686.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 40,
        "changed_files": 15,
        "created_at": "2018-12-11T16:54:54Z",
        "closed_at": "2020-05-24T12:07:31Z",
        "merged_at": null,
        "body": "fix some typos.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 318,
        "deletions": 111,
        "changed_files": 1,
        "created_at": "2018-12-10T18:36:21Z",
        "closed_at": "2018-12-11T07:23:55Z",
        "merged_at": "2018-12-11T07:23:55Z",
        "body": "Several improvements to the Cluster Manager's \"fix\" command:\r\n\r\n- The 'fix' command can now handle those situations where a slot is in migrating state in one node and in importing state in multiple nodes.\r\n- In some cases, fixing an uncovered slot could fail due to the fact that the in-memory slot's configuration wasn't updated after moving the slot from one node to another one (ie. when the slot was previously moved while fixing an open slot).\r\n- In order to avoid \"race conditions\" related to the fact that nodes update their configuration via the cluster's internal protocol, the process of setting a new slot's owner is now handled atomically (via MULTI..EXEC transactions).  \r\n- Is it now possibile to check for (and fix) multiple slot owners (multiple nodes who claim to own the slot and nodes who simply have keys for the slot, even if they don't actually have assigned the slot to themselves): this check can be activated using the `--cluster-search-multiple-owners` option in both 'fix' and 'check' commands. This can be a very slow operation, expecially for big clusters, but it will be optimized in the next days.\r\n- Code cleanup to make code more readable (by getting rid of many repeated sequences that have been moved to dedicated functions).\r\n\r\nNotes: there is some check in the code that currently parses the reply's error message in order to activate some behaviours. Since error message can change in the future, these checks will be replaced with more reliable actions in the next days.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-12-09T05:48:05Z",
        "closed_at": "2020-08-12T20:15:59Z",
        "merged_at": "2020-08-12T20:15:59Z",
        "body": "zslRandomLevel() returns the predictable value.\r\n\r\nThe random() function is called in dict.c, t_zset.c, cluster.c, but no srandom() can be found.\r\n\r\nhttps://github.com/antirez/redis/blob/5bfd8ae25301820ae3c321a838263925e70849b5/src/t_zset.c#L122-L127\r\nhttps://github.com/antirez/redis/blob/5bfd8ae25301820ae3c321a838263925e70849b5/src/cluster.c#L3018-L3023\r\nhttps://github.com/antirez/redis/blob/5bfd8ae25301820ae3c321a838263925e70849b5/src/dict.c#L622-L645\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 32011,
        "deletions": 6035,
        "changed_files": 247,
        "created_at": "2018-12-07T20:49:31Z",
        "closed_at": "2020-05-11T19:25:25Z",
        "merged_at": null,
        "body": "The CLUSTER NODES command is intended to report only the master's config epoch on replicas in cluster mode. However, that is not what is implemented currently. Replicas are reporting their own individual epochs. This commit fixes it. \r\n\r\nSee documentation https://redis.io/commands/cluster-nodes\r\n\r\nTested and it works.\r\n\r\nBefore fix: \r\n\r\n127.0.0.1:6382> cluster nodes\r\n6cb6540c43dd83bb07ec1db191b90063b3374ac0 127.0.0.1:6379@16379 master - 0 1544216055000 1 connected 1-3\r\nec1d5360e7270bf833fa675fbeea3fadd91b179a 127.0.0.1:6380@16380 master - 0 1544216056277 2 connected 4-6\r\n129fd991448e44502cb4a62cb08a873c514cfd4f 127.0.0.1:6382@16382 myself,slave ec1d5360e7270bf833fa675fbeea3fadd91b179a 0 1544216053000 3 connected\r\nff9c9805d1e3c83cfa04df00b918e5bf1bc57b9b 127.0.0.1:6381@16381 slave 6cb6540c43dd83bb07ec1db191b90063b3374ac0 0 1544216055264 1 connected\r\n\r\n\r\nAfter fix:\r\n\r\n127.0.0.1:6382> cluster nodes 3f8969fac2c3dc652d02cca4905bb739c7ed98f4 127.0.0.1:6381@16381 slave 8492f9a7d36ab632610da9aee75491466658cd67 0 1544212190000 1 connected\r\n8492f9a7d36ab632610da9aee75491466658cd67 127.0.0.1:6379@16379 master - 0 1544212190160 1 connected 1-3\r\n9dfbd151548a44ecf8a5dd39c69e830bca5cbd6a 127.0.0.1:6380@16380 master - 0 1544212191172 0 connected 4-6\r\n666925840c9780bf852bfbde46ae36fbbc44e67c 127.0.0.1:6382@16382 myself,slave 9dfbd151548a44ecf8a5dd39c69e830bca5cbd6a 0 1544212189000 0 connected",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-07T11:17:28Z",
        "closed_at": "2018-12-07T16:03:49Z",
        "merged_at": "2018-12-07T16:03:48Z",
        "body": "Hi @antirez , we should check `current_client` after `processCommand`, in case free slave which is the `current_client` when flush slave output buffers in `freeMemoryIfNeeded`, please check this.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 9,
        "changed_files": 3,
        "created_at": "2018-12-06T21:07:23Z",
        "closed_at": "2021-06-24T15:47:33Z",
        "merged_at": null,
        "body": "Part of https://github.com/NixOS/nixpkgs/issues/51517",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 39,
        "deletions": 27,
        "changed_files": 3,
        "created_at": "2018-12-06T11:55:55Z",
        "closed_at": "2018-12-11T11:58:27Z",
        "merged_at": null,
        "body": "Hi @antirez , there is an dangerous scenario when a mater turns into replica:\r\n\r\nWe have two instance, both run in master mode, master A listens on 6379 and master B listens on 7788.\r\n\r\n1. At first send half multi to master B:\r\n```\r\n127.0.0.1:7788> multi\r\nOK\r\n127.0.0.1:7788> set a b\r\nQUEUED\r\n127.0.0.1:7788> set c d\r\nQUEUED\r\n```\r\n\r\n2. And then turn master B into replica of master A:\r\n```\r\n127.0.0.1:7788> replicaof 127.0.0.1 6379\r\nOK\r\n```\r\n\r\n3. Execute the multi:\r\n```\r\n127.0.0.1:7788> exec\r\n1) OK\r\n2) OK\r\n```\r\n\r\nUnfortunately, instance B has two keys more than master A now.\r\n\r\nTo fix it I think we can disconnect the multi state clients when master turns into replica, just like what blocked clients do.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 24,
        "changed_files": 4,
        "created_at": "2018-12-06T09:25:21Z",
        "closed_at": "2021-12-30T10:18:08Z",
        "merged_at": null,
        "body": "Hi @antirez , we had a discussion about forbidden some commands in lua script in #4835.\r\n\r\nBut now I just found some commands are dangerous in multi/exec too... for instance:\r\n\r\nWe have a master instance listening on port 6379, and an replica listening on 7788, then execute `REPLICAOF` command in multi/exec on master:\r\n\r\n```\r\n127.0.0.1:6379> multi\r\nOK\r\n127.0.0.1:6379> replicaof 127.0.0.1 1234\r\nQUEUED\r\n127.0.0.1:6379> set a b\r\nQUEUED\r\n127.0.0.1:6379> set c d\r\nQUEUED\r\n127.0.0.1:6379> replicaof no one\r\nQUEUED\r\n127.0.0.1:6379> exec\r\n1) OK\r\n2) OK\r\n3) OK\r\n4) OK\r\n127.0.0.1:6379> keys *\r\n1) \"c\"\r\n2) \"a\"\r\n```\r\n\r\nBut the replica has nothing:\r\n```\r\n127.0.0.1:7788> keys *\r\n(empty list or set)\r\n127.0.0.1:7788> quit\r\n```\r\n\r\nIt's because in multi/exec after `REPLICAOF 127.0.0.1 1234` the master became an replica, then write commands would not be replicated, neither the `repl_backlog`.\r\n\r\nSo, I think we should forbid some replication commands like `REPLICAOF`, `SYNC`, `PSYNC`.\r\n\r\nI know this is a corner case, but it really can lead to data inconsistency between master and replica.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-05T09:38:57Z",
        "closed_at": "2018-12-11T12:03:10Z",
        "merged_at": "2018-12-11T12:03:10Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-04T09:42:46Z",
        "closed_at": "2018-12-07T10:28:18Z",
        "merged_at": "2018-12-07T10:28:18Z",
        "body": "redis-cli --cluster call command return reply in tty format which is hard to read\r\n```\r\n>>> Calling cluster info\r\n192.168.100.213:6379: \"cluster_state:ok\\r\\ncluster_slots_assigned:16384\\r\\ncluster_slots_ok:16384\\r\\ncluster_slots_pfail:0\\r\\ncluster_slots_fail:0\\r\\ncluster_known_nodes:12\\r\\ncluster_size:7\\r\\ncluster_current_epoch:114\\r\\ncluster_my_epoch:6\\r\\ncluster_stats_messages_ping_sent:440450\\r\\ncluster_stats_messages_pong_sent:433341\\r\\ncluster_stats_messages_meet_sent:2\\r\\ncluster_stats_messages_fail_sent:2\\r\\ncluster_stats_messages_update_sent:24\\r\\ncluster_stats_messages_sent:873819\\r\\ncluster_stats_messages_ping_received:433332\\r\\ncluster_stats_messages_pong_received:439186\\r\\ncluster_stats_messages_meet_received:9\\r\\ncluster_stats_messages_fail_received:10\\r\\ncluster_stats_messages_auth-req_received:3\\r\\ncluster_stats_messages_update_received:219\\r\\ncluster_stats_messages_received:872759\\r\\n\"\r\n```\r\nIt will be more appropriate in format in raw format the same as redis-trib.rb.\r\n```\r\n>>> Calling cluster info\r\n192.168.100.213:6379: cluster_state:ok\r\ncluster_slots_assigned:16384\r\ncluster_slots_ok:16384\r\ncluster_slots_pfail:0\r\ncluster_slots_fail:0\r\ncluster_known_nodes:12\r\ncluster_size:7\r\ncluster_current_epoch:114\r\ncluster_my_epoch:6\r\ncluster_stats_messages_ping_sent:441999\r\ncluster_stats_messages_pong_sent:434877\r\ncluster_stats_messages_meet_sent:2\r\ncluster_stats_messages_fail_sent:2\r\ncluster_stats_messages_update_sent:24\r\ncluster_stats_messages_sent:876904\r\ncluster_stats_messages_ping_received:434868\r\ncluster_stats_messages_pong_received:440735\r\ncluster_stats_messages_meet_received:9\r\ncluster_stats_messages_fail_received:10\r\ncluster_stats_messages_auth-req_received:3\r\ncluster_stats_messages_update_received:219\r\ncluster_stats_messages_received:875844\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-12-04T07:29:14Z",
        "closed_at": "2018-12-04T17:09:44Z",
        "merged_at": "2018-12-04T17:09:44Z",
        "body": "This happens when you send any invalid command to a monitor client. We saw clients doing this when they were trying to escape telnet connections using monitor, not a great experience.\r\n\r\nReproduction:\r\n```\r\ntelnet localhost 6379\r\nTrying ::1...\r\nConnected to localhost (::1).\r\nEscape character is '^]'.\r\nMONITOR\r\n+OK\r\nhi redis\r\nConnection closed by foreign host.\r\n```\r\n\r\nDo you want TCL tests for issues like these to prevent regressions in the future?",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2018-12-02T13:37:25Z",
        "closed_at": "2018-12-07T16:09:11Z",
        "merged_at": "2018-12-07T16:09:11Z",
        "body": "these metrics become negative when RSS is smaller than the used_memory.\r\nThis can easily happen when the program allocated a lot of memory and haven't\r\nwritten to it yet, in which case the kernel doesn't allocate any pages to the process",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2018-12-01T10:00:30Z",
        "closed_at": "2021-12-06T15:14:15Z",
        "merged_at": null,
        "body": "merge 5.0.2",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-11-30T08:53:35Z",
        "closed_at": "2020-05-24T12:07:15Z",
        "merged_at": null,
        "body": "The if condition \"strcasecmp(master_replid, server.replid)\" is already checked by the outer if statement, so no need to check again.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-30T04:10:43Z",
        "closed_at": "2018-12-07T16:09:56Z",
        "merged_at": "2018-12-07T16:09:56Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 15,
        "changed_files": 6,
        "created_at": "2018-11-29T01:44:08Z",
        "closed_at": "2021-08-21T13:10:18Z",
        "merged_at": null,
        "body": "The http://redis.io will cause an 301 status code:\r\n    HTTP/1.1 301 Moved Permanently\r\n    Server: nginx/1.10.2\r\n    Date: Wed, 28 Nov 2018 05:58:24 GMT\r\n    Content-Type: text/html\r\n    Content-Length: 185\r\n    Location: https://redis.io/\r\n\r\nSo update some http by using grep.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2018-11-28T07:02:59Z",
        "closed_at": "2018-11-28T15:18:17Z",
        "merged_at": null,
        "body": null,
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-28T02:11:31Z",
        "closed_at": "2018-11-28T15:32:48Z",
        "merged_at": null,
        "body": "I feel s should end with '\\0' similar to the sdstrim function.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-11-27T17:31:37Z",
        "closed_at": "2018-11-28T15:54:13Z",
        "merged_at": null,
        "body": "zslIsInLexRange and zzlIsInLexRange both have any early check to escape\r\nif `min > max`.   The underlying function does a `memcmp` which returns\r\n0, integer greater than 0, or integer less than 0.  However, these\r\nonly escape if the compare result is `> 1`.\r\n\r\nThe specific output of `memcmp` is implementation-dependent, but on\r\nmany systems, it will return 1 if the inputs differ by 1.\r\n\r\nDue to the rest of the algorithm, the zsl/zzlIsInLexRange functions have\r\noperated properly.  This change however is more correct (thus if the\r\nrest of the function changes, it will continue to work properly) and gives\r\na slight performance improvement for this edge case.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2018-11-26T07:37:27Z",
        "closed_at": "2021-07-25T06:21:05Z",
        "merged_at": null,
        "body": "Hi @antirez , I'm a little confused about the UNWATCH command:\r\n\r\n```\r\n127.0.0.1:6379> multi\r\nOK\r\n127.0.0.1:6379> set a b\r\nQUEUED\r\n127.0.0.1:6379> unwatch\r\nQUEUED\r\n127.0.0.1:6379> exec\r\n1) OK\r\n2) OK\r\n```\r\n\r\nIt seems that in multi context the UNWATCH is only queued not executed, so it becomes useless no matter the watched key is touched before or after the UNWATCH.\r\n\r\nMaybe we should execute the UNWATCH instead of queue it?",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-11-26T05:49:46Z",
        "closed_at": "2021-11-16T11:59:03Z",
        "merged_at": "2021-11-16T11:59:03Z",
        "body": "when aof rewrite is failed by fork(), It'll be indicated by aof_last_bgrewrite_status INFO field, same as when the fork child fails later on.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-11-26T00:52:23Z",
        "closed_at": "2020-05-24T12:07:48Z",
        "merged_at": null,
        "body": "In \"cluster help\" description, the \"cluster saveconfig\" command is missing.\r\nAlso, make the output format more consistent.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2018-11-25T13:40:37Z",
        "closed_at": "2021-11-17T08:12:54Z",
        "merged_at": null,
        "body": "It seems, the length of value has been checked already in o==NULL and the else block.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-11-25T11:22:31Z",
        "closed_at": "2018-11-25T13:20:15Z",
        "merged_at": null,
        "body": "if h is not equal newh, then we must first free memory h pointer to,in case of memory leak",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-25T09:19:10Z",
        "closed_at": "2020-09-09T16:25:39Z",
        "merged_at": null,
        "body": "Previous commit didnot fix this.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 170,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2018-11-24T15:50:36Z",
        "closed_at": "2018-11-28T16:01:34Z",
        "merged_at": "2018-11-28T16:01:34Z",
        "body": "FreeBSD/DragonFlyBSD does have backtrace only it does not\r\nbelong to libc.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-11-23T17:03:26Z",
        "closed_at": "2018-11-28T16:06:17Z",
        "merged_at": "2018-11-28T16:06:17Z",
        "body": "If we encounter an unsupported protocol in the `bind` list, don't ipso-facto consider it a fatal error. We continue to abort startup if there are no listening sockets at all.\r\n\r\nThis ensures that the lack of IPv6 support does not prevent Redis from starting on Debian where we try to bind to the `::1` interface by default (via `bind 127.0.0.1 ::1`). A machine with IPv6 disabled (such as some container systems) would simply fail to start Redis after the initiall call to `apt(8)`.\r\n\r\nThis is similar to the case for where `bind` is not specified (#3894) and was based on the corresponding PR #4108 but also adds `EADDRNOTAVAIL` to the list of errors to catch which I believe is missing from there.\r\n\r\n(This issue was raised in Debian as both [#900284](https://bugs.debian.org/900284) & [#914354](https://bugs.debian.org/914354).)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-23T16:59:22Z",
        "closed_at": "2018-11-28T16:06:51Z",
        "merged_at": "2018-11-28T16:06:51Z",
        "body": "This really helps spot it in the logs, otherwise it does not look like a warning/error. For example:\r\n\r\n```\r\nCreating Server TCP listening socket ::1:6379: bind: Cannot assign requested address\r\n```\r\n... is not nearly as clear as:\r\n\r\n```\r\nCould not create server TCP listening listening socket ::1:6379: bind: Cannot assign requested address\r\n```",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-11-23T16:14:28Z",
        "closed_at": "2020-09-09T16:25:42Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2018-11-23T09:03:30Z",
        "closed_at": "2018-11-28T17:13:04Z",
        "merged_at": "2018-11-28T17:13:04Z",
        "body": "It will choose a slave node for node assignment and result in an `incorrect` cluster configuration status.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-11-21T15:06:31Z",
        "closed_at": "2018-11-22T10:10:57Z",
        "merged_at": "2018-11-22T10:10:57Z",
        "body": "Skip slave nodes when send `cluster setslot [slot] node [nodeid]`, or it will return `ERR Please use SETSLOT only with masters` result in fix openslot failed.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-11-19T11:48:38Z",
        "closed_at": "2019-03-11T10:23:52Z",
        "merged_at": null,
        "body": "When using gcc 4.x on s390x with -O2, memefficiency.tcl can flag a false positive in terms of max. latency being lower than 120 msec. -O3 removes this problem by going for aggressive code generation.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-11-19T06:59:24Z",
        "closed_at": "2021-05-15T12:43:52Z",
        "merged_at": null,
        "body": "\r\n```\r\nsadd s1 a b \r\nsinterstore s1 s2 \r\n```\r\n\r\ngenerates no del keyspace events.\r\n\r\nthis patch fixes sinterstore by add missing keyspace del event when any source set not exists.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 35,
        "changed_files": 1,
        "created_at": "2018-11-18T13:38:42Z",
        "closed_at": "2020-09-09T16:29:13Z",
        "merged_at": null,
        "body": "Found some parts a little unclear on a first read, which prompted me to have a better look at the file and fix some minor things I noticed.\r\nFixing minor typos and grammar. There are no changes to configuration options.\r\nThese changes are only meant to help the user better understand the explanations to the various configuration options",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-11-16T09:32:30Z",
        "closed_at": "2018-11-20T09:39:00Z",
        "merged_at": "2018-11-20T09:39:00Z",
        "body": "The error message be accessed as `char*`, but they are `char **` which is unexpected.\r\n\r\nAnd a potential memory leak if error occurs in clusterManagerCommandSetTimeout.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 117,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2018-11-15T15:39:41Z",
        "closed_at": "2020-12-30T14:05:16Z",
        "merged_at": null,
        "body": "@antirez here's an updated version of the @sunheehnus patch for Feature request, zrevrangebyscore with STORE option #678 which, per your suggestion, adds a STORE dstkey clause to zrangebyscore/zrevrangebyscore.\r\n\r\n```\r\nZRANGEBYSCORE key min max [WITHSCORES] [STORE dstkey] [LIMIT offset count]\r\nZREVRANGEBYSCORE key min max [WITHSCORES] [STORE dstkey] [LIMIT offset count]\r\n```\r\n\r\nIf you're OK overall with the implementation (basically wrapping all replies with if (!withstore) conditionals), I'll move all the common keyspace notifications and clean-up code to the end and use a goto similar to the rest of Redis.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-15T09:01:32Z",
        "closed_at": "2018-11-28T16:14:22Z",
        "merged_at": "2018-11-28T16:14:22Z",
        "body": "fix comment typo in util.c",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2018-11-14T16:06:07Z",
        "closed_at": "2018-11-22T10:12:36Z",
        "merged_at": "2018-11-22T10:12:36Z",
        "body": "We are currently running redis4.0.11 with roughly 30K connections, but we have a lot of very shortlived connections. In very unfortunate situations, our flamegraphs reveals that most of the time was beeing spent in `unlinkClient`.\r\n\r\n![image](https://user-images.githubusercontent.com/801405/48494970-fac83600-e7fc-11e8-90f6-c52f09192c5f.png)\r\n\r\nBy applying this patch we were able to reduce the latency by a significant margin (currently getting accurate numbers for posterity), but I think back-porting this into redis4 is a win either way.\r\n\r\nThanks!",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2018-11-14T04:01:43Z",
        "closed_at": "2018-11-28T17:16:05Z",
        "merged_at": "2018-11-28T17:16:05Z",
        "body": "In MEMORY USAGE command, we count the key `argv[2]` into usage, but the argument in command may contains free spaces because of `sdsMakeRoomFor`.\r\n\r\nBut the key in db never contains free spaces because we use `sdsdup` in `dbAdd`, so using the real key to count the usage is more accurate.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 120,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2018-11-13T16:02:04Z",
        "closed_at": "2019-03-14T10:30:49Z",
        "merged_at": null,
        "body": "Redis module API exposes a low level API that allows the module writer low level fast access to redis keys.\r\nFor example, I can use RedisModule_StringSet to set some key value to STRING. This capability allows module writer to avoid the need to use RedisModule_Call which is much slower (because it performs a command lookup), it also allows the module writer to avoid parsing the result. In addition module writer can do things using RedisModule API which are not exposed as a command (such as register to time events or cluster messages).\r\n\r\nI would like modules to be able to expose such capabilities as well, for example:\r\n1. ReJson would be able to expose an API that allows direct low level access to the json datatypes.\r\n2. RediSearch would be able to expose an API for direct index search.\r\nOther modules would be able to use this API to access json datatypes or search on redisearch indexes.\r\n\r\nThis PR provide a mechanism to expose such an API. The idea is to use the same mechanism Redis uses to expose its own API. Modules will register their API functions and other modules will be able to get this API using RedisModule_GetApi function. \r\n\r\nThe PR introduces a new stage on module loading called RedisModule_RegisterApi, this function (if exposed by the module) is called before the usual RedisModule_OnLoad function. The function receives a pointer to moduleRegisterApi and allows the module to register its own API functions. Notice, in order to avoid problems like modules initialization order or bidirectional modules dependencies Redis will first call RedisModule_RegisterApi function of all modules and only then will call RedisModule_OnLoad of each module. \r\n\r\nNow, modules like Rjson or Redisearch will be able to expose their api using an .h file (just like redismodule.h) and other modules will be able to use it (perform initialization on RedisModule_OnLoad and then use all the function its exposes).\r\n\r\nPlease let me know if you have any question/objections or maybe ideas for different implementation, I will really like to be able to achieve such inner module communication.\r\n\r\nThanks.",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 48,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-11-13T00:02:35Z",
        "closed_at": "2018-11-28T16:13:28Z",
        "merged_at": null,
        "body": "Adding unit test for XCLAIM command for redis streams. The test covers the following:\r\n\r\n- A happy scenario of one client claiming a pending item from another client\r\n- 2 failure scenarios as mentioned in the v5.0.1 XCLAIM bug fix. This test passes on v5.0.1, but fails on v5.0.0 due to the bug. \r\nhttps://github.com/antirez/redis/commit/1ed821e28d19f58a44720761e1675cc324c2f458",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2018-11-11T18:50:56Z",
        "closed_at": "2018-11-22T10:16:31Z",
        "merged_at": "2018-11-22T10:16:31Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2018-11-11T07:23:50Z",
        "closed_at": "2018-11-28T17:17:06Z",
        "merged_at": "2018-11-28T17:17:06Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-10T03:09:29Z",
        "closed_at": "2022-06-23T15:41:33Z",
        "merged_at": "2022-06-23T15:41:33Z",
        "body": "I think parameter c is only useful to get client reply.\r\nBesides, other commands' host and port parameters may not be the at index 1 and 2.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-10T02:32:14Z",
        "closed_at": "2021-11-27T12:24:49Z",
        "merged_at": null,
        "body": "When port lengh is bigger thant strlen(\"65535\"), redis-cli prompt seems wrong.\r\nI started a server with port 30001\uff0cand connect it with \"redis-cli -p 300010000\"\uff0cwhile the prompt is \"127.0.0.1:300010000>\". \r\nI think it should be \"127.0.0.1:30001\".",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2018-11-08T14:11:19Z",
        "closed_at": "2020-09-09T16:25:45Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-08T05:28:38Z",
        "closed_at": "2022-03-02T08:40:39Z",
        "merged_at": "2022-03-02T08:40:39Z",
        "body": "1. since ZSKIPLIST_P is float, using it directly inside the condition used to causes floating point code to be used (gcc/x86)\r\n2. In some operating system(eg.Windows), the largest value returned from random() is 0x7FFF(15bit), so after bitwise AND with 0xFFFF, the probability of the less operation returning true in the while loop's condition is no more equal to ZSKIPLIST_P.\r\n3. In case some library has random() returning int in range [0~ZSKIPLIST_P*65535], the while loop will be an infinite loop.\r\n4. on Linux where RAND_MAX is higher than 0xFFFF, this change actually improves precision (despite not matching the result against a float value)",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-07T07:22:47Z",
        "closed_at": "2018-11-07T10:59:38Z",
        "merged_at": null,
        "body": "```\r\nif (start && newlen) memmove(s, s+start, newlen);\r\n    s[newlen] = 0;\r\n```\r\nI think it should be `s[newlen] = '\\0'`;",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 19,
        "changed_files": 8,
        "created_at": "2018-11-07T05:07:57Z",
        "closed_at": "2018-11-26T00:13:39Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 363,
        "deletions": 188,
        "changed_files": 14,
        "created_at": "2018-11-06T22:28:35Z",
        "closed_at": "2021-01-04T15:02:58Z",
        "merged_at": "2021-01-04T15:02:58Z",
        "body": "Given recent mergers, I reviewed the code and made some helpful (hopefully) adjustments.\r\n\r\n* man-like consistent long formatting\r\n* Uppercases commands, subcommands and options\r\n* Adds 'HELP' to HELP for all\r\n* Lexicographical order\r\n* Uses <value> notation and other .md (or not) likenesses\r\n* Moves const char *help to top\r\n* Keeps it under 80 chars\r\n* Misc help typos, consistent conjuctioning (i.e return and not returns)\r\n* Uses addReplySubcommandSyntaxError(c) all over\r\n* Trims some trailing spaces in various unrelated LoC\r\n\r\nSigned-off-by: Itamar Haber <itamar@redislabs.com>",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-06T02:56:10Z",
        "closed_at": "2018-11-06T11:05:24Z",
        "merged_at": "2018-11-06T11:05:24Z",
        "body": "fix zmalloc sizeof(*) instead of sizeof(**) in clusterManagerComputeReshardTable.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 132,
        "deletions": 36,
        "changed_files": 1,
        "created_at": "2018-11-05T17:50:09Z",
        "closed_at": "2018-11-06T11:05:07Z",
        "merged_at": "2018-11-06T11:05:07Z",
        "body": "This PR tries to improve the \"fix\" subcommand of redis-cli's cluster manager, by trying to cover more situations.\r\n\r\nMore specifically:\r\n\r\n- Fix Open Slot: it now checks if there are non-owner nodes that for some reason could have keys for the open slot, and it considers them to be \"owners\". These nodes are then set to importing state and finally they migrate keys to the \"owner\" node (the node having more keys related to the slot).\r\n- Fix Open Slot: it also fixes nodes that for some reason could remain in \"migrating\" state and that are owner of the slot.\r\n- Fix Open Slots/Fix Slots Coverage: it now set the slot to unassigned state on the owner itself before assigning it, in order to avoid errors due to the fact that the owner has already assigned the slot to another node. It also ensure that the other nodes are always updated so that they assign the slot to the new owner.\r\n- Move Slot: it now checks for \"slot not served\" error and retries to move the slot after assigning it to the target node.\r\n\r\nThanks to @funny-falcon for having reported many uncovered situations in his PR [#5270](https://github.com/antirez/redis/pull/5270).",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-05T09:38:47Z",
        "closed_at": "2018-11-06T12:00:16Z",
        "merged_at": "2018-11-06T12:00:16Z",
        "body": "server.hz was uninitialised between initServerConfig and initServer.\r\nthis can lead to someone (e.g. queued modules) doing createObject,\r\nand accessing an uninitialised variable, that can potentially be 0,\r\nand lead to a crash.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-11-05T06:39:30Z",
        "closed_at": "2020-09-09T16:25:49Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-11-02T20:59:20Z",
        "closed_at": "2018-11-06T11:28:08Z",
        "merged_at": "2018-11-06T11:28:08Z",
        "body": "Signed-off-by: Itamar Haber <itamar@redislabs.com>",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-11-02T02:37:40Z",
        "closed_at": "2022-02-06T05:54:51Z",
        "merged_at": null,
        "body": "For sentinel commands, the 'sflags' flag is useless.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2018-11-01T08:50:07Z",
        "closed_at": "2018-11-07T06:21:05Z",
        "merged_at": null,
        "body": "relative issue: https://github.com/antirez/redis/issues/5496",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2018-10-30T13:28:11Z",
        "closed_at": "2018-11-08T11:12:29Z",
        "merged_at": "2018-11-08T11:12:29Z",
        "body": "Some math functions require c11 standard.",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-30T12:38:25Z",
        "closed_at": "2018-11-28T15:57:20Z",
        "merged_at": null,
        "body": "relative  issue: https://github.com/antirez/redis/issues/5504",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-30T12:34:40Z",
        "closed_at": "2018-11-01T02:10:40Z",
        "merged_at": null,
        "body": "After we set the `min-replicas-to-write` and `min-replicas-max-lag`,   the  master can stop accepting writes if there are less than `min-replicas-to-write` replicas connected, having a lag greater or equal than `min-replicas-max-lag` seconds",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-10-27T17:13:59Z",
        "closed_at": "2020-09-09T16:25:52Z",
        "merged_at": null,
        "body": "Fix typos",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2018-10-27T11:44:19Z",
        "closed_at": "2018-10-29T16:26:03Z",
        "merged_at": null,
        "body": "Depending on compiler optimisations, memset might not suffice\r\nto guarantee hashing context's clearance.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2018-10-26T14:03:20Z",
        "closed_at": "2018-10-29T17:25:15Z",
        "merged_at": "2018-10-29T17:25:15Z",
        "body": "timezone global is a linux-ism whereas it is a function under BSD.\r\nHere a helper to get the timezone value in a more portable manner.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-10-26T12:02:48Z",
        "closed_at": "2018-10-26T17:40:45Z",
        "merged_at": null,
        "body": "Fixes #5487",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-23T11:26:44Z",
        "closed_at": "2020-09-09T16:25:55Z",
        "merged_at": null,
        "body": "Fixed typo for word 'dictionary'",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-22T15:32:58Z",
        "closed_at": "2021-06-29T05:50:03Z",
        "merged_at": null,
        "body": "It causes compilation error when includes both include files from single file.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 34,
        "deletions": 27,
        "changed_files": 1,
        "created_at": "2018-10-19T10:20:19Z",
        "closed_at": "2018-10-24T10:24:00Z",
        "merged_at": "2018-10-24T10:24:00Z",
        "body": "",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-19T10:17:33Z",
        "closed_at": "2018-10-24T10:31:29Z",
        "merged_at": "2018-10-24T10:31:29Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-10-18T21:15:07Z",
        "closed_at": "2018-10-24T10:33:24Z",
        "merged_at": null,
        "body": "Fixes #5458, but definitely breaks backward \"compatibility\" although undocumented and apparently wrong. Should be considered for backporting :)\r\n\r\nSigned-off-by: Itamar Haber <itamar@redislabs.com>",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-10-18T14:22:40Z",
        "closed_at": "2020-09-07T19:27:16Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2018-10-18T11:00:01Z",
        "closed_at": "2018-10-22T15:40:38Z",
        "merged_at": "2018-10-22T15:40:38Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-10-17T19:13:28Z",
        "closed_at": "2018-12-07T10:30:07Z",
        "merged_at": "2018-12-07T10:30:07Z",
        "body": "This adds support for passing a password through a REDISCLI_AUTH\r\nenvironment variable (which is safer than the CLI), which might often be\r\nsafer than passing it through a CLI argument.\r\n\r\nPassing a password this way does not trigger the warning about passing a\r\npassword through CLI arguments, and CLI arguments take precedence over\r\nit.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-17T16:42:09Z",
        "closed_at": "2018-10-25T09:50:05Z",
        "merged_at": "2018-10-25T09:50:05Z",
        "body": "This fix is important, but I feel it isn't complete yet.\r\n\r\nCurrently, with the fix, passing 0 or a negative count will return 0 results. However, other \"COUNT\" arguments in Redis behave differently. Specifically, `X[REV]RANGE` will treat such counts as \"all\".\r\n\r\n@antirez - what would be the way to resolve this?\r\n1. Leave as is (i.e. inconsistent behavior)\r\n2. Document XRANGE's count behavior, and make XPENDING behave the same\r\n3. Make XRANGE return 0 results for such counts\r\n\r\nAlso, @RoeyPrat found about the error in the documented syntax so if it makes sense I'll try to do a matching PR to the docs.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-10-17T08:46:36Z",
        "closed_at": "2018-10-25T11:00:20Z",
        "merged_at": "2018-10-25T11:00:20Z",
        "body": "Hi @antirez , I think we should just return OOM error if it is impossible to free enough memory and the client is in MULT/EXEC context, because `queueMultiCommand` holds the args memory, moreover `used_memory` can grow unlimited.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2018-10-16T18:53:37Z",
        "closed_at": "2018-10-24T10:28:29Z",
        "merged_at": "2018-10-24T10:28:29Z",
        "body": "`freeMemoryGetNotCountedMemory()` uses `sdslen(server.aof_buf)` when trying to remove memory that is not used as part of the main database, where it should use `sdsalloc` instead. This causes AOF to induce eviction although it is unecessary, and could end up resulting in OOM errors being returned to the client if there is not enough evictable keys available.\r\n\r\nWe were trying to debug why Redis was returning a OOM error on a write spike on a old 3.2.11 server with AOF enabled, and code review led us to this. We haven't confirmed that it solves the problem. I'm uploading this for discussion.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2018-10-16T15:47:54Z",
        "closed_at": "2018-10-16T16:56:55Z",
        "merged_at": "2018-10-16T16:56:55Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 6,
        "changed_files": 7,
        "created_at": "2018-10-16T07:54:04Z",
        "closed_at": "2018-10-22T10:17:10Z",
        "merged_at": "2018-10-22T10:17:10Z",
        "body": "fixed several typos; \r\nreuse max value in inset;\r\noptimize MSETNX, break if one key already exists.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-15T21:07:40Z",
        "closed_at": "2021-06-21T11:42:06Z",
        "merged_at": null,
        "body": "Fixes #5438\r\n\r\nDoes not abort the connection, but instead resets the db number to zero to follow the current behavior.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-10-15T13:46:08Z",
        "closed_at": "2020-09-09T12:09:42Z",
        "merged_at": "2020-09-09T12:09:42Z",
        "body": "I could swear something like this used to be there, but for the life of me, I can't find it. Anyway, this is needed so the API's documentation is more complete.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2018-10-15T08:59:47Z",
        "closed_at": "2020-08-18T18:46:32Z",
        "merged_at": null,
        "body": "Some DEBUG logs output as follows:\r\n```\r\n86004:M 15 Oct 16:25:05.050 . Unrecognized RDB AUX field: 'redis-ver'\r\n86004:M 15 Oct 16:25:05.050 . Unrecognized RDB AUX field: 'redis-bits'\r\n86004:M 15 Oct 16:25:05.050 . Unrecognized RDB AUX field: 'ctime'\r\n86004:M 15 Oct 16:25:05.050 . Unrecognized RDB AUX field: 'used-mem'\r\n86004:M 15 Oct 16:25:05.050 . Unrecognized RDB AUX field: 'aof-preamble'\r\n```\r\nThese AUX fields not start with `%`, so these aux fields can not be logged on `NOTICE` level while rdb loading.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-10-12T19:56:59Z",
        "closed_at": "2018-10-15T09:15:24Z",
        "merged_at": "2018-10-15T09:15:24Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2018-10-11T13:51:01Z",
        "closed_at": "2018-10-15T09:47:21Z",
        "merged_at": "2018-10-15T09:47:21Z",
        "body": "1. First, as your comment\r\n\r\n    ```c\r\n            /* Technically it could be more correct to update that only after\r\n             * checking for syntax errors, but this option is only used by\r\n             * the replication command that outputs correct syntax. */\r\n    ```\r\n\r\n    I know updating `LASTID` is only used by replication, but we cannot prevent user from doing it, so I still believe we should update it after checking.\r\n\r\n2. If `XCLAIM` didn't claim any stream entry but did update `LASTID`, this update cannot be propagated.\r\n\r\n3. argv in `streamPropagateXCLAIM` should be `group name` not `consumer name`.\r\n\r\n4. `XCALIM` should ignore `minidle` if NACK is created by `FORCE`\r\n\r\n    Because the NACK created by `FORCE` consumer is NULL, if idletime < minidle the NACK does not belong to any consumer, then redis will crash in `XPENDING`.\r\n\r\nPlease check @antirez .",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-10-09T12:35:51Z",
        "closed_at": "2018-10-09T16:02:56Z",
        "merged_at": "2018-10-09T16:02:56Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-09T08:43:00Z",
        "closed_at": "2021-06-29T04:04:03Z",
        "merged_at": null,
        "body": "This error message was misleading me when i was trying to debug my redis client library.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 165,
        "deletions": 16,
        "changed_files": 5,
        "created_at": "2018-10-09T08:10:23Z",
        "closed_at": "2018-10-16T11:10:37Z",
        "merged_at": "2018-10-16T11:10:37Z",
        "body": "Bug report:\r\n\r\n1. After aofrewrite, empty stream cannot be rewritten into AOF even it has consumer groups.\r\n2. After xdel last id and aofrewrite, stream's last id will be incorrect.\r\n\r\nBug located:\r\n\r\n```c\r\n    /* Reconstruct the stream data using XADD commands. */\r\n    while(streamIteratorGetID(&si,&id,&numfields)) {\r\n        /* Emit a two elements array for each item. The first is\r\n         * the ID, the second is an array of field-value pairs. */\r\n         /* Emit the XADD <key> <id> ...fields... command. */\r\n        if (rioWriteBulkCount(r,'*',3+numfields*2) == 0) return 0;\r\n        if (rioWriteBulkString(r,\"XADD\",4) == 0) return 0;\r\n        if (rioWriteBulkObject(r,key) == 0) return 0;\r\n        if (rioWriteBulkStreamID(r,&id) == 0) return 0;\r\n```\r\n\r\nWe can see that empty stream cannot go into the loop, and `lastid` is the last entry not last generated id.\r\n\r\nTo fix these, I added a new command `XSTREAM` with subcommands `CREATE` to create empty stream and `SETID` to set last-generated-id.\r\n\r\nPlease check @antirez .",
        "comments": 19
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 9,
        "changed_files": 8,
        "created_at": "2018-10-08T14:53:08Z",
        "closed_at": "2018-10-16T07:32:12Z",
        "merged_at": null,
        "body": "the zslRandomLevel fuction may cause unnecessary loop, changed it so it only loops for at most ZSKIPLIST_MAXLEVEL times;\r\n\r\nfixed two :) emoticon typos",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-10-08T13:06:03Z",
        "closed_at": "2020-09-07T19:25:22Z",
        "merged_at": null,
        "body": "Found while reading your very nice [article](http://antirez.com/news/124) :) \r\nThanks for taking the time to write it! ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-07T11:41:11Z",
        "closed_at": "2018-10-08T09:34:43Z",
        "merged_at": "2018-10-08T09:34:43Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-06T05:16:28Z",
        "closed_at": "2020-09-07T19:32:23Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 246,
        "deletions": 14,
        "changed_files": 13,
        "created_at": "2018-10-05T16:30:44Z",
        "closed_at": "2018-12-29T02:50:36Z",
        "merged_at": null,
        "body": "Sometimes `connected_clients` reach `maxclients` so quickly that DBAs have no time to increase the `maxclients` value. Why not reserve a speical ip(Let me call it the administrative ip) which is not limited to maxclients? DBAs can still visit Redis instance by the administrative ip, when the `server.clients` reached `maxclients`. This patch supports the feature mentioned above.  It is useful for DBAs to maintain Redis in my company.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-04T13:34:41Z",
        "closed_at": "2018-10-15T11:01:59Z",
        "merged_at": "2018-10-15T11:01:59Z",
        "body": "Fixes https://github.com/antirez/redis/issues/5415\r\nCurrently timer callback receives a context with a wrong selected database. This PR saves the selected context and apply it before calling the timer callback.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-02T00:45:06Z",
        "closed_at": "2018-10-31T17:17:21Z",
        "merged_at": null,
        "body": "Make the parameter naming consistent for 'lua-always-replicate-commands'.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-10-02T00:34:01Z",
        "closed_at": "2018-10-15T17:08:40Z",
        "merged_at": null,
        "body": "Fix a typo in redis.conf documentation for 'replica-ignore-maxmemory' parameter.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2018-09-30T09:53:43Z",
        "closed_at": "2018-10-01T11:28:54Z",
        "merged_at": "2018-10-01T11:28:54Z",
        "body": "When HAVE_MALLOC_SIZE is false, each call to zrealloc causes used_memory\r\nto increase by PREFIX_SIZE more than it should, due to mis-matched\r\naccounting between the original zmalloc (which includes PREFIX size in\r\nits increment) and zrealloc (which misses it from its decrement).\r\n\r\nI've also supplied a command-line test to easily demonstrate the\r\nproblem. It's not wired into the test framework, because I don't know\r\nTCL so I'm not sure how to automate it.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-09-30T09:33:57Z",
        "closed_at": "2018-10-01T11:30:43Z",
        "merged_at": "2018-10-01T11:30:43Z",
        "body": "sdsZmallocSize assumes a dynamically allocated SDS. When given a string\r\nobject created by createEmbeddedStringObject, it calls zmalloc_size on a\r\npointer that isn't the one returned by zmalloc",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 63,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2018-09-30T08:40:53Z",
        "closed_at": "2018-10-01T11:24:50Z",
        "merged_at": "2018-10-01T11:24:50Z",
        "body": "this was broken a while back by ba9154d7e7bf959b002533384319a1e90545447b\r\nthe purpose of which was to fix commandstats for GEOADD",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-09-28T04:09:10Z",
        "closed_at": "2018-10-02T14:09:22Z",
        "merged_at": "2018-10-02T14:09:22Z",
        "body": "There are two problems if we use lastcmd:\r\n\r\n1. BRPOPLPUSH cannot be rewrited as RPOPLPUSH in multi/exec\r\n    In mulit/exec context, the lastcmd is exec.\r\n2. Redis will crash when execute RPOPLPUSH loading from AOF\r\n    In fakeClient, the lastcmd is NULL.\r\n\r\nThis is introduced in commit 8a1219d93b3a9e3349c6f6726bf9923fcf2a40e5, sorry for that...",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-09-27T15:14:13Z",
        "closed_at": "2019-03-21T10:53:27Z",
        "merged_at": "2019-03-21T10:53:27Z",
        "body": "also, airty for COMMAND command was wrong.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 25,
        "changed_files": 2,
        "created_at": "2018-09-27T15:07:20Z",
        "closed_at": "2019-03-21T12:39:39Z",
        "merged_at": "2019-03-21T12:39:39Z",
        "body": "like in SUNIONSTORE etc, commands that perform writes are expected to open\r\nall keys, even input keys, with lookupKeyWrite",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2018-09-26T07:56:57Z",
        "closed_at": "2020-09-09T16:25:59Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-09-24T13:15:02Z",
        "closed_at": "2018-09-25T15:15:35Z",
        "merged_at": "2018-09-25T15:15:35Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-09-24T09:40:58Z",
        "closed_at": "2019-11-06T06:30:30Z",
        "merged_at": null,
        "body": "Cluster size is the number of master nodes with at least one slot. It used at fail-report and failover-auth. Only master with at least one slot can vote failover-auth, but all masters' fail-report is counted.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-09-20T23:46:41Z",
        "closed_at": "2018-09-25T15:18:51Z",
        "merged_at": "2018-09-25T15:18:51Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-09-19T17:23:21Z",
        "closed_at": "2018-10-31T10:42:05Z",
        "merged_at": "2018-10-31T10:42:05Z",
        "body": "During the full database resync we may still have unsaved changes\r\non the receiving side. This causes a race condition between\r\nsynced data rename/load and the rename of rdbSave tempfile.",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 3816,
        "deletions": 650,
        "changed_files": 68,
        "created_at": "2018-09-19T02:13:10Z",
        "closed_at": "2018-10-08T09:36:44Z",
        "merged_at": null,
        "body": "when is the Redis 5.0 GA version coming out?\r\nand We need to use the stream function.\r\n\r\nthanks.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2018-09-18T12:10:12Z",
        "closed_at": "2021-05-17T13:57:17Z",
        "merged_at": "2021-05-17T13:57:17Z",
        "body": "Hi @antirez , after we finish the pipeline optimization by avoiding too many `memmove`, I'am trying to find the others where `memmove` could be avoided, here I think one of them is about `aofrewrite`.\r\n\r\nBefore the `aofrewrite` we open some pipes, one is `server.aof_pipe_write_data_to_child` we use it to write appending data to child, and `memmove` appears in `aofChildWriteDiffData`:\r\n\r\n```c\r\n        if (block->used > 0) {\r\n            nwritten = write(server.aof_pipe_write_data_to_child,\r\n                             block->buf,block->used);\r\n            if (nwritten <= 0) return;\r\n            memmove(block->buf,block->buf+nwritten,block->used-nwritten);\r\n            block->used -= nwritten;\r\n            block->free += nwritten;\r\n        }\r\n```\r\n\r\nNormally in most linux, the pipe capacity is 64KB, and the `aofrwblock` size in redis is 10MB, so maybe it's a waste of time to call `memmove`.\r\n\r\nAt first, to check if it is needed to remove `memmove` I just record the latency of `aofChildWriteDiffData`, see bb18ff4c9ec40ced94cd599a581d581b0eed576e, and here is the result(redis-benchmark -d 1000 -r 10000000 -t set -n 10000000 -l):\r\n\r\n```\r\n127.0.0.1:6379> latency latest\r\n1) 1) \"aof-rewrite-write-data-to-child\"\r\n   2) (integer) 1537270575\r\n   3) (integer) 115\r\n   4) (integer) 119\r\n2) 1) \"aof-rewrite-diff-write\"\r\n   2) (integer) 1537270570\r\n   3) (integer) 513\r\n   4) (integer) 513\r\n```\r\n\r\nWe can see the latency, and then to avoid `memmove` I add a `pos` in `aofrwblock` to record the written position, and the latency disappears.\r\n\r\nBTW, after that we have to hold the 10MB buffer, but it is not a big deal I think.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2018-09-18T09:10:19Z",
        "closed_at": "2019-12-02T07:56:09Z",
        "merged_at": "2019-12-02T07:56:09Z",
        "body": "update comment",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 16,
        "changed_files": 1,
        "created_at": "2018-09-18T07:46:36Z",
        "closed_at": "2022-01-04T13:26:31Z",
        "merged_at": null,
        "body": "This commit fix the issue discussed in #5286.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2018-09-17T10:55:04Z",
        "closed_at": "2018-09-17T13:20:11Z",
        "merged_at": "2018-09-17T13:20:10Z",
        "body": "Extended the RedisModule_HashSet doc to mark that each call must end with NULL",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-09-16T12:21:34Z",
        "closed_at": "2018-09-17T13:41:24Z",
        "merged_at": "2018-09-17T13:41:24Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-09-13T12:42:28Z",
        "closed_at": "2018-09-17T13:42:51Z",
        "merged_at": "2018-09-17T13:42:51Z",
        "body": "No need to return \"+OK\" in this case since the result is an Array of all the nodes",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-09-12T21:22:13Z",
        "closed_at": "2018-10-02T20:29:57Z",
        "merged_at": null,
        "body": "**This PR:** Fixes some more minor spelling mistakes in the documentation for `lolwut.c`. I'm unsure if `caos` is a spelling mistake, as it's also present with that spelling in @antirez's blogpost about `lolwut`, so I'm happy to un-fix that if need be.\r\n\r\n<3 Redis!",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-09-11T11:35:21Z",
        "closed_at": "2018-09-11T13:29:59Z",
        "merged_at": null,
        "body": "FIx header of log about current redis role",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-09-06T15:37:47Z",
        "closed_at": "2021-03-15T13:16:39Z",
        "merged_at": null,
        "body": "This pull request changes the arity of [HSET](https://redis.io/commands/hset) to be in line with the documentation (arity of 4).\r\nCurrently HSET has arity of -4 (minimum of 4 arguments), which is incorrect per the documentation. Also, when more than 4 arguments are passed, HSET behaves similarly [HMSET](https://redis.io/commands/hmset) but with an integer reply instead of a simple string.\r\n\r\n[The documentation](https://redis.io/commands/hset) says the following about the reply of HSET:\r\n\r\n> Integer reply, specifically:\r\n>\r\n> 1 if field is a new field in the hash and value was set.\r\n> 0 if field already exists in the hash and the value was updated.\r\n\r\nHowever, with the current implementation that's not the case:\r\n\r\n```\r\n$ ./src/redis-cli\r\n127.0.0.1:6379> help hset\r\n\r\n  HSET key field value\r\n  summary: Set the string value of a hash field\r\n  since: 2.0.0\r\n  group: hash\r\n\r\n127.0.0.1:6379> hset myhash f1 v1\r\n(integer) 1\r\n127.0.0.1:6379> hset myhash f2 v2 f3 v3\r\n(integer) 2\r\n```\r\n\r\n\r\n\r\nWith my proposed changes:\r\n\r\n\r\n```\r\n$ ./src/redis-cli\r\n127.0.0.1:6379> hset myhash f1 v1\r\n(integer) 1\r\n127.0.0.1:6379> hset myhash f2 v2 f3 v3\r\n(error) ERR wrong number of arguments for 'hset' command\r\n```",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2018-09-06T13:15:10Z",
        "closed_at": "2018-10-17T05:50:32Z",
        "merged_at": "2018-10-17T05:50:32Z",
        "body": "`xadd` with id * generates random stream id\r\n\r\n`xadd` & `xtrim` with approximate maxlen count may trim stream randomly\r\n\r\n`xinfo` may get random radix-tree-keys/nodes\r\n\r\n`xpending` may get random idletime\r\n\r\n`xclaim`: master and slave may have different idletime in stream\r\n\r\nPing @antirez ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-09-06T05:42:38Z",
        "closed_at": "2018-09-06T15:41:31Z",
        "merged_at": "2018-09-06T15:41:31Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-09-05T12:10:12Z",
        "closed_at": "2022-02-09T10:07:50Z",
        "merged_at": null,
        "body": "It seems appendfilename/always-show-logo are missing for 'config get' command.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-09-05T09:08:59Z",
        "closed_at": "2018-09-07T10:06:03Z",
        "merged_at": "2018-09-07T10:06:03Z",
        "body": "If there is only one job and the thread calling bioWaitStepOfType() acquires the lock before decreasing bio_pending, the thread will wait even if there is no job. If the thread is the only thread creating jobs, it will cause deadlock.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-09-05T02:49:07Z",
        "closed_at": "2018-09-07T10:10:36Z",
        "merged_at": "2018-09-07T10:10:36Z",
        "body": "Sentinel uses randomized hz from 10 to 19 to avoid split-vote, but sentinelTimer is called every 100ms, so it is called every serverCron.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2018-09-03T16:05:37Z",
        "closed_at": "2018-09-04T10:49:51Z",
        "merged_at": "2018-09-04T10:49:51Z",
        "body": "If we are going to read a large object from network\r\ntry to make it likely that it will start at c->querybuf\r\nboundary so that we can optimize object creation\r\navoiding a large copy of data.\r\n\r\nBut only when the data we have not parsed is less than\r\nor equal to ll+2. If the data length is greater than\r\nll+2, trimming querybuf is just a waste of time, because\r\nat this time the querybuf contains not only our bulk.\r\n\r\nIt's easy to reproduce the that:\r\n\r\nTime1: call `client pause 10000` on slave.\r\n\r\nTime2: redis-benchmark -t set -r 10000 -d 33000 -n 10000.\r\n\r\nThen slave hung after 10 seconds.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2018-09-03T07:46:13Z",
        "closed_at": "2018-09-03T09:08:52Z",
        "merged_at": null,
        "body": "I think it's more efficiency to use pre-decrement  here. If I am wrong, I am eager to know what went wrong. best wishes!",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-09-02T07:23:35Z",
        "closed_at": "2018-09-07T10:31:28Z",
        "merged_at": "2018-09-07T10:31:28Z",
        "body": "This is the very essence of a trivial, 'drive-by' PR. Use it if it's useful.\r\n\r\nWas studying the geohash code and noticed some literals where there probably should have been `#define`d constants.\r\n\r\nAll tests passed locally. Probably good to merge this PR before #4291!",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-08-31T12:05:34Z",
        "closed_at": "2018-09-04T11:25:29Z",
        "merged_at": "2018-09-04T11:25:29Z",
        "body": "To avoid copying buffers to create a large Redis Object which\r\nexceeding PROTO_IOBUF_LEN 32KB, we just read the remaining data\r\nwe need, which may less than PROTO_IOBUF_LEN. But the remaining\r\nlen may be zero, if the bulklen+2 equals sdslen(c->querybuf),\r\nin client pause context.\r\n\r\nFor example:\r\n\r\nTime1:\r\n\r\n```\r\npython\r\n>>> import os, socket\r\n>>> server=\"127.0.0.1\"\r\n>>> port=6379\r\n>>> data1=\"*3\\r\\n$3\\r\\nset\\r\\n$1\\r\\na\\r\\n$33000\\r\\n\"\r\n>>> data2=\"\".join(\"x\" for _ in range(33000)) + \"\\r\\n\"\r\n>>> data3=\"\\n\\n\"\r\n>>> s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n>>> s.settimeout(10)\r\n>>> s.connect((server, port))\r\n>>> s.send(data1)\r\n28\r\n```\r\n\r\nTime2:\r\n```\r\nredis-cli client pause 10000\r\n```\r\nTime3:\r\n```\r\n>>> s.send(data2)\r\n33002\r\n>>> s.send(data3)\r\n2\r\n>>> s.send(data3)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nsocket.error: [Errno 104] Connection reset by peer\r\n```\r\nTo fix that, we should check if remaining is greater than zero.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2018-08-31T09:30:47Z",
        "closed_at": "2020-09-09T08:43:35Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-08-31T04:58:59Z",
        "closed_at": "2022-02-10T14:47:58Z",
        "merged_at": null,
        "body": "Only keep one address if duplicated.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-08-29T17:41:44Z",
        "closed_at": "2018-08-31T16:39:46Z",
        "merged_at": "2018-08-31T16:39:46Z",
        "body": "The conclusion, that a xread request can be answered syncronously in\r\ncase that the stream's last_id is larger than the passed last-received-id\r\nparameter, assumes, that there must be entries present, which could be\r\nreturned immediately.\r\nThis assumption fails for empty streams that actually contained some\r\nentries which got removed by xdel, ... .\r\n\r\nAs result, the client is answered synchronously with an empty result,\r\ninstead of blocking for new entries to arrive.\r\nAn additional check for a non-empty stream is required.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 63,
        "deletions": 182,
        "changed_files": 10,
        "created_at": "2018-08-29T13:04:58Z",
        "closed_at": "2018-09-05T03:34:45Z",
        "merged_at": null,
        "body": "Maybe a new way for lua replication.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2018-08-28T12:15:29Z",
        "closed_at": "2018-08-29T03:25:41Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 106,
        "deletions": 5,
        "changed_files": 6,
        "created_at": "2018-08-27T17:23:40Z",
        "closed_at": "2021-04-04T06:43:24Z",
        "merged_at": "2021-04-04T06:43:24Z",
        "body": "Previously (and by default after commit) when master loose its last slot\r\n(due to migration, for example), its replicas will migrate to new last slot\r\nholder.\r\n\r\nThere are cases where this is not desired:\r\n* Consolidation that results with removed nodes (including the replica, eventually).\r\n* Manually configured cluster topologies, which the admin wishes to preserve.\r\n\r\nNeedlessly migrating a replica triggers a full synchronization and can have a negative impact, so\r\nwe prefer to be able to avoid it where possible.\r\n\r\nThis commit adds 'cluster-allow-replica-migration' configuration option that is\r\nenabled by default to preserve existed behavior. When disabled, replicas will\r\nnot be auto-migrated.\r\n\r\nFixes #4896",
        "comments": 17
    },
    {
        "merged": false,
        "additions": 54,
        "deletions": 57,
        "changed_files": 4,
        "created_at": "2018-08-27T13:31:18Z",
        "closed_at": "2018-09-07T10:00:06Z",
        "merged_at": null,
        "body": "After PR #5244 we record the position we have read in querybuf,\r\nso we can use querybuf+qb_pos to feed replication stream\r\ninstead of pending_querybuf. The middle slave still performs as\r\na proxy, because the data in pending_querybuf is just a copy of\r\nquerybuf. Maybe querybuf+qb_pos will propagate partial command\r\nif the command contains some large objects, but it's safe because\r\nredis increment read_reploff only after processCommand succeed.\r\n\r\nThanks to @0xtonyxia suggestion.",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-08-27T05:54:56Z",
        "closed_at": "2018-09-07T10:29:04Z",
        "merged_at": null,
        "body": "### Steps to reproduce the issue:\r\n1. load a redis module, `helloworld.so` from redis source for example\r\n2. start a new redis-cli, the redis-cli process will enter an infinite loop and exhaust CPU to 100%\r\n\r\n## infinite loop reason\r\n1. argument number of a command loaded from module is always -1. https://github.com/antirez/redis/blob/unstable/src/module.c#L663\r\n2. If the command from a module sets firstkey = 1, lastkey = 1,  args will be reduced to -1 before the while loop condition\r\n\r\nWhat's more, passing a variable `arity` as argument number of a command in `RedisModule_CreateCommand` api may be a better solution, but it breaks compatibility of existing modules.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2018-08-27T04:11:07Z",
        "closed_at": "2018-08-29T14:13:43Z",
        "merged_at": "2018-08-29T14:13:43Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2018-08-27T03:03:16Z",
        "closed_at": "2018-08-30T02:06:34Z",
        "merged_at": null,
        "body": "This commit f63e81c202cac6566bbe06cc576ff3aa8e1863a4 has a merge error",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2018-08-25T16:00:00Z",
        "closed_at": "2018-08-29T10:45:32Z",
        "merged_at": null,
        "body": "This allows easy support for using distribution-suppled\r\nlibraries where the usage of embedded code copies is\r\nfrowned upon.",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2018-08-22T15:02:38Z",
        "closed_at": "2018-12-07T11:27:32Z",
        "merged_at": "2018-12-07T11:27:32Z",
        "body": "",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 332,
        "deletions": 16,
        "changed_files": 7,
        "created_at": "2018-08-22T14:08:01Z",
        "closed_at": "2021-03-13T22:42:15Z",
        "merged_at": null,
        "body": "Currently, `redis-cli --cluster fix` usually does a bad things when tries to fix half-inited or half-finished slot migration.\r\nAdd more special cases for all usual migration issues:\r\n1. when migration is half inited on \"migrating\" node\r\n2. when migration is half inited on \"importing\" node\r\n3. migration is inited, but not finished (already were existed and worked, but moved a bit higher)\r\n4. migration is half finished on \"migrating\" node\r\n5. migration is half finished on \"importing\" node",
        "comments": 15
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2018-08-22T10:37:34Z",
        "closed_at": "2018-09-14T11:31:46Z",
        "merged_at": null,
        "body": "sync with antirez/redis",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-08-22T10:08:23Z",
        "closed_at": "2018-08-29T14:19:14Z",
        "merged_at": "2018-08-29T14:19:14Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2018-08-21T09:34:52Z",
        "closed_at": "2021-03-25T11:02:14Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 17,
        "changed_files": 3,
        "created_at": "2018-08-21T08:57:58Z",
        "closed_at": "2018-08-27T11:19:32Z",
        "merged_at": "2018-08-27T11:19:32Z",
        "body": "Few tests had borderline thresholds that were adjusted.\r\n\r\nThe slave buffers test had two issues, preventing the slave buffer from growing:\r\n1) the slave didn't necessarily go to sleep on time, or woke up too early,\r\n   now using SIGSTOP to make sure it goes to sleep exactly when we want.\r\n2) the master disconnected the slave on timeout",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-08-20T07:29:38Z",
        "closed_at": "2018-09-03T11:57:33Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-08-16T08:37:33Z",
        "closed_at": "2020-09-09T16:26:02Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2018-08-16T03:17:55Z",
        "closed_at": "2020-08-11T06:22:19Z",
        "merged_at": null,
        "body": "sendfile is more efficient than the combination of read and write in the process of synchronization\uff0cwhich would avoid transferring data to and from user space .",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2018-08-15T08:03:50Z",
        "closed_at": "2018-09-06T08:27:15Z",
        "merged_at": null,
        "body": "freeMemoryIfNeeded() would propagate DEL before the lua script,\r\nthat will break lua scripts atomicity, and lead to inconsistency\r\nbetween master and slave and AOF.",
        "comments": 28
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2018-08-14T13:00:09Z",
        "closed_at": "2018-08-26T14:31:25Z",
        "merged_at": "2018-08-26T14:31:25Z",
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-08-14T01:42:59Z",
        "closed_at": "2018-08-15T01:12:10Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 56,
        "deletions": 54,
        "changed_files": 3,
        "created_at": "2018-08-13T16:53:03Z",
        "closed_at": "2018-08-26T14:30:50Z",
        "merged_at": "2018-08-26T14:30:50Z",
        "body": "This is an optimization for processing pipeline, we discussed a\r\nproblem in issue #5229: clients may be paused if we apply `CLIENT\r\nPAUSE` command, and then querybuf may grow too large, the cost of\r\nmemmove in sdsrange after parsing a completed command will be\r\nhorrible. The optimization is that parsing all commands in queyrbuf\r\n, after that we can just call sdsrange only once.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-08-13T14:44:57Z",
        "closed_at": "2019-03-21T10:54:10Z",
        "merged_at": "2019-03-21T10:54:10Z",
        "body": "this is very confusing to see the server disappears as if it got SIGKILL when it was not the case.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2018-08-13T14:42:41Z",
        "closed_at": "2018-10-02T14:03:06Z",
        "merged_at": "2018-10-02T14:03:06Z",
        "body": "before this commit, the MEMORY command reported only overheads and the INFO command reported only script code. i think that's misleading.\r\n@itamarhaber if/when this is merged, please update the docs again.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-08-13T05:52:34Z",
        "closed_at": "2021-06-24T06:56:32Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-08-12T16:51:07Z",
        "closed_at": "2018-09-07T10:28:22Z",
        "merged_at": "2018-09-07T10:28:22Z",
        "body": "Fix proposal for issue #5238",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2018-08-12T12:15:48Z",
        "closed_at": "2020-09-09T16:26:05Z",
        "merged_at": null,
        "body": "I hope this is the right branch.\r\n\r\n1. (ThreadSafeContextStart/Stop -> ThreadSafeContextLock/Unlock)\r\n2. minor typo.\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-08-09T14:40:18Z",
        "closed_at": "2021-07-23T16:25:10Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-08-08T09:33:42Z",
        "closed_at": "2020-07-29T12:24:16Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 154,
        "deletions": 155,
        "changed_files": 51,
        "created_at": "2018-08-08T07:35:16Z",
        "closed_at": "2020-09-09T16:29:10Z",
        "merged_at": null,
        "body": "I found some errors in the comments when I read the source code, so I checked most of the files and fixed the typos that looked obvious. I hope this will improve the accuracy of the description and the experience of reading.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-08-07T15:26:11Z",
        "closed_at": "2020-05-07T14:41:52Z",
        "merged_at": null,
        "body": "Hi.\r\nThere are no return statements in the redisIsSupervised function.\r\nI think it is bug.\r\nplease check it.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-08-07T06:22:41Z",
        "closed_at": "2021-06-30T03:09:54Z",
        "merged_at": null,
        "body": "The dict will not expand when dict_can_resize==0 && used==size.\r\nAfter, the dict may expand when used=size+1, so that:\r\n                      realsize=_dictNextPower(used X 2)=4 X size.\r\n      When the size is too large and the maxmemory is limited, this would be a disaster.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-08-07T02:36:54Z",
        "closed_at": "2021-11-24T09:24:28Z",
        "merged_at": null,
        "body": "Unix domain socket max path length is 107 bytes, need to check it before startup.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-08-06T08:42:38Z",
        "closed_at": "2018-08-06T11:58:45Z",
        "merged_at": null,
        "body": "The function definition in sds.h was guarded with an #ifdef REDIS_TEST,\r\nbut the function itself is guarded with an #ifdef SDS_TEST_MAIN, fix the\r\ndefinition in the header to match.\r\n\r\nCloses #5091",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-08-03T17:08:33Z",
        "closed_at": "2018-10-02T14:18:55Z",
        "merged_at": "2018-10-02T14:18:55Z",
        "body": "Hi @antirez , I think we should update the listpack with new pointer in `xdel`, because `lpReplaceInteger` may trigger `zrealloc`.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-08-03T09:16:34Z",
        "closed_at": "2021-06-28T13:28:09Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2018-08-03T07:57:20Z",
        "closed_at": "2021-12-21T06:37:24Z",
        "merged_at": null,
        "body": "Some non deterministic WRITE commands like `SPOP` should not be allowed in lua script.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2018-08-02T07:03:54Z",
        "closed_at": "2018-10-09T09:47:04Z",
        "merged_at": "2018-10-09T09:47:04Z",
        "body": "If we lost EXEC when loading aof, I think we should discard the transaction, or it will break the atomicity.",
        "comments": 22
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-08-02T02:46:23Z",
        "closed_at": "2018-09-18T11:05:32Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-08-01T16:51:03Z",
        "closed_at": "2021-11-09T06:28:37Z",
        "merged_at": null,
        "body": "In short connection scenario, if password is set,  it must auth password every time when create a connection. \r\n\r\nIn raw time_independent_strcmp we have to do 512 char to char xor.  In this PR, this  reduce to 64 unsigned long long to unsigned long long xor. \r\n\r\nI make a test in my machine. For 100 million time_independent_strcmp costs  73514 milli sec, and in this PR  time_independent_strcmp costs 16436 milli sec, 4.4 times faster.\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5516,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2018-08-01T12:46:47Z",
        "closed_at": "2021-04-27T05:15:52Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2018-08-01T11:37:43Z",
        "closed_at": "2022-01-04T13:26:05Z",
        "merged_at": null,
        "body": "The while loop is useless and it may send redundant commands to server.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-07-31T17:31:43Z",
        "closed_at": "2020-09-09T16:26:09Z",
        "merged_at": null,
        "body": "Made correction in spelling of some misspelled words.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-07-31T08:05:22Z",
        "closed_at": "2018-07-31T15:11:17Z",
        "merged_at": "2018-07-31T15:11:17Z",
        "body": "Fix cluster-announce-ip memory leak and refactor setting myself->ip.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2018-07-31T04:10:51Z",
        "closed_at": "2018-07-31T14:40:36Z",
        "merged_at": "2018-07-31T14:40:36Z",
        "body": "This PR is about expanding Lazyfree's option `lazyfree-lazy-server-del` to make it be available to `dbOverwrite`.\r\n\r\nKeys will be implicitly deleted not only in `dbDelete` like `rename` and `move` commands, but also in `dbOverwrite` like `set` and `bitop` commands, so I think we should implement that.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 83,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2018-07-30T16:15:28Z",
        "closed_at": "2018-07-31T15:20:09Z",
        "merged_at": "2018-07-31T15:20:09Z",
        "body": "This PR contains two commits:\r\n1. add DEBUG LOG, to to assist test suite debugging\r\n\r\n2. test suite conveniency improvements\r\n* allowing --single to be repeated\r\n* adding --only so that only a specific test inside a unit can be run\r\n* adding --skiptill useful to resume a test that crashed passed the problematic unit.\r\n  useful together with --clients 1\r\n* adding --skipfile to use a file containing list of tests names to skip\r\n* printing the names of the tests that are skiped by skipfile or denytags\r\n* adding --config to add config file options from command line",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2018-07-30T14:46:28Z",
        "closed_at": "2018-07-30T23:36:30Z",
        "merged_at": null,
        "body": "When do port sanity checking, 0 should be invalid, and be consistent with redis cluster.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2018-07-30T13:35:06Z",
        "closed_at": "2018-07-31T15:58:14Z",
        "merged_at": "2018-07-31T15:58:14Z",
        "body": "",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2018-07-30T13:19:17Z",
        "closed_at": "2018-07-30T15:59:13Z",
        "merged_at": "2018-07-30T15:59:13Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-07-27T02:25:22Z",
        "closed_at": "2020-09-01T00:24:42Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2018-07-26T09:13:32Z",
        "closed_at": "2020-09-01T05:04:42Z",
        "merged_at": null,
        "body": "Consider following commands order:\r\n\r\n1. set key 10\r\n2. expire key 120\r\n\r\nUsually the master and slave have different expire time point and the differ should be very small. But If the expire command is not replicated to slave and some network problem happens or slave down. \r\nWhen slave reboot, it send psync2 to master, and psync is accepted.  In this scenario, the differ may be large and slave have newer expire time point. More serious, if failover happens,   the slave becomes new master, a user may find that a already expired key  which is in old master that could visit in new master.\r\n\r\nData  inconsistent happens.\r\n",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2018-07-25T14:52:05Z",
        "closed_at": "2018-07-30T16:03:16Z",
        "merged_at": "2018-07-30T16:03:16Z",
        "body": "Issue: #5033 ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2018-07-24T10:50:02Z",
        "closed_at": "2018-07-28T09:39:05Z",
        "merged_at": null,
        "body": "Typo fix in comments AutMemEntry -> AutoMemEntry\r\nAnd Fix following compile warning..\r\n'''\r\nlistpack.c: In function \u2018lpSeek\u2019:\r\nlistpack.c:751:19: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         if (index >= numele) return NULL; /* Out of range the other side. */\r\n                   ^~\r\nlistpack.c:754:19: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         if (index > numele/2) {\r\n'''",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2018-07-22T09:53:57Z",
        "closed_at": "2018-07-24T08:59:56Z",
        "merged_at": "2018-07-24T08:59:56Z",
        "body": "",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-22T07:18:37Z",
        "closed_at": "2018-07-23T22:08:00Z",
        "merged_at": "2018-07-23T22:07:59Z",
        "body": "in case you don't merge https://github.com/antirez/redis/pull/5159, please merge this silly bug.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 173,
        "deletions": 27,
        "changed_files": 9,
        "created_at": "2018-07-22T07:15:06Z",
        "closed_at": "2020-10-01T08:37:21Z",
        "merged_at": null,
        "body": "This PR contains several commits handling related things. please feel free to cherry pick just the first 2 commits, or any other portions you like.\r\n\r\n1) Track and report memory used by clients argv.\r\nthis is very useful in case clients started sending a command and didn't\r\ncomplete it. in which case the first args of the command are already\r\ntrimmed from the query buffer.\r\n\r\n2) ~~report memory used by scripts (not the lua lib itself).\r\nthis is useful in case a bad client is flooding the server with many\r\nunique scripts, and their code in redis takes a lot of ram.~~ please use #4883 instead\r\n\r\n3) Include internal sds fragmentation in MEMORY reporting\r\n\r\n4) performance and memory reporting improvment - sds take control of it's internal frag\r\n\tthis commit has two aspects:\r\n\tA) improve memory reporting for all the places that use sdsAllocSize to compute\r\n   memory used by a string, in this case it'll include the internal fragmentation.\r\n\tB) reduce the need for realloc calls by making the sds implicitly take over\r\n   the internal fragmentation of the block it allocated.\r\n",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-07-21T18:16:42Z",
        "closed_at": "2018-07-24T08:28:16Z",
        "merged_at": "2018-07-24T08:28:54Z",
        "body": "Removed redundant check",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-21T17:54:19Z",
        "closed_at": "2020-09-09T16:26:12Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-07-21T02:01:59Z",
        "closed_at": "2018-07-24T15:24:37Z",
        "merged_at": "2018-07-24T15:24:37Z",
        "body": "There's no need to migrate already expired keys.\r\nThese keys should remain and wait for lazy deletion in activeExpireCycle().",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-07-21T00:54:32Z",
        "closed_at": "2018-07-30T16:10:07Z",
        "merged_at": "2018-07-30T16:10:07Z",
        "body": "I think this is a missing piece of code when processing lua script.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2018-07-20T10:12:14Z",
        "closed_at": "2018-08-03T11:39:33Z",
        "merged_at": "2018-08-03T11:39:33Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-20T03:09:22Z",
        "closed_at": "2018-07-25T08:41:32Z",
        "merged_at": null,
        "body": "..., so that we save one computation if rehash does not happen.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-19T13:06:58Z",
        "closed_at": "2018-08-03T11:45:28Z",
        "merged_at": "2018-08-03T11:45:28Z",
        "body": "Error is not found because that `min-idle-time` is kind of a valid stream id. But i think we'd better make it accurate and right :-). @antirez ",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 3585,
        "deletions": 501,
        "changed_files": 49,
        "created_at": "2018-07-19T00:30:51Z",
        "closed_at": "2022-01-04T23:20:29Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-18T22:59:11Z",
        "closed_at": "2020-01-30T18:10:17Z",
        "merged_at": null,
        "body": "Since the hash table size is always a power of 2, calculating a hash for every key is not needed when shrinking the hash table. The slot's index contains enough bits to determine where the key belongs in the new table. This improves performance.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 86,
        "deletions": 13,
        "changed_files": 2,
        "created_at": "2018-07-18T09:19:35Z",
        "closed_at": "2018-10-08T10:00:01Z",
        "merged_at": "2018-10-08T10:00:01Z",
        "body": "Bugs reported:\r\n\r\n1. XADD with MAXLEN > stream length will lead to inconsistency:\r\n\r\n    ```c\r\n            if (!streamTrimByLength(s,maxlen,approx_maxlen)) {\r\n                /* If no trimming was performed, for instance because approximated\r\n                 * trimming length was specified, rewrite the MAXLEN argument\r\n                 * as zero, so that the command is propagated without trimming. */\r\n                robj *zeroobj = createStringObjectFromLongLong(0);\r\n                rewriteClientCommandArgument(c,maxlen_arg_idx,zeroobj);\r\n                decrRefCount(zeroobj);\r\n            }\r\n    ```\r\n\r\n    If `maxlen` > stream length or in some cases using `approx_maxlen`, no trimming was performed, but we should not propagate zero `maxlen`, because `MAXLEN 0` means deleting all entries from stream.\r\n\r\n2. XADD/XTRIM with multi MAXLEN may behave wrong:\r\n\r\n    `XTRIM key MAXLEN ~ 10 MAXLEN 100`, what we want is the last `MAXLEN`: `XTRIM key MAXLEN 100`, but actually redis take it as `XTRIM key MAXLEN ~ 100`.\r\n\r\n3. Propagating approximated MAXLEN may lead to inconsistency:\r\n\r\n    Slaves and rebooting redis may have different radix tree struct, by different stream* config options. So propagating approximated `MAXLEN` to AOF/slaves may lead to date inconsistency.\r\n\r\n    Here I rewrite `~` as `=` to make `maxlen` specified.\r\n\r\n4. MAXLEN in XTRIM should not < 0.\r\n\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 16,
        "changed_files": 2,
        "created_at": "2018-07-18T07:47:30Z",
        "closed_at": "2020-09-27T14:06:47Z",
        "merged_at": null,
        "body": "",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2018-07-18T07:21:52Z",
        "closed_at": "2018-07-18T09:05:04Z",
        "merged_at": "2018-07-18T09:05:04Z",
        "body": "on slower machines, the active defrag test tended to fail.\r\nalthough the fragmentation ratio was below the treshold, the defragger was\r\nstill in the middle of a scan cycle.\r\n\r\nthis commit changes:\r\n- the defragger uses the current fragmentation state, rather than the cache one\r\n  that is updated by server cron every 100ms. this actually fixes a bug of\r\n  starting one excess scan cycle\r\n- the test lets the defragger use more CPU cycles, in hope that the defrag\r\n  will be faster, but also give it more time before we give up.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-17T10:50:31Z",
        "closed_at": "2018-07-17T15:50:31Z",
        "merged_at": "2018-07-17T15:50:31Z",
        "body": "Redis will always return an empty result when '$' ID is specified with xreadgroup command, i think it's meaningless.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 9,
        "changed_files": 3,
        "created_at": "2018-07-17T09:55:48Z",
        "closed_at": "2018-07-17T15:33:58Z",
        "merged_at": "2018-07-17T15:33:58Z",
        "body": "The slave sends \\n keepalive messages to the master while parsing the rdb,\r\nand later sends REPLCONF ACK once a second. rarely, the master recives both\r\na linefeed char and a REPLCONF in the same read, `\\n*3\\r\\n$8\\r\\nREPLCONF\\r\\n...`\r\nand it tries to trim two chars (\\r\\n) from the query buffer,\r\ntrimming the `*` from `*3\\r\\n$8\\r\\nREPLCONF\\r\\n...`\r\n\r\nthen the master tries to process a command starting with '3' and replies to\r\nthe slave a bunch of -ERR and one +OK.\r\nalthough the slave silently ignores these (prints a log message), this corrupts\r\nthe replication offset at the slave since the slave increases the replication\r\noffset, and the master did not.\r\n\r\nother than the fix in processInlineBuffer, i did several other improvments\r\nwhile hunting this very rare bug.\r\n\r\n- when redis replies with \"unknown command\" it includes a portion of the\r\n  arguments, not just the command name. so it would be easier to understand\r\n  what was recived, in my case, on the slave side,  it was -ERR, but\r\n  the \"arguments\" were the interesting part (containing info on the error).\r\n- about a year ago i added code in addReplyErrorLength to print the error to\r\n  the log in case of a reply to master (since this string isn't actually\r\n  trasmitted to the master), now changed that block to print a similar log\r\n  message to indicate an error being sent from the master to the slave.\r\n  note that the slave is marked as CLIENT_SLAVE only after PSYNC was received,\r\n  so this will not cause any harm for REPLCONF, and will only indicate problems\r\n  that are gonna corrupt the replication stream anyway.\r\n- two places were c->reply was emptied, and i wanted to reset sentlen\r\n  this is a precaution (i did not actually see such a problem), since a\r\n  non-zero sentlen will cause corruption to be transmitted on the socket.",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-07-17T03:23:51Z",
        "closed_at": "2018-07-24T16:57:06Z",
        "merged_at": "2018-07-24T16:57:06Z",
        "body": "Hi @antirez, we forgot `streamIteratorStop` after `streamIteratorStart` in `xdel`, that's where memory leak happens I think ...",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-07-16T12:51:14Z",
        "closed_at": "2018-07-16T14:04:33Z",
        "merged_at": "2018-07-16T14:04:33Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-07-16T07:58:46Z",
        "closed_at": "2018-07-16T10:39:39Z",
        "merged_at": "2018-07-16T10:39:38Z",
        "body": "",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-07-16T05:30:03Z",
        "closed_at": "2018-07-16T10:24:15Z",
        "merged_at": "2018-07-16T10:24:15Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-07-16T03:53:40Z",
        "closed_at": "2018-07-17T08:38:56Z",
        "merged_at": "2018-07-17T08:38:56Z",
        "body": "When we need free some memory, we first choose some keys to free. In dictGetSomeKeys,  the first loop check may be useless. \r\nSuppose we are in rehash that going from big to small table. The first table  has size 32, the second is  16, the  rehashidx is 27, and the first point is 20.  We get the first loop is useless.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-15T15:25:36Z",
        "closed_at": "2018-07-16T16:32:15Z",
        "merged_at": "2018-07-16T16:32:15Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 182,
        "deletions": 50,
        "changed_files": 10,
        "created_at": "2018-07-15T15:21:15Z",
        "closed_at": "2018-07-16T15:45:51Z",
        "merged_at": "2018-07-16T15:45:50Z",
        "body": "A) slave buffers didn't count internal fragmentation and sds unused space,\r\n   this caused them to induce eviction although we didn't mean for it.\r\n\r\nB) slave buffers were consuming about twice the memory of what they actually needed.\r\n- this was mainly due to sdsMakeRoomFor growing to twice as much as needed each time\r\n  but networking.c not storing more than 16k (partially fixed recently in 237a38737).\r\n- besides it wasn't able to store half of the new string into one buffer and the\r\n  other half into the next (so the above mentioned fix helped mainly for small items).\r\n- lastly, the sds buffers had up to 30% internal fragmentation that was wasted,\r\n  consumed but not used.\r\n\r\nC) inefficient performance due to starting from a small string and reallocing many times.\r\n\r\nwhat i changed:\r\n- creating dedicated buffers for reply list, counting their size with zmalloc_size\r\n- when creating a new reply node from, preallocate it to at least 16k.\r\n- when appending a new reply to the buffer, first fill all the unused space of the\r\n  previous node before starting a new one.\r\n\r\nother changes:\r\n- expose mem_not_counted_for_evict info field for the benefit of the test suite\r\n- add a test to make sure slave buffers are counted correctly and that they don't cause eviction",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-07-15T06:53:45Z",
        "closed_at": "2022-02-13T11:28:58Z",
        "merged_at": null,
        "body": "if we use 1023 bytes string for ERROR macro.\r\nIt can cause buffer overrun.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 46,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2018-07-15T02:48:04Z",
        "closed_at": "2020-06-10T12:34:34Z",
        "merged_at": null,
        "body": "since linux 3.15, linux could get / set the state of the\r\n\"THP disable\" flag for the calling thread. we can use this API\r\nto disable THP for self process without modifing any global\r\nkernel config.\r\n\r\nchanges in this patch :\r\n1, modify API THPIsEnabled to THPKernelIsEnabled, it means that\r\nthis API gets the global kernel THP config.\r\n2, add new API THPProcIsEnabled, this API could get process self\r\nTHP config by prctl(PR_GET_THP_DISABLE...).\r\n3, add new API THPProcDisable, this API could disable self THP\r\nconfig.\r\n4, server checks kernel THP config firstly, because this config\r\ncontrol khungepaged running or not. if kernel THP is enabled,\r\ntry to disable THP for self process.\r\n\r\nabout THP side effect :\r\naccording to http://www.antirez.com/news/84, we can see that\r\nredis latency spikes are caused by linux kernel THP feature.\r\nI have tested on E3-2650 v3, and found that 2M huge page costs\r\nabout 0.25ms to fix COW page fault.\r\n\r\ntests this patch :\r\nthis patch has been tested on ubuntu-1804(linux 4.15) and centos-7\r\n(linux 3.10), and work well.\r\n\r\nSigned-off-by: zhenwei pi <p_ace@126.com>",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2018-07-14T07:33:27Z",
        "closed_at": "2022-02-07T15:50:16Z",
        "merged_at": null,
        "body": "Fix some unclosed files before terminating the process.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-14T01:31:54Z",
        "closed_at": "2018-07-17T16:08:47Z",
        "merged_at": "2018-07-17T16:08:47Z",
        "body": "Accept write commands if persisting is disabled,\r\nevent if we do have problems persisting on disk\r\npreviously.\r\n\r\n\"server.aof_state != AOF_OFF\" is just the counterpart of \"server.saveparamslen > 0\".\r\nI think the logic should be consistent.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2018-07-13T10:05:45Z",
        "closed_at": "2018-07-13T15:35:16Z",
        "merged_at": "2018-07-13T15:35:16Z",
        "body": "I noticed that there's an inconsistency of slowlog-log-slower-than.\r\nSee\r\nhttps://github.com/antirez/redis/blob/d4182a0a0d36c97c42603fdd8ff6db750ec26580/src/slowlog.c#L124\r\nand\r\nhttps://github.com/antirez/redis/blob/d4182a0a0d36c97c42603fdd8ff6db750ec26580/src/latency.c#L297-L299\r\n\r\nI believe this is a mistake. After digging into the code history, I found that slowlog log would be disabled if we set slowlog-log-slower-than to -1. And redis take every command as slowlog when slowlog-log-slower-than set to 0.\r\n@antirez ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2018-07-12T13:22:13Z",
        "closed_at": "2018-07-12T17:07:41Z",
        "merged_at": "2018-07-12T17:07:41Z",
        "body": "When check rdb file, it is unnecessary to check role.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-07-11T13:11:01Z",
        "closed_at": "2018-07-11T20:07:47Z",
        "merged_at": "2018-07-11T20:07:47Z",
        "body": "* overflow in jemalloc fragmentation hint to the defragger\r\n\r\nsorry about that, same bug as before.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 10,
        "changed_files": 3,
        "created_at": "2018-07-10T08:49:09Z",
        "closed_at": "2018-07-16T16:34:35Z",
        "merged_at": "2018-07-16T16:34:35Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2018-07-10T08:29:02Z",
        "closed_at": "2018-07-12T11:04:22Z",
        "merged_at": "2018-07-12T11:04:22Z",
        "body": "For issue #5111.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 362,
        "deletions": 362,
        "changed_files": 1,
        "created_at": "2018-07-07T07:23:29Z",
        "closed_at": "2018-07-09T10:59:14Z",
        "merged_at": "2018-07-09T10:59:14Z",
        "body": "A simple patch that clears whitespace in src/redis-cli.c",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-07T00:46:35Z",
        "closed_at": "2018-07-09T10:57:13Z",
        "merged_at": "2018-07-09T10:57:13Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 448,
        "deletions": 3,
        "changed_files": 5,
        "created_at": "2018-07-06T09:38:37Z",
        "closed_at": "2018-12-15T09:04:14Z",
        "merged_at": null,
        "body": "For memcached 1.3.x and higher, you can enable and obtain detailed statistics about the get, set, and del operations on the individual keys stored in the cache, and determine whether the attempts hit (found) a particular key. These operations are only recorded while the detailed stats analysis is turned on. \r\n```\r\nstats detail on\r\n\r\nstats detail dump\r\nPREFIX user get 0 hit 0 set 1 del 0\r\nPREFIX post get 0 hit 0 set 1 del 0\r\nPREFIX abc get 1 hit 0 set 0 del 0\r\nPREFIX def get 3 hit 3 set 1 del 0\r\nEND\r\n```\r\n\r\nIt's very useful in product. We can find out hit rate of some kind of cache key are bad.\r\nAnd the key should be {prefix}:{id}, if not nothing happened.\r\n\r\nThis merge request finished some commands, like: set, get, zrem, zadd, set, hget, setex...\r\n\r\n```\r\n127.0.0.1:6379> set n:1 1\r\nOK\r\n127.0.0.1:6379> set n:2 2\r\nOK\r\n127.0.0.1:6379> get n:1\r\n\"1\"\r\n127.0.0.1:6379> info detail\r\n# Detail\r\nPREFIX n get 1 hit 1 set 2 del 0\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-05T07:27:31Z",
        "closed_at": "2018-07-09T11:03:58Z",
        "merged_at": "2018-07-09T11:03:58Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-04T15:37:49Z",
        "closed_at": "2018-07-09T11:14:58Z",
        "merged_at": "2018-07-09T11:14:58Z",
        "body": "http://help.github.com/send-pull-requests/ \r\nis no longer supported\r\n\r\nthis change modifies the link to the working one\r\nhttps://help.github.com/articles/creating-a-pull-request/",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-07-04T12:43:58Z",
        "closed_at": "2018-07-09T11:25:56Z",
        "merged_at": null,
        "body": "1. The 'cliConnect' is duplicated with the one called by 'cliIntegrateHelp',  it is unnecessary.\r\n2. Because of that, when client could not connect to the redis server, there will be two connection error log:\r\n```\r\n$ ~/redis-cli\r\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\r\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\r\nnot connected>\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2018-07-04T12:06:38Z",
        "closed_at": "2018-07-09T11:56:40Z",
        "merged_at": "2018-07-09T11:56:40Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-07-02T11:28:58Z",
        "closed_at": "2018-07-02T15:51:04Z",
        "merged_at": "2018-07-02T15:51:04Z",
        "body": "Issue #5063 added --wait-server option, but can not work properly.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2018-07-01T06:48:58Z",
        "closed_at": "2018-07-03T11:19:38Z",
        "merged_at": "2018-07-03T11:19:38Z",
        "body": "Bugfix:the usedmemory of slave will be too larger than master after set a big value or migrate a big list\u3001hash and so on\uff1a\r\n\r\ntest \"pending querybuf: check size of pending_querybuf after set a big value\" {\r\n+        $slave slaveof $master_host $master_port\r\n+        set _v [prepare_value [expr 32*1024*1024]]\r\n+        $master set key $_v \r\n+        after 2000\r\n+        set m_usedmemory [info_memory $master used_memory]\r\n+        set s_usedmemory [info_memory $slave used_memory]\r\n+        if { $s_usedmemory > $m_usedmemory + 10*1024*1024 } {\r\n+            fail \"the used_memory of slave is too larger than master.Master:$m_usedmemory Slave:$s_usedmemory\"\r\n+        }\r\n+    }",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 99,
        "deletions": 99,
        "changed_files": 40,
        "created_at": "2018-07-01T05:31:22Z",
        "closed_at": "2018-07-03T16:19:47Z",
        "merged_at": "2018-07-03T16:19:47Z",
        "body": "Fix some typo.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 78,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2018-06-30T01:42:03Z",
        "closed_at": "2018-07-03T10:36:54Z",
        "merged_at": "2018-07-03T10:36:54Z",
        "body": "When masters in a specific partition fails consecutively, slaves may not able to failover automatically while they should.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2018-06-28T15:52:50Z",
        "closed_at": "2019-11-06T06:30:24Z",
        "merged_at": null,
        "body": "* Misuse `keys_freed` as freed keys count and key freed flag.\r\n`keys_freed` is a local variable defined in while block, so `!(keys_freed % 16)` is always false.\r\n\r\n* Wrong `cant_free` handling.\r\nThere are three problems of `can_free` handling:\r\n  1. I want to figure out what `(mem_reported - zmalloc_used_memory()) + mem_freed) >= mem_tofree` means, but i failed. I think there is something wrong.\r\n  2. `cant_free` always returns `C_ERR` even if we reached target memory.\r\n  3. Maybe all lazyfree jobs finished between `goto cant_free` and `while (bioPendingJobsOfType(BIO_LAZY_FREE))`.  In this case, we simply return `C_ERR` now.\r\n\r\nemmm... I have 2 questions:\r\n1. Is it possible that `used_memory` less than `overhead`?\r\n```c\r\n    mem_used = mem_reported;\r\n    size_t overhead = freeMemoryGetNotCountedMemory();\r\n    mem_used = (mem_used > overhead) ? mem_used-overhead : 0;\r\n```\r\n  I checked it myself. I think it is impossible, so i use `zmalloc_used_memory() - mem_overhead <= server.maxmemory` directly.\r\n\r\n2. Why not use `bioWaitStepOfType()`? like:\r\n```c\r\n    while (bioWaitStepOfType(BIO_LAZY_FREE)) {\r\n        if (zmalloc_used_memory() - mem_overhead <= server.maxmemory)\r\n            return C_OK;\r\n    }\r\n```\r\nIs it wasteful to check on every lazyfree jobs finished? Is `usleep(1000)` too long?\r\n\r\nI'm glad to know your consideration. : )\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2018-06-28T14:12:21Z",
        "closed_at": "2018-07-04T04:04:27Z",
        "merged_at": null,
        "body": "change MIGRATE command help",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2018-06-28T10:46:16Z",
        "closed_at": "2018-06-29T11:32:58Z",
        "merged_at": "2018-06-29T11:32:58Z",
        "body": "This is for issue #5073.\r\n\r\nUsing `--no-auth-warning` option won't output warning message even if you use a `password` on command line interface.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2018-06-28T10:16:06Z",
        "closed_at": "2018-06-29T16:00:32Z",
        "merged_at": "2018-06-29T16:00:32Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2018-06-28T09:55:27Z",
        "closed_at": "2018-06-29T15:56:48Z",
        "merged_at": "2018-06-29T15:56:48Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-28T01:48:06Z",
        "closed_at": "2022-05-30T06:12:15Z",
        "merged_at": null,
        "body": "redis.conf, we set bind to localhost and internal address, such as:\r\n\r\n> bind 127.0.0.1 10.10.10.10\r\n\r\nafter restart redis, cluster reports: `Cluster state changed: fail`,  \r\nwe found redis use first bind addr(127.0.0.1) to connect other nodes, for example:\r\nuse 127.0.0.1 to connect 10.10.10.10\r\n\r\nafter the connect failure, other nodes will marked as: PFAIL.\r\nALL nodes in the cluster, mark itself alive, but other nodes failed.(Islanding?)\r\n\r\nIf this is a new initial cluster, the failure will make redis trib blocked too.\r\n\r\n1. Before initial cluster, assign the source addr by OS/kernel\r\n2. after the initial, use myself's ip(node's ip) to check cluster health",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-28T00:08:56Z",
        "closed_at": "2018-07-02T16:41:59Z",
        "merged_at": "2018-07-02T16:41:59Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-27T23:26:18Z",
        "closed_at": "2018-07-02T16:43:20Z",
        "merged_at": null,
        "body": "There isn't any link to the Google group. Shouldn't it be \"Redis Reddit Group\" rather than \"Redis Google Group\"?",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-06-27T21:53:41Z",
        "closed_at": "2018-06-28T09:55:23Z",
        "merged_at": null,
        "body": "BTW - please see https://github.com/antirez/redis/pull/4998 as well",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-06-27T16:52:13Z",
        "closed_at": "2018-07-02T16:40:36Z",
        "merged_at": "2018-07-02T16:40:36Z",
        "body": "The rename-command should not support an empty string in the command. Here are the reasons:\r\n1. It is useless. rename-command is unlike the others which use an empty string to disable 'sentinel set'. Instead, we need to do it like this: `SENTINEL SET mymaster rename-command PING PING`.\r\n\r\n2. Set a renamed command to empty string:  `SENTINEL SET mymaster rename-command PING \"\"`. Config rewrite process will record the renamed command like this\uff1a`sentinel rename-command mymaster ping`. Unfortunately, it is a wrong format. Once restart, sentinel can not startup successfully.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2018-06-27T13:37:12Z",
        "closed_at": "2018-06-27T16:54:28Z",
        "merged_at": "2018-06-27T16:54:28Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2018-06-27T12:34:47Z",
        "closed_at": "2018-06-27T16:56:49Z",
        "merged_at": "2018-06-27T16:56:49Z",
        "body": "sorry for another commit, realized that a few minutes after the previous PR.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2018-06-26T17:26:20Z",
        "closed_at": "2018-06-27T12:07:26Z",
        "merged_at": "2018-06-27T12:07:26Z",
        "body": "* fail the test (exit code) in case of timeout.\r\n* add --wait-server to allow attaching a debugger\r\n* add --dont-clean to keep log files when tests are done",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2018-06-26T04:16:55Z",
        "closed_at": "2018-06-26T07:23:08Z",
        "merged_at": null,
        "body": "Fix sentinel set error log format error",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 15331,
        "deletions": 4888,
        "changed_files": 151,
        "created_at": "2018-06-25T02:15:27Z",
        "closed_at": "2018-10-31T15:42:49Z",
        "merged_at": null,
        "body": "Modules may need to do something when Redis is shutdown.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-24T09:54:14Z",
        "closed_at": "2018-06-26T12:41:48Z",
        "merged_at": "2018-06-26T12:41:48Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 45995,
        "deletions": 17013,
        "changed_files": 276,
        "created_at": "2018-06-21T23:28:22Z",
        "closed_at": "2018-07-18T09:07:26Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-06-21T19:39:59Z",
        "closed_at": "2018-06-22T13:35:37Z",
        "merged_at": "2018-06-22T13:35:37Z",
        "body": "Unlike the BZPOP variants, these functions take a single key.  This fixes\r\nan erroneous CROSSSLOT error when passing a count to a cluster enabled\r\nserver.\r\n\r\nDemonstrating issue:\r\n```bash\r\n127.0.0.1:7000> zadd z1 1 one 2 two 3 three\r\n(integer) 3\r\n127.0.0.1:7000> zpopmax z1 1\r\n(error) CROSSSLOT Keys in request don't hash to the same slot\r\n127.0.0.1:7000> zpopmax z1\r\n1) \"three\"\r\n2) \"3\"\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2018-06-21T14:09:22Z",
        "closed_at": "2018-06-22T13:34:37Z",
        "merged_at": "2018-06-22T13:34:37Z",
        "body": "fix some typo",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-21T11:35:15Z",
        "closed_at": "2018-06-22T13:47:46Z",
        "merged_at": "2018-06-22T13:47:45Z",
        "body": "Same issue with incrDecrCommand(). See #5011 .",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 100,
        "deletions": 22,
        "changed_files": 5,
        "created_at": "2018-06-20T07:42:17Z",
        "closed_at": "2018-06-20T09:11:40Z",
        "merged_at": "2018-06-20T09:11:40Z",
        "body": "RESTORE now supports:\r\n1. Setting LRU/LFU\r\n2. Absolute-time TTL\r\n\r\nOther related changes:\r\n1. RDB loading will not override LRU bits when RDB file\r\n   does not contain the LRU opcode.\r\n2. RDB loading will not set LRU/LFU bits if the server's\r\n   maxmemory-policy does not match.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2018-06-19T14:09:26Z",
        "closed_at": "2018-06-19T15:22:37Z",
        "merged_at": "2018-06-19T15:22:37Z",
        "body": "this reduces the extra 8 bytes we save before each pointer.\r\nbut more importantly maybe, it makes the valgrind runs to be more similiar\r\nto our normal runs.\r\n\r\nnote: the change in malloc_stats struct in server.h is to eliminate an name conflict.\r\nstructs that are not typedefed are resolved from a separate name space.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-06-19T13:51:45Z",
        "closed_at": "2018-06-20T09:17:07Z",
        "merged_at": "2018-06-20T09:17:07Z",
        "body": "i was getting the following warning:\r\n```\r\nrdb.c: In function \u2018rdbLoadMillisecondTime\u2019:\r\nrdb.c:133:27: warning: suggest braces around empty body in an \u2018if\u2019 statement [-Wempty-body]\r\n         memrev64ifbe(&t64); /* Convert in big endian if the system is BE. */\r\n```\r\nfor the following code:\r\n```\r\n    if (rdbver >= 9) /* Check the top comment of this function. */\r\n        memrev64ifbe(&t64); /* Convert in big endian if the system is BE. */\r\n    return (long long)t64;\r\n```\r\nthis fix will also reduce the chance for some future bug in which an empty statement inside an 'if' can cause the 'if' to unintendedly catch the next line.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2018-06-19T13:45:32Z",
        "closed_at": "2018-06-20T09:18:31Z",
        "merged_at": "2018-06-20T09:18:31Z",
        "body": "this complication in the code is from times were rdbSaveLen didn't support 64 bits.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2018-06-19T06:04:45Z",
        "closed_at": "2018-06-26T12:33:21Z",
        "merged_at": "2018-06-26T12:33:21Z",
        "body": "Using a password with a space in it will cause replication based authentication to fail with:\r\n\"18866:S 19 Jun 06:00:00.371 # Unable to AUTH to MASTER: -ERR wrong number of arguments for 'auth' command\" \r\n\r\nReproduction:\r\n./redis-server --port 6378 --requirepass \"Broken Password\" &\r\n./redis-server --port 6379 --masterauth \"Broken Password\" --slaveof 127.0.01 6378 &\r\nsleep 1\r\npkill -9 redis-server\r\n\r\nThe proposed solution just wraps the AUTH command that is sent to the master to make sure it isn't parsed as a command with separate arguments.",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-19T05:43:37Z",
        "closed_at": "2018-06-20T15:42:38Z",
        "merged_at": "2018-06-20T15:42:38Z",
        "body": "fixed typo: 'th' -> 'the'",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-18T08:10:28Z",
        "closed_at": "2021-10-12T20:29:59Z",
        "merged_at": null,
        "body": "Redhat distributions have a different THP file location, need to handle it. #5030 ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-06-17T19:27:25Z",
        "closed_at": "2018-06-26T14:13:20Z",
        "merged_at": "2018-06-26T14:13:20Z",
        "body": "Redundant second if.\r\nAlso we may use \"not\" operation for boolean, I suppose it's faster.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-14T08:40:30Z",
        "closed_at": "2018-06-19T14:50:23Z",
        "merged_at": "2018-06-19T14:50:23Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-06-13T17:31:31Z",
        "closed_at": "2018-06-14T11:58:31Z",
        "merged_at": "2018-06-14T11:58:31Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 27,
        "changed_files": 1,
        "created_at": "2018-06-13T15:32:45Z",
        "closed_at": "2018-06-18T14:02:23Z",
        "merged_at": "2018-06-18T14:02:23Z",
        "body": "The min/max value of config_set_numerical_field() should be consistent with the type definition. Some of them will probably overflow.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-13T12:36:51Z",
        "closed_at": "2018-06-14T11:32:01Z",
        "merged_at": "2018-06-14T11:32:01Z",
        "body": "",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 98,
        "deletions": 13,
        "changed_files": 4,
        "created_at": "2018-06-12T15:58:55Z",
        "closed_at": "2021-07-05T06:30:17Z",
        "merged_at": "2021-07-05T06:30:17Z",
        "body": "After commit cec404f there are still some problem we should fix I think:\r\n\r\n1. `c->querybuf_peak` has not been updated correctly in `readQueryFromClient`.\r\n\r\n    ```c\r\n    void readQueryFromClient(aeEventLoop *el, int fd, void *privdata, int mask) {\r\n        qblen = sdslen(c->querybuf);\r\n        if (c->querybuf_peak < qblen) c->querybuf_peak = qblen;\r\n        c->querybuf = sdsMakeRoomFor(c->querybuf, readlen);\r\n        nread = read(fd, c->querybuf+qblen, readlen);\r\n        ...\r\n        sdsIncrLen(c->querybuf,nread);\r\n    ```\r\n\r\n    As we can see, we update `c->querybuf_peak` before `read` and `sdsIncrLen`, it means that `qblen` here is the last `c->querybuf` length. But the length of `c->querybuf` after `processInputBuffer` is always 0, unless we didn't read the whole request at one time.\r\n\r\n    So, we should update `c->querybuf_peak` after `sdsIncrLen` I think.\r\n\r\n2. We should use `sdsalloc` instead of `sdsAllocSize`.\r\n\r\n    Because we wanna get `c->querybuf` alloc space except for header, the `sdsAllocSize` value is always 32 * 1024 + `sdsHdrSize` + 1 grate than 32 * 1024.\r\n\r\n3. ~~Check if query buffer is > BIG_ARG and too big for latest peak only when `c->querybuf_peak` is not 0.~~\r\n     ~~Function `clientsCronResizeQueryBuffer` reset `c->querybuf_peak` to be 0 no matter resize the query buffer or not.~~\r\n    ~~If we don't get any requests after that, next time we will trigger the condition definitely.~~\r\n4. Query buffer shrinking improvements\r\nwhen tracking the peak, don't reset the peak to 0, reset it to the maximum of the current used, and the planned to be used by the current arg.\r\nwhen shrining, split the two separate conditions. the idle time shrinking will remove all free space. but the peak based shrinking will keep room for the current arg.\r\nwhen we resize due to a peak (rahter than idle time), don't trim all unused space, let the qbuf keep a size that's sufficient for the currently process bulklen, and the current peak.\r\n\r\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-06-12T10:02:51Z",
        "closed_at": "2018-06-12T11:05:19Z",
        "merged_at": "2018-06-12T11:05:19Z",
        "body": "The active-defrag-threshold-lower/active-defrag-threshold-upper min/max  value in redis.conf should be consistent with 'config set' command.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-12T09:25:07Z",
        "closed_at": "2018-06-19T03:55:07Z",
        "merged_at": null,
        "body": "When using incr/desc command to create a new key, the value object is a shared object, regardless of the maxmemory-policy. We need to check maxmemory-policy first, and use a shared object only if the maxmemory-policy is off.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-12T08:05:47Z",
        "closed_at": "2018-06-12T11:43:50Z",
        "merged_at": null,
        "body": "fix help message format [#5005](https://github.com/antirez/redis/issues/5005)",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-06-12T07:31:13Z",
        "closed_at": "2018-06-14T12:01:41Z",
        "merged_at": "2018-06-14T12:01:41Z",
        "body": "fix geohasEncode bug when step > 30",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-06-12T00:52:56Z",
        "closed_at": "2018-06-12T07:28:26Z",
        "merged_at": "2018-06-12T07:28:26Z",
        "body": "fix [#5005](https://github.com/antirez/redis/issues/5005)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2018-06-10T17:22:19Z",
        "closed_at": "2018-06-11T14:32:01Z",
        "merged_at": "2018-06-11T14:32:01Z",
        "body": "It's a supplement for PR #5001",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-06-10T16:17:01Z",
        "closed_at": "2018-06-11T14:33:11Z",
        "merged_at": "2018-06-11T14:33:11Z",
        "body": "Maybe using `lookupKeyWriteOrReply` in `xdel` and `xtrim` is more appropriate.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-10T15:59:04Z",
        "closed_at": "2018-06-11T14:30:50Z",
        "merged_at": "2018-06-11T14:30:50Z",
        "body": "Fixes segmentation fault in XGROUP if used against a key that is not a stream.\r\n\r\nReplicate the crash in current unstable like so:\r\n```bash\r\n127.0.0.1:9999> set foo bar\r\nOK\r\n127.0.0.1:9999> xgroup create foo $\r\nCould not connect to Redis at 127.0.0.1:9999: Connection refused\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-09T14:28:13Z",
        "closed_at": "2018-06-11T14:34:26Z",
        "merged_at": "2018-06-11T14:34:26Z",
        "body": "'unsigned' is short for unsigned int, but the goal is casting 'long long' to 'unsigned long'.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 92,
        "deletions": 74,
        "changed_files": 12,
        "created_at": "2018-06-07T18:17:24Z",
        "closed_at": "2018-07-02T16:46:57Z",
        "merged_at": "2018-07-02T16:46:57Z",
        "body": "This adds HELP to MODULE.\r\n\r\nThe new addReplySubSyntaxError function is more like a macro, so perhaps it should be - @antirez?",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-07T15:17:36Z",
        "closed_at": "2018-06-11T14:33:39Z",
        "merged_at": "2018-06-11T14:33:38Z",
        "body": "A typo in redis.conf which causes startup errror.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2018-06-07T12:24:48Z",
        "closed_at": "2019-10-28T08:55:12Z",
        "merged_at": "2019-10-28T08:55:12Z",
        "body": "Hi, @antirez \r\n\r\nAs we know if a module exports module-side data types, unload it is not allowed.\r\n\r\nThis rule is the same with blocked clients in module I think, because we use background threads to implement module blocked clients, and it's not safe to unload a module if there are background threads running:\r\n\r\n```\r\n127.0.0.1:6379> module load modules/helloblock.so\r\nOK\r\n127.0.0.1:6379> hello.block 10 1\r\nRequest timedout\r\n127.0.0.1:6379> module unload helloblock\r\nOK\r\n```\r\n\r\nAfter 10 seconds redis crashed. So it's necessary to check if any blocked clients running in this module when unload it.\r\n\r\nMoreover, after that we can ensure that if no modules, then no module blocked clients even module unloaded.\r\n\r\nSo, we can call `moduleHandleBlockedClients` only when we have modules.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-06T15:56:50Z",
        "closed_at": "2021-06-28T13:31:39Z",
        "merged_at": null,
        "body": "Before fix, the 's' will be double close if setsockopt return -1.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2018-06-06T15:35:48Z",
        "closed_at": "2018-06-08T10:37:22Z",
        "merged_at": null,
        "body": "Hi, @antirez \r\n\r\nUsing a loop to parse the count option in `XRANGE` is not necessary I think, it's easy to misuse:\r\n\r\n```\r\n127.0.0.1:6379> xadd key * a b\r\n1528299254819-0\r\n127.0.0.1:6379> xadd key * a b\r\n1528299257281-0\r\n127.0.0.1:6379> xrange key - + count 10 count 1\r\n1) 1) 1528297838306-0\r\n   2) 1) \"a\"\r\n      2) \"b\"\r\n```\r\n\r\nOnly the last `count 1` works.\r\n\r\nAnd `XREAD[GROUP]` also has the same problem:\r\n\r\n```\r\n127.0.0.1:6379> XREADGROUP GROUP mygroup Alice block 1 COUNT 10 COUNT 1 STREAMS mystream 0\r\n1) 1) \"mystream\"\r\n   2) 1) 1) 1528303462729-0\r\n         2) 1) \"message\"\r\n            2) \"banana\"\r\n```",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-06T12:24:04Z",
        "closed_at": "2018-07-09T11:37:59Z",
        "merged_at": null,
        "body": "As redisCommand::arity which means number of arguments includes the cmdname's count.\r\nBut redis-cli help info exclude cmdname, so one more `arg` display as follows:\r\n\r\n    127.0.0.1:6379> swapdb arg arg arg\r\n\r\nshould display as:\r\n\r\n    127.0.0.1:6379> swapdb arg arg\r\n\r\nThe same issue also exists during those commands which still not  been defined in commands.json.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 45,
        "deletions": 39,
        "changed_files": 6,
        "created_at": "2018-06-04T16:01:36Z",
        "closed_at": "2018-06-06T11:06:52Z",
        "merged_at": null,
        "body": "When executing write(), only if -1 is returned, the errno will be set appropriately. Otherwise the errno is used incorrectly. So, check the return value first to make the log more reliable.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2018-06-04T12:53:17Z",
        "closed_at": "2018-06-04T19:43:49Z",
        "merged_at": null,
        "body": "Pretty self-explanatory. My compiler complains without these fixes.\r\n\r\n```\r\nsiphash.c:210:15: warning: this statement may fall through [-Wimplicit-fallthrough=]\r\n     case 2: b |= ((uint64_t)siptlw(in[1])) << 8;\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 30,
        "changed_files": 4,
        "created_at": "2018-06-03T13:00:01Z",
        "closed_at": "2018-06-04T15:18:39Z",
        "merged_at": "2018-06-04T15:18:39Z",
        "body": "The redis-cli fix is courtesy of a gcc warning as well.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2018-06-03T12:39:10Z",
        "closed_at": "2019-09-26T09:39:30Z",
        "merged_at": "2019-09-26T09:39:30Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 18,
        "changed_files": 2,
        "created_at": "2018-06-03T12:13:30Z",
        "closed_at": "2020-07-29T12:25:31Z",
        "merged_at": null,
        "body": "Calls such as `RM_Replicate()` on a thread-safe context would push data to `server.also_propagate` but this would never be processed as `call()` swaps it and starts with a fresh array.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-03T01:26:08Z",
        "closed_at": "2018-06-07T16:49:10Z",
        "merged_at": "2018-06-07T16:49:09Z",
        "body": "Apologies if I am misunderstanding but I think we want to return early here when given a nonexistent consumer group.\r\n\r\nWithout the early return, I was seeing `*0\\r\\n*0\\r\\n` from Redis which was causing redis-cli to get out of sync.\r\n\r\nCheers!\r\nMike",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-02T06:42:02Z",
        "closed_at": "2018-06-16T09:05:04Z",
        "merged_at": "2018-06-16T09:05:04Z",
        "body": "Fix DEBUG LOADAOF so that redis-server will not crash unexpectedly\r\nand will not be inconsistent after we call debug loadaof.\r\nBefore this commit, there were 2 problems:\r\n\r\n1, When appendonly is set to no and there is not a appendonly file,\r\n   redis-server will crash if we call DEBUG LOADAOF.\r\n2, When appendonly is set to no and there is a appendonly file,\r\n   redis-server will hold different data after loading  appendonly\r\n   file.\r\n@antirez ",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-02T05:42:09Z",
        "closed_at": "2020-07-28T19:21:37Z",
        "merged_at": null,
        "body": "When object type is OBJ_ZSET and encoding is OBJ_ENCODING_SKIPLIST, MEMORY USAGE can not get the right size. @antirez ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2018-06-02T01:31:33Z",
        "closed_at": "2018-07-20T11:15:19Z",
        "merged_at": null,
        "body": "The double free of object may not be find for the refcount will not decrease to 0.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-06-01T10:11:11Z",
        "closed_at": "2022-02-14T10:01:45Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2018-05-31T15:22:08Z",
        "closed_at": "2018-05-31T16:40:54Z",
        "merged_at": "2018-05-31T16:40:54Z",
        "body": "If multiple clients blocked on one same sorted set, only one can be unblocked with `ZADD`, others get nil.\r\n\r\nIs this a bug or by design? @antirez ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2018-05-30T16:12:58Z",
        "closed_at": "2018-07-10T13:13:20Z",
        "merged_at": "2018-07-10T13:13:20Z",
        "body": "We use redis-cli to do incr command many times upon a key , to verify the data consistency  between master and slave when  a manual \"cluster failover\" is done.  like this: \r\n`redis-cli -c -p 7002 -r 50000 incr a`\r\n\r\nWe found that there are some issues about \"--repeat\" argument in redis-cli,  as follows:\r\n1. when we do a failover, redis server give a redirect response to redis-cli, this will cause \r\na wrong \"repeat\" count in the \"cliSendCommand\" function. and redis-cli will repeat to send \"incr a\" more than 50000 times, and we will get a wrong result.\r\n2. also in the \"cliSendCommand\", there maybe have many unnessary loops which can cause an obviously halt.\r\n\r\nLet's see how to reproduce this issue:\r\n1. deploy some redis instances and create a redis cluster\uff08port range: 7000-7005\uff09\r\n2. now all redis have no data, we use redis-cli to connect any master(eg: redis on the port 7005 ) in the cluster and do this:\r\n`redis-cli -c -p 7005 -r 50000 incr a`\r\n3. connect redis-server:7002, which is the slave of redis-server:7005, before the above step 2 finish , we do a manual failover:\r\n`redis-cli -c -p 7002 cluster failover`\r\nAfter press Enter on my keyboard, i can see an obvious halt for seconds on my terminal screen of redis-cli,  and now \"incr a\" has already processed 2820 times\r\n\r\n4. seconds later after failover is done, redis-cli recover from the halt , and continue to do the rest \"incr a\" tasks,  at the last,  we can see redis-cli will do \"incr a\" 50000 times but \uff0850000-2820\uff09times\uff0cso the value of \"a\"  is 52820 but 50000, which is obviously wrong.\r\n\r\nThis issue last for a long time from redis 3.0, maybe it should be merged on 3.0, 4.0, 5.0 and unstable branches.\r\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 82,
        "deletions": 173,
        "changed_files": 8,
        "created_at": "2018-05-30T15:21:41Z",
        "closed_at": "2021-12-21T06:34:15Z",
        "merged_at": null,
        "body": "Hi, @antirez \r\n\r\nHappy to see 5.0 RC1 released, it's a big step, redis becomes more strong and interesting, look forward to the GA version.\r\n\r\nFrom the last commit, we are aligned that RDB file does no longer represent just a persistence file, it represents also the **state** of the instance, so we persist all keys even expired to RDB file.\r\n\r\nIt looks good after the fix, but I just remember that we have met a similar problem about **lua scripts** once. After **psync** slave may lose some scripts because the rebooting RDB file doesn't contains, and we fix it by persisting all lua scripts to RDB file when **FULL RESYNC**. Lua scripts should be regarded as a part of state of the instance as well I think.\r\n\r\nBut there still have something bad which may result in inconsistency between master and slave:\r\n\r\n1. If slave accept `script flush` from a normal client, slave cannot execute `EVALSHA` correctly.\r\n\r\n    It's easy to understand and reproduce, because after `script flush` slave delete all lua scripts.\r\n\r\n2. `SCRIPT LOAD` on middle slave may lead to inconsistency.\r\n\r\n    This scenario is a little complex but serious, imagine that:\r\n\r\n    * We have three instances, master A, middle slave B and slave C like this: A --> B --> C\r\n    * Apply `script load \"redis.call('set','foo','bar')\"` on middle slave B, we can get the sha1 `a3862cfaeddaf7c3473803cba58cdd0bdca53397`\r\n    * Apply `evalsha a3862cfaeddaf7c3473803cba58cdd0bdca53397 0` on slave B, we will get an error because this script contains **write** command. And slave B will add this script to `server.repl_scriptcache_dict`, but not replicate to slave C, because all data in replication stream are generated by master.\r\n    * Then we promote slave B to be master, and apply `evalsha a3862cfaeddaf7c3473803cba58cdd0bdca53397 0` again. Now master B has key `foo` but slave C doesn't.\r\n\r\nThese bugs appear because `script flush/load` and `eval` command break the **lua scripting state**, we can fix them like this:\r\n\r\n1. Disable `script flush/load` and `eval` command on slave.\r\n\r\n    Then slave has no chance to break lua scripting state.\r\n\r\n2. For `EVAL` command, we can propagate scripts explicitly.\r\n\r\n    It means that we can split `EVAL` command to `SCRIPT LOAD` and `EVAL`, if it's the first time we meet this script.\r\n\r\n    Because some readonly scripts or scripts with `redis.replicate_command()` cannot be propagate to slave and aof, so after this all slaves can be consist with master.\r\n\r\n3. After these two above finished, we can remove replication script cache.\r\n\r\n    And consider aofrewrite without **replication script cache**, we should persist all lua scripts in AOF and RDB file.\r\n\r\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-05-30T13:36:20Z",
        "closed_at": "2018-05-31T15:20:34Z",
        "merged_at": "2018-05-31T15:20:34Z",
        "body": "When building on RHEL 7.5:\r\n\r\n```\r\nBUILDSTDERR: zmalloc.c: In function 'zmalloc_get_allocator_info':\r\nBUILDSTDERR: zmalloc.c:304:5: error: unknown type name 'uint64_t'\r\nBUILDSTDERR:      uint64_t epoch = 1;\r\nBUILDSTDERR:      ^\r\n```",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2018-05-29T15:09:46Z",
        "closed_at": "2021-09-16T09:29:29Z",
        "merged_at": null,
        "body": "Use the following code to set two keys into redis:\r\n\r\n\\#!/usr/bin/env python\r\n\\# -*- coding: utf-8 -*-\r\n\r\nimport struct\r\nimport socket\r\n\r\ndef format_binary_request(key):\r\n    key_len = struct.pack(\"!H\", len(key))\r\n    packet = key_len + key\r\n    return packet\r\n\r\ns1 = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\ns1.connect((\"127.0.0.1\", 6379))\r\n\r\nmy_key = format_binary_request(\"hell-redis\")\r\n\r\nkey=(\"$%d\\r\\n\") % (len(my_key))\r\nkey = key + my_key + \"\\r\\n\"\r\n\r\ns1.sendall(\"*3\\r\\n$3\\r\\nset\\r\\n\" + key + \"$2\\r\\nHI\\r\\n\")\r\nprint s1.recv(1024)\r\ns1.sendall(\"*2\\r\\n$3\\r\\nget\\r\\n\" + key)\r\nprint s1.recv(1024)\r\n\r\ns1.sendall(\"*3\\r\\n$3\\r\\nset\\r\\n$4\\r\\ntest\\r\\n$1\\r\\nI\\r\\n\")\r\nprint s1.recv(1024)\r\ns1.sendall(\"*2\\r\\n$3\\r\\nget\\r\\n$4\\r\\ntest\\r\\n\")\r\nprint s1.recv(1024)\r\n\r\nwe get:\r\n./bin/redis-cli   -p 6379\r\n127.0.0.1:9379> randomkey\r\n\"\\x00\\nhell-redis\"\r\n127.0.0.1:9379> randomkey\r\n\"test\"\r\n\r\nthe first key is '\\x00\\nhell-redis', which is unprintable, not ascii character starting, and has value length 2. the  second key is 'test',  which has a value length 1. It's obvious that the biggest key is '\\x00\\nhell-redis' .But when use redis-cli  -p 6379 --bigkeys, i get the following info:\r\n\r\n\\# Scanning the entire keyspace to find biggest keys as well as\r\n\\# average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec\r\n\\# per 100 SCAN commands (not usually needed).\r\n\r\n[00.00%] Biggest string found so far 'test' with 1 bytes\r\n\r\n-------- summary -------\r\n\r\nSampled 1 keys in the keyspace!\r\nTotal key length in bytes is 4 (avg len 4.00)\r\n\r\nBiggest string found 'test' has 1 bytes\r\n\r\n1 strings with 1 bytes (100.00% of keys, avg size 1.00)\r\n0 lists with 0 items (00.00% of keys, avg size 0.00)\r\n0 sets with 0 members (00.00% of keys, avg size 0.00)\r\n0 hashs with 0 fields (00.00% of keys, avg size 0.00)\r\n0 zsets with 0 members (00.00% of keys, avg size 0.00)\r\n0 streams with 0 entries (00.00% of keys, avg size 0.00)\r\n\r\nthe following is the modification:\r\n\r\n\\# Scanning the entire keyspace to find biggest keys as well as\r\n\\# average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec\r\n\\# per 100 SCAN commands (not usually needed).\r\n\r\n[00.00%] Biggest string found so far 'test' with 1 bytes\r\n[00.00%] Biggest string found so far '\\x00\\nhell-redis' with 2 bytes\r\n\r\n-------- summary -------\r\n\r\nSampled 2 keys in the keyspace!\r\nTotal key length in bytes is 16 (avg len 8.00)\r\n\r\nBiggest string found '\\x00\\nhell-redis' has 2 bytes\r\n\r\n2 strings with 3 bytes (100.00% of keys, avg size 1.50)\r\n0 lists with 0 items (00.00% of keys, avg size 0.00)\r\n0 sets with 0 members (00.00% of keys, avg size 0.00)\r\n0 hashs with 0 fields (00.00% of keys, avg size 0.00)\r\n0 zsets with 0 members (00.00% of keys, avg size 0.00)\r\n0 streams with 0 entries (00.00% of keys, avg size 0.00)",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-05-29T06:50:36Z",
        "closed_at": "2018-06-02T15:08:10Z",
        "merged_at": null,
        "body": "old: \r\nlocalhost:7000 (65de430b...) -> 0 keys | 5461 slots | 0 slaves.\r\n172.27.25.35:7002 (f62a9a8d...) -> 0 keys | 5461 slots | 0 slaves.\r\n172.27.25.35:7001 (5f03bdc2...) -> 0 keys | 5462 slots | 0 slaves.\r\n\r\n\r\nfixed:\r\nlocalhost:7000 (65de430b...) -> 0 keys | 5461 slots | 1 slaves.\r\n172.27.25.35:7002 (f62a9a8d...) -> 0 keys | 5461 slots | 1 slaves.\r\n172.27.25.35:7001 (5f03bdc2...) -> 0 keys | 5462 slots | 1 slaves.\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2018-05-27T13:52:50Z",
        "closed_at": "2018-05-29T13:42:27Z",
        "merged_at": null,
        "body": "set two keys in redis use the followding python program:\r\n\r\n#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport struct\r\nimport socket\r\n\r\ndef format_binary_request(key):\r\n    key_len = struct.pack(\"!H\", len(key))\r\n    packet = key_len + key\r\n    return packet\r\n\r\ns1 = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\ns1.connect((\"127.0.0.1\", 6379))\r\n\r\nmy_key = format_binary_request(\"hell-redis\")\r\n\r\nkey=(\"$%d\\r\\n\") % (len(my_key))\r\nkey = key + my_key + \"\\r\\n\"\r\n\r\ns1.sendall(\"*3\\r\\n$3\\r\\nset\\r\\n\" + key + \"$2\\r\\nHI\\r\\n\")\r\nprint s1.recv(1024)\r\ns1.sendall(\"*2\\r\\n$3\\r\\nget\\r\\n\" + key)\r\nprint s1.recv(1024)\r\n\r\ns1.sendall(\"*3\\r\\n$3\\r\\nset\\r\\n$4\\r\\ntest\\r\\n$1\\r\\nI\\r\\n\")\r\nprint s1.recv(1024)\r\ns1.sendall(\"*2\\r\\n$3\\r\\nget\\r\\n$4\\r\\ntest\\r\\n\")\r\nprint s1.recv(1024)\r\n\r\nwe get two keys:\r\n\r\n./bin/redis-cli   -p 6379\r\n127.0.0.1:9379> dbsize\r\n(integer) 2\r\n127.0.0.1:9379> randomkey\r\n\"\\x00\\nhell-redis\"\r\n127.0.0.1:9379> randomkey\r\n\"test\"\r\n\r\nThe first key is binary, un printable  key, the second is normal ascii key. \r\nThe key \"*hell-redis\" has value \"HI\", key \"test\" has value \"I\". It's obvious  the biggest key is \"*hell-redis\". But when use\r\n redis-cli -p 6379 --bigkeys\r\n\r\nI got the following result: \r\n\r\n\\# Scanning the entire keyspace to find biggest keys as well as\r\n\\# average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec\r\n\\# per 100 SCAN commands (not usually needed).\r\n\r\n[00.00%] Biggest string found so far 'test' with 1 bytes\r\n\r\n-------- summary -------\r\n\r\nSampled 1 keys in the keyspace!\r\nTotal key length in bytes is 4 (avg len 4.00)\r\n\r\nBiggest string found 'test' has 1 bytes\r\n\r\n1 strings with 1 bytes (100.00% of keys, avg size 1.00)\r\n0 lists with 0 items (00.00% of keys, avg size 0.00)\r\n0 sets with 0 members (00.00% of keys, avg size 0.00)\r\n0 hashs with 0 fields (00.00% of keys, avg size 0.00)\r\n0 zsets with 0 members (00.00% of keys, avg size 0.00)\r\n0 streams with 0 entries (00.00% of keys, avg size 0.00)\r\n\r\nit's obvious 'test' is not the biggest string. \r\n\r\nthe following is the bug fix version result:\r\n\r\n\\# Scanning the entire keyspace to find biggest keys as well as\r\n\\# average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec\r\n\\# per 100 SCAN commands (not usually needed).\r\n\r\n[00.00%] Biggest string found so far 'test' with 1 bytes\r\n[00.00%] Biggest string found so far '' with 2 bytes\r\n\r\n-------- summary -------\r\n\r\nSampled 2 keys in the keyspace!\r\nTotal key length in bytes is 16 (avg len 8.00)\r\n\r\nBiggest string found '\"test\"\"\\x00\\nhell-redis\"' has 2 bytes\r\n\r\n2 strings with 3 bytes (100.00% of keys, avg size 1.50)\r\n0 lists with 0 items (00.00% of keys, avg size 0.00)\r\n0 sets with 0 members (00.00% of keys, avg size 0.00)\r\n0 hashs with 0 fields (00.00% of keys, avg size 0.00)\r\n0 zsets with 0 members (00.00% of keys, avg size 0.00)\r\n0 streams with 0 entries (00.00% of keys, avg size 0.00)",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-05-27T13:34:35Z",
        "closed_at": "2020-08-12T05:05:22Z",
        "merged_at": null,
        "body": "Both master and slave can trigger eviction when used memory\r\nreaches maxmemory, which causes inconsistency. After introducing\r\nPSYN2, used memory gap between master and slave has reduced as\r\nslave now has backlog as master had.",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-05-27T08:32:49Z",
        "closed_at": "2018-05-31T15:31:25Z",
        "merged_at": "2018-05-31T15:31:24Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 33,
        "changed_files": 2,
        "created_at": "2018-05-26T02:11:44Z",
        "closed_at": "2019-05-23T06:50:17Z",
        "merged_at": null,
        "body": "Before this commit, there's a chance that slaves will hold on the expired key, which may consume a lot of memory. Let's see how it happens.\r\n\r\nSuppose there are 2 nodes in a partition, one is master A, the other is a slave of A, which is represented as B.\r\nSuppose there are a lot of keys with expire time in master A, and many keys should expire in every moment.\r\n\r\nDo the following steps:\r\n**Step1** , a new node C comes in and replicates master A, which makes C a new slave of A. \r\n**Step2**, Slave C does failover, either by manual or not.\r\n\r\nIf Step2 does not happen, when a expire key actived, master A would delete the key itself and propagate a DEL to AOF(if appendonly is yes) , backlog and slaves. Then slaves B and C got the DEL command, and do it themselves.\r\nHowever, when Step2 does happen, since rdb save filters out the expired keys, slave C will not have the expired keys, which could still be there in master A, which may be deleted later because of lazy delete., slave C will not have the chance to delete the expired keys, which were supposed to delete later. Slave B still has the expired keys, since slaves can only delete keys when they are asked by their master, and the expired keys will remain there forever since new master C do not have the expired keys.\r\n@antirez ",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-05-25T16:41:11Z",
        "closed_at": "2020-09-22T09:02:46Z",
        "merged_at": null,
        "body": "As noted by @loveCupid in #4943, this function checks twice if the string is \"0\".",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-05-25T12:26:55Z",
        "closed_at": "2018-05-25T14:12:19Z",
        "merged_at": "2018-05-25T14:12:19Z",
        "body": "Hi @antirez ,\r\n\r\nIt's really a big change that you upgrade jemalloc to 5.0.1, but unfortunately after the upgrade I met a link error:\r\n\r\n ```\r\n../deps/jemalloc/lib/libjemalloc.a(nstime.o): In function `nstime_get':\r\n/home/zhaozhao.zz/antirez/redis/deps/jemalloc/src/nstime.c:119: undefined reference to `clock_gettime'\r\ncollect2: ld returned 1 exit status\r\nmake: *** [redis-server] Error 1\r\n```\r\n\r\nI checked the function from man page:\r\n```\r\nNAME\r\n       clock_getres, clock_gettime, clock_settime - clock and time functions\r\n\r\nSYNOPSIS\r\n       #include <time.h>\r\n\r\n       int clock_getres(clockid_t clk_id, struct timespec *res);\r\n\r\n       int clock_gettime(clockid_t clk_id, struct timespec *tp);\r\n\r\n       int clock_settime(clockid_t clk_id, const struct timespec *tp);\r\n\r\n       Link with -lrt.\r\n\r\n   Feature Test Macro Requirements for glibc (see feature_test_macros(7)):\r\n\r\n       clock_getres(), clock_gettime(), clock_settime(): _POSIX_C_SOURCE >= 199309L\r\n```\r\n\r\nMy os is linux 2.6.32. So, I think we should append `-lrt` to `FINAL_LIBS` for linux.\r\n\r\nBTW, I guess your os is ubuntu and glibc is after 2.17, then you didn't met this problem:\r\n\r\n![image](https://user-images.githubusercontent.com/24804835/40544088-a15cb1ac-6059-11e8-85cf-f8248070dc03.png)\r\n ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-05-25T08:53:01Z",
        "closed_at": "2018-05-31T15:32:12Z",
        "merged_at": "2018-05-31T15:32:12Z",
        "body": "fix the help info to match code",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 49,
        "deletions": 55,
        "changed_files": 4,
        "created_at": "2018-05-24T07:27:22Z",
        "closed_at": "2022-02-07T16:15:15Z",
        "merged_at": null,
        "body": "refactor duplicated type checking code to use checkType function.\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2018-05-22T06:30:35Z",
        "closed_at": "2018-08-22T10:36:28Z",
        "merged_at": null,
        "body": "sync with antirez/redis",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-05-21T04:22:05Z",
        "closed_at": "2018-05-24T14:15:22Z",
        "merged_at": "2018-05-24T14:15:22Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2018-05-17T06:58:15Z",
        "closed_at": "2018-05-17T10:24:41Z",
        "merged_at": "2018-05-17T10:24:41Z",
        "body": "problems fixed:\r\n* failing to read fragmentation information from jemalloc\r\n* overflow in jemalloc fragmentation hint to the defragger\r\n* test suite not triggering eviction after population",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2018-05-16T08:28:12Z",
        "closed_at": "2018-05-24T14:14:43Z",
        "merged_at": "2018-05-24T14:14:43Z",
        "body": "",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 59,
        "deletions": 35,
        "changed_files": 3,
        "created_at": "2018-05-09T05:50:17Z",
        "closed_at": "2018-08-26T12:47:32Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-05-09T03:48:40Z",
        "closed_at": "2018-05-30T13:23:41Z",
        "merged_at": null,
        "body": "If slave accepts `script flush` from a normal client, that may lead to inconsistency between master and slave:\r\n\r\n![script](https://user-images.githubusercontent.com/24804835/39794809-d1769d86-537e-11e8-822a-0a70522bf5b2.png)\r\n\r\n\r\nBTW, let slave contains more scripts than master is not a good idea I think, that may lead to OOM. Slave should not accept `script load` from a normal client.\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2018-05-08T11:53:59Z",
        "closed_at": "2021-12-22T08:00:22Z",
        "merged_at": "2021-12-22T08:00:22Z",
        "body": "Let's keep the comments and implementation consistent.\r\n\r\nI don't think we need identify the mistake type.  I think 'len' < 0 or 'len == 0 ' should share same behavior.\r\n\r\nI checked all references of the function, set 'count' with 0 could break meaningless loop.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2018-05-08T11:46:49Z",
        "closed_at": "2018-05-23T15:11:01Z",
        "merged_at": "2018-05-23T15:11:00Z",
        "body": "for issue #4902",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2018-05-08T09:05:44Z",
        "closed_at": "2018-06-01T14:54:06Z",
        "merged_at": "2018-06-01T14:54:06Z",
        "body": "The only difference is when iterating over larger table, cursor is increased in reverse:\r\n```c\r\n        /* Iterate over indices in larger table that are the expansion\r\n         * of the index pointed to by the cursor in the smaller table */\r\n        do {\r\n            /* Emit entries at cursor */\r\n            if (bucketfn) bucketfn(privdata, &t1->table[v & m1]);\r\n            de = t1->table[v & m1];\r\n            while (de) {\r\n                next = de->next;\r\n                fn(privdata, de);\r\n                de = next;\r\n            }\r\n\r\n            /* Increment the reverse cursor not covered by the smaller mask.*/\r\n            v |= ~m1;\r\n            v = rev(v);\r\n            v++;\r\n            v = rev(v);\r\n\r\n            /* Continue while bits covered by mask difference is non-zero */\r\n        } while (v & (m0 ^ m1));\r\n```\r\nWhen jumping out of the loop, the highest bit of (v & m0) is increased, so move following code to if block:\r\n```c\r\n        /* Set unmasked bits so incrementing the reversed cursor\r\n         * operates on the masked bits */\r\n        v |= ~m0;\r\n\r\n        /* Increment the reverse cursor */\r\n        v = rev(v);\r\n        v++;\r\n        v = rev(v);\r\n```\r\n",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-05-07T14:25:15Z",
        "closed_at": "2019-03-14T10:43:55Z",
        "merged_at": "2019-03-14T10:43:55Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 10,
        "changed_files": 3,
        "created_at": "2018-05-07T12:24:59Z",
        "closed_at": "2018-06-16T09:39:15Z",
        "merged_at": null,
        "body": "1. It's a bit more efficient.\r\n2. If multi threads fopen the same log file every time in serverLogRaw() and use separate file pointers,  the writes of deferent threads may end up interleaved, because each of the threads uses its own lock in FILE object. But with only one FILE object, whole fprintf calls on that FILE will be atomic.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-05-06T18:24:07Z",
        "closed_at": "2018-06-11T14:32:41Z",
        "merged_at": "2018-06-11T14:32:41Z",
        "body": "This PR fix typo in preprocessor condition from zmalloc.c",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2018-05-06T10:45:10Z",
        "closed_at": "2021-10-14T06:25:38Z",
        "merged_at": null,
        "body": "Update upstream",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-05-05T21:25:57Z",
        "closed_at": "2020-09-09T16:29:07Z",
        "merged_at": null,
        "body": "Specifically: --host, --port, --verbose & --client",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-05-04T02:58:22Z",
        "closed_at": "2020-08-11T12:51:28Z",
        "merged_at": "2020-08-11T12:51:28Z",
        "body": "`hash` in benchmark needs real random fields instead of key name.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-05-03T08:03:08Z",
        "closed_at": "2018-05-21T02:49:08Z",
        "merged_at": null,
        "body": "The new sh should be checked right after it is malloced.\r\nOr if it is set to NULL, and init pointer is not set, the memset call will\r\ncause segment fault.\r\n\r\nChange-Id: I50f883d3061462e75174aca560eac300513f5245\r\nSigned-off-by: Jun He <jun.he@arm.com>",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-05-01T15:21:00Z",
        "closed_at": "2020-09-09T16:29:03Z",
        "merged_at": null,
        "body": "I found struct ```modulesDictType```'s comment just copy from struct ```clusterNodesBlackListDictType``` in **server.c.**\r\n\r\nSigned-off-by: charpty <charpty@gmail.com>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2018-04-30T16:37:13Z",
        "closed_at": "2018-07-23T16:43:05Z",
        "merged_at": "2018-07-23T16:43:05Z",
        "body": "Implementation notes: as INFO is \"already broken\", I didn't want to break it further. Instead of computing the server.lua_script dict size on every call, I'm keeping a running sum of the body's length and dict overheads.\r\n\r\nThis implementation is naive as it **does not** take into consideration dict rehashing, but that inaccuracy pays off in speed ;)\r\n\r\nDemo time:\r\n\r\n```bash\r\n$ redis-cli info memory | grep \"script\"\r\nused_memory_scripts:96\r\nused_memory_scripts_human:96B\r\nnumber_of_cached_scripts:0\r\n$ redis-cli eval \"\" 0 ; redis-cli info memory | grep \"script\"\r\n(nil)\r\nused_memory_scripts:120\r\nused_memory_scripts_human:120B\r\nnumber_of_cached_scripts:1\r\n$ redis-cli script flush ; redis-cli info memory | grep \"script\"\r\nOK\r\nused_memory_scripts:96\r\nused_memory_scripts_human:96B\r\nnumber_of_cached_scripts:0\r\n$ redis-cli eval \"return('Hello, Script Cache :)')\" 0 ; redis-cli info memory | grep \"script\"\r\n\"Hello, Script Cache :)\"\r\nused_memory_scripts:152\r\nused_memory_scripts_human:152B\r\nnumber_of_cached_scripts:1\r\n$ redis-cli eval \"return redis.sha1hex(\\\"return('Hello, Script Cache :)')\\\")\" 0 ; redis-cli info memory | grep \"script\"\r\n\"1be72729d43da5114929c1260a749073732dc822\"\r\nused_memory_scripts:232\r\nused_memory_scripts_human:232B\r\nnumber_of_cached_scripts:2\r\n\u2714 19:03:54 redis [lua_scripts-in-info-memory L \u271a\u2026\u2691] $ redis-cli evalsha 1be72729d43da5114929c1260a749073732dc822 0\r\n\"Hello, Script Cache :)\"\r\n```",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2018-04-30T14:33:08Z",
        "closed_at": "2020-09-09T16:26:15Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 380,
        "deletions": 20,
        "changed_files": 6,
        "created_at": "2018-04-29T23:18:00Z",
        "closed_at": "2018-05-15T08:19:20Z",
        "merged_at": "2018-05-15T08:19:20Z",
        "body": "An implementation of the\r\n[Ze POP Redis Module](https://github.com/itamarhaber/zpop) as core\r\nRedis commands.\r\n\r\nResolves #1861.\r\n\r\n@antirez per our RedisConf discussion. Does it make sense IYO to put\r\neffort into designing the zset equivalent of `BRPOPLPUSH`?",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2018-04-27T07:29:34Z",
        "closed_at": "2018-06-21T11:13:53Z",
        "merged_at": null,
        "body": "If the 'logfile' is a relative path, when loading 'dir', the chdir changes the working directory to the new path. And it is possibly different from the path specified by \u2018logfile\u2019.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-04-23T10:48:45Z",
        "closed_at": "2021-06-29T06:06:54Z",
        "merged_at": null,
        "body": "This is a major security hole. Please merge !!!",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 218,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2018-04-23T08:14:36Z",
        "closed_at": "2023-02-20T09:51:30Z",
        "merged_at": null,
        "body": "Redis implements SHA1 with software look table.\r\nThe patch optimized SHA1 with ARM NEON Intrinsics\r\nthat is supported by ARM64v8 Crytpto Extension.\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-04-23T03:05:08Z",
        "closed_at": "2021-06-29T10:27:09Z",
        "merged_at": null,
        "body": "Stumbled upon it while learning the codebase.\r\nFor validation, built Redis with the change and `make test` was happy. ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2018-04-22T23:56:29Z",
        "closed_at": "2022-05-23T14:44:26Z",
        "merged_at": null,
        "body": "While reading through some code I just noticed that generally it seems the preferred use of `strcasecmp` is to simply use the return value from the call in a conditional, but in some cases the result would then be explicitly compared to 0, which seemed superfluous so I thought I'd put up a refactor.\r\n\r\nAlso when testing my changes there just found that a missing newline on line 1422 would cause the buffered output to not get flushed:\r\n```\r\n127.0.0.1:6379> restart\r\n127.0.0.1:6379> Use 'restart' only in Lua debugging mode.\r\n127.0.0.1:6379> \r\n```\r\nAdding it seemed to fix that and seems more consistent with how other output is formatted.\r\n```\r\n127.0.0.1:6379> restart\r\nUse 'restart' only in Lua debugging mode.\r\n127.0.0.1:6379> \r\n```\r\nHope these tiny refactors are helpful and not just white noise among real patches \ud83d\ude00 .",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-04-22T14:41:05Z",
        "closed_at": "2018-06-08T10:12:35Z",
        "merged_at": "2018-06-08T10:12:35Z",
        "body": "Hi @antirez , maybe we forgot `dictExpand()` when loading `hash` and `zset` just like what `set` does. \r\n\r\nAnd this can halve the loading time on an `hash` or `zset` object with 5 millions elements:\r\n\r\n\r\nLoding time without `dictExpand` is `8.375 seconds`:\r\n```\r\n5116:M 22 Apr 22:08:01.534 # Server initialized\r\n5116:M 22 Apr 22:08:09.909 * DB loaded from disk: 8.375 seconds\r\n5116:M 22 Apr 22:08:09.909 * Ready to accept connections\r\n```\r\n\r\nLoding time with `dictExpand` is `4.676 seconds`:\r\n```\r\n23014:M 22 Apr 22:27:04.732 # Server initialized\r\n23014:M 22 Apr 22:27:09.409 * DB loaded from disk: 4.676 seconds\r\n23014:M 22 Apr 22:27:09.409 * Ready to accept connections\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2018-04-20T12:26:11Z",
        "closed_at": "2018-06-18T11:54:39Z",
        "merged_at": "2018-06-18T11:54:39Z",
        "body": "Redis crashed when using `command getkeys eval \"return 'hello, world'\" -12` or `command getkeys ZUNIONSTORE out -2 zset1 zset2 WEIGHTS 2 3` .",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2869,
        "deletions": 183,
        "changed_files": 44,
        "created_at": "2018-04-19T22:13:00Z",
        "closed_at": "2019-10-16T23:25:54Z",
        "merged_at": null,
        "body": "See SSL_README.md for more information:\r\n\r\nmake test\r\nBUILD_SSL=yes make test\r\nBUILD_SSL=yes make test-ssl\r\n\r\npass 48/48 tests for all 3 combinations\r\n\\o/ All tests passed without errors!\r\n\r\nThe cluster tests also generally pass, redis-trib doesn't fully support ssl though which causes some failures.",
        "comments": 40
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-04-19T09:29:36Z",
        "closed_at": "2018-07-25T14:30:11Z",
        "merged_at": "2018-07-25T14:30:11Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 105,
        "deletions": 26,
        "changed_files": 5,
        "created_at": "2018-04-16T09:17:22Z",
        "closed_at": "2021-06-01T10:43:48Z",
        "merged_at": "2021-06-01T10:43:48Z",
        "body": "This PR contains three changes:\r\n1. CONFIG REWRITE command will rewrite modules as `loadmodule` options into config file, in case someone uses MODULE LOAD or UNLOAD commands in runtime.\r\n2. free elements in `server.loadmodule_queue` after module loaded from config file or UNLOAD to save memory.\r\n3. show `path` and `args` in MODULE LIST reply.",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2018-04-15T21:46:53Z",
        "closed_at": "2020-08-20T21:20:20Z",
        "merged_at": null,
        "body": "Manually ported jinoos:keyspace-key-hit-miss #1767 to current unstable",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-04-15T14:50:03Z",
        "closed_at": "2018-08-28T05:53:57Z",
        "merged_at": null,
        "body": "1. In `sdsNewLen`, it looks like the `sh` check is invalid because before `if (sh == NULL) return NULL;` execute, `memset(sh, 0, hdrlen+initlen+1);` might be executed, I fixed this problem by modifying their order.\r\n\r\n2. In `sdstrim`, there is not any instruction will change `sds s`, I think `s` in `sdstrim` won't be changed.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 24,
        "changed_files": 3,
        "created_at": "2018-04-13T00:33:38Z",
        "closed_at": "2021-06-25T01:15:32Z",
        "merged_at": null,
        "body": "With default options gcc will complain about cascading case\r\nswitch statements, annotate these to indicate this is being\r\nused intentionally in hyperloglog.c and siphash.c\r\n\r\nThe recent streams code has some cases where void pointers\r\nare passed under \"%s\" to printf-alike error handling code;\r\nthese can be safely cast to character pointers here.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2018-04-11T07:44:52Z",
        "closed_at": "2020-04-30T08:43:13Z",
        "merged_at": "2020-04-30T08:43:13Z",
        "body": "Hi @antirez \r\n\r\nThere are some problems about lazyfree eviction I think:\r\n\r\n1. We forgot the latency generated by waiting for `BIO_LAZY_FREE`\r\n\r\n   It's necessary to record this kind of latency.\r\n\r\n2. Maybe we misuse `latencyRemoveNestedEvent `\r\n\r\n    In my opinion the `eviction-cycle` should contains all latencies, including `eviction-del` and `eviction-lazyfree`, or we may lose the accumulated latency, for example:\r\n\r\n    ```\r\n    127.0.0.1:6379> latency latest\r\n       1) 1) \"command\"\r\n       2) (integer) 1523431788\r\n       3) (integer) 10\r\n       4) (integer) 10\r\n    2) 1) \"eviction-cycle\"\r\n       2) (integer) 1523431892\r\n       3) (integer) 23\r\n       4) (integer) 23\r\n    3) 1) \"eviction-del\"\r\n       2) (integer) 1523431892\r\n       3) (integer) 11\r\n       4) (integer) 47\r\n    ```\r\n\r\n    We can see than eviction-del's latency is greater than eviction-cycle's, and if `latency-monitor-threshold` is smaller than eviction-del's, the evict latency will be lost.\r\n\r\n    So we should not remove `eviction-del` from `eviction-cycle` I think.\r\n\r\n3. The last problem is about `cant_free`\r\n\r\n    When we goto `cant_free`, we wait for `BIO_LAZY_FREE` finished or break if already freed enough memory.\r\n\r\n    But, we use `mem_freed` duplicately.\r\n\r\n    ```\r\n        while(bioPendingJobsOfType(BIO_LAZY_FREE)) {\r\n            if (((mem_reported - zmalloc_used_memory()) + mem_freed) >= mem_tofree)\r\n                break;\r\n            usleep(1000);\r\n        }\r\n    ```\r\n\r\n    I think we can replace it with the new function `getMaxmemoryState()`.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-04-11T07:00:42Z",
        "closed_at": "2018-04-11T16:17:52Z",
        "merged_at": null,
        "body": "We should use the pointers not the values they point to, or redis will crash when `getMaxmemoryState`.\r\n\r\nPlease check @antirez ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2018-04-10T12:42:05Z",
        "closed_at": "2018-08-29T10:13:51Z",
        "merged_at": "2018-08-29T10:13:51Z",
        "body": "see more details in issue #4834 ",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2018-04-10T09:32:29Z",
        "closed_at": "2022-02-06T06:11:40Z",
        "merged_at": null,
        "body": "Using a special id '^' to indicate that an empty stream will be created, just like the special meaning of ID '*'.\r\nThis's is PR for issue #4824 . Set up a consumer group with an empty stream before any messages coming sounds quite  reasonable. :)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-04-10T09:21:00Z",
        "closed_at": "2018-05-25T14:27:22Z",
        "merged_at": "2018-05-25T14:27:22Z",
        "body": "strtoull() in libc may not set errno to EINVAL when the string contains invalid digits(tested with glibc 2.20 and 2.24) and string2ull() in t_stream.c will return wrong value.\r\n\r\n```\r\n> xadd s1 maxlen 5 invalidid sdf dsf\r\n(error) ERR The ID specified in XADD is equal or smaller than the target stream top item\r\n```\r\n\r\nthe above command should return `(error) ERR Invalid stream ID specified as stream command argument`",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-04-10T09:13:53Z",
        "closed_at": "2018-05-24T14:07:19Z",
        "merged_at": "2018-05-24T14:07:19Z",
        "body": "missing parenthesis causes wrong arithmetic priority.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-04-08T09:31:47Z",
        "closed_at": "2019-09-17T11:01:54Z",
        "merged_at": null,
        "body": "`c->woff` should be assigned before `call`, or the offset is not the latest, it's the offset when the client do the last command and may be way behind the current offset.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-04-06T10:48:18Z",
        "closed_at": "2018-07-12T11:42:16Z",
        "merged_at": "2018-07-12T11:42:16Z",
        "body": "Found a small problem while reading the source code D:)~\r\nSigned-off-by: charpty <charpty@gmail.com>",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 81,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2018-04-04T12:31:01Z",
        "closed_at": "2018-07-09T11:14:22Z",
        "merged_at": null,
        "body": "Curently, Redis uses localtime() in its log function to format time, but it's not thread safe.\r\n\r\nlocaltime_r() is thread safe, but it isn't safe for the bgsave subprocess if localtime_r() is called in another thread(like bio thread or thread created in redis module), because global shared lock is used in localtime_r() and localtime().",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 102,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2018-04-02T15:56:22Z",
        "closed_at": "2019-02-21T11:54:16Z",
        "merged_at": "2019-02-21T11:54:16Z",
        "body": "when setting repl-diskless-sync yes, and sending SYNC.\r\nredis-cli needs to be able to understand the EOF marker protocol\r\nin order to be able to skip or download the rdb file\r\n\r\nthis is useful mainly in order to test and possibly benchmark diskless master.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-03-30T08:05:34Z",
        "closed_at": "2021-06-29T03:56:41Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-03-29T15:22:04Z",
        "closed_at": "2018-10-09T10:56:09Z",
        "merged_at": "2018-10-09T10:56:09Z",
        "body": "This PR fixes issue #4803",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2018-03-29T12:24:09Z",
        "closed_at": "2018-05-28T03:26:03Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-03-29T09:39:39Z",
        "closed_at": "2018-06-08T10:32:19Z",
        "merged_at": "2018-06-08T10:32:19Z",
        "body": "BTW, PR #4774 and #3833 is interesting.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2018-03-22T10:25:12Z",
        "closed_at": "2018-03-22T15:21:20Z",
        "merged_at": "2018-03-22T15:21:20Z",
        "body": "After a client is unblocked (not by timeout), appropriate keyspace notifications should be sent.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-03-21T08:43:22Z",
        "closed_at": "2018-03-22T08:02:37Z",
        "merged_at": "2018-03-22T08:02:37Z",
        "body": "@antirez \r\nThere is missing decrRefCount for argv[11]\r\nThanks.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-03-21T07:17:00Z",
        "closed_at": "2018-03-22T15:22:32Z",
        "merged_at": "2018-03-22T15:22:32Z",
        "body": "If bind or listen failed in `anetListen`, we will close the listen socket fd, so do not double close when create tcp server failed.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2018-03-20T10:35:34Z",
        "closed_at": "2020-08-28T07:50:56Z",
        "merged_at": null,
        "body": "In the case of undefined HAVE_MALLOC_SIZE, the function zmalloc_size is not used in the function zmalloc etc. , then the number of bytes for memory alignment may still need to be added.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2018-03-20T09:46:41Z",
        "closed_at": "2018-06-08T10:36:12Z",
        "merged_at": null,
        "body": "Performance Testing\uff1a\r\n```\r\n#include <stdio.h>\r\n#include <limits.h>\r\n#include <time.h>\r\n\r\n/* Our hash table capability is a power of two */\r\nstatic unsigned long _dictNextPower(unsigned long size) {\r\n    unsigned long prev;\r\n    unsigned long iter = size;\r\n    if (size >= LONG_MAX) return LONG_MAX + 1LU;\r\n    while (iter) {\r\n        prev = iter;\r\n        iter = iter & (iter - 1);\r\n    }\r\n    return prev == size ? prev : prev << 1;\r\n}\r\n\r\n/* Our hash table capability is a power of two */\r\nstatic unsigned long _dictNextPower2(unsigned long size) {\r\n    unsigned long i = 4;\r\n\r\n    if (size >= LONG_MAX) return LONG_MAX;\r\n    while (1) {\r\n        if (i >= size)\r\n            return i;\r\n        i *= 2;\r\n    }\r\n}\r\n\r\nint main(void) {\r\n    int start;\r\n    int end;\r\n    start = time((time_t *) NULL);\r\n    for (int i = 0; i < 1000000000; ++i) {\r\n        _dictNextPower2(i);\r\n    }\r\n    end = time((time_t *) NULL);\r\n    printf(\"%d\\n\", end - start);\r\n\r\n    start = time((time_t *) NULL);\r\n    for (int j = 0; j < 1000000000; ++j) {\r\n        _dictNextPower(j);\r\n    }\r\n    end = time((time_t *) NULL);\r\n    printf(\"%d\\n\", end - start);\r\n    return 0;\r\n}\r\n```\r\n\r\nprint\uff1a\r\n```\r\n68\r\n42\r\n```\r\n\r\nthe algorithm avoid unnecessary calculation,for example:\r\n`when size = 129(10000001)`\r\nfor old algorithm,from 4 to 256\uff08result\uff09,need 6 step,but the new algorithm just need 2 step\u3002\r\n1.use value&(value-1) get highest bits value.until value = 0,and prev = 128(10000000)\r\n2.if prev == value ,return prev.\r\n3.else result = prev<<1 (100000000)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-03-19T16:06:05Z",
        "closed_at": "2020-08-28T07:45:05Z",
        "merged_at": null,
        "body": "The alignment/padding calculations in update_zmalloc_stat_(alloc|free) currently do nothing, haven't been used since thread safe mode was removed, and were probably broken years before that as they were used only for incrementation in the thread safe case. As the padding is accounted for in all calls to update_zmalloc_stat_(alloc|free) by the caller either using zmalloc_size or explicit addition of PREFIX_SIZE, they no longer appear to be needed here.\r\n\r\nSee issue #4739 \r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-03-18T15:31:44Z",
        "closed_at": "2021-06-28T13:35:53Z",
        "merged_at": null,
        "body": "If sh is NULL and init is neither SDS_NOINIT nor NULL, memset will cause segment default.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 49,
        "deletions": 138,
        "changed_files": 6,
        "created_at": "2018-03-18T14:29:00Z",
        "closed_at": "2018-05-30T13:22:54Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2018-03-18T05:11:57Z",
        "closed_at": "2022-02-13T16:50:54Z",
        "merged_at": null,
        "body": "While reading the code, I noticed the EXISTS command just calls `expireIfNeeded()` and `dictFind()` (via `dbExists()`), instead of going through `lookupKeyReadWithFlags()` like the other read-only commands. That means that EXISTS behaves differently from GET with keys that exist on the slave but are \"logically expired\", meaning the master hasn't yet had a chance to delete the expired key on the slave (see #1768).\r\n\r\nI tried fixing this by having `existsCommand()` call `lookupKeyReadWithFlags()`, with a new flag `LOOKUP_NOSTATS` that disables updating the hits/misses stats. By passing `LOOKUP_NOTOUCH | LOOKUP_NOSTATS`, we should get the exact same behavior as before, but with the extra key expiry logic included.\r\n\r\nI don't know if this is a bug or intentional, but I had fun fixing it and thought a PR might be helpful.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1007,
        "deletions": 213,
        "changed_files": 13,
        "created_at": "2018-03-17T19:17:09Z",
        "closed_at": "2020-08-11T06:24:31Z",
        "merged_at": null,
        "body": "Hi, @dvirsky (cc @antirez), it's too late to apply your reviews.\r\n\r\n# Design \r\n It follows RCP 1 https://github.com/redis/redis-rcp/blob/master/RCP1.md\r\n\r\n## Why does it take to merge command acls and group acls.\r\n ACLs is not determistic, it can affect by order, \r\n and some commands can be in multiple group.\r\n let's see the examples.\r\n\r\n```\r\n+#READONLY -#ZSET +#SLOW // we can use zrange\r\n+#READONLY +#SLOW -#ZSET // we can't use zrange\r\n```\r\n\r\n so I think to merge ACLs is more clear to make acls and make it to check easiler\r\n \r\n# Tests\r\n I added tests in tests/unit/acls.tcl\r\n\r\n# Compliance with RCP-1\r\n## group\r\n It follows RCP 1.\r\n it only added more group RCP was written before redis has geo and stream\r\n so it has geo and stream group to support those commands.\r\n\r\n## modules\r\n actullay, in RCP 1, there is no spec of MODULES.\r\n so I implemented, anyone can use commands in module.\r\n\r\n# Supporting slaves and sentinels\r\n I added auth code for slaves and sentinels\r\n\r\n# hashed passwords\r\n currenlty, not support hashed passwords.\r\n but it is very easy to support, redis always has sha1.c\r\n ",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2018-03-16T10:03:17Z",
        "closed_at": "2019-10-28T08:58:39Z",
        "merged_at": "2019-10-28T08:58:39Z",
        "body": "I came across some minor typo's and style inconsistencies while reading through the source code. Deliberately kept the changes very small after advice on HN. \ud83d\ude42 ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2018-03-16T04:42:59Z",
        "closed_at": "2021-08-05T15:09:08Z",
        "merged_at": null,
        "body": "Fix for #4753 with added unit test.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 13,
        "changed_files": 9,
        "created_at": "2018-03-15T16:57:14Z",
        "closed_at": "2018-06-16T08:59:38Z",
        "merged_at": "2018-06-16T08:59:37Z",
        "body": "Hi, @antirez \r\n\r\nAbout persistence, we have `aof-rewrite-incremental-fsync` option to avoid big latency spikes by committing AOF file to the disk incrementally, but we forgot RDB file.\r\n\r\nSo, I add an option `rdb-save-incremental-fsync`, please check. ^_^",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-03-15T11:33:54Z",
        "closed_at": "2018-03-22T08:20:18Z",
        "merged_at": "2018-03-22T08:20:17Z",
        "body": "I belive it should be listpack.o instead of listpack.c in Makefile\r\n\r\n@antirez if you have any intend to put it as listpack.c?",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 127,
        "deletions": 120,
        "changed_files": 1,
        "created_at": "2018-03-10T19:47:07Z",
        "closed_at": "2018-03-16T16:00:09Z",
        "merged_at": "2018-03-16T16:00:09Z",
        "body": "The current implementation uses the LogLog-Beta approach to estimate the cardinality from the HyperLogLog registers. Unfortunately, the method relies on magic constants that have been empirically determined. The formula presented in \"New cardinality estimation algorithms for HyperLogLog sketches\", https://arxiv.org/abs/1702.01284 has a better theoretical foundation and has already been independently verified in https://www.biorxiv.org/content/biorxiv/suppl/2018/02/09/262956.DC1/262956-1.pdf. The new implementation finds the histogram of registers first, which is finally fed into the new cardinality estimation formula.",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-03-07T14:22:00Z",
        "closed_at": "2022-02-13T14:45:22Z",
        "merged_at": null,
        "body": "should be `int prepareForShutdown(int flags)`",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-03-07T04:00:51Z",
        "closed_at": "2019-03-14T10:44:58Z",
        "merged_at": "2019-03-14T10:44:58Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2018-03-06T12:52:56Z",
        "closed_at": "2018-11-28T15:53:33Z",
        "merged_at": "2018-11-28T15:53:33Z",
        "body": "```\r\n127.0.0.1:6379> ZADD z 0 x\r\n(integer) 1\r\n127.0.0.1:6379> ZLEXCOUNT z (maxstring +\r\n(integer) 0\r\n```\r\n\r\nZLEXCOUNT should return 1\r\n\r\nThe problem occurs because of a call to `sdscmp` with `shared.maxstring` (which has the actual value of \"maxstring\") so it is basically equivalent to an empty range\r\n\r\nAnother problem was that in order to test empty ranges, `sdscmplex(range->min,range->max) > 1` was used instead of `sdscmplex(range->min,range->max) > 0`\r\n\r\nFinally, I changed `shared.maxstring` and `shared.minstring` to be exclusive so that the test for empty ranges will exit ASAP in case of `ZLEXCOUNT z - -` or `ZLEXCOUNT z + +`",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-03-04T04:41:36Z",
        "closed_at": "2020-09-09T16:26:18Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-03-03T02:33:54Z",
        "closed_at": "2020-09-09T16:26:22Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2018-03-03T02:22:40Z",
        "closed_at": "2020-09-09T16:26:25Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-03-02T09:49:26Z",
        "closed_at": "2018-07-19T10:21:35Z",
        "merged_at": "2018-07-19T10:21:35Z",
        "body": "The genRedisInfoString function use getClientsMaxBuffers for calculating clients longest output list and biggest input buffer, getClientsMaxBuffers  will check every client for these information. When there are too many clients (it's common when clients use short connections scenario), check every client will consume a lot of time.   For info command, the raw genRedisInfoString function call getClientsMaxBuffers every time, which is unnecessary for info server, info  memory, etc. So it is better for info default, info all, info clients calling  genRedisInfoString, the other command don't.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-03-01T13:33:34Z",
        "closed_at": "2018-03-22T15:23:41Z",
        "merged_at": "2018-03-22T15:23:41Z",
        "body": "We don't need to call aeDeleteFileEvent twice.\r\nwe can remove one system call to merge these conditions.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2018-03-01T05:39:57Z",
        "closed_at": "2020-03-31T10:42:19Z",
        "merged_at": "2020-03-31T10:42:19Z",
        "body": "If lots of clients `PSUBSCRIBE` to same patterns, multiple patterns matching will take place. This commit change it into just one single pattern matching by using a `dict *` to store the unique pattern and which clients subscribe to it.\r\n\r\nFor example, if there are two clients `PSUBSCRIBE f*o` and one clients `PSUBSCRIBE b*r`, redis will perform pattern match 3 times:\r\n\r\n![7](https://user-images.githubusercontent.com/3157740/36828546-78d6d2fe-1d55-11e8-86ab-e99b5c1c4bcc.png)\r\n\r\nAfter the modification by this commit, it will only perform 2 times of pattern match:\r\n\r\n![8](https://user-images.githubusercontent.com/3157740/36828569-96709750-1d55-11e8-8b8b-0b01a73a0f49.png)\r\n\r\nSo when there is 100 clients subscribe the same pattern, this commit will reduce the pattern match time from 100 to 1. **This feature is already tested and run in production environment at Alibaba Group without any issue for about 2 months.**",
        "comments": 20
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-03-01T02:18:17Z",
        "closed_at": "2019-03-14T11:21:41Z",
        "merged_at": null,
        "body": "1. \"LREM\" notification should be of type NOTIFY_LIST\r\n2. In RPOPLPUSH: \"RPOP\" notification should come before \"LPUSH\"",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 284,
        "deletions": 108,
        "changed_files": 8,
        "created_at": "2018-02-28T18:46:09Z",
        "closed_at": "2018-07-15T15:22:01Z",
        "merged_at": null,
        "body": "- slave buffers didn't count internal fragmentation and sds unused space,\r\n  this caused them to induce eviction although we didn't mean for it.\r\n- slave buffers were consuming about twice the memory of what they actually needed\r\n  this was mainly due to sdsMakeRoomFor growing to twice as much as needed each time\r\n  but networking.c never storing more than 16k and usually less since wasn't able to\r\n  store half of the new string into one buffer and the other half into the next.\r\n- inefficient performance due to starting from a small string and reallocing many times.\r\n\r\nwhat i changed:\r\n- counting slave buffers size with zmalloc_size\r\n- when creating a new sds reply node from char*, preallocate it to 16k\r\n- when creating a new sds reply node from an existing sds, keep the sds (without memcpy)\r\n  but grow the logical size of the sds so that it takes over it's internal fragmentation\r\n- when appending a new reply to the buffer, first fill all the unused space of the previous\r\n  node before starting a new one\r\n\r\nother changes:\r\n- bugfix in sdsReqType creating 64bit headers on 32bit systems\r\n- additional intefaces for sds library in order to support the above listed changes\r\n- expose mem_not_counted_for_evict info field for the benefit of the test suite\r\n- add a test to make sure slave buffers are counted correctly and that they don't cause eviction",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-02-27T13:04:46Z",
        "closed_at": "2018-02-27T18:15:28Z",
        "merged_at": "2018-02-27T18:15:28Z",
        "body": "Because of 2nd and 3rd conditions parenthesis position,\r\nit only plus 0 to len in 2nd and 3rd conditions.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-02-27T12:47:59Z",
        "closed_at": "2018-03-01T11:39:15Z",
        "merged_at": "2018-03-01T11:39:15Z",
        "body": "TYPE_NONE is 6\r\nbut array is allocated with 5.\r\nit causes index range issue.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-02-26T08:55:29Z",
        "closed_at": "2020-09-27T04:20:59Z",
        "merged_at": null,
        "body": "@antirez Thanks for your review",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-02-24T12:51:28Z",
        "closed_at": "2021-03-18T14:39:38Z",
        "merged_at": null,
        "body": "In order to maintain consistency with the rest of the API and docs.\r\n\r\nFixes #4709",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-02-23T03:13:26Z",
        "closed_at": "2018-03-22T15:33:07Z",
        "merged_at": null,
        "body": "OBJ_ENCODING_EMBSTR_SIZE_LIMIT Revised to 44",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-02-23T02:58:58Z",
        "closed_at": "2018-03-22T15:32:45Z",
        "merged_at": "2018-03-22T15:32:45Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-02-22T22:03:29Z",
        "closed_at": "2020-09-09T16:26:28Z",
        "merged_at": null,
        "body": "Realized there was a typo by watching Writing System Software Ep1 ( https://youtu.be/VBrnmciV9fM?t=14m27s )",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-02-21T09:18:12Z",
        "closed_at": "2019-03-21T11:18:05Z",
        "merged_at": "2019-03-21T11:18:05Z",
        "body": "According to C11, the behavior of realloc with size 0 is now deprecated.\r\nit can either behave as free(ptr) and return NULL, or return a valid pointer.\r\nBut in zmalloc it can lead to zmalloc_oom_handler and panic.\r\n\r\nNote that this can affect modules that use it, and modules can even pass the allocation function to other libraries they use, which may rely on the scenario crash instead.\r\n\r\nIt looks like both glibc allocator and jemalloc behave like so:\r\n  realloc(malloc(32),0) returns NULL\r\n  realloc(NULL,0) returns a valid pointer\r\n\r\nThis commit changes zmalloc to behave the same",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2018-02-19T22:13:31Z",
        "closed_at": "2021-07-15T22:47:11Z",
        "merged_at": null,
        "body": "The monotonic clock can be used for block requests\r\nor user timeouts as settimeofday does not affect\r\nthis clock.\r\n\r\nThis was based on tycho:monotonic-clock work but with a smaller scope. ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 884,
        "deletions": 205,
        "changed_files": 10,
        "created_at": "2018-02-18T15:57:45Z",
        "closed_at": "2018-03-22T08:16:32Z",
        "merged_at": "2018-03-22T08:16:32Z",
        "body": "First commit: **Avoid latency spikes when defragging huge keys**\r\n* big keys are not defragged in one go from within the dict scan  instead they are scanned in parts after the main dict hash bucket is done.\r\n* add latency monitor sample for defrag\r\n* change default active-defrag-cycle-min to induce lower latency\r\n* make active defrag start a new scan right away if needed, so it's easier (for the test suite) to detect when it's done\r\n* make active defrag quit the current cycle after each db / big key\r\n* defrag  some non key long term global allocations\r\n* some refactoring for smaller functions and more reusable code\r\n\r\nSecond commit: **Adding real allocator fragmentation to INFO and MEMORY command + active defrag test**\r\n    \r\nother fixes / improvements:\r\n* LUA script memory isn't taken from zmalloc (taken from libc malloc) so it can cause high fragmentation ratio to be displayed (which is false)\r\n* there was a problem with \"fragmentation\" info being calculated from RSS and used_memory sampled at different times (now sampling them together)\r\n    \r\nother details:\r\n* adding a few more allocator info fields to INFO and MEMORY commands\r\n* improve defrag test to measure defrag latency of big keys\r\n* increasing the accuracy of the defrag test (by looking at real grag info), this way we can use an even lower threshold and still avoid false positives\r\n* keep the old (total) \"fragmentation\" field unchanged, but add new ones for spcific things\r\n* add these the MEMORY DOCTOR command\r\n* deduct LUA memory from the rss in case of non jemalloc allocator (one for which we don't \"allocator active/used\")\r\n* reduce sampling rate of the rss and allocator info\r\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-02-14T02:57:44Z",
        "closed_at": "2022-02-10T07:18:59Z",
        "merged_at": "2022-02-10T07:18:59Z",
        "body": "From the source code, we could tell the master will send the PINGs to its slaves not the other way round.\r\n\r\n> /* First, send PING according to ping_slave_period. */\r\n>     if ((replication_cron_loops % server.repl_ping_slave_period) == 0 &&\r\n>         listLength(server.slaves))\r\n>     {\r\n>         ping_argv[0] = createStringObject(\"PING\",4);\r\n>         replicationFeedSlaves(server.slaves, server.slaveseldb,\r\n>             ping_argv, 1);\r\n>         decrRefCount(ping_argv[0]);\r\n>     }",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 5,
        "created_at": "2018-02-13T01:05:53Z",
        "closed_at": "2021-11-03T09:37:44Z",
        "merged_at": null,
        "body": "CodeAI, the first non-human contributor to your software project, found 19 defects and fixed 10 in Redis.  It wants to merge commits fixing 5 of these issues- 3 Null Pointer Dereference and 2 Dead Code. To view all 10 fixed issues from the run, claim your free open source account at mycode.ai. \r\n A screenshot of the results as well as the Dockerfile used to build and run your project in CodeAI can be found here- https://drive.google.com/drive/folders/1fR0qB4JqHscGPVYIqT-ZP4H5dNukVvSH?usp=sharing.\r\n\r\nIf you have any questions about these results or have general inquiries about CodeAI, please send an email to techsupport@mycode.ai",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-02-12T07:44:07Z",
        "closed_at": "2020-09-09T16:26:32Z",
        "merged_at": null,
        "body": "Fix `five behaviors` to `eight behaviors` in [this sentence ](https://github.com/antirez/redis/blob/unstable/redis.conf#L564)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2018-02-11T13:29:43Z",
        "closed_at": "2018-03-22T15:26:01Z",
        "merged_at": "2018-03-22T15:26:01Z",
        "body": "dbOverwrite() is called only after a lookup method or in setKey(). It should not update lfu-counter, because lookupKey() has updated it. After remove updateLFU() in it, some commands such as getset, set xx when key is exist still update lfu-counter twice in one call, but many other commands not.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-02-10T08:11:27Z",
        "closed_at": "2018-02-12T07:22:02Z",
        "merged_at": null,
        "body": "Hi, \r\nWhen I wrote a script for Redis, I found a small problem.\r\nWhen I use '\\n', Redis will lose the first letter of the next command.\r\n\r\n![image](https://user-images.githubusercontent.com/8869963/36060073-7b382c38-0e7c-11e8-999b-235a5e9abf70.png)\r\n\r\nSigned-off-by: charpty <charpty@gmail.com>",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-02-08T08:16:37Z",
        "closed_at": "2018-07-18T11:15:29Z",
        "merged_at": null,
        "body": "Sometimes in order to complete a full resync on a write traffic burst situation, we need to increase the size of replication backlog. \r\n\r\nI have encountered a situation where replication backlog is set to 4GB which caused most of the keys in database evicted. This may be beyond my expectation and other Redis users'.  \r\n\r\nSo i think exclude the replication backlog when computing used memory is a better idea, after all replication log is kind of cache and shouldn't be considered as part of data set. ",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 33307,
        "deletions": 45857,
        "changed_files": 261,
        "created_at": "2018-02-07T03:26:43Z",
        "closed_at": "2022-02-06T05:55:18Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-02-06T04:09:44Z",
        "closed_at": "2018-02-07T12:47:48Z",
        "merged_at": null,
        "body": "hi, sdsnewlen s_malloc call failed, sh == NULL, and memset(sh, 0, ...) cause segment fault",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-02-02T03:03:14Z",
        "closed_at": "2022-02-07T15:51:34Z",
        "merged_at": null,
        "body": "support list all sha and get lua script for specific sha, it's good choice to use for different redis master to be consistent with other's lua scripts.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-01-28T15:24:19Z",
        "closed_at": "2018-01-31T03:24:02Z",
        "merged_at": null,
        "body": "\"j = j+seplen-1; \" don't  \"skip the separator\", I think it should be \"j = j+seplen\"",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-01-26T14:52:37Z",
        "closed_at": "2018-06-22T13:48:34Z",
        "merged_at": "2018-06-22T13:48:34Z",
        "body": "About special configuration item \"\", loading configuration file should be consistent with the behavior of `config set`",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2018-01-26T13:51:45Z",
        "closed_at": "2020-08-11T10:59:31Z",
        "merged_at": "2020-08-11T10:59:30Z",
        "body": "Hi, proto-max-bulk-len is a good option for user, we can enlarge string limit by ourselves.\r\n\r\nBut it introduces some new problems at the same time... just like the action below:\r\n\r\n```\r\n127.0.0.1:6379> config get proto-max-bulk-len\r\n1) \"proto-max-bulk-len\"\r\n2) \"536870912\"\r\n127.0.0.1:6379> config set proto-max-bulk-len 1\r\nOK\r\n127.0.0.1:6379> ping\r\n(error) ERR Protocol error: invalid bulk length\r\n127.0.0.1:6379> config set proto-max-bulk-len 512mb\r\n(error) ERR Protocol error: invalid bulk length\r\n```\r\n\r\nOops... redis can never execute any commands, so I think `proto-max-bulk-len` must be 512mb or greater.\r\n\r\nBTW, string's limit should be replaced with `proto-max-bulk-len` instead of 512MB.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 8,
        "changed_files": 5,
        "created_at": "2018-01-26T07:02:27Z",
        "closed_at": "2022-02-07T21:23:40Z",
        "merged_at": null,
        "body": "I think include a .c file is not so pretty.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-01-23T15:49:27Z",
        "closed_at": "2018-01-24T09:48:04Z",
        "merged_at": "2018-01-24T09:48:04Z",
        "body": "Older versions might not have this function.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-01-23T03:14:23Z",
        "closed_at": "2021-11-02T12:08:52Z",
        "merged_at": null,
        "body": "instantaneous_input_kbps made me confused when I pay attention to the bandwith. As we know kbps means 1000 bits per second, but actually this field wants to show nums of 1024 byes per second as I saw in the code.\r\nAnd I see that all info displayed is lower case, so kbps -> KBps may not a good idea. :(\r\nBut some other may be confused by kbps too as follows:\r\nhttps://stackoverflow.com/questions/41097459/is-redis-statitic-value-instantaneous-output-kbps-in-bytes-or-bits",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2018-01-22T22:33:43Z",
        "closed_at": "2020-11-24T17:06:16Z",
        "merged_at": null,
        "body": "This is my go at a low-level API version for PUBLISH.\r\n\r\nAs you can see it doesn't include much. Interestingly there was no need for a module context to publish, so the command doesn't accept a RedisModuleCtx at all. It looks weird, so if you think that for the sake of consistency or future proofing we should keep it there - I'll add it as an unused parameter for now. \r\n\r\nAlso proper testing is not really possible from within the test module as we can't subscribe. So the test just checks that publishing works and that since there are no listeners the return value is 0. I've subscribed manually and saw the messages being received to the client. ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2018-01-22T09:10:21Z",
        "closed_at": "2021-02-28T12:11:33Z",
        "merged_at": null,
        "body": "This PR makes the client used by the module to perform RM_Call, part of the module context. If repeated calls to RM_Call are made, we reuse the same client, only destroying it along with the module.\r\n\r\nThis achieves a huge performance gain in repeated RM_Call's. For comparison, here is an example of **calling PING a million times** with and without this patch.\r\n\r\n**Without:**\r\n\r\n```\r\n127.0.0.1:6379> BENCH.CALL\r\nOKAY\r\n(3.48s)\r\n127.0.0.1:6379> BENCH.CALL\r\nOKAY\r\n(3.42s)\r\n```\r\n\r\n**After the patch is applied:**:\r\n```\r\n127.0.0.1:6379> BENCH.CALL\r\nOKAY\r\n(1.28s)\r\n127.0.0.1:6379> BENCH.CALL\r\nOKAY\r\n(1.27s)\r\n```\r\n\r\nNotice that this only optimizes repeated calls within the same context, which is a rather common case but not always the case. If we just used the same client for ALL RM_Call's, we would optimize even for the case of a module performing a single call. I wasn't sure how safe that is, so I want with the more conservative option. If you think that we can recycle all the way - let's go for it!",
        "comments": 18
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2018-01-20T12:46:53Z",
        "closed_at": "2018-01-22T15:22:58Z",
        "merged_at": null,
        "body": "Is there a problem about __ziplistDelete function in ziplist.c as follows:\r\n```bash\r\n            /* When the tail contains more than one entry, we need to take\r\n             * \"nextdiff\" in account as well. Otherwise, a change in the\r\n             * size of prevlen doesn't have an effect on the *tail* offset. */\r\n            zipEntry(p, &tail);\r\n            if (p[tail.headersize+tail.len] != ZIP_END) {\r\n                ZIPLIST_TAIL_OFFSET(zl) =\r\n                   intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+nextdiff);\r\n            }\r\n```\r\nAs we know, ZIPLIST_TAIL_OFFSET macro return the offset of the last item(entry) inside the ziplist.\r\nWhen the tail contians just one entry instead of more than one entry , should we take \"nextdiff\"\r\n in account as well?  issue: https://github.com/antirez/redis/issues/4621\r\n\r\nThanks!",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2018-01-19T05:52:44Z",
        "closed_at": "2018-03-22T08:46:57Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-01-18T11:56:10Z",
        "closed_at": "2021-06-30T09:14:11Z",
        "merged_at": "2021-06-30T09:14:11Z",
        "body": "1. Add one key-value pair to myhash, which the length of key and value both less than hash-max-ziplist-value, for example:\r\n>hset myhash key value\r\n\r\n2. Then execute the following command\r\n>hsetnx myhash key value1 (the length greater than hash-max-ziplist-value)\r\n\r\n3. This will add nothing, but the code type of \"myhash\" changed from ziplist to dict even there are only one key-value pair in \"myhash\", and both of them less than hash-max-ziplist-value.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 35,
        "changed_files": 5,
        "created_at": "2018-01-17T11:09:44Z",
        "closed_at": "2020-09-09T16:29:20Z",
        "merged_at": null,
        "body": "Fix the typos in comments of cluster.c and latency.c, and also fix two typos in code:\r\n\r\n     line 345 in latency.c:   !strcasecmp(event,\"rdb-**unlik**-temp-file\")) {\r\n     line 4883 in cluster.c:  migrateCloseTimedoutSockets",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2018-01-17T09:13:03Z",
        "closed_at": "2020-08-12T02:25:25Z",
        "merged_at": "2020-08-12T02:25:25Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-01-16T14:50:04Z",
        "closed_at": "2018-01-17T09:45:12Z",
        "merged_at": "2018-01-17T09:45:12Z",
        "body": "fix assert problem in ZIP_DECODE_PREVLENSIZE macro(in ziplist.c)\r\n, see issue: https://github.com/antirez/redis/issues/4587",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2018-01-16T08:18:46Z",
        "closed_at": "2018-01-16T14:32:59Z",
        "merged_at": "2018-01-16T14:32:59Z",
        "body": "after a slave is promoted (assuming it has no slaves\r\nand it booted over an hour ago), it will lose it's replication\r\nbacklog at the next replication cron, rather than waiting for slaves\r\nto connect to it.\r\nso on a simple master/slave faiover, if the new slave doesn't connect\r\nimmediately, it may be too later and PSYNC2 will fail.\r\n\r\n```98355:M 16 Jan 10:08:11.384 # Connection with master lost.\r\n98355:M 16 Jan 10:08:11.384 * Caching the disconnected master state.\r\n98355:M 16 Jan 10:08:11.384 * Discarding previously cached master state.\r\n98355:M 16 Jan 10:08:11.384 * MASTER MODE enabled (user request from 'id=6 addr=127.0.0.1:53740 fd=9 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveof')\r\n98488:M 16 Jan 10:08:11.384 # Connection with slave 127.0.0.1:12345 lost.\r\n98355:M 16 Jan 10:08:12.283 * Replication backlog freed after 3600 seconds without connected slaves.\r\n```",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-01-14T17:09:11Z",
        "closed_at": "2018-01-15T11:43:56Z",
        "merged_at": "2018-01-15T11:43:56Z",
        "body": "Hi, @antirez \r\n\r\nWe have talked about lazyfree's memory leak problem in issue #4323 , and already fixed a bug about `FLUSHALL ASYNC` and `SLOWLOG`, but now I find another problem...about `lazyfree-lazy-server-del`.\r\n\r\nThe `lazyfree-lazy-server-del` works in `rename` and `move` command, more accurately it's in `dbDelete()`\r\n```\r\nint dbDelete(redisDb *db, robj *key) {\r\n    return server.lazyfree_lazy_server_del ? dbAsyncDelete(db,key) :\r\n                                             dbSyncDelete(db,key);\r\n}\r\n```\r\n\r\nTake `move` command as an example:\r\n```\r\nvoid moveCommand(client *c) {\r\n...\r\n    o = lookupKeyWrite(c->db,c->argv[1]);\r\n...\r\n    incrRefCount(o);\r\n\r\n    /* OK! key moved, free the entry in the source DB */\r\n    dbDelete(src,c->argv[1]);\r\n...\r\n}\r\n```\r\n\r\nFrom the codes above, `o` is the source object, we call `incrRefCount` to increase `o->refcount`, after that and before lazyfree works `o->refcount` is 2...that is the problem which means that we create a race about the shared object, and may result in memory leak.\r\n\r\nThis PR fixed the memory leak, please check.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-01-10T16:28:12Z",
        "closed_at": "2018-05-08T15:27:19Z",
        "merged_at": "2018-05-08T15:27:19Z",
        "body": "negative nextdb will cause a segmentation fault.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2018-01-10T08:26:33Z",
        "closed_at": "2018-08-02T02:51:24Z",
        "merged_at": null,
        "body": "ldo.c: In function \u2018f_parser\u2019:\r\nldo.c:496:7: warning: unused variable \u2018c\u2019 [-Wunused-variable]\r\n   int c = luaZ_lookahead(p->z);\r\n          ^",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-01-09T08:15:01Z",
        "closed_at": "2018-01-12T16:16:13Z",
        "merged_at": "2018-01-12T16:16:12Z",
        "body": "instuction -> instruction ?",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2018-01-08T09:20:47Z",
        "closed_at": "2020-08-18T16:41:03Z",
        "merged_at": null,
        "body": "\r\n#### 1.Slave Evicting\r\n\r\nslave should not perform evicting , even if it reaches it`s \"maxmemory\"\r\n\r\nThe way Redis calculate how much memory should be released is  how much memory we have used in the entire instance minus \"maxmemory\" directive . But master and slave may not always have exactly the same memory usage, there are always some slightly different !(for example : slave have much more connection than master) \r\n\r\nIf slave have reached it`s \"maxmemory\", but master did not, then some keys would be evicted from slave, but still existing in master. It might be fine if our key was just a simply string and we never use commands like \"INCR\" or \"DECR\", but considering if the key is a more complex key, a \"list\" for example, this would result data was incomplete between master and slave \r\n\r\nThe way we fix this was disable memory eviction if the current instance is a slave. The evicting shall be performed by master and replicate to slave\r\n\r\n### 2. maximum evicting \r\nThe way we evicting key if \"maxmemory\" reached was calculate how much memory should be release as a \"memory_to_be_free\" variable, and keep deleting objects till there are exactly \"memory_to_be_free\" bytes of memory was freed. \r\n\r\nThere are special cases : the \"memory_to_be_free\" variable may be a too big value. \r\nfor example :\r\n\r\n* dict rehashing \r\n    \r\n    This really happens in our product environment ! And was the major motivation of this update. Redis would always perform rehash size of a hash table was reached a specific number . And as the size of hash table grows, the rehashing memory allocation grows too. If the size reaches \"8013608\" \uff0c 128Mb of memory shall be alloced at once. The \"memory_to_be_free\" variable would be about 128Mb, in the next call of \"freeMemoryIfNeeded()\", that`s were could block Redis. \r\n    \r\n* manual config change\r\n\r\n    User changes \"maxmemory\" to a smaller value would also block server. But that`s acceptable\r\n    \r\n\r\nI set a maximum iterate limitation for \"freeMemoryIfNeeded()\". The value was set to 5, which means 5 keys shall be evicted at most per call . \r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 56,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2018-01-07T14:43:27Z",
        "closed_at": "2018-01-12T16:41:10Z",
        "merged_at": "2018-01-12T16:41:10Z",
        "body": "This PR adds the RedisModule_UnlinkKey, which deletes a key in an async fashion, whether lazyfree is configured or not. It includes a basic test for this. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-01-05T04:18:21Z",
        "closed_at": "2018-01-05T10:53:12Z",
        "merged_at": "2018-01-05T10:53:12Z",
        "body": "When using the command MEMORY USAGE to estimate a list key, weird stuff came out as below:\r\n```\r\n127.0.0.1:6379> keys *\r\n1) \"mylist\"\r\n127.0.0.1:6379> info memory\r\n# Memory\r\nused_memory:1038371263\r\nused_memory_human:990.27M\r\nused_memory_rss:1045626880\r\nused_memory_rss_human:997.19M\r\nused_memory_peak:1038371263\r\nused_memory_peak_human:990.27M\r\nused_memory_peak_perc:100.00%\r\nused_memory_overhead:884838\r\nused_memory_startup:835104\r\nused_memory_dataset:1037486425\r\nused_memory_dataset_perc:100.00%\r\ntotal_system_memory:67467202560\r\ntotal_system_memory_human:62.83G\r\nused_memory_lua:37888\r\nused_memory_lua_human:37.00K\r\nmaxmemory:0\r\nmaxmemory_human:0B\r\nmaxmemory_policy:noeviction\r\nmem_fragmentation_ratio:1.01\r\nmem_allocator:libc\r\nactive_defrag_running:0\r\nlazyfree_pending_objects:0\r\n\r\n127.0.0.1:6379> llen mylist\r\n(integer) 10000159\r\n\r\n127.0.0.1:6379> memory usage mylist\r\n(integer) 70265117278\r\n\r\n127.0.0.1:6379> debug object mylist\r\nValue at:0x7ace78 refcount:1 encoding:quicklist serializedlength:23291534 lru:5174912 lru_seconds_idle:175 ql_nodes:126585 ql_avg_node:79.00 ql_ziplist_max:-2 ql_compressed:0 ql_uncompressed_size:1031408812\r\n\r\n```\r\nObviously, it's not the right estimation. By checking the source code, I found the sizeof quicklist should be used instead of the total number of ziplist entry.\r\n\r\nAfter fixing this, we can get the right memory usage estimation as below:\r\n\r\n```\r\n127.0.0.1:6379> memory usage mylist\r\n(integer) 889436925\r\n(3.10s)\r\n\r\n127.0.0.1:6379> memory usage mylist samples 10000\r\n(integer) 1035392366\r\n(5.96s)\r\n```\r\n  ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-01-03T05:10:16Z",
        "closed_at": "2018-01-12T16:43:02Z",
        "merged_at": "2018-01-12T16:43:02Z",
        "body": "When I use `redis-benchmark`, something unexpected happened, see the end of the result below:\r\n\r\n```\r\n====== GET ======\r\n  100001 requests completed in 1.23 seconds\r\n  100 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n\r\n98.23% <= 1 milliseconds\r\n99.95% <= 2 milliseconds\r\n100.00% <= 2 milliseconds\r\n81500.41 requests per second\r\n\r\n\r\nAll clients disconnected... aborting.\r\n```\r\n\r\n**All clients disconnected... aborting.** can only appear when we have no liveclients:\r\n\r\n```\r\nint showThroughput(...\r\n    if (config.liveclients == 0) {\r\n         fprintf(stderr,\"All clients disconnected... aborting.\\n\");\r\n         exit(1);\r\n     }\r\n}\r\n```\r\n\r\nIt seems like we lost all connections, but no errors are printed.\r\n\r\n`config.liveclients--` happens only when we call `freeClient()`, I check all the codes about `freeClient()` in `redis-benchmark.c` and finally find the problem:\r\n\r\n```\r\nstatic void writeHandler(aeEventLoop *el, int fd, void *privdata, int mask) {\r\n    /* Initialize request when nothing was written. */\r\n    if (c->written == 0) {\r\n        /* Enforce upper bound to number of requests. */\r\n        if (config.requests_issued++ >= config.requests) {\r\n            freeClient(c);\r\n            return;\r\n        }\r\n```\r\n\r\nWe call `freeClient()` when the `config.requests_issued` is greater than `config.requests` or all requests finished.\r\n\r\n```\r\nstatic void clientDone(client c) {\r\n    if (config.requests_finished == config.requests) {\r\n        freeClient(c);\r\n        aeStop(config.el);\r\n        return;\r\n    }\r\n```\r\n\r\nBut, after `aeStop`, the time event still have chance to execute, that is the problem, I fix it as below:\r\n\r\n```\r\n--- a/src/redis-benchmark.c\r\n+++ b/src/redis-benchmark.c\r\n@@ -614,7 +614,7 @@ int showThroughput(struct aeEventLoop *eventLoop, long long id, void *clientDat\r\n     UNUSED(id);\r\n     UNUSED(clientData);\r\n\r\n-    if (config.liveclients == 0) {\r\n+    if (config.liveclients == 0 && config.requests_finished != config.requests) {\r\n         fprintf(stderr,\"All clients disconnected... aborting.\\n\");\r\n         exit(1);\r\n     }\r\n```",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2018-01-03T03:59:54Z",
        "closed_at": "2021-03-24T15:48:48Z",
        "merged_at": null,
        "body": "In case of the query buffer abuse, it's often confusing to get such the memory metrics:\r\n\r\n> 127.0.0.1:6379> info memory\r\n> # Memory\r\n> used_memory:5873901248\r\n> **used_memory_human:5.47G**\r\n> used_memory_rss:5255127040\r\n> used_memory_rss_human:4.89G\r\n> used_memory_peak:5873902224\r\n> used_memory_peak_human:5.47G\r\n> used_memory_peak_perc:100.00%\r\n> used_memory_overhead:5121658606\r\n> used_memory_startup:765576\r\n> used_memory_dataset:752242642\r\n> used_memory_dataset_perc:12.81%\r\n> total_system_memory:67467202560\r\n> total_system_memory_human:62.83G\r\n> used_memory_lua:37888\r\n> used_memory_lua_human:37.00K\r\n> maxmemory:1073741824\r\n> **maxmemory_human:1.00G**\r\n> maxmemory_policy:noeviction\r\n> mem_fragmentation_ratio:0.89\r\n> mem_allocator:jemalloc-4.0.3\r\n> active_defrag_running:0\r\n> lazyfree_pending_objects:0\r\n\r\nIn this pr, three metrics have been added. They can give a big help for users to know where his memory goes:\r\n\r\n> 127.0.0.1:6379> info memory\r\n> # Memory\r\n> used_memory:6122779168\r\n> used_memory_human:5.70G\r\n> used_memory_rss:6439333888\r\n> used_memory_rss_human:6.00G\r\n> used_memory_peak:6122779168\r\n> used_memory_peak_human:5.70G\r\n> used_memory_peak_perc:100.00%\r\n> used_memory_overhead:5122885286\r\n> used_memory_startup:765960\r\n> used_memory_dataset:999893898\r\n> **used_memory_dataset_human:953.57M**\r\n> used_memory_dataset_perc:16.33%\r\n> **used_memory_query_buffer: 5120033374\r\n> used_memory_query_buffer_human: 4.77G**\r\n> total_system_memory:67467202560\r\n> total_system_memory_human:62.83G\r\n> used_memory_lua:37888\r\n> used_memory_lua_human:37.00K\r\n> maxmemory:1073741824\r\n> maxmemory_human:1.00G\r\n> maxmemory_policy:noeviction\r\n> mem_fragmentation_ratio:1.05\r\n> mem_allocator:jemalloc-4.0.3\r\n> active_defrag_running:0\r\n> lazyfree_pending_objects:0\r\n  ",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2018-01-01T04:33:26Z",
        "closed_at": "2021-06-24T06:55:32Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-12-31T02:25:38Z",
        "closed_at": "2019-03-25T22:15:12Z",
        "merged_at": null,
        "body": "fix typo in README.md\r\nanitrez/redis -> antirez/redis",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 34,
        "changed_files": 11,
        "created_at": "2017-12-29T10:49:56Z",
        "closed_at": "2018-01-11T10:20:03Z",
        "merged_at": "2018-01-11T10:20:03Z",
        "body": "Reasoning:\r\nRedis limits individual strings to 512MB, but obviously it is possible to create a hash object containing many fields each of 512MB, such a key can be serialized to well over 4GB, and while saving and loading it from RDB works perfectly well, trying to DUMP / RESTORE such a key will fail.\r\n\r\nThis PR has two commits:\r\n* one is about changing variable types in various places from int (always 32bit) to long / size_t (64bit on 64bit platforms).\r\n* the other adds config options that increase query buffer and bulk size limits.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2017-12-28T12:14:37Z",
        "closed_at": "2020-09-09T16:26:35Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-12-26T04:18:27Z",
        "closed_at": "2018-08-02T02:51:01Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-12-22T08:36:27Z",
        "closed_at": "2021-06-27T09:14:31Z",
        "merged_at": null,
        "body": "Before commit there are no more than <-P numreq>*<-c clients>(such as 2450) redunant requests during pipeline mode.\r\n\r\n    $redis-benchmark -n 100000 -P 50 -q incr foo;redis-cli get foo\r\n    incr foo: 2173913.00 requests per second\r\n    \"102450\"\r\n\r\nAfter commit redunant nums reduce to no more than <-P numreq>.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 17,
        "changed_files": 3,
        "created_at": "2017-12-21T06:37:10Z",
        "closed_at": "2018-01-15T12:01:12Z",
        "merged_at": null,
        "body": "Hi, @antirez \r\n\r\nUnfortunately, the `sds` also has the int casting problem, another PR #4519 is about `zset`.\r\n\r\nI think we should use -Wconversion to check all the casting carefully.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-12-18T10:00:31Z",
        "closed_at": "2018-01-17T10:21:56Z",
        "merged_at": "2018-01-17T10:21:56Z",
        "body": "see  https://github.com/antirez/redis/issues/4545",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2017-12-18T08:32:10Z",
        "closed_at": "2018-04-11T07:19:30Z",
        "merged_at": null,
        "body": "Hi @antirez \r\n\r\nThere are three problems about lazyfree eviction I think:\r\n\r\n1. We forgot the latency generated by waiting for `BIO_LAZY_FREE`\r\n\r\n    I think it's necessary to record this kind of latency.\r\n\r\n2. Maybe we misuse `latencyRemoveNestedEvent `\r\n\r\n    In my opinion the `eviction-cycle` should contains all latency, including `eviction-del` and `eviction-lazyfree`, so we should not remove `eviction-del` from `eviction-cycle`.\r\n\r\n3. The last problem is about `cant_free`\r\n\r\n    When we goto `cant_free`, we wait for `BIO_LAZY_FREE` finished or break if already freed enough memory.\r\n\r\n    But, we use `mem_freed` duplicately.\r\n\r\n    ```\r\n        while(bioPendingJobsOfType(BIO_LAZY_FREE)) {\r\n            if (((mem_reported - zmalloc_used_memory()) + mem_freed) >= mem_tofree)\r\n                break;\r\n            usleep(1000);\r\n        }\r\n    ```\r\n\r\n    I add a variable `mem_used_init` to record the initial used memory, please check.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2017-12-15T03:12:48Z",
        "closed_at": "2017-12-15T11:41:03Z",
        "merged_at": "2017-12-15T11:41:03Z",
        "body": "see https://github.com/antirez/redis/issues/4538",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-12-15T03:10:45Z",
        "closed_at": "2021-10-28T10:33:50Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2017-12-13T23:07:34Z",
        "closed_at": "2017-12-14T21:14:45Z",
        "merged_at": "2017-12-14T21:14:45Z",
        "body": "This change originates from [my Windows fork of Redis 4.0.2](https://github.com/tporadowski/redis) where original ```redis-cli.c``` code could not successfully determine path to ```.rediscli_history``` file due to absence of ```HOME``` environment variable on Windows platform. This prevented usage of command history altogether, which can be just always kept in-memory when ```redis-cli``` runs on a TTY. I suppose chances are rather low that ```HOME``` variable is not found on *nix systems, but it's possible ;)\r\n\r\nOriginal commit message:\r\n- when redis-cli is running on a TTY - always enable command history buffering, regardless if history file path can be successfully determined",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2017-12-13T06:34:17Z",
        "closed_at": "2018-01-15T12:17:16Z",
        "merged_at": null,
        "body": "Currently redis set funtions (etc: incrbyfloat) treat the key as\r\nexpriration(-2) when the expired time is 0. So ttl must return\r\n-2 in this case to consistent with document (return -2 when expired)",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-12-12T08:31:11Z",
        "closed_at": "2017-12-14T10:41:43Z",
        "merged_at": null,
        "body": "fixed #4293 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-12-12T00:42:03Z",
        "closed_at": "2017-12-14T11:21:33Z",
        "merged_at": "2017-12-14T11:21:33Z",
        "body": "This commit aims to fix #4527 by making sure that enough characters are left before incrementing `pattern` and trying to index `pattern[0]`. The same case was already handled here:\r\nhttps://github.com/antirez/redis/blob/522760fac79536eb68dc5fc70e9166f689eb76dc/src/util.c#L136-L140",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 18,
        "changed_files": 1,
        "created_at": "2017-12-10T15:53:07Z",
        "closed_at": "2018-11-06T17:13:18Z",
        "merged_at": "2018-11-06T17:13:18Z",
        "body": "@antirez naturally, I totally forgot about the reason I started the previous PRs. This is sort of tied to https://github.com/antirez/redis-doc/pull/851 (bump).",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2017-12-10T15:41:22Z",
        "closed_at": "2018-01-28T15:18:39Z",
        "merged_at": null,
        "body": "sh should be check at first, otherwise, memset may be failed.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-12-08T09:29:50Z",
        "closed_at": "2020-08-11T19:01:16Z",
        "merged_at": "2020-08-11T19:01:15Z",
        "body": "Run test directly without make first cause such errors:\r\n    \"src/redis-benchmark\": no such file or directory\r\n    \"src/redis-cli\": no such file or directory\r\nAdd dependencies in Makefile for test maybe more reasonable.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2017-12-08T08:18:34Z",
        "closed_at": "2018-06-08T10:45:11Z",
        "merged_at": "2018-06-08T10:45:11Z",
        "body": "Another int problem found in zset ... @antirez \r\n\r\nBTW, I think we fall into the trap of implicit cast, also happens when flush aof in PR #4498 , so we should check all the type carefully. \r\n\r\nAnd I think we can get more detail memory analysis in issue #4504",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-12-07T08:13:29Z",
        "closed_at": "2018-11-12T03:03:54Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2017-12-05T09:43:47Z",
        "closed_at": "2017-12-05T14:28:09Z",
        "merged_at": "2017-12-05T14:28:09Z",
        "body": "Another int problem found @antirez ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2017-12-05T06:46:09Z",
        "closed_at": "2017-12-05T14:47:05Z",
        "merged_at": "2017-12-05T14:47:05Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 91,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-12-04T15:50:23Z",
        "closed_at": "2021-09-09T19:48:15Z",
        "merged_at": null,
        "body": "Logic copied from #3409",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2017-12-04T11:50:36Z",
        "closed_at": "2017-12-04T16:25:34Z",
        "merged_at": "2017-12-04T16:25:34Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 29,
        "changed_files": 3,
        "created_at": "2017-12-04T07:31:40Z",
        "closed_at": "2017-12-04T17:20:11Z",
        "merged_at": null,
        "body": "In function `luaCreateFunction`, `dictFind` needs a `sds` but a `char*` is given.\r\n\r\n```\r\nint luaCreateFunction(client *c, lua_State *lua, char *funcname, robj *body, int allow_dup) {\r\n    sds funcdef = sdsempty();\r\n    char fname[43];\r\n\r\n    if (funcname == NULL) {\r\n        fname[0] = 'f';\r\n        fname[1] = '_';\r\n        sha1hex(fname+2,body->ptr,sdslen(body->ptr));\r\n        funcname = fname;\r\n    }\r\n\r\n    if (allow_dup && dictFind(server.lua_scripts,funcname+2) != NULL)\r\n        return C_OK;\r\n```\r\n\r\nAnd I think we did too much work about fix the lua problem, just fix it as `script load` is the simplest way.\r\n\r\nThis a critical bug, please check it @antirez \r\n",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2017-12-04T05:06:27Z",
        "closed_at": "2018-06-28T10:15:05Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-11-30T15:05:38Z",
        "closed_at": "2017-12-14T11:15:46Z",
        "merged_at": "2017-12-14T11:15:46Z",
        "body": "",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2017-11-30T10:13:19Z",
        "closed_at": "2017-12-05T14:51:16Z",
        "merged_at": "2017-12-05T14:51:16Z",
        "body": "this PR is a new version based on #4210",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 20,
        "changed_files": 6,
        "created_at": "2017-11-30T03:36:40Z",
        "closed_at": "2017-12-05T14:38:13Z",
        "merged_at": null,
        "body": "This PR fix the problem in issue #4493",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 862,
        "deletions": 265,
        "changed_files": 20,
        "created_at": "2017-11-29T08:27:07Z",
        "closed_at": "2019-07-01T14:15:36Z",
        "merged_at": null,
        "body": "Slave won't store rdb to file when syncing, plus some me other related fixes\r\n\r\nThe implementation of the diskless replication was currently diskless only on the master side.\r\nThe slave side still stores the received rdb file to the disk before loading it back in and parsing it.\r\n\r\nuser can chose between these options: disabled, on-empty-db, flushdb, swapdb.\r\n\r\n### other changes:\r\n\r\n1)\r\nDon't save rdb / aof file when we're a slave that is not synced (sync failed and dataset is empty),\r\nso that we don't override an existing one and end up loosing data on failover.\r\n\r\n2)\r\nloadAppendOnlyFile (loadDataFromDisk) would have exit() if the file doesn't exist, but in practice that would never happen since the file was always already created in initServer before that check (i.e. the exit is a dead code).\r\nInstead: don't create an empty aof file on startup (before reading it) and only create it when we start writing to it.\r\nthis allows us to distinguish between success to load an empty file and a failure to load a non-existing file.\r\n\r\nHowever, currently we don't use the above since the process startup should succeed even if the file doesn't exist (i.e. first server startup).\r\nmaybe we need to add another config called \"preload-file\" or \"load-on-startup\" or \"abort-when-load-fails\", so that whoever starts the process can tell it that it expects it to come from persistence, and fail if it can't rather than coming up empty.\r\n\r\n3)\r\nDistinguish between aof configuration and aof state, so that we can re-enable aof only when sync eventually succeeds (and not when exiting from readSyncBulkPayload after a failed attempt).\r\ni.e. if there was a failure to load the rdb from the master, you don't want to start an AOFRW just yet (and lose the previous one you had), you rather keep the aof disabled until another replication attempt succeeds.\r\nAlso note that a CONFIG GET and INFO command during rdb loading would have lied about the configuration, so it's better to distinguish between the configuration, and the state anyway.\r\n\r\n4)\r\nSLAVEOF NO ONE, will have an argument to succeed only if the slave is in sync (a specific offset can be provided)\r\nthis can assist to prevent a race in which someone monitors the slave, concludes that it's healthy and decides to promote it to a master, but then just before it happens, the slave disconnects from the master and decides to full sync and wipe the data in ram.\r\n\r\n5)\r\nWhen loading rdb from the network, don't kill the server on short read (that can be a network error), instead the rdb loading should fail, and another replication attempt should be made. if we're using the SWAPDB approach, this is critical!\r\n\r\n6)\r\nFix rdb check when performed on preamble AOF (it would have used the wrong file name)\r\n\r\n### fixes to diskless master\r\n- Redis kept streaming RDB to a disconnected slave\r\n- in diskless replication - master not notifing the slave that rdb transfer\r\nterminated on error, and lets slave wait for replication timeout\r\n- when starting a replication for a certain mincapa, we must take care to\r\nexclude slaves that didn't declare that capa, otherwise they may get something\r\nthat they can't handle\r\n\r\n\r\n### tests:\r\n- add test for not saving on exit for unsynced slave\r\n- replication tests for diskless slave and diskless master\r\n- other replication tests improvements (not related to diskless slave)\r\n- test database recovery when diskless sync fails",
        "comments": 42
    },
    {
        "merged": true,
        "additions": 176,
        "deletions": 91,
        "changed_files": 10,
        "created_at": "2017-11-28T19:30:54Z",
        "closed_at": "2017-12-06T11:08:00Z",
        "merged_at": "2017-12-06T11:08:46Z",
        "body": "This adds a new `addReplyHelp` helper that's used by commands\r\nwhen returning a help text. The following commands have been\r\nhelped and sorted: DEBUG, OBJECT, COMMAND, PUBSUB, SCRIPT, \r\nSLOWLOG, CLUSTER and CONFIG.\r\n\r\nOnce merged, the relevant PR to redis-doc needs to be made.",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2017-11-28T18:42:02Z",
        "closed_at": "2017-12-05T14:29:42Z",
        "merged_at": "2017-12-05T14:29:42Z",
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-11-28T18:41:23Z",
        "closed_at": "2021-06-30T03:18:29Z",
        "merged_at": null,
        "body": "The current behavior returns the entire slowlog and is not documented. This breaks backward compatability, so monitoring scripts relying on it may malfunction.\r\n\r\nRisk: low :)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-11-28T11:36:08Z",
        "closed_at": "2021-03-14T16:09:09Z",
        "merged_at": null,
        "body": "In one line, have write \"anitrez/redis\".",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-11-28T09:39:26Z",
        "closed_at": "2017-11-30T12:50:11Z",
        "merged_at": null,
        "body": "replication script cache should also be flushed\r\nafter a partial resynchronization,\r\nbecause slave may lose the cached scripts after a restart and\r\nwhen it reconnects with master, unmatched evalsha command would\r\nlead to inconsistent with master.",
        "comments": 36
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-11-28T04:49:15Z",
        "closed_at": "2017-12-03T15:51:46Z",
        "merged_at": null,
        "body": "I think these two infos are useful, and maybe more informations we should show in `info` command.\r\n\r\nBTW, why we take `overhead` memory into `bytes_per_key`?\r\n```\r\n    /* Metrics computed after subtracting the startup memory from\r\n     * the total memory. */\r\n    size_t net_usage = 1;\r\n    if (zmalloc_used > mh->startup_allocated)\r\n        net_usage = zmalloc_used - mh->startup_allocated;\r\n    mh->dataset_perc = (float)mh->dataset*100/net_usage;\r\n    mh->bytes_per_key = mh->total_keys ? (net_usage / mh->total_keys) : 0;\r\n```\r\nI think just take `dataset` into account is enough and evident.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-11-28T00:22:42Z",
        "closed_at": "2018-10-02T00:25:03Z",
        "merged_at": null,
        "body": "The rax documentation states that the raxCompare API takes in operator arguments in the format of \">=\" and \"<=\", but the actual implementation in code does it in \"=>\" and \"=<\" today. See https://github.com/antirez/rax",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 302,
        "deletions": 36,
        "changed_files": 5,
        "created_at": "2017-11-27T21:35:27Z",
        "closed_at": "2018-02-15T20:36:32Z",
        "merged_at": "2018-02-15T20:36:32Z",
        "body": "This PR implements a mechanism for keyspace notifications in modules. \r\nThe implementation details and API are documented inside, but here is the gist of it:\r\n\r\n1. The module subscribes to notifications based on a mask of event types (identical to what core redis uses).\r\n2. When notifying modules, the dispatchers selects the subscribers by flags.\r\n3. It is the subscriber's job to filter further by key name, actual event, etc.\r\n4. Notifications aren't reentrant - i.e. if you subscribe to SET, and you set something from within the notification callback, this will not trigger the callback again (that would crash redis). \r\n5. However subscriber A can call a command that will trigger subscriber B (but B cannot trigger A back). \r\n6. Subscriptions work without any configuration - the server only dispatches events to modules if there are actualy subsribers. Checking if there are doesn't reduce performance in a measurable way (profiled to take 0.1% CPU time during purely write loads.\r\n\r\nThe test module now includes an example of a working notification subscriber. \r\n",
        "comments": 34
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-11-27T19:58:45Z",
        "closed_at": "2020-09-09T16:26:38Z",
        "merged_at": null,
        "body": "As per issue #4476, it wasn't immediately obvious another repository existed, so this PR points to that repo in the contributing guidelines.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2017-11-24T18:03:44Z",
        "closed_at": "2017-11-27T11:41:02Z",
        "merged_at": "2017-11-27T11:41:02Z",
        "body": "This bundles two commits:\r\n\r\n1. The `freq` subcommand shouldn't be called with `noeviction`\r\n2. A `help` subcommand\r\n\r\nIf and when merged, the relevant page at redis-io needs to be added with the new subcommand.",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2017-11-23T14:49:22Z",
        "closed_at": "2017-11-24T07:59:24Z",
        "merged_at": "2017-11-24T07:59:24Z",
        "body": "\u2026even if the string has more data.\r\n\r\ngetLongLongFromObject calls string2ll which has this line:\r\n/* Return if not all bytes were used. */\r\nso if you pass an sds with 3 characters \"1\\01\" it will fail.\r\n\r\nbut getLongDoubleFromObject calls strtold, and considers it ok if eptr[0]==`\\0`\r\ni.e. if the end of the string found by strtold ends with null terminator\r\n\r\n127.0.0.1:6379> set a 1\r\nOK\r\n127.0.0.1:6379> setrange a 2 2\r\n(integer) 3\r\n127.0.0.1:6379> get a\r\n\"1\\x002\"\r\n127.0.0.1:6379> incrbyfloat a 2\r\n\"3\"\r\n127.0.0.1:6379> get a\r\n\"3\"",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2017-11-22T20:04:08Z",
        "closed_at": "2017-11-23T12:38:44Z",
        "merged_at": "2017-11-23T12:38:44Z",
        "body": "For example:\r\n1. A module command called within a MULTI section.\r\n2. A Lua script with replicate_commands() called within a MULTI section.\r\n3. A module command called from a Lua script in the above context.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-11-22T15:29:29Z",
        "closed_at": "2021-06-28T13:39:01Z",
        "merged_at": null,
        "body": "`a` is correct in this usage.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-11-22T13:13:06Z",
        "closed_at": "2017-11-22T15:29:19Z",
        "merged_at": null,
        "body": "This happens when replicate_commands() is used when Lua is executed\r\ninside `MULTI`, e.g.:\r\n\r\n```\r\nMULTI\r\nEVAL \"redis.replicate_commands(); redis.call('SET','a','1');\" 0\r\nEXEC\r\n```\r\n\r\nThe above will result with nested `MULTI/EXEC`, which may lead to invalid\r\ndata in AOF/Slaves.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 36,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2017-11-21T07:25:00Z",
        "closed_at": "2021-04-27T04:15:51Z",
        "merged_at": null,
        "body": "Without the option:\r\n\r\n> 0.00% <= 6 milliseconds\r\n> 0.05% <= 7 milliseconds\r\n> 19.50% <= 8 milliseconds\r\n> 90.96% <= 9 milliseconds\r\n> 99.57% <= 10 milliseconds\r\n> 99.91% <= 11 milliseconds\r\n> 99.92% <= 14 milliseconds\r\n> 99.94% <= 15 milliseconds\r\n> 99.97% <= 16 milliseconds\r\n> 99.97% <= 18 milliseconds\r\n> 99.98% <= 19 milliseconds\r\n> 100.00% <= 19 milliseconds\r\n> 1108513.00 requests per second\r\n\r\nWith the option:\r\n\r\n> 0% <= 564 microseconds\r\n> 10% <= 7870 microseconds\r\n> 20% <= 8091 microseconds\r\n> :\r\n> 90% <= 9063 microseconds\r\n> 91% <= 9087 microseconds\r\n> 92% <= 9113 microseconds\r\n> :\r\n> 99% <= 9784 microseconds\r\n> 100% <= 21022 microseconds\r\n> 1099089.88 requests per second\r\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2017-11-20T13:13:29Z",
        "closed_at": "2017-11-24T17:21:36Z",
        "merged_at": "2017-11-24T17:21:36Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2017-11-19T16:25:36Z",
        "closed_at": "2017-11-28T16:23:49Z",
        "merged_at": "2017-11-28T16:23:49Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-11-17T13:36:58Z",
        "closed_at": "2017-11-21T08:50:23Z",
        "merged_at": null,
        "body": "see #4446",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2017-11-15T12:34:12Z",
        "closed_at": "2021-11-10T08:48:46Z",
        "merged_at": null,
        "body": "This PR adds the `REDISMODULE_CTX_FLAGS_AOF_LOADING` to the module context flags, when the client running a module command is redis itself loading an AOF file. See #3994 ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 29673,
        "deletions": 12285,
        "changed_files": 229,
        "created_at": "2017-11-14T16:40:17Z",
        "closed_at": "2017-11-14T17:57:46Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2017-11-07T23:24:04Z",
        "closed_at": "2017-12-01T09:31:41Z",
        "merged_at": null,
        "body": "It also updates comments to match stream id format.\r\n\r\n@antirez, this supersedes #4351. As discussed there, I'm using `sep` instead of the actual separator name.\r\n\r\nHTH, thanks.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2017-11-06T22:37:53Z",
        "closed_at": "2021-10-28T11:15:09Z",
        "merged_at": null,
        "body": "Solving issue #4422 as suggested by @badboy ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2017-11-06T08:34:29Z",
        "closed_at": "2018-09-03T16:31:03Z",
        "merged_at": "2018-09-03T16:31:03Z",
        "body": "hi, @antirez \r\n\r\nClient may unblocked more than once in the function `clientsArePaused()`, when we apply `BLPOP`, `LPUSH`, `CLIENT PAUSE` successively, and then the stack is:\r\n\r\n```\r\naeProcessEvents()\r\n|----rfileProc(readQueryFromClient)\r\n        |----processInputBuffer()\r\n                |----processCommand() client A: BLPOP\r\n                |----processCommand() client B: LPUSH, then client A will be unblocked\r\n                |----processCommand() client C: CLIENT PAUSE\r\n|----processTimeEvents()\r\n        |----serverCron\r\n                |----updateCachedTime()\r\n                |----clientsArePaused() OK, now we will put all clients including unblocked client A\r\n                                        in unblocked clients queue, so client A will be unblocked twice.\r\n|----beforeSleep()\r\n        |----processUnblockedClients() Here client A will be processed twice.\r\n```\r\n\r\nBecause in function `serverCron()` we call `clientsArePaused` after `updateCachedTime`, so that may let redis reach `server.clients_pause_end_time` and unblock the `BLPOP` client twice in one eventLoop.\r\n\r\nMoreover, now client MASTER maybe unblocked in Lua timedout context in #5297, we should consider it seriously.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-11-02T06:13:40Z",
        "closed_at": "2021-07-15T22:42:23Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 50,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2017-11-01T09:35:21Z",
        "closed_at": "2017-11-24T09:56:19Z",
        "merged_at": "2017-11-24T09:56:19Z",
        "body": "This PR is a bugfix for issue #4407 .\r\n\r\n1. Safe free backlog when reach the time limit \r\n\r\n    When we free the backlog, we should use a new\r\nreplication ID and clear the ID2. Since without\r\nbacklog we can not increment master_repl_offset\r\neven do write commands, that may lead to inconsistency\r\nwhen we try to connect a \"slave-before\" master\r\n(if this master is our slave before, our replid\r\nequals the master's replid2). As the master have our\r\nhistory, so we can match the master's replid2 and\r\nsecond_replid_offset, that make partial sync work,\r\nbut the data is inconsistent.\r\n\r\n2. Fix the missing of rdbSaveInfo for BGSAVE\r\n\r\n3. Clarify the scenario when repl_stream_db can be -1\r\n\r\n4. Make `repl_stream_db` never be -1\r\n\r\n    \r\n    It means that after this change all the replication\r\ninfo in RDB is valid, and it can distinguish us from\r\nthe older version.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-10-25T13:20:26Z",
        "closed_at": "2020-09-09T16:26:41Z",
        "merged_at": null,
        "body": "Quick fix a typo on the redis-cli.c file.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-10-24T21:58:11Z",
        "closed_at": "2019-08-03T04:53:04Z",
        "merged_at": null,
        "body": "Fix for 2 small typos I noticed in the config file \ud83d\ude42 ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-10-24T04:58:47Z",
        "closed_at": "2021-06-28T13:39:57Z",
        "merged_at": null,
        "body": "return NULL if sh== NULL before detected init",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 62,
        "deletions": 62,
        "changed_files": 24,
        "created_at": "2017-10-24T03:58:21Z",
        "closed_at": "2018-06-01T15:24:17Z",
        "merged_at": null,
        "body": "I noticed a couple typos in the code comments. I ran all the files in ./src/ through a spellchecker and tried to fix all the ones I knew were incorrect.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 188,
        "deletions": 29,
        "changed_files": 6,
        "created_at": "2017-10-24T01:39:32Z",
        "closed_at": "2017-11-29T16:16:50Z",
        "merged_at": null,
        "body": "1. We have added a new LFU mode for `maxmemory-policy`, but forgot `config get` and `config rewrite`.\r\n\r\n    And I think `int` is enough for the LFU factors.\r\n\r\n2. Another commit about LFU: do some changes about LFU to find hotkeys\r\n\r\n    Firstly, use access time to replace the decreas time of LFU.\r\n    For function LFUDecrAndReturn,\r\n    it should only try to get decremented counter,\r\n    not update the LFU fields, we will update it in an explicit way.\r\n    And we will times halve the counter according to the times of\r\n    elapsed time than server.lfu_decay_time.\r\n    Everytime a key is accessed, we should update the LFU\r\n    including update access time, and increment the counter after\r\n    call function LFUDecrAndReturn.\r\n    If a key is overwritten, the LFU should be also updated.\r\n    Then we can use `OBJECT freq` command to get a key's frequence,\r\n    and LFUDecrAndReturn should be called in `OBJECT freq` command\r\n    in case of the key has not been accessed for a long time,\r\n    because we update the access time only when the key is read or\r\n    overwritten.",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-10-22T03:23:01Z",
        "closed_at": "2019-10-02T14:36:40Z",
        "merged_at": null,
        "body": "in order to make builds reproducible.\nSee https://reproducible-builds.org/ for why this is good.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-10-19T06:46:23Z",
        "closed_at": "2018-03-22T16:09:53Z",
        "merged_at": null,
        "body": "adding current_timestamp in info command.\r\nfor monitoring, if we add current_timestamp we can sort info result with current_timestamp.\r\n\r\nwhy not used uptime.\r\nwhen redis server is down, redis uptime will be set as 0, so sometimes it is hard to sort in some time range. ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 5,
        "changed_files": 5,
        "created_at": "2017-10-18T13:22:32Z",
        "closed_at": "2021-09-29T11:44:00Z",
        "merged_at": null,
        "body": "The vanilla version use listSearchKey(server.clients,c) to find a client when free a client, it costs too much cpu cycle and has bad performance, in the new code we use client_list_node to save the node of  the double linked list, so we can remove the node from the list immediately. the complexity of algorithm change from o(N) to o(1). it has 30 percent performance improvement in short connection benchmark test.\r\n\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-10-15T03:33:15Z",
        "closed_at": "2017-10-15T17:59:31Z",
        "merged_at": null,
        "body": "issue #4371 The event library has a thread bug\r\nWhen thread 1 add file event to thread 2 and thread 1 run on line 147, while thread 2 is calling file event (rfileProc or wfileProc), but the variable rfileProc, wfileProc and clientData of fe is old version, is from last registrar, not current, then thread 2 would call wrong callback funtion with wrong clientData. At last, the program may crash.",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2017-10-14T19:47:16Z",
        "closed_at": "2018-03-02T19:58:47Z",
        "merged_at": null,
        "body": "succesfully -> successfully\r\ndissapeared -> disappeared",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2017-10-13T17:22:09Z",
        "closed_at": "2017-11-28T16:33:24Z",
        "merged_at": "2017-11-28T16:33:24Z",
        "body": "There is a file descriptor leak and one place where and errors is not caught.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-10-13T07:53:15Z",
        "closed_at": "2021-10-17T15:48:15Z",
        "merged_at": "2021-10-17T15:48:15Z",
        "body": "[src/bitops.c:512] -> [src/bitops.c:507]: (warning) Either the condition 'if(o&&o->encoding==1)' is redundant or there is possible null pointer dereference: o.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 184,
        "deletions": 24,
        "changed_files": 6,
        "created_at": "2017-10-13T03:36:09Z",
        "closed_at": "2017-10-24T01:34:48Z",
        "merged_at": null,
        "body": "1. We have added a new LFU mode for `maxmemory-policy`, but forgot `config get` and `config rewrite`.\r\n\r\n    And I think `int` is enough for the LFU factors.\r\n\r\n2. Another commit about LFU: do some changes about LFU to find hotkeys\r\n\r\n    Firstly, use access time to replace the decreas time of LFU.\r\n    For function LFUDecrAndReturn,\r\n    it should only try to decrement the counter,\r\n    not update the access time, we will update it in an explicit way.\r\n    And we will times halve the counter according to the times of\r\n    elapsed time than server.lfu_decay_time.\r\n    Everytime a key is accessed, we should update the LFU\r\n    including update access time, and increment the counter after\r\n    call function LFUDecrAndReturn.\r\n    If a key is overwritten, the LFU should be also updated.\r\n    Then we can use `OBJECT freq` command to get a key's frequence,\r\n    and LFUDecrAndReturn should be called in `OBJECT freq` command\r\n    in case of the key has not been accessed for a long time,\r\n    because we update the access time only when the key is read or\r\n    overwritten.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-10-09T08:25:51Z",
        "closed_at": "2017-11-08T20:45:52Z",
        "merged_at": null,
        "body": "Provide a clear error message in the event of a bad slot number\r\nfrom the redis cluster config file, instead of continuing on and\r\npotentially crashing shortly thereafter when the migrating_slots\r\narrays are written to.\r\n\r\nResolves https://github.com/antirez/redis/issues/4278",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2017-10-08T09:26:58Z",
        "closed_at": "2018-12-26T09:46:33Z",
        "merged_at": null,
        "body": "Before Change:\r\nThe boundary conditions to check for North and South poles are checked twice in geohashEncode function of geohash.c. \r\n\r\nAfter Change:\r\nI've replaced the check having hard coded constants with the lat_range &  lon_range variables. These variables are set  to the same boundary constants by geohashGetCoordRange function.\r\n\r\nImpact:\r\nNo external impact as only the duplicated condition is removed. \r\nHow? The lat_range and lon_range variables are always set before calling geohashEncode in geohashEncodeType by calling geohashGetCoordRange. \r\nWhy? The dependency of calling geohashGetCoordRange before geohashEncode always has to be there because the same lat_range & lon_range variables are used to calculate offsets later inside geohashEncode.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-10-06T14:04:34Z",
        "closed_at": "2017-10-06T19:54:21Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-10-06T14:02:04Z",
        "closed_at": "2020-09-07T19:50:27Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2017-10-03T10:36:59Z",
        "closed_at": "2020-09-09T16:26:45Z",
        "merged_at": null,
        "body": "Although it might be a trivial PR, I think keeping comments precise is as important as code.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2688,
        "deletions": 248,
        "changed_files": 21,
        "created_at": "2017-10-03T10:22:27Z",
        "closed_at": "2017-11-07T23:25:36Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-09-30T04:38:54Z",
        "closed_at": "2020-09-09T16:26:48Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 47,
        "deletions": 23,
        "changed_files": 2,
        "created_at": "2017-09-28T14:08:45Z",
        "closed_at": "2017-11-24T08:37:07Z",
        "merged_at": "2017-11-24T08:37:07Z",
        "body": "See #4343 ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2017-09-28T09:09:28Z",
        "closed_at": "2020-09-09T12:28:39Z",
        "merged_at": "2020-09-09T12:28:39Z",
        "body": "According to the [code](https://github.com/antirez/redis/blob/unstable/src/quicklist.c#L808), when `'after'==0`,  the returned node will have elements [0, OFFSET-1], and the input node keeps elements [OFFSET, END].",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2017-09-28T07:20:54Z",
        "closed_at": "2020-08-11T07:57:21Z",
        "merged_at": null,
        "body": "See #4331 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 185,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2017-09-27T09:00:46Z",
        "closed_at": "2017-09-28T12:40:56Z",
        "merged_at": "2017-09-28T12:40:56Z",
        "body": "This PR adds support for module context flags as described in #4328 \r\n\r\nThis allows for modules to properly handle the case of blocking commands from multi or lua by checking the context flags. ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2385,
        "deletions": 248,
        "changed_files": 21,
        "created_at": "2017-09-27T08:31:44Z",
        "closed_at": "2017-09-27T10:40:10Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2017-09-24T13:19:45Z",
        "closed_at": "2017-09-28T07:18:04Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2017-09-23T07:36:00Z",
        "closed_at": "2017-09-23T11:21:29Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2017-09-22T03:28:50Z",
        "closed_at": "2021-11-01T09:09:29Z",
        "merged_at": "2021-11-01T09:09:29Z",
        "body": "These definitions already exist in server.h.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-09-21T07:22:02Z",
        "closed_at": "2017-09-21T10:32:02Z",
        "merged_at": "2017-09-21T10:32:02Z",
        "body": "See #4323.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 9,
        "changed_files": 4,
        "created_at": "2017-09-20T05:58:33Z",
        "closed_at": "2017-09-20T15:11:11Z",
        "merged_at": null,
        "body": "This commit is a reinforcement of commit c1c99e9 attempts to fix issue #4316.\r\n\r\n1. Replication information can be stored when the RDB file is\r\ngenerated by a mater using server.slaveseldb when server.repl_backlog\r\nis not NULL, or set repl_stream_db be -1. That's safe, because\r\nNULL server.repl_backlog will trigger full synchronization,\r\nthen master will send SELECT command to replicaiton stream.\r\n2. Only do rdbSave* when rsiptr is not NULL,\r\nif we do rdbSave* without rdbSaveInfo, slave will miss repl-stream-db.\r\n3. Save the replication informations also in the case of\r\nSAVE command, FLUSHALL command and DEBUG reload.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 9,
        "changed_files": 4,
        "created_at": "2017-09-19T12:00:46Z",
        "closed_at": "2017-09-20T15:10:42Z",
        "merged_at": null,
        "body": "only slave can load replication info from RDB file\r\nand also load the repl-stream-db\r\n\r\nsee issue #4316 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-09-17T04:26:36Z",
        "closed_at": "2017-09-18T10:03:41Z",
        "merged_at": "2017-09-18T10:03:41Z",
        "body": "when SHUTDOWN command is recived it is possible that some of the recent\r\ncommand were not yet flushed from the AOF buffer, and the server\r\nexperiences data loss at shutdown.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-09-14T01:46:57Z",
        "closed_at": "2020-09-09T16:26:51Z",
        "merged_at": null,
        "body": "The value of OBJ_ENCODING_EMBSTR_SIZE_LIMIT is 44 now instead of 39.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2017-09-12T07:08:53Z",
        "closed_at": "2020-09-09T16:26:54Z",
        "merged_at": null,
        "body": "According to the definition of ZIP_BIG_PREVLEN and other related code,\r\nthe guard of single byte <prevlen> should be 254 instead of 255.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-09-12T03:53:33Z",
        "closed_at": "2020-12-08T21:52:31Z",
        "merged_at": null,
        "body": "add module unload hook in function moduleUnload",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 38481,
        "deletions": 8897,
        "changed_files": 339,
        "created_at": "2017-09-08T16:05:15Z",
        "closed_at": "2017-11-28T16:40:44Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 46946,
        "deletions": 16189,
        "changed_files": 328,
        "created_at": "2017-09-07T03:47:46Z",
        "closed_at": "2017-09-28T21:30:17Z",
        "merged_at": null,
        "body": "      while(c->multibulklen) {\r\n         /* Read bulk length if unknown */\r\n           if (c->bulklen == -1) {\r\n                newline = strchr(c->querybuf+pos,'\\r');\r\n            if (newline == NULL) {\r\n                if (sdslen(c->querybuf) > PROTO_INLINE_MAX_SIZE) {\r\n                    addReplyError(c,\r\n                        \"Protocol error: too big bulk count string\");\r\n                       setProtocolError(\"too big bulk count string\",c,0);\r\n                    return C_ERR;\r\n                }\r\n                break;\r\n            }\r\n\r\n            /* Buffer should also contain \\n */\r\n            if (newline-(c->querybuf) > ((signed)sdslen(c->querybuf)-2))\r\n                break;\r\n\r\n            if (c->querybuf[pos] != '$') {\r\n                addReplyErrorFormat(c,\r\n                    \"Protocol error: expected '$', got '%c'\",\r\n                    c->querybuf[pos]);\r\n                setProtocolError(\"expected $ but got something else\",c,pos);\r\n                return C_ERR;\r\n            }\r\n\r\n            ok = string2ll(c->querybuf+pos+1,newline-(c->querybuf+pos+1),&ll);\r\n            if (!ok || ll < 0 || ll > 512*1024*1024) {\r\n                addReplyError(c,\"Protocol error: invalid bulk length\");\r\n                setProtocolError(\"invalid bulk length\",c,pos);\r\n                return C_ERR;\r\n            }\r\n\r\n            pos += newline-(c->querybuf+pos)+2;\r\n            if (ll >= PROTO_MBULK_BIG_ARG) {\r\n                size_t qblen;\r\n\r\n                /* If we are going to read a large object from network\r\n                 * try to make it likely that it will start at c->querybuf\r\n                 * boundary so that we can optimize object creation\r\n                 * avoiding a large copy of data. */\r\n                sdsrange(c->querybuf,pos,-1);\r\n                pos = 0;\r\n                qblen = sdslen(c->querybuf);\r\n                /* Hint the sds library about the amount of bytes this string is\r\n                 * going to contain. */\r\n                if (qblen < (size_t)ll+2)\r\n                    c->querybuf = sdsMakeRoomFor(c->querybuf,ll+2-qblen);\r\n            }\r\n            c->bulklen = ll;\r\n        }\r\n\r\n        /* Read bulk argument */\r\n        if (sdslen(c->querybuf)-pos < (unsigned)(c->bulklen+2)) {\r\n            /* Not enough data (+2 == trailing \\r\\n) */\r\n            break;\r\n        } else {\r\n            /* Optimization: if the buffer contains JUST our bulk element\r\n             * instead of creating a new object by *copying* the sds we\r\n             * just use the current sds string. */\r\n            if (pos == 0 &&\r\n                c->bulklen >= PROTO_MBULK_BIG_ARG &&\r\n                (signed) sdslen(c->querybuf) == c->bulklen+2)\r\n            {\r\n                c->argv[c->argc++] = createObject(OBJ_STRING,c->querybuf);\r\n                sdsIncrLen(c->querybuf,-2); /* remove CRLF */\r\n                /* Assume that if we saw a fat argument we'll see another one\r\n                 * likely... */\r\n                c->querybuf = sdsnewlen(NULL,c->bulklen+2);\r\n                sdsclear(c->querybuf);\r\n                pos = 0;\r\n            } else {\r\n                c->argv[c->argc++] =\r\n                    createStringObject(c->querybuf+pos,c->bulklen);\r\n                pos += c->bulklen+2;\r\n            }\r\n            c->bulklen = -1;\r\n            c->multibulklen--;\r\n        }\r\n    }\r\nabove code,if c->querybuf length is equal to bulk length+2  ,execute     pos += c->bulklen+2;  when enter while again, c->querybuf+pos  address out of c->querybuf \r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 29,
        "changed_files": 3,
        "created_at": "2017-09-06T13:38:29Z",
        "closed_at": "2018-09-07T13:48:14Z",
        "merged_at": null,
        "body": "Suggestion: Change GEO_LAT_MIN and GEO_LAT_MAX default values (geohash.h) to use the same range as in the Geohash standard, instead of EPSG:3785. This removes the need to decode/reencode hash values each time the geohashCommand (geo.c) is called. Need to adjusted the unit tests. Preexisting stored geo data would have to be adjusted",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 74,
        "deletions": 29,
        "changed_files": 1,
        "created_at": "2017-09-04T22:04:25Z",
        "closed_at": "2018-07-13T16:19:35Z",
        "merged_at": null,
        "body": "Use --password='secret'\r\nAdded in: info, check, create, add-node, del-node, set-timeout, reshard,\r\nrebalance and fix\r\n\r\nIssues #2866 and #3389\r\n\r\nGoogle groups: https://groups.google.com/forum/#!topic/redis-db/Z8lMxTfDct8\r\n\r\n",
        "comments": 33
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-09-04T08:28:52Z",
        "closed_at": "2017-09-18T10:07:30Z",
        "merged_at": null,
        "body": "poiting -> pointing\r\nchilden -> children",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-09-01T16:01:31Z",
        "closed_at": "2018-02-08T14:08:42Z",
        "merged_at": null,
        "body": "If the disk for the AOF of a slave is full, write(2) may return a number less\r\nthan the size of the data, then we will truncate the AOF to undo the incomplete\r\nwrite, and accumulate the data in buffer.\r\n\r\nAt last, the buffer maybe larger than 2G. From that time on, even if we make\r\nspace for the AOF, Linux always return 2147479552(2G-4K), IOW short write\r\ncontinues. We have to restart redis-server to recover.\r\n\r\nAfter applying this patch, we handle short-write as normal, and continue\r\nwriting until no more data to send or any error.\r\n\r\nThe corresponding issue is #4066.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-08-31T17:57:00Z",
        "closed_at": "2017-09-18T10:10:59Z",
        "merged_at": null,
        "body": "Fixes bug [4033](https://github.com/antirez/redis/issues/4033).",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-08-31T12:22:23Z",
        "closed_at": "2021-06-28T13:40:37Z",
        "merged_at": null,
        "body": "To prevent out of memory",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-08-31T09:34:30Z",
        "closed_at": "2017-09-19T08:36:53Z",
        "merged_at": null,
        "body": "This PR fix issue #4268 ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12053,
        "deletions": 4436,
        "changed_files": 123,
        "created_at": "2017-08-31T09:14:21Z",
        "closed_at": "2017-08-31T11:18:33Z",
        "merged_at": null,
        "body": "Sorry, wrong PR\r\nshould be deleted",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-08-31T02:22:34Z",
        "closed_at": "2018-01-24T09:55:26Z",
        "merged_at": "2018-01-24T09:55:26Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-08-29T06:47:32Z",
        "closed_at": "2018-12-11T12:02:29Z",
        "merged_at": "2018-12-11T12:02:29Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-08-24T16:24:38Z",
        "closed_at": "2017-11-28T16:42:19Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-08-24T16:23:28Z",
        "closed_at": "2017-11-28T16:42:39Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-08-21T18:24:37Z",
        "closed_at": "2017-12-01T04:38:38Z",
        "merged_at": null,
        "body": "fixes https://github.com/antirez/redis/issues/4247",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2017-08-21T14:26:19Z",
        "closed_at": "2021-04-01T05:44:58Z",
        "merged_at": "2021-04-01T05:44:57Z",
        "body": "Working with redis logging I keep forgetting the right levels and making typos when writing them. \r\n\r\nI've added macros for them so I'll have them readily available in the editor's autocomplete and avoid this problem. Would be cool if you can merge it and save me some headache :)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-08-21T05:08:56Z",
        "closed_at": "2022-02-06T11:17:38Z",
        "merged_at": "2022-02-06T11:17:38Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-08-18T10:07:39Z",
        "closed_at": "2017-10-19T12:01:00Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-08-18T04:22:24Z",
        "closed_at": "2018-07-17T16:04:56Z",
        "merged_at": "2018-07-17T16:04:56Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2017-08-17T02:30:49Z",
        "closed_at": "2017-08-17T07:41:21Z",
        "merged_at": null,
        "body": "4.0",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2017-08-16T09:53:28Z",
        "closed_at": "2017-12-26T22:53:04Z",
        "merged_at": null,
        "body": "This commit add the possibility to use OSS-fuzz a project by google that provides continuos fuzz testing.\r\n\r\nThe main page of the project is: https://github.com/google/oss-fuzz\r\n\r\nThere are some issues that had to be solved first.\r\n\r\n1. The main should be removed since the fuzz library already provide a\r\nmain.\r\n2. The testing interfaces does not fit very well a client - server architecture, so, the raw fuzz data are directly send to the redis internal, namely to client->querybuf.\r\n3. In the testing phase we do not need anymore to bind the sockets.\r\n4. We cannot call anymore os.exit(1) when the SHUTDOWN command is\r\ntested, otherwise the application will exit\r\n\r\nTo achieve all this I introduce a compile time option.\r\n\r\nAnother notable changes is in deps/Makefile.\r\n\r\nBefore it, lua was compiled down to an executable and as \"side-effect\"\r\nthe liblua.a archive was generate. I believe that we don't need the lua\r\nexecutable so I remove it in the compilation phase.\r\n\r\nFor sure there will be some rought edge to smooth, but I believe that this can be an interesting starts.\r\n\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-08-15T00:12:23Z",
        "closed_at": "2020-09-09T16:26:57Z",
        "merged_at": null,
        "body": "suceed -> succeed",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-08-14T21:51:01Z",
        "closed_at": "2020-09-09T16:27:01Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-08-14T13:41:07Z",
        "closed_at": "2017-08-14T15:34:29Z",
        "merged_at": null,
        "body": "- 2 lookupKeyWrite calls -> 1 dbExists\r\n- Because there is a lookupKeyWrite call in setKey, we don't need to call lookupKeyWrite in setGenericCommand. We just need to check whether the key exists or not.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 83,
        "deletions": 11,
        "changed_files": 3,
        "created_at": "2017-08-14T08:13:31Z",
        "closed_at": "2017-11-28T16:50:09Z",
        "merged_at": null,
        "body": "A hash gets converted to dict encoding if it's getting, depends on the configuration, more than 512 entries by default.  Iterating such a large hash table using the iteration api takes quite some cpu cycles and generated more cache misses then necessary. This merge request uses a dedicated iterate all function to traverse the whole dict to generate hgetall, hkeys and hvals response, which can reduce latency by 1 ~ 5% depends on size of the dict.\r\n\r\nThe code to generate test data and benchmark the difference [can be found here](https://gist.github.com/sunxiaoguang/f92c705bff965eb6854da1e5ffb7b4af). Please use the '[optimize_hgetall_unstable_comparaison](https://github.com/sunxiaoguang/redis/tree/optimize_hgetall_unstable_comparaison)' branch which contains commands for both the original implementation and new implementation for reference.\r\n\r\nSome test run of the benchmark program on a E5-2670 v3 server demonstrate for a 16000 fields hash table (the test h5) the new way can save couple hundreds of microseconds on average for a hgetall call. Running the same test on servers with less cache can observe some more improvements as the iterator way access data in a more scattered way therefore renders cache less efficient.\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-08-14T02:03:33Z",
        "closed_at": "2022-02-06T05:55:41Z",
        "merged_at": null,
        "body": "Add commands with docker",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 6,
        "created_at": "2017-08-13T05:26:57Z",
        "closed_at": "2018-08-29T14:18:12Z",
        "merged_at": "2018-08-29T14:18:12Z",
        "body": "",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-08-13T05:21:52Z",
        "closed_at": "2017-11-28T17:08:33Z",
        "merged_at": "2017-11-28T17:08:33Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 5,
        "changed_files": 5,
        "created_at": "2017-08-11T06:50:03Z",
        "closed_at": "2017-12-05T14:51:09Z",
        "merged_at": null,
        "body": "I also see #4204, but he forgot replication.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-08-11T06:31:27Z",
        "closed_at": "2017-09-20T15:30:48Z",
        "merged_at": null,
        "body": "when requirepass is \"\" in config file, redis will set requirepass be \"\".\r\nit is not a good idea using redis-cli to send 'auth \"\"', and we can not pass auth by telnet yet.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2017-08-10T16:19:56Z",
        "closed_at": "2020-06-03T04:34:14Z",
        "merged_at": null,
        "body": "An incredibly simple PR that does two things:\r\n\r\n* Updates the Redis URL in the ASCII art to use `https://` instead of `http://`\r\n* Updates the copyright year in `COPYING` to 2017",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2017-08-10T11:57:53Z",
        "closed_at": "2021-10-29T09:35:50Z",
        "merged_at": null,
        "body": "Signed-off-by: Bo Cai <charpty@gmail.com>",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2017-08-10T05:31:30Z",
        "closed_at": "2017-12-16T03:31:21Z",
        "merged_at": null,
        "body": "This commit contains following changes:\r\n1 Add a node pointer member in client structure\r\n\r\n2 Add a helper function in adlist to return the\r\nclient node after appending a new client object\r\n\r\n3 When about to delete the client node in list,\r\nWe use clients's node pointer instead of iterating\r\nthe whole list by listSearchKey function\r\n\r\nThis commit looks like a workaround but it actually\r\nworks in production. When redis is dealing with \r\nshort connections(connect. do stuff, disconnect),\r\nthis change could make redis perform 8% more qps. ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-08-08T09:47:34Z",
        "closed_at": "2017-11-28T17:44:11Z",
        "merged_at": "2017-11-28T17:44:11Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2017-08-08T08:51:37Z",
        "closed_at": "2020-05-27T11:53:01Z",
        "merged_at": null,
        "body": "anetTcpKeepAlive is redundant for anetKeepAlive has implemented it.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2017-08-08T08:00:42Z",
        "closed_at": "2020-09-09T16:27:04Z",
        "merged_at": null,
        "body": "servewr => server\r\nreplicationFeedSlavesFromMaster() => replicationFeedSlavesFromMasterStream()",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-08-07T19:20:35Z",
        "closed_at": "2017-12-15T16:17:34Z",
        "merged_at": null,
        "body": "Three options are listed, not two.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-08-07T16:04:45Z",
        "closed_at": "2017-08-08T02:24:35Z",
        "merged_at": null,
        "body": "",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2017-08-05T10:01:27Z",
        "closed_at": "2021-10-29T09:51:31Z",
        "merged_at": null,
        "body": "Cache the listNode pointer in server.clients list of current the client, so we can simple use it when free the client.\r\nMainly in a large number of short-connected scenarios, traversing the active clients list consumes CPU.\r\n\r\nThis is my test report, please check.\r\n[Test-Result.txt](https://github.com/antirez/redis/files/1202276/Test-Result.txt)\r\n\r\n\r\nSigned-off-by: Bo Cai <charpty@gmail.com>",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2017-08-04T13:45:16Z",
        "closed_at": "2017-11-28T17:27:18Z",
        "merged_at": null,
        "body": "In listJoin(), when list 'l' is not empty and list 'o' is empty, 'tail' point of list 'l' is NULL.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-08-03T08:29:26Z",
        "closed_at": "2017-08-03T10:18:59Z",
        "merged_at": "2017-08-03T10:18:59Z",
        "body": "add missing `fclose()` in `src/redis-cli.c : getRDB()`",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2017-08-02T11:14:01Z",
        "closed_at": "2020-12-08T21:49:25Z",
        "merged_at": null,
        "body": "This solves the issue of logging from a context that was initialized with `RedisModule_GetThreadSafeContext(NULL);` by putting a default string instead of the module name.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2017-07-31T05:24:22Z",
        "closed_at": "2017-08-03T07:55:57Z",
        "merged_at": null,
        "body": "add missing fclose() in `src/redis-cli.c : getRDB()`",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-30T10:12:13Z",
        "closed_at": "2017-11-28T17:37:46Z",
        "merged_at": null,
        "body": "Duplicate call for delete, not sure if that intended ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-28T17:04:58Z",
        "closed_at": "2017-11-28T17:39:44Z",
        "merged_at": "2017-11-28T17:39:44Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-27T13:03:29Z",
        "closed_at": "2019-05-17T02:51:04Z",
        "merged_at": null,
        "body": "in `freeMemoryIfNeeded`, when enter `cant_free`, the original implentation is to wait for object to be lazyfreed to free.   \r\nbut when finally we got enough memory, redis return `C_ERR` which can be avoided.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-27T06:28:13Z",
        "closed_at": "2017-11-28T17:40:03Z",
        "merged_at": "2017-11-28T17:40:03Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-27T00:38:05Z",
        "closed_at": "2017-11-28T17:40:43Z",
        "merged_at": "2017-11-28T17:40:43Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12031,
        "deletions": 4427,
        "changed_files": 123,
        "created_at": "2017-07-27T00:25:25Z",
        "closed_at": "2017-07-27T05:58:12Z",
        "merged_at": null,
        "body": "Should be \"loopback\" not \"lookback\".",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-26T13:54:19Z",
        "closed_at": "2020-09-09T16:27:08Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-26T13:35:29Z",
        "closed_at": "2017-11-28T17:41:28Z",
        "merged_at": "2017-11-28T17:41:28Z",
        "body": "Signed-off-by: Bo Cai <charpty@gmail.com>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-26T13:25:28Z",
        "closed_at": "2017-11-28T17:41:52Z",
        "merged_at": "2017-11-28T17:41:51Z",
        "body": "Signed-off-by: Bo Cai <charpty@gmail.com>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2017-07-26T12:11:21Z",
        "closed_at": "2017-11-28T17:43:45Z",
        "merged_at": "2017-11-28T17:43:45Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-26T11:31:27Z",
        "closed_at": "2021-06-29T03:41:00Z",
        "merged_at": null,
        "body": "Long size is platform dependent, so to avoid potential porting problems I suggest to follow a consistent approach.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2017-07-26T01:46:15Z",
        "closed_at": "2018-07-13T08:20:22Z",
        "merged_at": null,
        "body": "Fixes antirez/redis#4134.\r\n\r\nPlease let me know where to make documentation additions, and whether this is the correct branch to target.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2017-07-21T09:05:32Z",
        "closed_at": "2021-06-30T16:12:45Z",
        "merged_at": null,
        "body": "I ran the infer static analyser. It found some small issues (mostly false alarms). Here are two that I could easily fix:\r\n\r\n* Memory was allocated after which a file was checked. It leaks when the file could not be openened, fixed by checking the file before allocating memory.\r\n\tmodified:   src/config.c\r\n* Adds an fclose in the case of errors.\r\n\tmodified:   src/redis-check-rdb.c",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2017-07-19T08:50:43Z",
        "closed_at": "2017-07-20T13:37:45Z",
        "merged_at": null,
        "body": "This fixes issue #4131 ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2017-07-18T07:29:00Z",
        "closed_at": "2017-07-24T12:18:29Z",
        "merged_at": "2017-07-24T12:18:29Z",
        "body": "",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-07-15T02:38:53Z",
        "closed_at": "2017-07-24T13:18:57Z",
        "merged_at": "2017-07-24T13:18:57Z",
        "body": "I found that using 'config set auto-aof-rewrite-min-size <value>' and '<value>' can only be number which mean that I can only rewrite auto-aof-rewrite-min-size by bytes, which is not convenient.\r\n\r\n**Full Demo**\r\n```\r\n127.0.0.1:6379> config get auto-aof-rewrite-min-size\r\n1) \"auto-aof-rewrite-min-size\"\r\n2) \"67108864\"\r\n127.0.0.1:6379> config set auto-aof-rewrite-min-size 65mb\r\n(error) ERR Invalid argument '65mb' for CONFIG SET 'auto-aof-rewrite-min-size'\r\n```\r\n\r\nI fixed that by changing the code a little bit. After fixing, it behaves like this:\r\n```\r\n127.0.0.1:6379> config get auto-aof-rewrite-min-size\r\n1) \"auto-aof-rewrite-min-size\"\r\n2) \"67108864\"\r\n127.0.0.1:6379> config set auto-aof-rewrite-min-size 65mb\r\nOK\r\n127.0.0.1:6379> config get auto-aof-rewrite-min-size\r\n1) \"auto-aof-rewrite-min-size\"\r\n2) \"68157440\"\r\n```",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-14T21:54:17Z",
        "closed_at": "2017-07-24T13:19:22Z",
        "merged_at": "2017-07-24T13:19:22Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2017-07-11T05:43:30Z",
        "closed_at": "2017-07-25T09:49:00Z",
        "merged_at": null,
        "body": "Add optional unload callback to redis module to make it possible to perform\r\ncleanup steps before it's actually being unloaded. For example, stop\r\nrunning background threads.\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-07-10T18:40:26Z",
        "closed_at": "2017-07-11T17:02:46Z",
        "merged_at": null,
        "body": "Redis memory usage command reports incorrect usage for list. Steps to reproduce the issue \r\n\r\n1. Run redis server \r\n./redis-server --daemonize yes \r\n2. Create a list with 100000 elements with each item of 5 bytes. \r\nfor i in {1..100000}; do echo \"lpush key foooo\"; done | ./redis-cli \r\n./redis-cli llen key \r\n(integer) 100000 \r\n3. Check memory usage, it is reporting very high memory usage (~700MB) for the list \r\n./redis-cli memory usage key \r\n(integer) 759180081 \r\n\r\nIt is using \"total count of all entries in all ziplists\", I think it should be \"number of quicklistNodes\" \r\n\r\nAfter the fix, memory usage returned is correct (5 bytes * 100000 + overhead)\r\n./redis-cli memory usage key                                                                                                                                                          \r\n(integer) 652975",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 30,
        "changed_files": 1,
        "created_at": "2017-07-10T11:47:33Z",
        "closed_at": "2017-07-10T17:14:35Z",
        "merged_at": "2017-07-10T17:14:35Z",
        "body": "In the recent change of the module RDB format, there was a bug: The return value of rdbSaveLen was not entirely accumulated in io->bytes.\r\nThis messes up DUMP and RESTORE.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2017-07-07T10:23:32Z",
        "closed_at": "2019-05-23T06:43:01Z",
        "merged_at": null,
        "body": "Function objectComputeSize is not correct with list, set, zset and hash.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-07-07T06:12:48Z",
        "closed_at": "2020-10-21T08:23:40Z",
        "merged_at": null,
        "body": "Extend the error check to EPROTONOSUPPORT to which errno is set in\r\ncertain contexts (especially some containers) and to other error codes\r\nwhich may be reported in more situations, adopting the same list used\r\nin the ISC DHCP implementation upon socket() and bind() calls.\r\n\r\nSolves issue #3894 ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2017-07-05T14:50:12Z",
        "closed_at": "2017-07-06T12:31:09Z",
        "merged_at": "2017-07-06T12:31:09Z",
        "body": "",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-07-03T21:03:13Z",
        "closed_at": "2017-07-04T10:01:52Z",
        "merged_at": "2017-07-04T10:01:52Z",
        "body": "See #4100 ",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2017-07-03T08:54:11Z",
        "closed_at": "2018-05-24T14:16:12Z",
        "merged_at": "2018-05-24T14:16:12Z",
        "body": "Added aarch64 CFLAGS to generate stack trace.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2017-07-01T14:30:23Z",
        "closed_at": "2021-11-02T12:38:18Z",
        "merged_at": null,
        "body": "Addresses #4095. \r\n\r\nBefore this change, going to werr when fclose(fp) failed resulted in calling fclose(fp) twice and undefined behavior.\r\n\r\nMy earlier idea was to create a second goto statement after werr that skipped the fclose, but this didn't work well because serverLog needs to be called before fclose.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 79,
        "deletions": 79,
        "changed_files": 41,
        "created_at": "2017-06-30T06:24:08Z",
        "closed_at": "2020-09-09T16:27:10Z",
        "merged_at": null,
        "body": "Fix typo",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 77,
        "deletions": 3,
        "changed_files": 6,
        "created_at": "2017-06-29T15:19:45Z",
        "closed_at": "2017-06-29T19:23:51Z",
        "merged_at": null,
        "body": "Hello!\r\nMay be possible merge the port of https://github.com/antirez/redis/pull/3363 pull request into 3.2 stable branch?",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 252,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2017-06-27T08:32:17Z",
        "closed_at": "2019-11-18T05:33:58Z",
        "merged_at": null,
        "body": "Created optimized CRC computation using ARM's PMULL instruction.\r\nBenchmark shows that it is ~25X faster than generic LUT on arm64v8.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 112,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-06-26T07:11:18Z",
        "closed_at": "2017-06-27T03:41:13Z",
        "merged_at": null,
        "body": "Change-Id: If7220ec6158c72c64b9735d1477fb8013a1d8731\r\nSigned-off-by: Jun He <jun.he@arm.com>",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2017-06-23T07:33:59Z",
        "closed_at": "2019-11-19T10:00:02Z",
        "merged_at": "2019-11-19T10:00:02Z",
        "body": "This is a light-weight replace function, useful for use cases such as\r\nrealloc()ing an existing value, etc.  Using RM_ModuleTypeSetValue() in\r\nsuch cases is wasteful and complex as it attempts to delete the old\r\nvalue, call its destructor, etc.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2017-06-23T07:03:18Z",
        "closed_at": "2017-06-27T15:51:20Z",
        "merged_at": "2017-06-27T15:51:20Z",
        "body": "1. Redis command table entry for brpop command looks to be incorrect. As a result of this, we are not checking all the keys in the command for slot and not giving CROSSSLOT error correctly.\r\n2. We can get rid of a loop to check slot for all keys client is blocked on. Slot must be same for all the keys.\r\n3. dictIterator is not freed in case of redirection. \r\n\r\nCurrent behavior:\r\n127.0.0.1:8000> cluster keyslot b\r\n(integer) 3300\r\n127.0.0.1:8000> cluster keyslot f\r\n(integer) 3168\r\n127.0.0.1:8000> brpop b f 0\r\n[blocked]\r\n\r\nAfter Fix:\r\n127.0.0.1:8000> brpop b f 0\r\n(error) CROSSSLOT Keys in request don't hash to the same slot",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2017-06-21T12:02:54Z",
        "closed_at": "2017-06-22T08:46:59Z",
        "merged_at": "2017-06-22T08:46:59Z",
        "body": "Translate set command with ex/px option to set and expireat commands when updating aof.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2017-06-18T02:54:09Z",
        "closed_at": "2017-06-20T14:40:49Z",
        "merged_at": null,
        "body": "Addresses #4061. New REDISMODULE_CREATE flag added. It must be combined with REDISMODULE_WRITE to open a non-existent key, otherwise NULL is returned. E.g.: \r\n\r\n`RedisModule_OpenKey(ctx, keyname, REDISMODULE_WRITE | REDISMODULE_CREATE)`\r\n\r\nLet me know if this isn't what you think is best or if I can make any improvements!",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2017-06-16T23:10:13Z",
        "closed_at": "2017-06-20T14:41:11Z",
        "merged_at": "2017-06-20T14:41:11Z",
        "body": "Fix a few typos/adjust wording in `create-cluster` README",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-06-14T08:45:01Z",
        "closed_at": "2017-06-20T14:55:29Z",
        "merged_at": "2017-06-20T14:55:29Z",
        "body": "I think unit of slowlog duration is microseconds\u3002\r\nissue: [4054](https://github.com/antirez/redis/issues/4054)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2017-06-10T11:29:22Z",
        "closed_at": "2020-11-24T19:31:26Z",
        "merged_at": null,
        "body": "Simplest possible implementation to support a callback on unload. I could not find where to add a test.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-06-10T09:37:08Z",
        "closed_at": "2021-10-17T17:37:53Z",
        "merged_at": "2021-10-17T17:37:53Z",
        "body": "Hi, I found a small problem:\r\nthere is no need to compare the value of ep and sp\r\n```\r\n    sp = start = s;\r\n    // the only way that make ep > sp is sdslen(s) == 0\r\n    // so when ep > sp,must exist ep-sp == -1\r\n    ep = end = s+sdslen(s)-1;\r\n    while(sp <= end && strchr(cset, *sp)) sp++;\r\n    while(ep > sp && strchr(cset, *ep)) ep--;\r\n    // -1 + 1 already equals 0\r\n    len = (sp > ep) ? 0 : ((ep-sp)+1);\r\n```\r\nSo I think there is no need to determine the size of ep and sp, I am not really sure, is that right?\r\n\r\nSigned-off-by: Bo Cai <charpty@gmail.com>",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 116,
        "deletions": 73,
        "changed_files": 1,
        "created_at": "2017-06-10T09:27:30Z",
        "closed_at": "2017-06-10T22:47:47Z",
        "merged_at": null,
        "body": "",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-06-05T00:35:39Z",
        "closed_at": "2020-09-09T16:27:13Z",
        "merged_at": null,
        "body": "Fix the minorest of typos.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-06-04T16:10:25Z",
        "closed_at": "2021-12-09T23:26:17Z",
        "merged_at": null,
        "body": "the original statement:\r\n`This is not possible with X nodes and Y replicas per node.`\r\nwill make me to think:\r\nI want to create X*Y nodes\r\nin actual:\r\nI want to create X nodes only.\r\nSo I advice to change the statement to make more clear:\r\n`This is not possible with total X nodes and Y replicas per master node.`\r\n\r\nWDYT? Thanks",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-06-04T14:27:11Z",
        "closed_at": "2017-06-13T13:27:31Z",
        "merged_at": "2017-06-13T13:27:31Z",
        "body": "@antirez for review",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-06-04T14:10:28Z",
        "closed_at": "2017-06-13T13:28:24Z",
        "merged_at": "2017-06-13T13:28:24Z",
        "body": "CLUSTER_PROTO_VER is defined as 1, so the comment seems to be wrong as it is referring to version 0.\r\nMessage header is built using CLUSTER_PROTO_VER: https://github.com/antirez/redis/blob/unstable/src/cluster.c#L2204\r\n@antirez for review",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-06-03T11:01:20Z",
        "closed_at": "2018-09-11T08:44:05Z",
        "merged_at": null,
        "body": "**Issue:**\r\nThis PR is a fix for #3081\r\n\r\n**Reproduce Step:**\r\nThe following code has printed 1 instead of 0.\r\n\r\n```\r\n// file name: test_t_zset.c\r\n#include \"server.h\"\r\n#include <stdio.h>\r\n    \r\nint main()\r\n{       \r\n        zskiplist* zsl = zslCreate();\r\n        zslInsert(zsl, 2, sdsnewlen(\"test\", 4));\r\n        zslInsert(zsl, 4, sdsnewlen(\"test\", 4));\r\n        int res = zslGetRank(zsl, 3, sdsnewlen(\"test\", 4));\r\n        printf(\"%d\\n\", res);\r\n        zslFree(zsl);\r\n        return 0;\r\n} \r\n```\r\n\r\nTo compile and run this piece of code. You can:\r\n\r\n1. change [main()](https://github.com/antirez/redis/blob/unstable/src/server.c#L3646) to any other name in server.c\r\n2.  add test_t_zset.o to [REDIS_SERVER_OBJ](https://github.com/antirez/redis/blob/unstable/src/Makefile#L142).\r\n3. make -f [Makefile ](https://github.com/antirez/redis/blob/unstable/src/Makefile)\r\n4. run redis-server",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2017-06-03T04:26:21Z",
        "closed_at": "2017-06-03T06:20:52Z",
        "merged_at": null,
        "body": "The condition seems not strict enough although it works for practice and probility of hang is very low. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2017-06-01T21:29:11Z",
        "closed_at": "2017-06-23T09:39:02Z",
        "merged_at": "2017-06-23T09:39:02Z",
        "body": "Proposed fix to https://github.com/antirez/redis/issues/4027",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-05-27T07:33:26Z",
        "closed_at": "2020-09-09T16:27:16Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2017-05-25T20:40:15Z",
        "closed_at": "2018-07-07T16:39:26Z",
        "merged_at": null,
        "body": "Minor fixes",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-05-22T20:41:55Z",
        "closed_at": "2020-09-09T16:27:19Z",
        "merged_at": null,
        "body": "changed 'why' -> 'while'",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 84,
        "deletions": 86,
        "changed_files": 43,
        "created_at": "2017-05-20T08:02:27Z",
        "closed_at": "2018-07-23T06:30:39Z",
        "merged_at": null,
        "body": "This PR is part of a campaign to fix a lot of typos on github!\r\nYou can see the progress on https://github.com/fixTypos/fix_typos/\r\n\r\nhttps://github.com/client9/misspell",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 10989,
        "deletions": 1524,
        "changed_files": 109,
        "created_at": "2017-05-18T04:09:10Z",
        "closed_at": "2017-06-22T12:53:45Z",
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2017-05-17T15:08:50Z",
        "closed_at": "2022-02-08T01:14:44Z",
        "merged_at": null,
        "body": "Added new option `--dump` to `redis-check-aof` that shows line-based redis commands for human readability.\r\n\r\n#### test.aof\r\n\r\n```\r\n*2\r\n$6\r\nSELECT\r\n$1\r\n0\r\n*3\r\n$3\r\nSET\r\n$3\r\nfoo\r\n$4\r\ntest\r\n*2\r\n$3\r\nDEL\r\n$3\r\nfoo\r\n```\r\n\r\n```shell\r\n% redis-check-aof --dump test.aof\r\nSELECT 0\r\nSET foo test\r\nDEL foo\r\n```\r\n\r\nIt acts same as before when `--dump` is not given.\r\nThanks.\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-05-15T18:52:07Z",
        "closed_at": "2020-09-09T12:06:05Z",
        "merged_at": "2020-09-09T12:06:05Z",
        "body": "THP can also be set to madvise, in which case it shouldn't cause\r\nproblems for Redis.\r\n\r\nFixes #3895",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 2633,
        "deletions": 18,
        "changed_files": 13,
        "created_at": "2017-05-11T10:06:00Z",
        "closed_at": "2018-01-11T10:17:56Z",
        "merged_at": "2018-01-11T10:17:56Z",
        "body": "### What changes were proposed in this pull request?\r\n----\r\n\r\n+ Supported nonblocking data migration on branch `redis/unstable`. (Related to issue #3958).\r\n\r\n### How was this patch tested?\r\n----\r\n\r\n+ Verified by a batch of stress tests under valgrind.\r\n+ Tested by a lots of unit tests in `tests/unit/migrate-async.tcl`.\r\n    + Here is the testing results [Travis-CI](https://travis-ci.org/spinlock/redis).",
        "comments": 37
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2017-05-11T08:32:46Z",
        "closed_at": "2021-10-29T09:44:25Z",
        "merged_at": null,
        "body": "Redis-server terminate by signal such as sigint, If server is performing AOF rewrite. It will create temp file.  Then server exit but leave AOF temp file not be deleted.\r\nFix it like RDB temp file clear.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2017-05-10T14:09:24Z",
        "closed_at": "2019-03-18T17:44:24Z",
        "merged_at": null,
        "body": "Client id is accessible to modules via RedisModule_GetClientId() and should be initialised when loading data from AOF.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-05-09T15:58:58Z",
        "closed_at": "2017-05-10T14:07:52Z",
        "merged_at": null,
        "body": "Client id is accessible to modules via RedisModule_GetClientId() and should be initialised when loading data from AOF.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 48,
        "deletions": 25,
        "changed_files": 1,
        "created_at": "2017-05-09T10:51:26Z",
        "closed_at": "2017-05-09T12:58:53Z",
        "merged_at": null,
        "body": "When calling RM_UnblockClient inside module code, it is possible, at some time, it is running inside exactly the same thread running the main server loop. For this condition, calling lock on mutex is unnecessary. Even though it certainly doesn't block for this case, but it always good to save something.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 58,
        "deletions": 14,
        "changed_files": 5,
        "created_at": "2017-05-04T21:33:38Z",
        "closed_at": "2019-01-28T23:07:57Z",
        "merged_at": null,
        "body": "This PR proposes an extension to the `ZRANK` and `ZREVRANK` commands: an optional tail argument `WITHSCORE`, which changes behaviour in order to return both rank and score.\r\n\r\nPros:\r\n* Avoid common pitfall of having to call both `ZRANK`/`ZREVRANK` and `ZSCORE`\r\n* It is quite consistent with other commands that already return ranges and, optionally, scores as well (e.g. `ZRANGE)`\r\n* Fairly low overhead\r\n* Doesn't break existing interface\r\n\r\nCons:\r\n* Return type is different between scoreless and _scoreful_ invocations. \r\n\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-05-04T14:14:06Z",
        "closed_at": "2021-10-28T10:30:38Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2017-05-02T16:20:49Z",
        "closed_at": "2017-06-14T07:58:18Z",
        "merged_at": null,
        "body": "In activeExpireCycle(),variable **_timelimit_** is used as a condition of exiting the loop to limit the cpu usage.When **_elapsed_**(calculated by ustime()-start) is greater than **_timelimit_**,the method returns immediately.\r\n\r\nHowever if the system clock happens to be adjusted forward a lot during the loop,the cpu usage will become unpredictable.The probability of above case may be very small,but since redis is single-threaded,I think it is better to exit the loop as soon as possible if **_elapsed_** appears to be negative.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-05-02T15:43:20Z",
        "closed_at": "2019-10-16T12:39:56Z",
        "merged_at": null,
        "body": "Nautical Miles = Air Miles\r\nA nautical mile is a unit of measurement defined as exactly 1852 meters (about 6,076.1 feet or 1.1508 statute miles). Historically, it was defined as one minute of latitude, which is equivalent to one sixtieth of a degree of latitude.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-05-02T07:51:51Z",
        "closed_at": "2017-06-20T15:03:09Z",
        "merged_at": null,
        "body": "Redis counts more bytes than it actually wrote when dealing with pending replies, and this would make \r\n metrics of instantaneous_output_kbps and total_net_output_bytes increase very fast.\r\n\r\nIn src/networking.c writeToClient function:\r\n\r\n```\r\nwhile(clientHasPendingReplies(c)) {\r\n    //...\r\n    nwritten = write(/*...*/);\r\n    totwritten += nwritten;\r\n    //...\r\n    server.stat_net_output_bytes += totwritten;\r\n}\r\n\r\n```\r\nand totwritten is counted(and not reset) each time the loop runs.\r\n",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 29339,
        "deletions": 12224,
        "changed_files": 225,
        "created_at": "2017-05-01T03:49:39Z",
        "closed_at": "2017-05-17T15:46:47Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-04-29T00:06:57Z",
        "closed_at": "2021-10-28T06:42:52Z",
        "merged_at": null,
        "body": "",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-04-26T14:50:55Z",
        "closed_at": "2020-09-09T16:27:23Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 674,
        "deletions": 267,
        "changed_files": 16,
        "created_at": "2017-04-25T11:12:48Z",
        "closed_at": "2017-11-29T08:32:59Z",
        "merged_at": null,
        "body": "\u2026me other related fixes\r\n\r\nThe implementation of the diskless replication was currently diskless only on the master side.\r\nThe slave side still stores the received rdb file to the disk before loading it back in and parsing it.\r\n\r\nother changes:\r\n--------------\r\ndon't save rdb / aof file when we're a slave that is not synced (sync failed and dataset is empty),\r\nso that we don't override an existing one and end up loosing data on failover.\r\n\r\nloadAppendOnlyFile (loadDataFromDisk) would have exit() if the file doesn't exist, but that would never happen\r\nsince the file was always already created in initServer before that check.\r\ninstad: don't create an empty aof file on startup before reading it and only create it when we start writing to it.\r\nthis allows us to distinguish between success to load an empty file and a failure to load a non-existing file,\r\n\r\ncurrently we don't use the above since the startup should succeed even if the file doesn't exist (first server startup).\r\nmaybe we need to add another config called \"preload-file\" or \"load-on-startup\" or \"abort-when-load-fails\"\r\n\r\ndistinguish between aof configuration and state so that we can re-enable aof only when sync eventually\r\nsucceeds (and not when exiting from readSyncBulkPayload after a failed attempt)\r\nalso a CONFIG GET and INFO during rdb loading would have lied\r\n\r\nSLAVEOF NO ONE, will have an argument to succeed only if the slave is in sync (a specific offset can be provided)\r\n\r\ntests:\r\nadd test for not saving on exit for unsynced slave\r\nreplication tests for diskless slave and diskless master\r\nother replication tests improvements (not related to diskless slave)",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-04-25T01:02:33Z",
        "closed_at": "2019-12-23T16:42:04Z",
        "merged_at": null,
        "body": "changed craete-cluster stop to create-cluster stop",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-04-21T15:15:53Z",
        "closed_at": "2021-07-01T07:04:06Z",
        "merged_at": null,
        "body": "`sdsfreesplitres` and `linenoiseFree` are used to free string memory of command, but they haven't applied in \"restart\" command handling, add deallocation before leaving repl function.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-04-19T21:25:33Z",
        "closed_at": "2020-09-09T16:27:26Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-04-18T12:21:38Z",
        "closed_at": "2017-04-20T05:55:52Z",
        "merged_at": "2017-04-20T05:55:52Z",
        "body": "This may avoid to create an unnecessary new block in some case.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2017-04-18T06:54:16Z",
        "closed_at": "2017-04-18T08:56:58Z",
        "merged_at": "2017-04-18T08:56:58Z",
        "body": "Hi @antirez, as we've discussed in issue #3912, I reopen this pull request.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-04-17T11:39:04Z",
        "closed_at": "2017-04-18T14:31:19Z",
        "merged_at": "2017-04-18T14:31:19Z",
        "body": "Fixes #3944",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-04-16T23:57:54Z",
        "closed_at": "2021-10-27T10:28:34Z",
        "merged_at": null,
        "body": "Jemalloc url download pattern is not supported. URL pattern is replaced with the newest version address that is provided by github. curl command is changed with wget command in order to access file on github (amazon servers certification and redirects may cause problem while downloading.)",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2017-04-16T08:45:27Z",
        "closed_at": "2017-04-21T14:56:29Z",
        "merged_at": null,
        "body": "fix issue https://github.com/antirez/redis/issues/3848",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-04-15T19:36:03Z",
        "closed_at": "2021-06-21T11:09:30Z",
        "merged_at": null,
        "body": "Resolves #3938",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2017-04-13T14:19:28Z",
        "closed_at": "2017-08-02T10:51:27Z",
        "merged_at": "2017-08-02T10:51:27Z",
        "body": "With the addition of modules, looping over the redisCommandTable\r\nmisses any added commands. By moving to dictionary iteration this\r\nis resolved.\r\n\r\nFixes #3934",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2017-04-12T11:23:30Z",
        "closed_at": "2017-04-26T10:18:45Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 41,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2017-04-08T18:43:37Z",
        "closed_at": "2017-06-14T16:12:18Z",
        "merged_at": "2017-06-14T16:12:18Z",
        "body": "Previously I reported a bug where Redis cluster slot check was not performed on the store key in GEORADIUS and GEORADIUSBYMEMBER commands. See https://github.com/antirez/redis/issues/3921\r\n\r\nHere is a patch to fix the issue. Essentially I implemented a custom method for getting the index of keys specifically for those 2 commands. Fix has been tested and cluster redirect is now working correctly as expected for both commands. \r\n\r\n> src/redis-cli \r\n\r\n127.0.0.1:6379> cluster keyslot Sicily\r\n(integer) 10713\r\n127.0.0.1:6379> cluster addslots 10713\r\nOK\r\n127.0.0.1:6379> GEOADD Sicily 13.361389 38.115556 \"Palermo\" 15.087269 37.502669 \"Catania\"\r\n(integer) 2\r\n127.0.0.1:6379> GEORADIUS Sicily 15 37 200 km WITHDIST\r\n1) 1) \"Palermo\"\r\n   2) \"190.4424\"\r\n2) 1) \"Catania\"\r\n   2) \"56.4413\"\r\n\r\n127.0.0.1:6379> GEORADIUS Sicily 15 37 200 km store mylocations\r\n(error) CROSSSLOT Keys in request don't hash to the same slot\r\n127.0.0.1:6379> GEORADIUS Sicily 15 37 200 km store {Sicily}-mylocations\r\n(integer) 2\r\n127.0.0.1:6379> GEORADIUSBYMEMBER Sicily Palermo 200 km\r\n1) \"Palermo\"\r\n2) \"Catania\"\r\n\r\n127.0.0.1:6379> GEORADIUSBYMEMBER Sicily Palermo 200 km storedist mylocations\r\n(error) CROSSSLOT Keys in request don't hash to the same slot\r\n127.0.0.1:6379> GEORADIUSBYMEMBER Sicily Palermo 200 km storedist {Sicily}-mylocations2\r\n(integer) 2\r\n127.0.0.1:6379> scan 0\r\n1) \"0\"\r\n2) 1) \"{Sicily}-mylocations2\"\r\n   2) \"{Sicily}-mylocations\"\r\n   3) \"Sicily\"\r\n127.0.0.1:6379> ",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2017-04-08T15:28:33Z",
        "closed_at": "2022-02-10T15:36:54Z",
        "merged_at": null,
        "body": "We don't need to set decr_step = 16 in every loop.\r\nIt is always 16. and not changed.\r\nso I extract it as global define.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-04-08T07:20:54Z",
        "closed_at": "2017-04-11T14:31:55Z",
        "merged_at": "2017-04-11T14:31:55Z",
        "body": "The macro REDIS_EXPIRELOOKUPS_TIME_PERC has been replaced by\r\nACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC in commit 6500fabfb881a7ffaadfbff74ab801c55d4591fc.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 41,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2017-04-07T22:54:45Z",
        "closed_at": "2017-04-08T18:37:31Z",
        "merged_at": null,
        "body": "Previously I reported a bug where Redis cluster slot check was not performed on the store key in GEORADIUS and GEORADIUSBYMEMBER commands. See https://github.com/antirez/redis/issues/3921\r\n\r\nHere is a patch to fix the issue. Essentially I implemented a custom method for getting the index of keys specifically for those 2 commands. Fix has been tested and cluster redirect is now working correctly as expected for both commands. \r\n\r\n> src/redis-cli \r\n\r\n127.0.0.1:6379> cluster keyslot Sicily\r\n(integer) 10713\r\n127.0.0.1:6379> cluster addslots 10713\r\nOK\r\n127.0.0.1:6379> GEOADD Sicily 13.361389 38.115556 \"Palermo\" 15.087269 37.502669 \"Catania\"\r\n(integer) 2\r\n127.0.0.1:6379> GEORADIUS Sicily 15 37 200 km WITHDIST\r\n1) 1) \"Palermo\"\r\n   2) \"190.4424\"\r\n2) 1) \"Catania\"\r\n   2) \"56.4413\"\r\n\r\n127.0.0.1:6379> GEORADIUS Sicily 15 37 200 km store mylocations\r\n(error) CROSSSLOT Keys in request don't hash to the same slot\r\n127.0.0.1:6379> GEORADIUS Sicily 15 37 200 km store {Sicily}-mylocations\r\n(integer) 2\r\n127.0.0.1:6379> GEORADIUSBYMEMBER Sicily Palermo 200 km\r\n1) \"Palermo\"\r\n2) \"Catania\"\r\n\r\n127.0.0.1:6379> GEORADIUSBYMEMBER Sicily Palermo 200 km storedist mylocations\r\n(error) CROSSSLOT Keys in request don't hash to the same slot\r\n127.0.0.1:6379> GEORADIUSBYMEMBER Sicily Palermo 200 km storedist {Sicily}-mylocations2\r\n(integer) 2\r\n127.0.0.1:6379> scan 0\r\n1) \"0\"\r\n2) 1) \"{Sicily}-mylocations2\"\r\n   2) \"{Sicily}-mylocations\"\r\n   3) \"Sicily\"\r\n127.0.0.1:6379> ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-04-05T08:55:38Z",
        "closed_at": "2021-06-29T03:42:07Z",
        "merged_at": null,
        "body": "Change redis URL in ascii logo from http to https",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 20,
        "changed_files": 2,
        "created_at": "2017-04-04T16:07:25Z",
        "closed_at": "2020-11-10T19:03:48Z",
        "merged_at": null,
        "body": " Quick fix for [#3841](https://github.com/antirez/redis/issues/3841).\r\n`127.0.0.1:6379> SLOWLOG get 5\r\n1) 1) (integer) 3\r\n   2) (integer) 1491321639\r\n   3) (integer) 30\r\n   4) 1) \"incrbyfloat\"\r\n      2) \"foo\"\r\n      3) \"3.14\"\r\n2) 1) (integer) 2\r\n   2) (integer) 1491321633\r\n   3) (integer) 21\r\n   4) 1) \"hincrbyfloat\"\r\n      2) \"bar\"\r\n      3) \"x\"\r\n      4) \"3.4\"\r\n3) 1) (integer) 1\r\n   2) (integer) 1491321632\r\n   3) (integer) 51\r\n   4) 1) \"hincrbyfloat\"\r\n      2) \"bar\"\r\n      3) \"x\"\r\n      4) \"3.4\"\r\n4) 1) (integer) 0\r\n   2) (integer) 1491321629\r\n   3) (integer) 9\r\n   4) 1) \"config\"\r\n      2) \"set\"\r\n      3) \"slowlog-log-slower-than\"\r\n      4) \"0\"`",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2017-03-31T13:58:57Z",
        "closed_at": "2017-04-17T05:20:20Z",
        "merged_at": null,
        "body": "#### What changes were proposed in this pull request?\r\n\r\nIf we serialise a big skiplist value in reversed order, the deserialisation can be optimised from O(nlgn) to O(n).",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-03-29T17:21:02Z",
        "closed_at": "2021-12-24T12:29:50Z",
        "merged_at": null,
        "body": "This pull request corrects the following bug (fixes #3888):\r\n- Using `RedisModule_Call` write commands on a `HASH` data type passing a `'l' (long long)` formatter on `field` or `value` results in a segmentation fault.\r\n\r\nI wrote a [simple redis module](https://github.com/aviggiano/pr-redis) to better explain the problem. \r\nThe fix consists of calling `getDecodedObject` before the `hashTypeSet` call in the `hsetCommand` function. The `getDecodedObject` properly decodes a `(void*)(long long)` to a `sds` when the object is of type `OBJ_ENCODING_INT`, which is the case of this issue.\r\n\r\nNote: I'm not sure if this bug exists in other commands, since some of them use `getDecodedObject` internally (such as those from the `LIST` data type) and others do not (e.g. `HASH` write commands). It would be probably better to change all data types write calls to pass a `robj` instead of a `sds` as a function argument, but I wasn't not sure about how @antirez would feel about this structural change. \r\n\r\n*****\r\n\r\nDive deeping into the crash: \r\n1. The `RM_Call` command would call the [`moduleCreateArgvFromUserFormat`](https://github.com/antirez/redis/blob/unstable/src/module.c#L2457) function to parse the clients' command arguments\r\n2. The `moduleCreateArgvFromUserFormat` command would create a list of `robj`. Those with the `long long` formatter would have an `->encoding` of type `OBJ_ENCODING_INT` and `->ptr` equals to `(void*) (long long) ll`.\r\n3. The `hsetCommand` would eventually be called and would then call [`hashTypeSet`](https://github.com/antirez/redis/blob/unstable/src/t_hash.c#L520) passing `argv[ ]->ptr` which would be received as an `sds`\r\n4. Any `sds` function call to the encoded `void*` would result in a segmentation fault. \r\n\r\n```\r\n# server\r\n$ gdb ./redis/src/redis-server\r\n...\r\n(gdb) r --loadmodule ./pr-redis/pr-redis.so\r\n...\r\n32666:M 29 Mar 13:51:41.091 * Module 'PR_REDIS' loaded from ./pr-redis/pr-redis.so\r\n32666:M 29 Mar 13:51:41.091 * Ready to accept connections\r\nThread 1 \"redis-server\" received signal SIGSEGV, Segmentation fault.\r\nhashTypeSet (o=0x7ffff6a6f4e0, field=0x7ffff6a1c0f3 \"field\", \r\n    value=0x1 <error: Cannot access memory at address 0x1>, flags=flags@entry=0)\r\n    at t_hash.c:231\r\n231\t            zl = ziplistPush(zl, (unsigned char*)value, sdslen(value),\r\n(gdb) bt\r\n#0  hashTypeSet (o=0x7ffff6a6f4e0, field=0x7ffff6a1c0f3 \"field\", \r\n    value=0x1 <error: Cannot access memory at address 0x1>, flags=flags@entry=0)\r\n    at t_hash.c:231\r\n#1  0x000000000045a4c3 in hsetCommand (c=0x7ffff6b394c0) at t_hash.c:520\r\n#2  0x000000000042b736 in call (c=c@entry=0x7ffff6b394c0, flags=3) at server.c:2152\r\n#3  0x0000000000492dde in RM_Call (ctx=0x7fffffffc7b0, cmdname=<optimized out>, \r\n    fmt=0x7ffff4ffc23a \"ssl\") at module.c:2502\r\n#4  0x00007ffff4ffb5e0 in CrashCommand () from ./pr-redis/pr-redis.so\r\n#5  0x000000000049268f in RedisModuleCommandDispatcher (c=<optimized out>) at module.c:454\r\n#6  0x000000000042b736 in call (c=c@entry=0x7ffff6b1ae00, flags=flags@entry=15)\r\n    at server.c:2152\r\n#7  0x000000000042be37 in processCommand (c=0x7ffff6b1ae00) at server.c:2432\r\n#8  0x000000000043ba1d in processInputBuffer (c=0x7ffff6b1ae00) at networking.c:1299\r\n#9  0x00000000004256a8 in aeProcessEvents (eventLoop=eventLoop@entry=0x7ffff6a300a0, \r\n    flags=flags@entry=3) at ae.c:412\r\n#10 0x0000000000425a4b in aeMain (eventLoop=0x7ffff6a300a0) at ae.c:455\r\n#11 0x0000000000422765 in main (argc=<optimized out>, argv=0x7fffffffcac8) at server.c:3791\r\n```\r\n\r\n```\r\n# client\r\n$ redis-cli\r\n127.0.0.1:6379> CRASH\r\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\r\n(53.54s)\r\nnot connected> \r\n```\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 45766,
        "deletions": 16973,
        "changed_files": 276,
        "created_at": "2017-03-25T08:17:23Z",
        "closed_at": "2017-09-25T11:21:13Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-03-23T21:27:30Z",
        "closed_at": "2019-11-29T03:04:46Z",
        "merged_at": null,
        "body": "To create a new key, it is required REDISMODULE_WRITE instead of REDISMODULE_READ.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-03-23T15:03:49Z",
        "closed_at": "2021-06-28T13:42:54Z",
        "merged_at": null,
        "body": "null pointer",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-03-20T02:49:38Z",
        "closed_at": "2021-03-16T14:56:44Z",
        "merged_at": null,
        "body": "add close(s) when error return, otherwise it could leak socket s after `anetSetReuseAddr` returns ANET_ERR.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-03-18T14:30:52Z",
        "closed_at": "2021-05-05T12:12:53Z",
        "merged_at": "2021-05-05T12:12:53Z",
        "body": "I think the second fd lookup is unnecessary. Though it might be optimized by compiler or cached by CPU, remove one may help and make the code clean.\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-03-17T12:38:44Z",
        "closed_at": "2020-09-09T16:27:29Z",
        "merged_at": null,
        "body": "`falure` to failure",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 42,
        "changed_files": 23,
        "created_at": "2017-03-16T14:39:37Z",
        "closed_at": "2020-09-09T16:27:32Z",
        "merged_at": null,
        "body": "fix some typos.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-03-14T02:19:23Z",
        "closed_at": "2020-09-09T16:27:36Z",
        "merged_at": null,
        "body": "Fixed two typos in the warning log message.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-03-12T18:39:01Z",
        "closed_at": "2021-06-28T14:57:00Z",
        "merged_at": null,
        "body": "@antirez va_list 'ap' was opened but not closed by va_end()",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-03-11T02:24:42Z",
        "closed_at": "2021-10-28T09:45:18Z",
        "merged_at": null,
        "body": "1 Update repl_no_slaves_since timestamp when role changed from slave to master, avoid replication backlog free in replication cron(role changed to master and repl-backlog-ttl is done with no slaves connected, so it will free backlog result in fullsync). \r\n2 Create backlog before psync begin if this slave psync with master after server startup.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 21,
        "changed_files": 1,
        "created_at": "2017-03-07T19:00:00Z",
        "closed_at": "2018-02-14T02:56:07Z",
        "merged_at": null,
        "body": "It is possible to do BGREWRITEAOF even if appendonly=no. This is by design.\r\nstopAppendonly() didn't turn off aof_rewrite_scheduled (it can be turned on\r\nagain by BGREWRITEAOF even while appendonly is off anyway).\r\nAfter configuring `appendonly yes` it will see that the state is AOF_OFF,\r\nthere's no RDB fork, so it will do rewriteAppendOnlyFileBackground() which\r\nwill fail since the aof_child_pid is set (was scheduled and started by cron).\r\n\r\nSolution:\r\nstopAppendonly() will turn off the schedule flag (regardless of who asked for it).\r\nstartAppendonly() will terminate any existing fork and start a new one (so it is the most recent).",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-03-07T16:18:04Z",
        "closed_at": "2017-04-18T14:13:24Z",
        "merged_at": "2017-04-18T14:13:24Z",
        "body": "fix [#3847](https://github.com/antirez/redis/issues/3847)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-03-06T16:48:00Z",
        "closed_at": "2018-01-22T13:13:34Z",
        "merged_at": null,
        "body": "When feeding the master with a high rate traffic the the slave's feed is much slower.\r\nThis causes the replication buffer to grow (indefinitely) which leads to slave disconnection.\r\nThe problem is that writeToClient() decides to stop writing after NET_MAX_WRITES_PER_EVENT\r\nwrites (In order to be fair to clients).\r\nWe should ignore this when the client is a slave.\r\nIt's better if clients wait longer, the alternative is that the slave has no chance to stay in\r\nsync in this situation.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-03-06T12:38:07Z",
        "closed_at": "2017-07-06T13:02:11Z",
        "merged_at": "2017-07-06T13:02:11Z",
        "body": "Fixes #3851",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-03-01T14:53:19Z",
        "closed_at": "2017-04-10T11:14:53Z",
        "merged_at": "2017-04-10T11:14:53Z",
        "body": "`unblockClient(c);` triggered `unblockClientFromModule` which refered to the blocked context after it's been freed, resulting in possible crash and an error in valgrind. \r\n\r\nReversing the order of lines fixes this.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2017-02-23T12:06:30Z",
        "closed_at": "2018-06-08T10:36:02Z",
        "merged_at": null,
        "body": "note that the previous implementation uses LONG_MAX which is not a power of 2, not even an even number\r\n\r\nAntirez, i know i already pushed this PR in the past, please consider it again. even if a small improvement in efficiency, there's no good reason not to take it.\r\na small function with that name, doesn't need to have readable implementation.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-02-23T12:05:11Z",
        "closed_at": "2018-02-13T14:55:27Z",
        "merged_at": "2018-02-13T14:55:27Z",
        "body": "since slave isn't replying to it's master, these errors go unnoticed.\r\nsince we don't expect the master to send garbadge to the slave, this should be safe.\r\n(as long as we don't log OOM errors there)\r\n\r\nAntirez, please consider if you want to add this. it helped us uncover some problems.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2017-02-23T12:01:52Z",
        "closed_at": "2018-06-28T11:33:55Z",
        "merged_at": null,
        "body": "- if defrag was enabled via config file, we should terminate the server if not supported\r\n- GeoHashArea initialization warnings\r\n- resetRefCount should not clear OBJ_SHARED_REFCOUNT",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2017-02-23T12:01:02Z",
        "closed_at": "2019-03-22T16:41:41Z",
        "merged_at": "2019-03-22T16:41:41Z",
        "body": "old title: several bugfixes to diskless replication and the CAPA mechanism\r\n\r\n1) in diskless replication - master not notifing the slave that rdb transfer terminated on error, and lets slave wait for replication timeout\r\n\r\n2) when the main process disconnected a slave due to slave buffer overflow, the rdbsave child kept streaming the rdb, and the slave only found out about it after reading the entire rdb.\r\n\r\n\r\n3) [redundant fix, commit removed from the PR]\r\nwhen starting a replication for a certain mincapa, we must take care to exclude slaves that didn't declare that capa, otherwise they may get something that they can't handle\r\n\r\n\r\n\r\n@guybe7 these are your fixes, please add details if i messed them up.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2017-02-23T11:59:39Z",
        "closed_at": "2019-03-21T16:15:56Z",
        "merged_at": "2019-03-21T16:15:55Z",
        "body": "since retry will get negative.\r\n\r\nalso reduce an excess sleep at the end\r\n\r\nplease note that this bug existed in the original code (redis 3.2), before this code was promoted into a separate function.\r\n\r\n@guybe7 this is your fix, please add details if i messed them up.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 11,
        "changed_files": 6,
        "created_at": "2017-02-23T11:53:13Z",
        "closed_at": "2018-02-27T12:04:33Z",
        "merged_at": "2018-02-27T12:04:33Z",
        "body": "this commit also contains small bugfix in rdbLoadLzfStringObject\r\na bug that currently has no implications.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2017-02-22T16:21:52Z",
        "closed_at": "2021-08-08T11:36:23Z",
        "merged_at": null,
        "body": "Fixes #3824",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-02-22T05:27:42Z",
        "closed_at": "2018-03-22T08:19:27Z",
        "merged_at": "2018-03-22T08:19:27Z",
        "body": "Before returning error. We should call dlclose(handle);\r\n\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 369,
        "deletions": 6,
        "changed_files": 7,
        "created_at": "2017-02-20T21:42:02Z",
        "closed_at": "2020-11-24T17:00:44Z",
        "merged_at": null,
        "body": "This is a better implementation of my previous pull request - https://github.com/antirez/redis/pull/3814\r\nThe callbacks are no longer stored per client but are now stored in the redisServer struct.\r\n\r\ncreateClient (networking.c) and freeClient (networking.c) fire the list of connection callbacks and disconnection callbacks respectively.\r\n\r\nModule API now exposes 2 methods (instead of 1):\r\n\r\nint RM_HookToConnection(RedisModuleCtx *ctx, void (*cb)(RedisModuleCtx *));\r\nint RM_HookToDisconnection(RedisModuleCtx *ctx, void (*cb)(RedisModuleCtx *));\r\n\r\nThe id of the connected/disconnected client can be extracted from the RedisModuleCtx pointer via RM_GetClientId.\r\n\r\n(I though it would be more appropriate to expose RedisModuleCtx instead of the actual client id because it provides another layer of abstraction that is consistent with the rest of the module API + It can be possible to run commands with RedisModuleCtx).\r\n\r\nClients are no longer required to access the module for their connections/disconnection to be detected.\r\n\r\nCallbacks get unhooked at module unloading (serverLog is thrown if unhooking has failed).",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-02-19T22:58:05Z",
        "closed_at": "2018-10-26T17:24:55Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2017-02-18T21:46:21Z",
        "closed_at": "2020-09-05T19:53:55Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2017-02-18T11:08:23Z",
        "closed_at": "2021-10-17T12:20:09Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 80,
        "deletions": 1,
        "changed_files": 6,
        "created_at": "2017-02-18T00:20:47Z",
        "closed_at": "2017-02-20T21:42:42Z",
        "merged_at": null,
        "body": "I had noticed that this feature was missing when i began working on a module that's supposed to act as a simple count server - each client would have the ability to raise the count of some token and when a client disconnects, **his** contribution would be removed.\r\n\r\nSo here is my contribution for this matter.\r\n\r\nEach client is assigned with a list of disconnection callbacks and an hash set of visited modules.\r\nI expose and API - RM_HookToDisconnection that sets a disconnection callback (with client id as argument) to the module under the ctx that was passed to it.\r\n\r\nI use the 'RedisModuleCommandDispatcher' (module.c) proxy to add the visited module to the client's visited modules dictionary, if dictAdd returns with no errors (which means that this is the first command that is issued from the client to the module) - i add the callback that was registered by RM_HookToDisconnection to the client's disconnection callbacks list.\r\n\r\nThe callbacks are called in the freeClient function (networking.c).",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2017-02-17T23:04:24Z",
        "closed_at": "2021-10-17T17:26:04Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-02-10T12:21:17Z",
        "closed_at": "2017-06-20T15:01:17Z",
        "merged_at": "2017-06-20T15:01:17Z",
        "body": "fix server.stat_net_output_bytes, we should add server.stat_net_output_bytes with totwritten after the while loop.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-02-09T07:46:48Z",
        "closed_at": "2021-08-10T09:01:33Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 166,
        "deletions": 31,
        "changed_files": 7,
        "created_at": "2017-01-30T20:57:00Z",
        "closed_at": "2018-02-18T15:58:41Z",
        "merged_at": null,
        "body": "adding a test for active defrag.\r\ni had to make it slow so in order to wait for the relatively long intervals in active defrag decisions.\r\ni also had to eat a lot of memory since the info command doesn't report accurate fragmentation information.\r\nyou may want to tag it as slow test so that it is excluded from your normal test runs.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2017-01-28T03:26:39Z",
        "closed_at": "2017-09-18T10:18:57Z",
        "merged_at": "2017-09-18T10:18:57Z",
        "body": "default size of SET/GET value in usage should be 3 bytes as in main code.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 0,
        "changed_files": 6,
        "created_at": "2017-01-23T20:46:06Z",
        "closed_at": "2021-01-12T15:28:49Z",
        "merged_at": null,
        "body": "Add `SO_BUSY_POLL` support.\r\nBenchmarks example. Port 6379 is without busy polling, 6380 with:\r\n```\r\n% ./target/release/rpc-perf --server 127.0.0.1:6379 --config configs/default.toml --protocol redis | grep Latency:\r\n2017-01-23 22:20:59.994 INFO  [rpc-perf] Latency: min: 649593 ns max: 184280941 ns\r\n2017-01-23 22:21:10.074 INFO  [rpc-perf] Latency: min: 707265 ns max: 192334005 ns\r\n2017-01-23 22:21:20.153 INFO  [rpc-perf] Latency: min: 605029 ns max: 143076099 ns\r\n2017-01-23 22:21:30.257 INFO  [rpc-perf] Latency: min: 614466 ns max: 209513874 ns\r\n2017-01-23 22:21:40.341 INFO  [rpc-perf] Latency: min: 722469 ns max: 111132279 ns\r\n\r\n% ./target/release/rpc-perf --server 127.0.0.1:6380 --config configs/default.toml --protocol redis | grep Latency:\r\n2017-01-23 22:22:46.682 INFO  [rpc-perf] Latency: min: 587727 ns max: 208842785 ns\r\n2017-01-23 22:22:56.789 INFO  [rpc-perf] Latency: min: 579863 ns max: 26625442 ns\r\n2017-01-23 22:23:06.881 INFO  [rpc-perf] Latency: min: 458752 ns max: 127641060 ns\r\n2017-01-23 22:23:16.985 INFO  [rpc-perf] Latency: min: 601359 ns max: 126902862 ns\r\n2017-01-23 22:23:27.079 INFO  [rpc-perf] Latency: min: 702022 ns max: 116500988 ns\r\n```\r\nTo verify if polling is set (systemtap):\r\n```\r\n%{\r\n#include <asm-generic/socket.h>\r\n%}\r\n\r\nprobe kernel.function(\"sys_setsockopt\").return\r\n{\r\n        if ($optname == %{ SO_BUSY_POLL %} && $return == 0)\r\n                printf(\"%s busy_poll: %d\\n\", execname(), user_int($optval));\r\n}\r\nredis-server-bp busy_poll: 50\r\n```",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 244,
        "deletions": 0,
        "changed_files": 7,
        "created_at": "2017-01-23T15:52:18Z",
        "closed_at": "2023-05-19T08:27:01Z",
        "merged_at": null,
        "body": "Hello Antirez, I add the lockless programming CAS API for string type's storage.\r\n\r\nThe reason why I add CAS API is: I know Redis can implement CAS operations by using WATCH and MULTI; But many business always package Redis server, for example, using a proxy to access Redis, in this scenario, WATCH and MULTI is hard to use. So I added the CAS API for string type storage.\r\n\r\nHow I implemented CAS API for string type: \r\nI introduced three APIs: \r\n\r\n>getcas key\r\nreturn version and value\r\nsetcas\r\n\r\n>setcas key value version\r\nif key is not exist, version must = 0; otherwise setcas will be failed\r\nif key is exist, version must = cas version in key storage, otherwise setcas will be failed\r\ndelcas\r\n\r\n>delcas key\r\nreturn version and value\r\n\r\nFor string type:\r\nwhen encoding = INT or EMBSTR, rasing INT to RAW, then appending cas version to sds->buf;\r\nwhen encoding = RAW, appending cas version to sds->buf;\r\n\r\nBy this easy way, my CAS API supported RDB\\AOF\\master-slave model\\cluster model easily.\r\nI have test my API successfully.\r\n\r\nSo can you share your idea or give me some advise?",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-01-18T15:03:06Z",
        "closed_at": "2017-11-22T20:05:02Z",
        "merged_at": null,
        "body": "The problem is easily reproduced:\r\n\r\n```\r\nMULTI\r\n<some module command>\r\nEXEC\r\n```\r\n\r\nWould result with two pairs of MULTI/EXEC wrapping the module replicated commands.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 119807,
        "deletions": 32691,
        "changed_files": 518,
        "created_at": "2017-01-18T02:15:28Z",
        "closed_at": "2017-01-20T10:01:01Z",
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2017-01-17T14:14:58Z",
        "closed_at": "2021-10-12T20:01:17Z",
        "merged_at": null,
        "body": "bigkeys sleep is defined each 100 scanned keys, and it is checked it only between scan cycles. In cases that scan does not return exactly 10 keys it will stop sleeping. In addition the comment was sleep each 100 SCANs but it was 100 keys or 10 SCANs.\r\n\r\nMy proposal is to change the sleep to be carried every scan, and modify the suggestion to 0.01 secs as an interval.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-01-17T09:22:31Z",
        "closed_at": "2021-12-02T15:12:10Z",
        "merged_at": null,
        "body": "When notification-script return 1, the running_scripts count will not decrease to right number, so running_scripts will soon grow to SENTINEL_SCRIPT_MAX_RUNNING, and notification-script will never be called",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-01-15T17:37:29Z",
        "closed_at": "2020-09-09T16:27:38Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-01-15T12:09:12Z",
        "closed_at": "2021-10-13T13:54:35Z",
        "merged_at": "2021-10-13T13:54:35Z",
        "body": "Adding -i option (sleep interval) of repeat and bigkeys  to redis-cli --scan.\r\nWhen the keyspace contains many already expired keys scanning the dataset with redis-cli --scan can impact the performance",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2017-01-14T21:32:16Z",
        "closed_at": "2017-08-20T18:35:36Z",
        "merged_at": null,
        "body": "When `syncWrite` fails, it will leak `ap` because it is not `va_end`-ed. This commit fixes it.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2017-01-11T17:25:07Z",
        "closed_at": "2018-02-13T14:50:21Z",
        "merged_at": "2018-02-13T14:50:21Z",
        "body": "",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2017-01-08T17:30:19Z",
        "closed_at": "2017-01-13T10:32:23Z",
        "merged_at": "2017-01-13T10:32:23Z",
        "body": "Currently, everytime a command is launched through redis-cli it will first send the `COMMAND` command. This is unnecessary overhead, so I just removed initialization until it is really needed.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-01-05T13:44:57Z",
        "closed_at": "2021-06-24T06:45:07Z",
        "merged_at": null,
        "body": "Just an update of the year in the copyright. Happy new year ! ;-)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2017-01-04T08:11:22Z",
        "closed_at": "2018-03-22T16:10:30Z",
        "merged_at": null,
        "body": "Swap Memory is a factor to affect performance of redis.\r\nSo I just add \r\n\r\nredis:6379> info Memory\r\n# Memory\r\nused_memory:2164072904\r\nused_memory_human:2.02G\r\nused_memory_rss:1929404416\r\nused_memory_rss_human:1.80G\r\nswap_memory:292462592\r\nswap_memory_human:278.91M\r\nused_memory_peak:2164072904\r\nused_memory_peak_human:2.02G\r\n......\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2017-01-01T15:32:05Z",
        "closed_at": "2020-09-05T19:51:15Z",
        "merged_at": null,
        "body": "should be\r\n`void RedisModule_FreeString(RedisModuleCtx *ctx, RedisModuleString *str)`\r\n\r\nwas\r\n`void RedisModule_FreeString(RedisModuleString *str)`",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 774,
        "deletions": 8,
        "changed_files": 13,
        "created_at": "2017-01-01T07:03:03Z",
        "closed_at": "2017-01-11T14:46:50Z",
        "merged_at": "2017-01-11T14:46:50Z",
        "body": "gradually scan the keyspace and all the allocations in it, and re-allocate them in order to reduce memory fragmentation.\r\n\r\nnotes:\r\n* we may want to make it disabled by default in the released version, but for now i prefer to keep it enabled so that it can be tested.\r\n* i've also added an optional size parameter to DEBUG POPULATE",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2016-12-29T23:03:47Z",
        "closed_at": "2022-04-11T05:34:44Z",
        "merged_at": "2022-04-11T05:34:44Z",
        "body": "`RM_OpenKey` currently returns `void*`, however the rest of the API takes the `RedisModuleKey*` type.  Is this intentional?\r\n\r\nWith C++ compilers (or perhaps any C compiler with higher warning levels), a line like this:\r\n```\r\nRedisModuleKey *key = RedisModule_OpenKey(ctx, keyname, REDISMODULE_WRITE);\r\n```\r\nresults in\r\n```\r\nerror: invalid conversion from \u2018void*\u2019 to \u2018RedisModuleKey*\u2019 [-fpermissive]\r\n```\r\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 110,
        "deletions": 16,
        "changed_files": 6,
        "created_at": "2016-12-29T20:44:02Z",
        "closed_at": "2021-06-22T09:26:48Z",
        "merged_at": "2021-06-22T09:26:48Z",
        "body": "Previously, passing 0 for newlen would not truncate the string at all.\r\nThis adds handling of this case, freeing the old string and creating a new empty string.\r\n\r\nOther changes:\r\n- Move `src/modules/testmodule.c` to `tests/modules/basics.c`\r\n- Introduce that basic test into the test suite\r\n- Add tests to cover StringTruncate\r\n- Add `test-modules` build target for the main makefile\r\n- Extend `distclean` build target to clean modules too",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2016-12-29T13:47:28Z",
        "closed_at": "2020-09-05T22:30:10Z",
        "merged_at": null,
        "body": "information about Automatic Memory Management is in INTRO.md,\r\nnot API.md\r\n\r\nUpdate comments in code and text in API.md",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-12-26T20:15:32Z",
        "closed_at": "2017-01-20T10:01:43Z",
        "merged_at": "2017-01-20T10:01:43Z",
        "body": "getExpire calls dictFind which can do rehashing.\r\nfound by calling computeDatasetDigest from serverCron and running the test suite.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-12-26T08:29:08Z",
        "closed_at": "2021-06-21T20:04:14Z",
        "merged_at": null,
        "body": "### Problem\r\n\r\n`redis-cli` via unixsock run into infinite loop when cluster returns redirection.\r\n\r\n### Scenario\r\n1. connect to server via unixsock\r\n2. run some cluster command, and got some redirection like `MOVED`\r\n3. redis-cli sets `config.host` and `config.port` and `reissue`\r\n4. but `cliConnect` always uses `config.hostsocket` if exists\r\n5. goto 1\r\n\r\n#### unstable (and 3.2, ...)\r\n\r\n``` shell\r\n% ./redis-cli --version\r\nredis-cli 999.999.999 (git:6712bce9)\r\n\r\n% ./redis-cli -s /tmp/redis.sock get foo\r\n(error) MOVED 12182 127.0.0.1:6003\r\n\r\n% ./redis-cli -c -s /tmp/redis.sock get foo\r\n# infinite loop\r\n```\r\n\r\n#### This PR\r\n\r\n``` shell\r\n% ./redis-cli --version\r\nredis-cli 999.999.999 (git:e25cf19e)\r\n\r\n% ./redis-cli -s /tmp/redis.sock get foo\r\n(error) MOVED 12182 127.0.0.1:6003\r\n\r\n% ./redis-cli -c -s /tmp/redis.sock get foo\r\n(nil)\r\n```\r\n\r\nThanks.\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-12-22T12:51:15Z",
        "closed_at": "2018-08-29T14:03:22Z",
        "merged_at": null,
        "body": "this bug may cause segmentation fault \uff1a\uff09",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-12-22T11:47:46Z",
        "closed_at": "2020-09-09T15:10:17Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-12-21T12:29:32Z",
        "closed_at": "2020-09-09T16:27:42Z",
        "merged_at": null,
        "body": "Fix typo they->the in helloworld.c",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24316,
        "deletions": 7775,
        "changed_files": 246,
        "created_at": "2016-12-21T00:50:32Z",
        "closed_at": "2017-04-19T14:20:13Z",
        "merged_at": null,
        "body": "When master all down, slave not selected to master, cluster failed.\r\nWe test cluster in 2 labs with 3 master and each one has 1 slave. All master in lab1, and slave in lab2.\r\nWhen we shutdown the machine of all 3 master int the lab1(shutdown 2 is fail too, shutdown 1 is OK) in almost same time, all the master stay with PFAIL status. No slave voted and became to master instead. And the cluster stay down, can not provide service.\r\nWe have 2 lab, and one lab maybe power off by accident. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-12-19T22:38:32Z",
        "closed_at": "2016-12-20T12:32:54Z",
        "merged_at": "2016-12-20T12:32:54Z",
        "body": "Fix missing brackets around encoding variable in ZIP_DECODE_LENGTH macro",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-12-19T08:41:05Z",
        "closed_at": "2020-09-09T16:27:46Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2016-12-16T11:18:28Z",
        "closed_at": "2017-07-12T04:23:17Z",
        "merged_at": null,
        "body": "Since ll2string() returns the length, we can skip the call of strlen.\r\n\r\nActually, we can write like this:\r\n\r\nwrite(fd, buf, ll2string(buf,sizeof(buf), getpid()));\r\n\r\nHowever in this way the readability is less.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-12-14T22:08:47Z",
        "closed_at": "2016-12-16T08:20:48Z",
        "merged_at": "2016-12-16T08:20:48Z",
        "body": "Due to the wrong element being compared in the low level API functions for ZSET iteration, we would read one extra element on these ranges. \r\n\r\nThis PR fixes this.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-12-12T18:47:08Z",
        "closed_at": "2021-09-05T06:10:57Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 76076,
        "deletions": 2236,
        "changed_files": 380,
        "created_at": "2016-12-12T17:13:12Z",
        "closed_at": "2016-12-19T07:18:11Z",
        "merged_at": null,
        "body": "Is it possible to use Redis modules in the MSOpenTech Redis fork?\r\nhttps://github.com/RedisLabsModules\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 57,
        "deletions": 27,
        "changed_files": 4,
        "created_at": "2016-12-09T09:04:42Z",
        "closed_at": "2016-12-16T10:08:31Z",
        "merged_at": null,
        "body": "LogLog-\u03b2 is a new algorithm for estimating cardinalities based on LogLog counting. The new algorithm uses only one formula and needs no additional bias corrections for the entire range of cardinalities, therefore, it is more efficient and simpler to implement. \r\nRefer to https://arxiv.org/pdf/1612.02557.pdf for more details.\r\nThis changeset adds the support to compute carinality based on loglog-beta alogorithm and a config option to use the same.\r\n\r\nI have tested the changes using PFSELFTEST command with new config option set.\r\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-12-08T16:47:35Z",
        "closed_at": "2017-07-17T22:22:46Z",
        "merged_at": null,
        "body": "Signed-off-by: Kyle Swanson <k@ylo.ph>",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2016-12-08T16:26:52Z",
        "closed_at": "2021-04-27T07:49:05Z",
        "merged_at": null,
        "body": "This revamps some of the usage clause as well as clarifies an ambiguity regarding the datasize of custom commands (see http://stackoverflow.com/questions/41026808/redis-benchmarking-for-hmset-hgetall-with-a-data-size/41029167)",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-12-08T13:29:05Z",
        "closed_at": "2016-12-14T11:41:00Z",
        "merged_at": "2016-12-14T11:41:00Z",
        "body": "Before, if a previous key had a TTL set but the current one didn't, the\r\nTTL was reused and thus resulted in wrong expirations set.\r\n\r\nThis behaviour was experienced, when `MigrateDefaultPipeline` in\r\nredis-trib was set to >1\r\n\r\nFixes #3655",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-12-08T12:30:47Z",
        "closed_at": "2022-05-02T09:46:49Z",
        "merged_at": "2022-05-02T09:46:49Z",
        "body": "update macros ZIPLIST_ENTRY_END \uff0c although ZIPLIST_END_SIZE is defined  (sizeof(uint8_t)) , it must be equal 1, but  I think the right definition is ((zl)+intrev32ifbe(ZIPLIST_BYTES(zl))-ZIPLIST_END_SIZE). ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 49,
        "deletions": 23,
        "changed_files": 4,
        "created_at": "2016-12-07T11:13:23Z",
        "closed_at": "2019-10-16T10:01:16Z",
        "merged_at": null,
        "body": "This allows modules to declare commands as having raw output (like INFO), and\r\nhave redis-cli be aware of it and properly display the output.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-12-06T03:16:35Z",
        "closed_at": "2016-12-06T08:20:37Z",
        "merged_at": "2016-12-06T08:20:37Z",
        "body": "when num_reports is 0 or empty is 1 ,memory doctor output message without a '\\n' while other output message end up with '\\n'.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2016-12-05T16:35:44Z",
        "closed_at": "2016-12-16T15:53:13Z",
        "merged_at": "2016-12-16T15:53:13Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3036,
        "deletions": 83,
        "changed_files": 10,
        "created_at": "2016-12-04T23:52:36Z",
        "closed_at": "2016-12-05T16:32:02Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-12-03T05:06:46Z",
        "closed_at": "2017-06-20T14:53:57Z",
        "merged_at": "2017-06-20T14:53:57Z",
        "body": "Fixes #3647 \r\n\r\nI tested this by running a script which spinlocks for three seconds. With existing behaviour, each of these would print a line containing the time elapsed.\r\n\r\n```bash\r\n$ echo 'evalsha d367aeb6cc5b4d74c4eda50408bd10efbe3aeb18 0' | ./src/redis-cli     \r\n(nil)\r\n(3.00s)\r\n\r\n$ echo 'evalsha d367aeb6cc5b4d74c4eda50408bd10efbe3aeb18 0' | ./src/redis-cli --csv\r\nNIL                                     \r\n\r\n$ echo 'evalsha d367aeb6cc5b4d74c4eda50408bd10efbe3aeb18 0' | ./src/redis-cli --raw\r\n\r\n# empty output\r\n```\r\n\r\nIt was mentioned in #3647 that it might be a good idea to always print the elapsed time when using the standard output format. I played with this for a little while, but most commands run so fast that the elapsed time is written as 0, so I left that logic as-is.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-12-02T22:09:37Z",
        "closed_at": "2022-01-08T10:10:20Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-12-02T16:20:11Z",
        "closed_at": "2017-01-25T08:31:48Z",
        "merged_at": "2017-01-25T08:31:48Z",
        "body": "Fixes #3639",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2016-12-02T09:38:45Z",
        "closed_at": "2016-12-16T15:52:43Z",
        "merged_at": null,
        "body": "funcation sdsnewlen is not static , so it is the API for the sds.  but when s_malloc is faild and init is null , memset a null pointer can make the program crash .",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-11-30T20:10:36Z",
        "closed_at": "2016-12-01T08:09:37Z",
        "merged_at": "2016-12-01T08:09:37Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 17,
        "changed_files": 8,
        "created_at": "2016-11-30T19:51:15Z",
        "closed_at": "2019-03-21T10:49:26Z",
        "merged_at": "2019-03-21T10:49:26Z",
        "body": "A first go at exposing key names to rdb_save and rdb_load data type callbacks.  Seems like these should also be available for RESTORE and DUMP command handling, although my main focus was RDB processing and I may have overlooked some implications of those scenarios.\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-11-30T14:28:27Z",
        "closed_at": "2019-02-27T21:33:22Z",
        "merged_at": "2019-02-27T21:33:22Z",
        "body": "Small typo fix",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-11-29T08:36:49Z",
        "closed_at": "2016-12-19T07:19:11Z",
        "merged_at": "2016-12-19T07:19:11Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 29030,
        "deletions": 12191,
        "changed_files": 222,
        "created_at": "2016-11-23T13:15:38Z",
        "closed_at": "2016-12-05T22:17:52Z",
        "merged_at": null,
        "body": "",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2016-11-21T03:52:39Z",
        "closed_at": "2021-01-28T09:12:40Z",
        "merged_at": "2021-01-28T09:12:40Z",
        "body": "if option `setproctitle' is no, then do nothing for proc title.\r\n\r\nThe reason has been explained in [here](https://github.com/antirez/redis/commit/6356cf6808be9ea88ab97be33ebf1576eeb57da6#commitcomment-6119128), with the copy bellow.\r\n\r\nWe update redis to 2.8.8, then found there are some side effect when redis always change the process title.\r\n\r\nWe run several slave instance on one computer, and all these salves listen on unix socket only, then ps will show:\r\n1 S redis 18036 1 0 80 0 - 56130 ep_pol 14:02 ? 00:00:31 /usr/sbin/redis-server *:0\r\n1 S redis 23949 1 0 80 0 - 11074 ep_pol 15:41 ? 00:00:00 /usr/sbin/redis-server *:0\r\nfor redis 2.6 the output of ps is like following:\r\n1 S redis 18036 1 0 80 0 - 56130 ep_pol 14:02 ? 00:00:31 /usr/sbin/redis-server /etc/redis/a.conf\r\n1 S redis 23949 1 0 80 0 - 11074 ep_pol 15:41 ? 00:00:00 /usr/sbin/redis-server /etc/redis/b.conf\r\n\r\nLater is more informational in our case. The situation is worse when we manage the config and process running state by salt. Salt check the process by running \"ps | grep SIG\" (for Gentoo System) to check the running state, where SIG is the string to search for when looking for the service process with ps. Previously, we define sig as \"/usr/sbin/redis-server /etc/redis/a.conf\". Since the ps output is identical for our case, so we have no way to check the state of specified redis instance.\r\n\r\nSo, for our case, we prefer the old behavior, i.e, do not change the process title for the main redis process. Or add an option such as \"setproctitle [yes|no]\" to control this behavior.\r\n\r\nThere are some discussion in issue https://github.com/antirez/redis/issues/694 , https://github.com/antirez/redis/issues/1979, https://github.com/antirez/redis/issues/2081 , but in our case, keep the title untouched is the best option, thanks.",
        "comments": 16
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-11-19T08:30:27Z",
        "closed_at": "2016-12-16T15:54:14Z",
        "merged_at": null,
        "body": "@antirez Do you intend to add \":\" char to host: command?\r\nI just remove it. :)",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2016-11-16T19:34:42Z",
        "closed_at": "2016-12-16T16:43:20Z",
        "merged_at": "2016-12-16T16:43:20Z",
        "body": "currently there was a chance that an AOFRW fork will run during RDB parsing, which created huge CoW damage unnecessarily.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-11-16T03:16:19Z",
        "closed_at": "2016-11-18T09:45:09Z",
        "merged_at": "2016-11-18T09:45:09Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 7,
        "changed_files": 4,
        "created_at": "2016-11-13T20:42:40Z",
        "closed_at": "2017-07-11T07:50:01Z",
        "merged_at": null,
        "body": "When checking the slowlog for performance issues, it is often hard to tell\r\nwhere n slow command is coming from. This becomes a lot easier if you have\r\nmeaningful client names.\r\n\r\nExamples include the name of the app, the name of the host, a unique id\r\nper request.\r\n\r\nI also saw that #2043 is requsting the slowlog to include the db number, so I added that as well.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-11-13T15:32:38Z",
        "closed_at": "2016-12-19T07:20:02Z",
        "merged_at": "2016-12-19T07:20:02Z",
        "body": "Fixing typo in the usage of redis-benchmark",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-11-10T15:02:42Z",
        "closed_at": "2016-12-19T07:48:45Z",
        "merged_at": "2016-12-19T07:48:45Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-11-05T22:00:37Z",
        "closed_at": "2021-07-20T06:00:36Z",
        "merged_at": null,
        "body": "See conversation at https://twitter.com/Habbie/status/784404440661057536\r\n\r\nI did not find a neat way to do this in Redis, for lack of comment syntax in commands. So here is my alternative - make MONITOR log client names. I did not touch the Lua call because it appears it does not have the client name. Suggestions on that welcome.\r\n\r\nPasses all tests, which tells me MONITOR is not actually part of the tests today..",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-11-05T19:52:59Z",
        "closed_at": "2022-06-23T15:26:44Z",
        "merged_at": null,
        "body": null,
        "comments": 3
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-11-04T15:07:38Z",
        "closed_at": "2021-08-09T06:28:02Z",
        "merged_at": null,
        "body": "Currently when press Ctrl+C in redis monitor command redis-cli just died.\r\nThis patch just go to shell again.\r\n\r\n```\r\ncharsyam@ubuntu001:~/redis$ src/redis-cli\r\n127.0.0.1:6379> monitor\r\nOK\r\n^C(0.51s)\r\n127.0.0.1:6379> \r\n```",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-11-02T19:36:59Z",
        "closed_at": "2020-08-11T18:57:21Z",
        "merged_at": "2020-08-11T18:57:21Z",
        "body": "This option was very useful to me.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2016-11-01T06:14:04Z",
        "closed_at": "2016-11-02T10:09:12Z",
        "merged_at": null,
        "body": "Because in the rdbLoad() function aux field must had a '%' prefix.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-10-31T16:49:00Z",
        "closed_at": "2016-11-02T09:58:30Z",
        "merged_at": "2016-11-02T09:58:30Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2016-10-31T13:11:16Z",
        "closed_at": "2016-10-31T14:34:16Z",
        "merged_at": "2016-10-31T14:34:16Z",
        "body": "sizeof(client) returns sizeof(struct client*) instead of sizeof(struct client)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-10-28T11:49:44Z",
        "closed_at": "2016-11-02T10:00:24Z",
        "merged_at": "2016-11-02T10:00:24Z",
        "body": "",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2016-10-28T09:25:15Z",
        "closed_at": "2016-11-02T10:00:59Z",
        "merged_at": null,
        "body": "antirez 3.2\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-10-28T02:21:45Z",
        "closed_at": "2016-10-31T14:36:38Z",
        "merged_at": "2016-10-31T14:36:38Z",
        "body": "add missing `fclose()` in `src/redis-cli.c:cliLoadPreferences()`\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-10-24T15:28:00Z",
        "closed_at": "2016-11-02T14:18:44Z",
        "merged_at": "2016-11-02T14:18:44Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 92698,
        "deletions": 24158,
        "changed_files": 524,
        "created_at": "2016-10-23T05:21:32Z",
        "closed_at": "2016-11-02T14:19:19Z",
        "merged_at": null,
        "body": "2.8\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-10-21T17:59:45Z",
        "closed_at": "2016-10-31T14:40:47Z",
        "merged_at": "2016-10-31T14:40:46Z",
        "body": "fix https://github.com/antirez/redis/issues/3564 :-)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-10-20T14:38:16Z",
        "closed_at": "2018-11-23T08:44:32Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 44321,
        "deletions": 15672,
        "changed_files": 311,
        "created_at": "2016-10-13T14:31:59Z",
        "closed_at": "2016-12-26T06:03:24Z",
        "merged_at": null,
        "body": "### Problem\n\n`redis-cli` via unixsock run into infinite loop when cluster returns redirection.\n### Scenario\n1. connect to server via unixsock\n2. run some cluster command, and got some redirection like `MOVED`\n3. redis-cli sets `config.host` and `config.port` and `reissue`\n4. `cliConnect` always uses `config.hostsocket` if set\n5. goto 1\n#### 3.2.4\n\n``` shell\n% redis-cli --version\nredis-cli 3.2.4 (git:070d0471)\n% redis-cli -s /tmp/redis.sock get foo\n(error) MOVED 12182 127.0.0.1:6005\n% redis-cli -c -s /tmp/redis.sock get foo\n# -> infinite loop\n```\n#### PR\n\n``` shell\n% ./src/redis-cli --version\nredis-cli 3.2.4 (git:309fdc3a)\n% ./src/redis-cli -s /tmp/redis.sock get foo\n(error) MOVED 12182 127.0.0.1:6005\n% ./src/redis-cli -c -s /tmp/redis.sock get foo\n(nil)\n```\n\nThanks.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-10-13T06:54:20Z",
        "closed_at": "2017-09-18T10:18:16Z",
        "merged_at": "2017-09-18T10:18:16Z",
        "body": "The same code as the line above it.\n`if (strstr(color,\"red\")) ccode = 31;`\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2016-10-12T14:07:32Z",
        "closed_at": "2016-12-20T09:21:41Z",
        "merged_at": null,
        "body": "re: Issue #3512 \nReturning nullbulk instead of emptybulk for geodist seems very reasonable.\nBefore:\n\n```\n127.0.0.1:6379> geoadd mygeoset 8.66 49.52 Weinheim\n(integer) 1\n127.0.0.1:6379> geodist mygeoset Weinheim Weinheim\n\"0.0000\"\n127.0.0.1:6379> geodist mygeoset Weinheim Unknown\n(nil)\n127.0.0.1:6379> geodist unknownset Weinheim Unknown\n\"\"\n```\n\nAfter:\n\n```\n127.0.0.1:6379> geodist unknownset Weinheim Unknown\n(nil)\n```\n\nI'm less certain about the design intent for geo{pos,hash} but chose to add a db helper to return a multi-reply consistent in cardinality with the case of a key that is devoid of the target elements. eg.\n\nBefore:\n\n```\n127.0.0.1:6379> geohash Sicily Palermo Unknown Catania\n1) \"sqc8b49rny0\"\n2) (nil)\n3) \"sqdtr74hyu0\"\n127.0.0.1:6379> geohash unknownset Palermo Unknown Catania\n(empty list or set)\n```\n\nAfter:\n\n```\n127.0.0.1:6379> geohash unknownset Palermo Unknown Catania\n1) (nil)\n2) (nil)\n3) (nil)\n```\n\nI deferred making any changes at all to the georadius family of commands, pending feedback from @antirez \n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2016-10-11T19:32:42Z",
        "closed_at": "2021-07-15T15:18:06Z",
        "merged_at": null,
        "body": "This code allows you to convert a GEODIST value from meter (default) to nautical mile.\n\n127.0.0.1:6379> GEOADD Sicily 13.361389 38.115556 \"Palermo\" 15.087269 37.502669 \"Catania\"\n(integer) 2\n127.0.0.1:6379> GEODIST Sicily Palermo Catania\n\"166274.1516\"\n127.0.0.1:6379> GEODIST Sicily Palermo Catania nm\n\"89.7809\"\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-10-09T16:40:44Z",
        "closed_at": "2021-03-14T16:08:24Z",
        "merged_at": null,
        "body": "Add `call` command usage in README.\n\n``` bash\n$ ./create-cluster\nUsage: ./create-cluster [start|create|stop|watch|tail|call|clean]\nstart       -- Launch Redis Cluster instances.\ncreate      -- Create a cluster using redis-trib create.\nstop        -- Stop Redis Cluster instances.\nwatch       -- Show CLUSTER NODES output (first 30 lines) of first node.\ntail <id>   -- Run tail -f of instance at base port + ID.\ncall        -- Run the given command to each nodes.\nclean       -- Remove all instances data, logs, configs.\n```\n\n<!-- Reviewable:start -->\n\n---\n\nThis change is [<img src=\"https://reviewable.io/review_button.svg\" height=\"34\" align=\"absmiddle\" alt=\"Reviewable\"/>](https://reviewable.io/reviews/antirez/redis/3548)\n\n<!-- Reviewable:end -->\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-10-09T16:14:29Z",
        "closed_at": "2016-11-02T10:02:32Z",
        "merged_at": "2016-11-02T10:02:32Z",
        "body": "Remove unused variable.\n\n<!-- Reviewable:start -->\n\n---\n\nThis change is [<img src=\"https://reviewable.io/review_button.svg\" height=\"34\" align=\"absmiddle\" alt=\"Reviewable\"/>](https://reviewable.io/reviews/antirez/redis/3547)\n\n<!-- Reviewable:end -->\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-10-09T06:05:15Z",
        "closed_at": "2021-08-12T05:03:05Z",
        "merged_at": null,
        "body": "Lists the cached lua script\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2016-10-06T11:28:03Z",
        "closed_at": "2021-03-16T14:43:53Z",
        "merged_at": "2021-03-16T14:43:53Z",
        "body": "prettify\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-10-04T19:43:52Z",
        "closed_at": "2021-06-28T13:45:12Z",
        "merged_at": null,
        "body": "Minor cleanup left over from https://github.com/antirez/redis/commit/6854c7b9ee16595f74a773b911b6a195384e9665#diff-dc7ec9f7ebd3abdba3c14c2fb519a974L311\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-09-30T18:21:24Z",
        "closed_at": "2018-03-22T16:14:41Z",
        "merged_at": null,
        "body": "just simple refactoring for kevent check\n\nfilter can have only one value.\nso we reduce if condition check using if and else if\n\n``` c\nif (e->filter == EVFILT_READ) mask |= AE_READABLE;\nif (e->filter == EVFILT_WRITE) mask |= AE_WRITABLE;\n```\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2016-09-29T11:07:24Z",
        "closed_at": "2021-07-15T12:36:25Z",
        "merged_at": null,
        "body": "I want to set a key that will expire tomorrow\u3002Now I have to `SET` first, then `EXPIRE`.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-09-27T09:58:29Z",
        "closed_at": "2017-07-11T10:29:53Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-09-27T00:25:05Z",
        "closed_at": "2021-07-29T12:10:17Z",
        "merged_at": null,
        "body": "The earlier behavior was to exit out if bind to any ip failed. The new\nbehavior allows redis-server to come up if even a single bind succeeds.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-09-26T18:03:02Z",
        "closed_at": "2016-12-20T12:38:42Z",
        "merged_at": null,
        "body": "this is in order to avoid memory corruptions as show in #3519\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-09-25T22:55:59Z",
        "closed_at": "2016-09-27T00:11:02Z",
        "merged_at": null,
        "body": "The earlier behavior was to exit out if bind to any ip failed. The new\nbehavior allows redis-server to come up if even a single bind succeeds.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-09-24T06:55:23Z",
        "closed_at": "2016-11-02T10:04:52Z",
        "merged_at": "2016-11-02T10:04:52Z",
        "body": "in replicationFeedSlaves in replication.c \nwe don't need to listrewind for server.slaves\njust we can use slaves parameters\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2016-09-20T17:09:21Z",
        "closed_at": "2016-09-24T22:16:31Z",
        "merged_at": null,
        "body": "- Changing AF_INET to AF_UNSPEC\n- Only use AF_UNSPEC for the check, removing the IPv4-centric code\n- Depending on the environment, getaddrinfo() result might be cached by\n  e.g. nscd\n\nThe original code caused problems on our IPv6-only production systems.\nI tested the patch on other systems, both dual-stack and single-stack deployments and see no issues introduced.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 50247,
        "deletions": 20451,
        "changed_files": 303,
        "created_at": "2016-09-18T01:38:54Z",
        "closed_at": "2016-11-02T09:59:05Z",
        "merged_at": null,
        "body": "fix memory error on module unload\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-09-17T14:56:18Z",
        "closed_at": "2020-12-06T12:55:09Z",
        "merged_at": null,
        "body": "Dear @antirez ,\n\nI found a yet-undisclosed buffer over read vulnerability in redis-server 3.2.0 and later. I have reproduced this vulnerability by poc/poc.sh  in a Debian system and written a report (README.pdf) about it. A patch is also provided for your reference. You can find the POC and the report at [https://github.com/jingleyang/security_ctf/tree/master/opensource/redis_vul_0906_2016](https://github.com/jingleyang/security_ctf/tree/master/opensource/redis_vul_0906_2016)\n\nThis is a brief introduction about this vulnerability:\n\nThe last parameter named len in serverLogHexDump() can be controlled by a corrupted rdb database file. A very large value will trigger a buffer over read, resulting sensitive information on the stack, such as password, exposed.\n\nI also reported the issue to the Debian security team (security@debian.org), but I have not got any confirmed reply.\n\nThank you very much.\n\nBest Regards,\nJingyu\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2016-09-14T09:53:52Z",
        "closed_at": "2020-06-17T02:33:36Z",
        "merged_at": null,
        "body": "``` c\n#L2418 in server.c\nint prepareForShutdown(int flags) {\n//...\n}\n```\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-09-12T10:54:03Z",
        "closed_at": "2016-12-20T12:39:30Z",
        "merged_at": "2016-12-20T12:39:30Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-09-10T19:47:43Z",
        "closed_at": "2019-03-14T13:16:10Z",
        "merged_at": null,
        "body": "When implementing a module data type, callbacks are made without a\nRedisModuleCtx so it becomes impossible to free previously allocated\nstrings when necessary (e.g. on a free callback).\n\nThe other alternative is to push the context on all callbacks but it may\nbe reasonable to require that modules with custom data types will not\nrely on auto memory for internal types.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-09-09T15:23:58Z",
        "closed_at": "2020-12-17T12:49:43Z",
        "merged_at": null,
        "body": "",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-09-09T02:26:04Z",
        "closed_at": "2016-09-09T14:05:07Z",
        "merged_at": "2016-09-09T14:05:07Z",
        "body": "line 3088: dictDelete(modules,module->name);\nline 3089: moduleFreeModuleStructure(module);\n\nwill free module->name and module twice\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 90603,
        "deletions": 23398,
        "changed_files": 510,
        "created_at": "2016-09-08T01:45:09Z",
        "closed_at": "2016-09-09T14:08:18Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-08-29T21:45:16Z",
        "closed_at": "2020-05-08T14:25:02Z",
        "merged_at": null,
        "body": "When looking through the `config.c` file I noticed a few copy-paste fails. Thought I'd submit a PR to fix them real quick.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 17,
        "changed_files": 546,
        "created_at": "2016-08-29T11:20:40Z",
        "closed_at": "2016-08-31T08:04:48Z",
        "merged_at": null,
        "body": "dynamic adjust the loop events capacity when number of clients more than the event loop initial size.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-08-29T09:05:54Z",
        "closed_at": "2016-11-30T15:59:38Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-08-24T08:06:22Z",
        "closed_at": "2021-08-01T21:17:22Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2016-08-21T17:43:36Z",
        "closed_at": "2016-09-11T08:18:24Z",
        "merged_at": null,
        "body": "when no binding address was provided, binding should succeed only when both IPv4 and IPv6 are successfull (and fail if one of them is already taken), but it should not fail if the binding fails because the OS doesn't support IPv6.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2016-08-21T00:29:06Z",
        "closed_at": "2021-08-03T11:49:05Z",
        "merged_at": null,
        "body": "Previously any unit for hard/soft limit (e.g. GB) would be\nignored. Additionally, you could include a unit for the time and Redis\nwould not give a invalid argument error.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-08-20T23:36:58Z",
        "closed_at": "2021-07-15T22:42:05Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-08-17T17:32:21Z",
        "closed_at": "2021-06-28T13:49:27Z",
        "merged_at": null,
        "body": "If sh is NULL, maybe lead to crash.\nSo judge the null pointer before memset.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-08-16T09:17:10Z",
        "closed_at": "2021-07-29T14:33:10Z",
        "merged_at": null,
        "body": "1] symptom\n Even though, AOF is disabled, Infinite AOF Rewriting is happend.\n\n2] Precondition\n appendonly no\n aof_current_size > aof_request_min_size\n\n3] Why?\n current aof rewriting rule doesn't check AOF is disabled.\n it checks only server.aof_current_size > server.aof_rewrite_min_size and growth > percent\n\n and AOF is disabled, after rewriting AOF, it doesn't update aof_current_size\n\n4] solution?\n The easiest way is just check AOF state when check aof rewriting.\n and other solution is just updating aof_current_size in all codes.\n But I think this is more intutive solution :)\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-08-14T07:33:41Z",
        "closed_at": "2021-07-26T13:15:31Z",
        "merged_at": null,
        "body": "Hi antirez,\n\nI fixed a ressource leak and it should be easy to merge.\nCheers,\ntpltnt\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 182,
        "deletions": 90,
        "changed_files": 2,
        "created_at": "2016-08-13T16:37:30Z",
        "closed_at": "2020-08-08T12:24:05Z",
        "merged_at": null,
        "body": "Global variables defined in headers will cause duplicate symbols linkage error when this header is included in multiple source files. We should declare it by extern in header and define it in c source file.\nhttp://stackoverflow.com/questions/1433204/how-do-i-use-extern-to-share-variables-between-source-files-in-c\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-08-12T07:54:59Z",
        "closed_at": "2020-05-20T01:39:28Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 43,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-08-11T10:41:42Z",
        "closed_at": "2021-08-12T21:59:24Z",
        "merged_at": null,
        "body": "",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 58,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2016-08-10T01:59:14Z",
        "closed_at": "2020-09-02T13:05:49Z",
        "merged_at": null,
        "body": "a very quick and simple implement of list iterator for redis module, @antirez please have a review. #3438 \n",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 28845,
        "deletions": 12156,
        "changed_files": 220,
        "created_at": "2016-08-09T06:36:19Z",
        "closed_at": "2016-09-09T13:52:22Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-08-09T04:49:40Z",
        "closed_at": "2021-06-30T13:04:48Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-08-08T22:27:01Z",
        "closed_at": "2021-07-01T06:59:39Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1056,
        "deletions": 2,
        "changed_files": 7,
        "created_at": "2016-08-07T16:41:41Z",
        "closed_at": "2021-06-22T04:21:06Z",
        "merged_at": null,
        "body": "Adds Interval set support as sorted sets for this don't support overlap and/or boundary space\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-08-04T07:11:31Z",
        "closed_at": "2016-08-04T08:25:53Z",
        "merged_at": "2016-08-04T08:25:53Z",
        "body": "Fix warning: ISO C does not support '**FUNCTION**' predefined identifier\n[-Wpedantic]\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-08-01T19:49:31Z",
        "closed_at": "2016-08-02T08:43:05Z",
        "merged_at": "2016-08-02T08:43:05Z",
        "body": "The logic implemented in code was inconsistent with the intention that BGSAVE to be delayed when AOF rewrite was in progress as part of 3.2.2 release. I am attempting to fix this bug. \n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2016-07-28T13:33:36Z",
        "closed_at": "2020-02-17T19:14:23Z",
        "merged_at": null,
        "body": "How do people feel about the semantics of not replicating the publish to slaves? The module author can do their own replication, but I've included the cluster broadcast functionality of publishCommand. \n\nPerhaps this should just be the equivalent of RM_Call(\"publish\"...), and perform the replication? Perhaps multiple functions or boolean parameters so the user can specify how to replicate?\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 136,
        "deletions": 21,
        "changed_files": 6,
        "created_at": "2016-07-27T16:50:02Z",
        "closed_at": "2020-08-19T16:08:08Z",
        "merged_at": null,
        "body": "We are running Redis in 24/7/365 production where the password must be regularly updated for security reasons. It's nice if we can use multiple passwords to manage both old and new users like this.\n\n```\nrequirepass pass2016 pass2017 ...\n```\n\nThis PR offers the feature where the implementation is mostly copied from `bindaddrs`.\nAll tests have been passed on both `unstable` and `3.2.1` branches.\nThanks.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2016-07-26T07:03:38Z",
        "closed_at": "2020-09-07T21:14:26Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2016-07-24T12:56:06Z",
        "closed_at": "2021-05-09T10:25:22Z",
        "merged_at": null,
        "body": "Fixes https://github.com/antirez/redis/issues/3410\n\nAdds error printing in case selecting the database fails. Also moves interactive help setup to be invoked only for repl mode.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 89,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-07-23T18:50:00Z",
        "closed_at": "2017-11-27T11:34:25Z",
        "merged_at": null,
        "body": "Resolves #3399.\n\nAdds the ability to connect to a Redis server with a URI as specified by the [provisional specification](https://www.iana.org/assignments/uri-schemes/prov/redis).\n\nIf and when this PR is merged, https://github.com/antirez/redis-doc/pull/743 should also be considered for merging.\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 49,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2016-07-23T14:57:17Z",
        "closed_at": "2017-02-20T09:26:27Z",
        "merged_at": null,
        "body": "The bug described in #2883, #2857 is a pipes file descriptors leak on failed fork() in aof.c.\n\nCorresponding Tcl test is ongoing.\n\nTo verify behavior:\n- run redis_server\n- get pid\n- $ prlimit --nproc=0 --pid ${pid}\n- $ echo \"BGREWRITEAOF\" | ./redis-cli\n  will result into ERR\n- lsof +E -p ${pid} | grep FIFO\n\nbefore patch there will be leaked pipes.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-07-20T22:17:18Z",
        "closed_at": "2020-09-09T16:28:53Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2016-07-18T05:56:11Z",
        "closed_at": "2020-09-09T16:27:49Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-07-17T23:00:43Z",
        "closed_at": "2019-05-24T20:54:14Z",
        "merged_at": null,
        "body": "OSX has been renamed to macOS\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-07-16T11:34:59Z",
        "closed_at": "2020-09-05T20:05:15Z",
        "merged_at": null,
        "body": "Commit solve the minor fix for issue https://github.com/antirez/redis/issues/3379\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 250,
        "deletions": 1,
        "changed_files": 6,
        "created_at": "2016-07-11T15:01:37Z",
        "closed_at": "2019-11-19T09:55:43Z",
        "merged_at": "2019-11-19T09:55:43Z",
        "body": "These proposed API functions make it possible to implement custom DUMP/RESTORE kind of commands.\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2016-07-11T14:11:30Z",
        "closed_at": "2021-07-29T14:50:01Z",
        "merged_at": null,
        "body": "fix compile warning in gcc 5.3.1 in ubuntu 16.04\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2016-07-10T03:31:57Z",
        "closed_at": "2016-07-19T03:35:03Z",
        "merged_at": null,
        "body": "mstime is  tv.sec \\* 1000000  and then div 1000, that is unnecessary\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 138,
        "deletions": 12,
        "changed_files": 5,
        "created_at": "2016-07-05T14:57:57Z",
        "closed_at": "2019-10-31T20:36:57Z",
        "merged_at": null,
        "body": "The core requirement here is to be able to store and retrieve from RDB global state which is not associated with a specific key.\n\nI've also took the opportunity to propose a more generic mechanism that can be later extended to support additional hooks. The guide line here was to avoid per-hook prototype while still providing type safety (i.e. avoid passing void*'s all over).\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-07-04T20:34:51Z",
        "closed_at": "2016-09-09T09:01:16Z",
        "merged_at": null,
        "body": "$ redis-cli debug segfault\n\n```\n21175:M 04 Jul 23:31:50.719 # symbol: debugCommand (base 0x462a10), in module: /home/oran/work/redis/src/redis-server 127.0.0.1:6379 (base: 0x400000)\n21175:M 04 Jul 23:31:50.719 # dump of function (hexdump):\n415741564155415455534889fb4881ec58100000448b673064488b042528000000488984244810000031c04183fc010f847b0300004c8b6f38be28984e00498b4508488b68084889efe882cefbff85c00f84fa010000beaf2d4f004889efe86dcefbff85c00f8465010000beb82d4f004889efe858cefbff85c00f8488010000bec02d4f004889efe843cefbff85c00f8473010000be1bce4f004889efe82ecefbff85c00f845e040000bed22d4f004889efe819cefbff85c00f84f70c0000bee02d4f004889efe804cefbff85c00f84ec030000be032e4f004889efe8efcdfbff85c00f84d7020000be0d9c4e004889efe8dacdfbff85c00f85420300004183fc030f842b060000be8b2e4f004889efe8bbcdfbff85c0750e418d4424fd83f8010f86c9060000bea52e4f004889efe89ccdfbff85c0750a4183fc020f8424080000beac2e4f004889efe881cdfbff85c00f85d90300004183fc030f84c0080000bec42e4f004889efe862cdfbff85c00f855a040000662e0f1f840000000000bee52e4f004889efe843cdfbff85c0750a4183fc020f8412090000be4f2f4f004889efe828cdfbff85c00f85a00400004183fc030f84140800004889eabee8274f004889df31c0e8345dfdffeb0a6690c60425ffffffff78488b8424481000006448330425280000000f85010b00004881c4581000005b5d415c415d415e415fc30f1f80000000004531f64183fc0248c7442430000000000f8f520100004889efbeb82d4f00e8a5ccfbff83f8014c89f619ff83e703e88584fcffbe80264f004889dfe8485afdffeb8e660f1f4400004889dfe88838fdff4889dfbe18204f004889c5e8c864fdff4889dfbe50204f00e8bb64fdff4889dfbe80204f00e8ae64fdff4889dfbeb8204f00e8a164fdff4889dfbe10214f00e89464fdff4889dfbe38214f00e88764fdff4889dfbe78214f00e87a64fdff4889dfbec0214f00e86d64fdff4889dfbe08224f00e86064fdff4889dfbe58224f00e85364fdff4889dfbee0224f00e84664fdff4889dfbe30234f00e83964fdff4889dfbe78234f00e82c64fdff4889dfbe30244f00e81f64fdff4889dfbee8244f00e81264fdff4889dfbe70254f00e80564fdff4889dfbeb8254f00e8f863fdff4889dfbe08264f00e8eb63fdff4889dfbe40264f00e8de63fdffba130000004889ee4889dfe86e70fdffe971feffff660f1f840000000000498b7510488d54243031c94889dfe8ed9dfdff85c00f854dfeffff488b4424304885c00f8887000000488b53384989c6488b5208488b6a08e971feffff0f1f00bed01f4f00e8d658fdffe919feffff90833d311a2d00010f845703000031d231f6bfffffffffe855c7fdff488b3d1e1a2d00e8e9cbffff85c00f85b900000048c705b61a2d0000000000bee0264f00bf0300000031c0e82550fcff488b35061e2d004889dfe85651fdffe9b9fdffff90488b433848c744243000000000488b4008488b6808e9e4fdffff660f1f440000be692e4f004889efe883cafbff85c00f85b3fcffff4183fc030f85a9fcffff498b4510488b7008488b4310488b38e80d3bfcff4885c00f8479080000488b5008488b280fb602a80f7508a8700f84fd060000be702e4f004889dfe8f157fdffe934fdffff0f1f4000488b35711d2d004889dfe8b950fdffe91cfdffff0f1f4000488b3d111a2d00e85437feff85c075d831d231f6bfffffffffe852c6fdff488b3df3192d00e83649feff85c00f8428020000beb8264f004889dfe89157fdffe9d4fcffff0f1f400048c7c7ffffffffe86ce5fcff4889c7e854e7fcff488b35f51c2d004889dfe84550fdffe9a8fcffffbeb22e4f004889efe893c9fbff85c0753f4183fc030f8535fcffff498b451031f6ba0a000000488b7808e861d0fbff488b35b21c2d004889df890525182d00e8fc4ffdffe95ffcffff0f1f8000000000bec42e4f004889efe843c9fbff85c0753f4183fc030f85e5fbffff498b451031f6ba0a000000488b7808e811d0fbff488b35621c2d004889df8905ed1b2d00e8ac4ffdffe90ffcffff0f1f8000000000bec1e34e004889efe8f3c8fbff85c00f859bfbffff4183fc030f8591fbffffbe\n\n```\n\n$ xxd -r -p ~/temp/dump.hex ~/temp/dump.bin\n\n```\n$ objdump -D -b binary -m i386:x86-64 ~/temp/dump.bin |head\n/home/oran/temp/dump.bin:     file format binary\n\nDisassembly of section .data:\n\n0000000000000000 <.data>:\n       0:   41 57                   push   %r15\n       2:   41 56                   push   %r14\n       4:   41 55                   push   %r13\n\n```\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 25,
        "changed_files": 17,
        "created_at": "2016-07-03T09:25:19Z",
        "closed_at": "2020-09-09T16:27:52Z",
        "merged_at": null,
        "body": "This is a safe to apply commit with no functional changes.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-07-01T05:36:20Z",
        "closed_at": "2016-07-05T13:12:06Z",
        "merged_at": "2016-07-05T13:12:06Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1900,
        "deletions": 201,
        "changed_files": 11,
        "created_at": "2016-06-29T16:27:08Z",
        "closed_at": "2018-03-22T16:38:05Z",
        "merged_at": null,
        "body": "Adding cmd acls for redis. but it needs more reivews.\n1. adding user command acl.\n\nIt supports only below groups. we get this information from command's flag\n-readonly\n-write\n-slow\n-admin\n-pubsub\n-scripting\n-all\n\nsample acl files\n\n```\nantirez \"antirez\" +#all\ncharsyam \"passwd\" +#all -#admin\ndefault \"\" +#readonly\n```\n\nand change acls config name to users-acls.\n\nif you set users-acls, and client get default user right when they connected.\n\n@antirez please review this :)\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-06-29T11:43:14Z",
        "closed_at": "2016-12-20T14:39:04Z",
        "merged_at": null,
        "body": "From the GEORADIUS wiki \"The command default is to return unsorted items.\"  But it seems that the actual default is an ascending ordered set, same as if the ASC option was used.\n\n```\nGEOADD a 0 0.18 1\nGEOADD a 0 0.12 2\nGEOADD a 0 0.30 3\nGEOADD a 0 0.21 4\nGEOADD a 0 0.25 5\nGEORADIUS a 0 0 40 km withcoord\n\n1) 1) \"2\"\n   2) 1) \"0.00000268220901489\"\n      2) \"0.12000003648600455\"\n2) 1) \"1\"\n   2) 1) \"0.00000268220901489\"\n      2) \"0.1799994210487128\"\n3) 1) \"4\"\n   2) 1) \"0.00000268220901489\"\n      2) \"0.21000038069065141\"\n4) 1) \"5\"\n   2) 1) \"0.00000268220901489\"\n      2) \"0.2500008153061799\"\n5) 1) \"3\"\n   2) 1) \"0.00000268220901489\"\n```\n\nThis PR adds a new option \"RAND\" to be used when we want the items in the result set to be in random order.\n\nExample usage:\n\n```\nGEORADIUS a 0 0 40 km withcoord rand\n```\n\nThis is especially helpful when we only want a random subset of the results such as:\n\n```\nGEORADIUS a 0 0 40 km withcoord rand count 3\n```\n- The code below has not be tested, or even compiled. It is a straight edit on a single file so as to provide a platform for discussion.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-06-29T09:20:24Z",
        "closed_at": "2018-01-09T17:58:31Z",
        "merged_at": "2018-01-09T17:58:31Z",
        "body": "The `module` command cannot be properly called in the cluster mode due to its wrong `firstkey`, `lastkey`, and `keystep` in `redisCommandTable`.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2016-06-28T02:17:17Z",
        "closed_at": "2020-09-09T16:27:55Z",
        "merged_at": null,
        "body": "It should now refer to server.c file.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-06-26T02:04:41Z",
        "closed_at": "2018-08-23T22:49:39Z",
        "merged_at": null,
        "body": "Honoring `$DESTDIR` is useful for staged installs (https://www.gnu.org/prep/standards/html_node/DESTDIR.html)\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-06-24T23:32:17Z",
        "closed_at": "2021-06-28T14:27:26Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-06-23T19:32:53Z",
        "closed_at": "2016-06-24T06:58:23Z",
        "merged_at": "2016-06-24T06:58:23Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-06-23T16:38:14Z",
        "closed_at": "2016-08-05T13:10:41Z",
        "merged_at": "2016-08-05T13:10:41Z",
        "body": "After the cluster meet and join was done, when the summary was shown, it\nwas giving info regarding the nodes. This fix ensures that confusion\nwhere the slaves were shown as masters.\nFix would be to reset the nodes and reload the cluster information\nbefore checking the cluster status after creating it.\nTo fix #3337\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-06-23T11:58:33Z",
        "closed_at": "2016-06-23T14:06:15Z",
        "merged_at": "2016-06-23T14:06:15Z",
        "body": "A 64-bit integer ranges from -9223372036854775808 and 9223372036854775807. The maximum number of chars is 20 (not 21).  Therefore, a string with 21 chars is not necessarily tried to convert to a 64-bit integer.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2016-06-22T18:03:56Z",
        "closed_at": "2016-06-23T14:16:29Z",
        "merged_at": "2016-06-23T14:16:29Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2016-06-22T14:33:42Z",
        "closed_at": "2016-06-23T14:19:14Z",
        "merged_at": "2016-06-23T14:19:14Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2016-06-21T07:23:46Z",
        "closed_at": "2016-06-23T14:12:07Z",
        "merged_at": "2016-06-23T14:12:07Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 38,
        "changed_files": 13,
        "created_at": "2016-06-20T20:13:36Z",
        "closed_at": "2016-06-23T10:29:53Z",
        "merged_at": "2016-06-23T10:29:53Z",
        "body": "Use case: it's useful to use `const RedisModuleString *` in Redis modules where we need a reference to a string and we want to be sure we won't accidentally change it (or, for example, pass it as a reply etc.).\n\nTo do that, Redis Module API needed to be a bit more const friendly (which further required other parts of Redis to do the same).\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-06-20T10:17:48Z",
        "closed_at": "2018-10-31T15:33:18Z",
        "merged_at": null,
        "body": "Sentinel itself do not have any configs by default to support authentication.\n\nSince sentinel is built kind of wrapper over the redis-server and so all configs are accessible, the authentication feature can be enabled manually adding the configuration `requirepass` in `sentinel.conf`.\n\nAnd then we need to add the AUTH command to the sentinel's supported command list for it to work properly. Committing same here to fix #3279\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-06-16T23:52:33Z",
        "closed_at": "2016-06-23T14:39:51Z",
        "merged_at": "2016-06-23T14:39:51Z",
        "body": "The comment should be \"argv[0] contains ...\" since `strstr()` is used.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-06-16T20:48:44Z",
        "closed_at": "2016-07-29T09:30:19Z",
        "merged_at": null,
        "body": "The default CLI history file (`.rediscli_history`) is being written with default user file creation mask, which makes the file readable by group and others. This type of file may contain sensible information and should not be world-readable.\n\nCloses #3284\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2016-06-16T08:56:21Z",
        "closed_at": "2017-06-30T05:44:32Z",
        "merged_at": null,
        "body": "Fixed bitcount bug: when end < -strlen, return non-zero value.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-06-16T07:40:24Z",
        "closed_at": "2021-04-27T12:25:21Z",
        "merged_at": null,
        "body": "LRANGE_500 is supposed to take `LRANGE mylist 0 499` as raw request string.\noriginal code:\n\n```\nif (test_is_selected(\"lrange\") || test_is_selected(\"lrange_500\")) {\n    len = redisFormatCommand(&cmd,\"LRANGE mylist 0 449\");\n    benchmark(\"LRANGE_500 (first 450 elements)\",cmd,len);\n    free(cmd);\n}\n```\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-06-15T15:05:27Z",
        "closed_at": "2016-06-17T05:30:33Z",
        "merged_at": null,
        "body": "This minor fix ensure that cli do not try for a new connection on\ndefault port if a custom port is mentioned. The cliInitHelp() and\ncliIntegrateHelp(), can be done after parsing the command line argument.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2016-06-15T13:35:47Z",
        "closed_at": "2016-06-23T10:17:16Z",
        "merged_at": null,
        "body": "@antirez here's a quick one, it's needed for sure.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2016-06-14T20:07:58Z",
        "closed_at": "2016-06-15T09:18:04Z",
        "merged_at": "2016-06-15T09:18:04Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-06-07T21:14:35Z",
        "closed_at": "2016-06-10T06:59:50Z",
        "merged_at": "2016-06-10T06:59:50Z",
        "body": "Fix a building-on-MSYS2 `error: unknown type name \u2018fd_set\u2019`\n\nThis resolves the following errors/warnings:\n\n```\nIn file included from ae.c:58:0:\nae_select.c:35:5: error: unknown type name \u2018fd_set\u2019\n     fd_set rfds, wfds;\n     ^\nae_select.c:38:5: error: unknown type name \u2018fd_set\u2019\n     fd_set _rfds, _wfds;\n     ^\nae_select.c: In function \u2018aeApiCreate\u2019:\nae_select.c:45:5: warning: implicit declaration of function \u2018FD_ZERO\u2019 [-Wimplicit-function-declaration]\n     FD_ZERO(&state->rfds);\n     ^\nae_select.c: In function \u2018aeApiResize\u2019:\nae_select.c:53:20: error: \u2018FD_SETSIZE\u2019 undeclared (first use in this function)\n     if (setsize >= FD_SETSIZE) return -1;\n                    ^\nae_select.c:53:20: note: each undeclared identifier is reported only once for each function it appears in\nae_select.c: In function \u2018aeApiAddEvent\u2019:\nae_select.c:64:29: warning: implicit declaration of function \u2018FD_SET\u2019 [-Wimplicit-function-declaration]\n     if (mask & AE_READABLE) FD_SET(fd,&state->rfds);\n                             ^\nae_select.c: In function \u2018aeApiDelEvent\u2019:\nae_select.c:72:29: warning: implicit declaration of function \u2018FD_CLR\u2019 [-Wimplicit-function-declaration]\n     if (mask & AE_READABLE) FD_CLR(fd,&state->rfds);\n                             ^\nae_select.c: In function \u2018aeApiPoll\u2019:\nae_select.c:80:46: error: \u2018fd_set\u2019 undeclared (first use in this function)\n     memcpy(&state->_rfds,&state->rfds,sizeof(fd_set));\n                                              ^\nae_select.c:83:14: warning: implicit declaration of function \u2018select\u2019 [-Wimplicit-function-declaration]\n     retval = select(eventLoop->maxfd+1,\n              ^\nae_select.c:91:43: warning: implicit declaration of function \u2018FD_ISSET\u2019 [-Wimplicit-function-declaration]\n             if (fe->mask & AE_READABLE && FD_ISSET(j,&state->_rfds))\n                                           ^\n```\n\nThis was encountered when building https://github.com/armon/statsite/pull/165 using MSYS2 (for Windows).\n\nThe fix was recommended here: https://github.com/Claudio-Sjo/HID_linux_xbmc_driver/issues/1.\n\nI verified this change did not break the build on Linux; haven't had a chance to check Mac OS X yet.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-06-07T10:39:34Z",
        "closed_at": "2016-06-07T15:20:47Z",
        "merged_at": "2016-06-07T15:20:47Z",
        "body": "If a module is composed of several files, gcc complains about `RedisModule_Init()` defined but not used in all files except the one implementing RedisModule_OnLoad().\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-06-07T06:44:49Z",
        "closed_at": "2016-06-10T07:14:30Z",
        "merged_at": "2016-06-10T07:14:30Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 56,
        "deletions": 42,
        "changed_files": 3,
        "created_at": "2016-06-05T14:51:41Z",
        "closed_at": "2016-06-10T08:10:08Z",
        "merged_at": "2016-06-10T08:10:08Z",
        "body": "This is part of #1232 adapted to the recent Redis codebase.\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 10,
        "changed_files": 3,
        "created_at": "2016-06-05T14:50:48Z",
        "closed_at": "2016-06-10T08:20:28Z",
        "merged_at": "2016-06-10T08:20:28Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2016-06-05T10:30:12Z",
        "closed_at": "2016-06-10T08:18:49Z",
        "merged_at": "2016-06-10T08:18:49Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 40,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2016-06-05T08:55:30Z",
        "closed_at": "2016-06-13T07:31:59Z",
        "merged_at": "2016-06-13T07:31:59Z",
        "body": "There is a need to provide modules with configuration data.  In some cases it must be done at load time, so creating a custom module config command doesn't work.\n\nI also considered using the current Redis config file/CONFIG commands but I don't see how this can be done in a simple and clean way.\n\nThe best alternative I came up with is simply passing arguments at load time.  This works well in all module loading approaches (i.e. MODULE LOAD command or config argument) and is even quite elegant (IMHO) at the API level because the RedisModule_OnLoad prototype is then equivalent to command callbacks.\n\nThe only problem I see is breaking the API but I believe it may be acceptable to do in this early stage.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-30T09:00:57Z",
        "closed_at": "2016-06-15T13:53:19Z",
        "merged_at": "2016-06-15T13:53:19Z",
        "body": "should be OBJ_ENCODING_EMBSTR_SIZE_LIMIT\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-05-30T08:38:10Z",
        "closed_at": "2016-06-15T10:15:34Z",
        "merged_at": "2016-06-15T10:15:34Z",
        "body": "There are some points to be considerd  when start/end index are both negative and bigger than strlen.\nFor example ,when we send 2 command to redis:\n    set key 1111;\n   \u2460 bitcount key -6 -7  or \u2461 bitcount key -5 -5_\n- \u2460 should return 0 for logic consistency, (bitcount key -3 -4 returns 0).\n- but \u2461's return value varies when we treat out-of-range area as 0 or we just treat negative index(bigger than strlen) as 0.\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2016-05-29T22:54:16Z",
        "closed_at": "2016-06-15T10:51:15Z",
        "merged_at": "2016-06-15T10:51:15Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-28T17:03:09Z",
        "closed_at": "2016-05-30T15:47:23Z",
        "merged_at": "2016-05-30T15:47:23Z",
        "body": "The existing `R` flag appears to be sufficient and there's no apparent reason why the command should be blocked.\n\nRef: #2139\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2016-05-28T09:58:08Z",
        "closed_at": "2021-07-12T07:48:46Z",
        "merged_at": null,
        "body": "Original config.c has ignored invalid memory sizes in `redis.conf`\nand set the default value(= 0) to such a field.\n\nMemory sizes are sensitive, so Redis should fail when\n`redis.conf` has wrong memory sizes.\n\nI have got an experience that unexpected maxmemory was set\nand I had not noticed that for a while when I wrote \"maxmemory 1.5gb\" in `redis.conf`.\nThis is because `memtoll` fails when a decimal is given.\n\nIt is true that I mistook a configuration rule\nbut Redis could have inform me of the mistake.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-26T05:12:03Z",
        "closed_at": "2016-06-15T15:24:11Z",
        "merged_at": "2016-06-15T15:24:11Z",
        "body": "ref: https://github.com/antirez/redis/issues/3273\n\nthis commit introduce this bug, https://github.com/antirez/redis/commit/1029276c0d46e643a5740120d44a9cce8ba652b9#diff-45b8aaf53e1ac2b9134c1e0ad96a0e03L3684\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1056,
        "deletions": 2,
        "changed_files": 7,
        "created_at": "2016-05-25T19:05:51Z",
        "closed_at": "2016-08-07T16:30:08Z",
        "merged_at": null,
        "body": "The fourth code update from base to support interval sets\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2016-05-24T23:11:18Z",
        "closed_at": "2021-07-26T12:56:37Z",
        "merged_at": null,
        "body": "Since two methods in dict.h both use `dict *d` as argument, but the dict.c uses `dict *ht`, so I guess using  `d` will make it more clear that it represents dictionary rather than hash table.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2016-05-24T11:59:13Z",
        "closed_at": "2016-06-16T10:52:36Z",
        "merged_at": "2016-06-16T10:52:36Z",
        "body": "this was a bug in the recent refactoring: bee963c4459223d874e3294a0d8638a588d33c8e\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-23T21:56:42Z",
        "closed_at": "2017-07-03T11:15:37Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-05-23T11:06:52Z",
        "closed_at": "2016-06-16T10:57:59Z",
        "merged_at": "2016-06-16T10:57:59Z",
        "body": "> geoadd asdf 1 2 asdf\n> (integer) 1\n> georadius asdf 1 2 3 fv\n> (error) ERR unsupported unit provided. please use m, km, ft, mi\n> ping\n> (error) ERR radius cannot be negative\n> ping\n> (error) ERR radius must be >= 0\n> ping\n> PONG\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-23T08:45:19Z",
        "closed_at": "2016-06-16T10:59:05Z",
        "merged_at": "2016-06-16T10:59:05Z",
        "body": "wasn't sure if you want to update with more details (which will make it fragile), or reduce detail and make it more stable.\nif you don't like it, reject..\n\nfound by @benamizilber\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2016-05-23T01:52:17Z",
        "closed_at": "2017-02-15T17:40:54Z",
        "merged_at": null,
        "body": "FIXED a typo dict *ht should be dict *d\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-05-22T17:37:56Z",
        "closed_at": "2016-06-17T12:48:41Z",
        "merged_at": "2016-06-17T12:48:41Z",
        "body": "support for negative values for this field was only in the config file parsing.\ni've verified that all the calls to config_set_numerical_field defined min as 0 or positive number, so there was no need for the excess negative check.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-05-21T11:50:33Z",
        "closed_at": "2016-05-22T21:32:03Z",
        "merged_at": "2016-05-22T21:32:03Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2016-05-19T10:55:29Z",
        "closed_at": "2016-06-23T07:39:58Z",
        "merged_at": null,
        "body": "This is an alternative to #3244  - choose either one. \n\nIt uses a heuristic suggested over IRC, to check either the oldest element, or scan from top to bottom. \nI think it's an overkill for most cases, but let me know what you think.\n\nBTW Another approach would be zigzag scanning (bottom, top, bottom+1, top-1, .... middle). \n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2016-05-19T09:20:13Z",
        "closed_at": "2016-06-23T06:59:38Z",
        "merged_at": "2016-06-23T06:59:38Z",
        "body": "This fixes two issues:\n1. If we have a lot of objects allocated and we're freeing, it's a better heuristic to start from the recent elements than from the oldest elements if we want to find the element freed.\n2. The loop didn't have a `break` after marking the element as freed, making it slow as the element list grew.\n\nIn a huge SCAN loop adding this made it run X100 faster\n",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2016-05-19T05:20:04Z",
        "closed_at": "2020-04-09T17:15:27Z",
        "merged_at": null,
        "body": "hi, as we discussed, adding a DEBUG command like RELOAD that doesn't first save to disk.\ncan be used by power users to load data without the need to restart the process.\n\ni decided to support AOF too in this function and also concluded it will be more powerful if it won't flush the database (emptyDb) before loading the file (people can use it to load multiple rdb / aof files)\n\nplease note however, that FLUSHALL command saves an empty RDB or adds a flush command to the AOF, so if someone wishes to do a combination of FLUSHALL and then DEBUG LOAD, he'll have to put the file he wishes to load in the right location only after the FLUSHALL.\nstill since this is a debug command i feel making it more raw and powerful is the right thing, if you think otherwise let me know.\n",
        "comments": 18
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-18T08:49:34Z",
        "closed_at": "2016-12-20T14:39:57Z",
        "merged_at": "2016-12-20T14:39:57Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2016-05-17T19:58:49Z",
        "closed_at": "2021-07-13T09:27:37Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2016-05-17T13:48:56Z",
        "closed_at": "2016-05-18T14:15:15Z",
        "merged_at": "2016-05-18T14:15:15Z",
        "body": "This caused crashes in my module, seems like you reset the field ptr even if it wasn't a c string.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-16T17:13:38Z",
        "closed_at": "2016-05-17T07:49:11Z",
        "merged_at": "2016-05-17T07:49:11Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2016-05-16T12:08:59Z",
        "closed_at": "2021-07-13T11:22:15Z",
        "merged_at": null,
        "body": "When I run a redis server instance listening on port 4000 and  use the redis-cli with arguement \"-p 4000\" to connect it, I get the following warnings:\n\n```\n./redis-cli -p 4000\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\n127.0.0.1:4000> \n```\n\nIf I run the redis-cli without any argument, redis client give the warnings:\n\n```\n\n./redis-cli\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\nnot connected> \n\n```\n\nTo prevent such wrong alerts, I add a variable parse_flag for struct config of redis-cli.c to check whether redis client has got user's arguments.\n\nAfter add this variable,  redis-cli gives the right alerts: \n\n```\nlinux-vt7e:~/test/c/redis/redis3/src> ./redis-cli   \nCould not connect to Redis at 127.0.0.1:6379: Connection refused\nnot connected> \nlinux-vt7e:~/test/c/redis/redis3/src> ./redis-cli -p 4000\n127.0.0.1:4000> \n```\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-16T10:31:19Z",
        "closed_at": "2020-09-07T20:39:27Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-05-15T20:52:27Z",
        "closed_at": "2021-11-27T11:38:23Z",
        "merged_at": null,
        "body": "This commit fixes two kill all issues:\n1. Client kill id 0\n2. Client kill skipme no (or client kill skipme yes)\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-05-15T14:51:52Z",
        "closed_at": "2016-05-15T19:35:49Z",
        "merged_at": null,
        "body": "This commit fixes two kill all issues:\n1. Client kill id 0\n2. Client kill skipme no (or client kill skipme yes)\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-14T20:54:05Z",
        "closed_at": "2016-05-21T12:04:30Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 35440,
        "deletions": 12661,
        "changed_files": 236,
        "created_at": "2016-05-14T01:19:38Z",
        "closed_at": "2016-06-10T07:02:38Z",
        "merged_at": null,
        "body": "try\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-13T09:54:17Z",
        "closed_at": "2020-09-07T20:39:48Z",
        "merged_at": null,
        "body": "SLAVECONF->REPLCONF in comment\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 64,
        "deletions": 31,
        "changed_files": 1,
        "created_at": "2016-05-13T09:48:58Z",
        "closed_at": "2016-06-15T09:20:01Z",
        "merged_at": "2016-06-15T09:20:01Z",
        "body": "This PR adds the ability to execute the installation script non-interactively, useful for automated provisioning scripts such as Chef, Puppet, Ansible, Salt, etc.\nSimply feed the environment variables into the install script to skip the prompts.\n\nFor debug and verification purposes, the script will still output the selected config variables.\nThe plus side is that the environment variables also support command substitution (see REDIS_EXECUTABLE).\n\n```\nsudo REDIS_PORT=1234 REDIS_CONFIG_FILE=/etc/redis/1234.conf REDIS_LOG_FILE=/var/log/redis_1234.log REDIS_DATA_DIR=/var/lib/redis/1234 REDIS_EXECUTABLE=`command -v redis-server` ./utils/install_server.sh\n\nWelcome to the redis service installer\nThis script will help you easily set up a running redis server\n\nSelected config:\nPort           : 1234\nConfig file    : /etc/redis/1234.conf\nLog file       : /var/log/redis_1234.log\nData dir       : /var/lib/redis/1234\nExecutable     : /usr/local/bin/redis-server\nCli Executable : /usr/local/bin/redis-cli\nCopied /tmp/1234.conf => /etc/init.d/redis_1234\nInstalling service...\nSuccessfully added to chkconfig!\nSuccessfully added to runlevels 345!\nStarting Redis server...\nInstallation successful!\n```\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 399,
        "deletions": 86,
        "changed_files": 11,
        "created_at": "2016-05-12T17:47:09Z",
        "closed_at": "2016-09-21T09:49:41Z",
        "merged_at": null,
        "body": "Hi,\nThese are some improvements and optimizations which may required a closer inspection.\n- DEBUG MEM-OVERHEAD\n- DEBUG KEY-RAM-SIZE (objectComputeSize)\n- INFO MEMORY (added ram-overhead and key-overhead)\n- optimizations to avoid double dict lookup (hashing, strcmp, etc)\n\ni had to do quite a lot changes to adapt this code from my 3.2 to the unstable (since it's so different). i hope i got it right.\nif you want to backport these to 3.2 / 3.0 maybe i can help.\n\nplease first look at sentinel.c / lazyfree.c / zset.c to see the benefit of not doing key hashing / lookup twice (find+del / find+add) before you look at the massive changes in dict.c.\nthis way you'll know if you think it wroth to do a careful review of how i did that.\n\nthe changes in dict.c are explained at the bottom.\n\nBTW, i only applied these optimizations on a few random places, i guess there are many other places in the code which do find+del / find+add in which we can do that optimization.\n\nbesides the places in redis code that call find+add, the original implementation of dictReplace had repeated lookup internally (fixed)\n\nthe code in objectComputeSize() needs your review, i'm not 100% sure it is correct.\n\nthe code in debug.c looks a bit dirty since i originally wrote it for my own debugging, please feel free to refactor / rename it if you choose to accept it.\n\nhere's a description of the changes in dict.c (i'm quite sure there are no bugs there):\n- add dictMove can be used to move an entry from one dict to the other without double key-hashing and memory allocation overheads.\n- avoid repeated key-hashing like find followed by add or by delete (see previous implementation of dictReplace)\n- responsibilities and scope of _dictKeyIndex and dictGenericDelete changed so that they are flexible and better serve wrapper functions.\n- dictDeleteNoFree can optionally return a copy of the dictEntry it deleted (someone can use it instead of doing dictFind before calling dictDelete)\n- dictAddRaw optionally returns the pre-existing entry in case the add failed (so someone doesn't need to call find when it fails if he needs the entry)\n- comment improvements on function names: dictReplace = \"Add or Overwrite\", dictReplaceRaw = \"Add or Find\", etc.\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 16,
        "changed_files": 6,
        "created_at": "2016-05-12T17:33:28Z",
        "closed_at": "2016-05-18T14:50:53Z",
        "merged_at": "2016-05-18T14:50:53Z",
        "body": "hi, this one contains several small fixes (two are important for v3.2):\n- GEORADIUS with negative radius doesn't reply causing client to hung\n- sdsReqType didn't use the full potential of the size (off by one in my original code)\n- htNeedsResize - excess test may prevent dict resize when dict is reduced to empty (used==0)\n- missing calls to signalModifiedKey in spopWithCountCommand\n- moving existing calls to signalModifiedKey to always be after the modifications are done (my fork actually depends on that)\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-05-12T17:25:35Z",
        "closed_at": "2016-05-18T12:53:53Z",
        "merged_at": "2016-05-18T12:53:53Z",
        "body": "hi, i didn't see you committed a fix for this yet, if you have, then discard this PR.\n\nreproduce with:\nset foo 1\nbitfield foo get i1 0\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2016-05-11T03:02:38Z",
        "closed_at": "2016-06-14T12:40:18Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 44,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2016-05-10T18:17:57Z",
        "closed_at": "2021-10-26T12:16:24Z",
        "merged_at": null,
        "body": "How about adding this feature for module commands\n\nmodule list helloworld\n\n```\n127.0.0.1:6379> module list helloworld\n 1) \"hello.push.call\"\n 2) \"hello.zsumrange\"\n 3) \"hello.push.call2\"\n 4) \"hello.list.splice.auto\"\n 5) \"hello.more.expire\"\n 6) \"hello.simple\"\n 7) \"hello.toggle.case\"\n 8) \"hello.hcopy\"\n 9) \"hello.push.native\"\n10) \"hello.list.sum.len\"\n11) \"hello.rand.array\"\n12) \"hello.lexrange\"\n13) \"hello.repl1\"\n14) \"hello.repl2\"\n15) \"hello.list.splice\"\n\n127.0.0.1:6379> module list helloworld2\n(error) ERR no such module with that name\n```\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-09T04:16:57Z",
        "closed_at": "2016-05-18T10:00:22Z",
        "merged_at": null,
        "body": "When the user wants to build Redis with different jemalloc version. `LG_QUANTUM` can be forgotten to be changed from 4 to 3 easily. Hence, it will be handy if `LG_QUANTUM` is updated whenever `update-jemalloc.sh` is executed.\n\nrelated issue: #2510 \n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-05-06T14:33:23Z",
        "closed_at": "2016-05-10T20:55:40Z",
        "merged_at": null,
        "body": "Fix one character typo in README.md\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-05-02T15:11:05Z",
        "closed_at": "2016-05-04T07:01:41Z",
        "merged_at": null,
        "body": "Requests will hang on `GEORADIUS` when there is a negative unit specified.\n\nreproduce:\n\n``` bash\ngeoadd fleet -115 33 truck1   # add a truck\ngeoradius fleet -115 33 -1 m  # hang\n```\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-04-30T16:59:29Z",
        "closed_at": "2016-05-05T06:57:30Z",
        "merged_at": "2016-05-05T06:57:30Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 71,
        "deletions": 58,
        "changed_files": 17,
        "created_at": "2016-04-25T13:55:46Z",
        "closed_at": "2016-05-04T07:11:36Z",
        "merged_at": "2016-05-04T07:11:36Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-04-23T02:54:50Z",
        "closed_at": "2017-08-04T05:43:32Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2016-04-22T15:50:28Z",
        "closed_at": "2016-05-05T07:02:25Z",
        "merged_at": "2016-05-05T07:02:25Z",
        "body": "I've renamed maxmemoryToString to evictPolicyToString since that is\nmore accurate (and easier to mentally connect with the correct data), as\nwell as updated the function to user server.maxmemory_policy rather than\nserver.maxmemory. Now with a default config it is actually returning\nthe correct policy rather than volatile-lru. This fixes #3187 \n\n@antirez \nBTW, this will need to go into 3.2 before final release as that is actually where I found the bug. ;)\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-04-21T13:56:09Z",
        "closed_at": "2020-09-09T16:27:58Z",
        "merged_at": null,
        "body": "I thought the previous comment is not accurate. If the running time of dictRehash(d,100) > 1 ms, then this function may exceed (ms+1) milliseconds. \n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-04-19T11:47:31Z",
        "closed_at": "2016-06-10T07:02:06Z",
        "merged_at": "2016-06-10T07:02:06Z",
        "body": "In file **utils/redis_init_script**\n`PIDFILE=/var/run/redis_${REDISPORT}.pid`\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-04-19T05:39:48Z",
        "closed_at": "2020-09-09T16:28:01Z",
        "merged_at": null,
        "body": "Just noticed this typo while reading the top level README.md.\n\nThanks for making redis btw, using it everyday lately and it's a great piece of software, zero configuration for most people, very reliable, powerful and easy to use.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2016-04-18T03:40:16Z",
        "closed_at": "2016-04-18T10:27:59Z",
        "merged_at": "2016-04-18T10:27:59Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-04-14T05:22:03Z",
        "closed_at": "2021-03-14T16:10:21Z",
        "merged_at": null,
        "body": "fix typo\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-04-13T08:15:42Z",
        "closed_at": "2016-05-05T07:05:43Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 54,
        "deletions": 19,
        "changed_files": 1,
        "created_at": "2016-04-05T12:17:08Z",
        "closed_at": "2022-02-06T06:13:25Z",
        "merged_at": null,
        "body": "Hi,\nI think it will be better if the return value of redisCommand() is tested whether it is NULL in exmaple.c. Otherwise, there will be a segmentation fault if the reply is NULL.\n\nI reproduced the segmentation fault in the following way.\n1. A client (hiredis) continues to call redisCommand() and show the reply->str.\n2. At the same time, kill the redis-server.\n3. In this case, the reply will be NULL, so reply->str will cause a segmentation fault.\n\nI have tested my patch and got the following result.\n\n```\nPING: PONG\nSET: OK\nSET (binary API): OK\nGET foo: hello world\nINCR counter: 1\nINCR counter: 2\n0) element-9\n1) element-8\n2) element-7\n3) element-6\n4) element-5\n5) element-4\n6) element-3\n7) element-2\n8) element-1\n9) element-0\n```\n\nThis is just my suggestion. And I am looking forward to your opinion.\nThank you for your time.\n\nCheers,\nJingle\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2016-03-30T13:04:57Z",
        "closed_at": "2016-05-05T06:56:29Z",
        "merged_at": "2016-05-05T06:56:29Z",
        "body": "Hello. :D\n\n[Fix 1]\nDoes redis-cluster-node have two state(importing and migrating) at the same time?\nIf so, I think this PR will be needed to merge.\n\n[Fix 2]\nVariable `n` is not defined.\nI think it is mistake.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 88688,
        "deletions": 23398,
        "changed_files": 490,
        "created_at": "2016-03-23T01:38:53Z",
        "closed_at": "2016-05-05T07:10:53Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 30,
        "changed_files": 18,
        "created_at": "2016-03-22T02:12:28Z",
        "closed_at": "2020-09-09T16:28:05Z",
        "merged_at": null,
        "body": "simple typos patch \n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2016-03-13T17:53:44Z",
        "closed_at": "2021-06-17T12:30:24Z",
        "merged_at": null,
        "body": "the idea is to cache hash value in dictEntry, to get following effects\n- less dictHashKey() call\n- less memory fetch when we're following linked list, since entry->key is not followed if hash value is different. (this happens a lot when hash table size is not too big)\n\nexperiment result\nSET P16\norg : 788022.06 814000.81   799808  793021.44   807297.94\npatched : 827472.06 734537.94   829668.94   826446.25   823316.31\n\nGET P16\norg : 1064282.62    1064282.62  941708.25   1054963.62  1106806.88\npatched : 1141943.62    1147183.62  1143510.62  1112842.25  1122460.38\n\nGET P16 1M keys\norg : 1305312.62    1297185.12  1310959.62  1299883 1286008.25\npatched : 1334044.88    1336898.38  1322051.75  1320829.5   1319957.75\n\ni'm seeing a few % overall performance boost.\nbenchmark command used for testing\nSET P16 - redis-benchmark -t set -r 100000 -n 10000000 -P 16\nGET P16 - redis-benchmark -t get -r 100000 -n 10000000 -P 16\nGET P16 1M keys - redis-benchmark -t get -r 1000000 -n 10000000 -P 16\n\nGET P16 1M keys is to test a situation where a lot of non-existent keys are being queried - more hash value mismatches ( more linked list traversal )\nGET tests are performed after SET\n\nI didn't test other ops thoroughly but not much difference.\n\nany ideas?\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-03-11T20:27:54Z",
        "closed_at": "2016-05-05T07:32:07Z",
        "merged_at": "2016-05-05T07:32:07Z",
        "body": "Fixes #2515.\n\nI can provide a change for unstable as well if desired.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 31200,
        "deletions": 3691,
        "changed_files": 243,
        "created_at": "2016-03-10T21:52:43Z",
        "closed_at": "2016-06-23T14:39:19Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-03-09T06:21:42Z",
        "closed_at": "2020-09-09T16:28:08Z",
        "merged_at": null,
        "body": "Typo correction: \"faield\" --> \"failed\"\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-03-09T05:38:40Z",
        "closed_at": "2021-12-09T23:22:15Z",
        "merged_at": null,
        "body": "(Minor) Changed display message from \"Redis Sentinel\" to \"Redis Cluster\". The script is for testing Redis Cluster (Redis Sentinel has another test script).\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2016-03-03T01:26:56Z",
        "closed_at": "2020-09-05T20:14:25Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2016-03-02T10:16:35Z",
        "closed_at": "2016-03-02T14:12:38Z",
        "merged_at": "2016-03-02T14:12:38Z",
        "body": "related to https://github.com/antirez/redis/pull/3114\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2016-02-29T11:43:46Z",
        "closed_at": "2016-02-29T16:52:40Z",
        "merged_at": null,
        "body": "",
        "comments": 23
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-02-27T00:20:24Z",
        "closed_at": "2017-07-07T14:17:41Z",
        "merged_at": null,
        "body": "This will allow to run build and test on [travis-ci.org](https://travis-ci.org/cyberdelia/redis/builds/112161629).\nOnce enable on the repo, it will allow for a quick health overview for each PR, branches and commit.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 82,
        "changed_files": 3,
        "created_at": "2016-02-26T23:52:21Z",
        "closed_at": "2016-12-20T07:39:30Z",
        "merged_at": null,
        "body": "Redis ASCII logo is a special snow flake in logs, it doesn't respect log level formatting, and makes it hard to lookup/search for some precious information it conveys. \n\nRemoving it is certainly a bit sad, but should fix all of the above mention issues.\n",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-02-25T08:49:39Z",
        "closed_at": "2021-03-14T16:11:44Z",
        "merged_at": null,
        "body": "Corrected typo which said \"craete\"instead of \"create\".\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-02-25T08:32:31Z",
        "closed_at": "2020-09-09T16:28:11Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-02-25T01:42:06Z",
        "closed_at": "2022-04-10T14:03:39Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 25934,
        "deletions": 11309,
        "changed_files": 199,
        "created_at": "2016-02-22T09:47:25Z",
        "closed_at": "2016-05-05T07:09:56Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2016-02-18T23:19:19Z",
        "closed_at": "2016-02-19T11:08:24Z",
        "merged_at": "2016-02-19T11:08:24Z",
        "body": "Thanks for adding addReplyHumanLongDouble @antirez :earth_africa: \n\nClosing #3100 as it is included here too.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-02-18T18:12:20Z",
        "closed_at": "2016-02-18T23:20:06Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-02-16T21:54:41Z",
        "closed_at": "2021-10-27T08:57:52Z",
        "merged_at": null,
        "body": "I have written a clear, easy-to-comprehend introduction for Redis. I believe this introduction, free of complex technical terms, will swiftly and thoroughly inform anyone about the use-cases, merit, and advantages, including the celebrated features, of Redis. Seasoned developers, new programmers, and even nontechnical visitors will benefit from the clear language, exposing Redis to a broader range of users. \n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-02-15T04:32:18Z",
        "closed_at": "2021-03-04T19:19:07Z",
        "merged_at": null,
        "body": "To make Redis more approachable to a larger number of users\u2014from informed developers to curious passersby and new and interested visitors\u2014I have added a new section to the Redis README. The section is entitled What to Use Redis for and When to Use It. As the name suggests, the new section, a brief paragraph, explains in plain and straightforward language what technologyRepo is used for and when one should use it.\n\nOther repositories may begin to use this new informative and catchy section soon, as my colleagues contact open-source technologies in an effort to make them more accessible to the tens of thousands of new users learning to code and thousands joining GitHub every day.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-02-12T15:52:22Z",
        "closed_at": "2022-04-14T08:15:04Z",
        "merged_at": null,
        "body": "To make Redis more approachable to a larger number of users\u2014from informed developers to curious passersby and new and interested visitors\u2014I have added a new section to the Redis README. The section is entitled What to Use Redis for and When to Use It. As the name suggests, the new section, a brief paragraph, explains in plain and straightforward language what Redis is used for and when one should use it.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 60,
        "deletions": 3,
        "changed_files": 6,
        "created_at": "2016-02-10T14:38:20Z",
        "closed_at": "2020-10-28T18:01:12Z",
        "merged_at": null,
        "body": "This PR adds support for the SMISMEMBER command, which is useful when wanting to check multiple members at once.\n\nIts also possible to implement using lua, but so does every \"multiple\" command..\n",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-02-09T14:07:08Z",
        "closed_at": "2016-05-05T08:13:00Z",
        "merged_at": "2016-05-05T08:13:00Z",
        "body": "Fix issue in case the redirect address is in ipv6 format. Parse from behind to extract last part of the response which represents actual port.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2016-02-07T18:33:44Z",
        "closed_at": "2016-02-10T08:03:29Z",
        "merged_at": "2016-02-10T08:03:29Z",
        "body": "LUA allocator should always receive a correct value for osize when calling reaclloc/free. This enables customizing the LUA allocator and adding things like accounting of used memory. Currently the cmsgpack extension erroneously passes only 'buf->len' instead of 'buf->len + buf->free' to mp_realloc/mp_buf_append which is wrong.\nAdditionally it turns out mp_buf_append doesn't correctly update the free size size after allocating x2 of the needed size.\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 208,
        "deletions": 26,
        "changed_files": 12,
        "created_at": "2016-02-07T15:55:32Z",
        "closed_at": "2022-03-01T12:19:27Z",
        "merged_at": null,
        "body": "Hi,\n\nYet another PR from us, this time for allowing empty masters to optionally be part of the \"cluster size\" (and so, both vote on failovers and epoch changes and be counted as part of the quorum size).\n\nThis is a feature that is supported by both Redis Sentinel (and RLEC, and many other systems) and is missing in Redis Cluster.\n\nIt has \"conflicts\" with #3069, which pretty much sum up to \"which flag receives the value of 512 and which 1024\".\n\nLike #3069, this one also seems to work OK with Redis 3.2 (but we're developing over 3.0 because that's what we are running in production).\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 294,
        "deletions": 21,
        "changed_files": 13,
        "created_at": "2016-02-05T09:23:47Z",
        "closed_at": "2022-03-01T12:19:13Z",
        "merged_at": null,
        "body": "Hi Salvatore,\n\nThis is the patch that we're using.\n\nIt technically works, and with basic renaming rules, also works for 3.2. We're using this one over 3.0, for a tiny bit more stability.\n\nWaiting to hear your comments.\n\nAs discussed on https://www.reddit.com/r/redis/comments/43zwkc/heterogeneous_redis_cluster_roles/\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-02-05T02:54:10Z",
        "closed_at": "2016-05-10T06:53:09Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-02-04T19:56:34Z",
        "closed_at": "2021-06-24T16:42:36Z",
        "merged_at": null,
        "body": "Fix for potential crash when list iterator argument for function listNext is NULL. fix for issue #3014 \n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2016-02-02T18:01:16Z",
        "closed_at": "2016-02-05T14:52:57Z",
        "merged_at": "2016-02-05T14:52:57Z",
        "body": "Fixes #2512. If merged, doc page needs to be updated as well.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2016-02-01T07:06:51Z",
        "closed_at": "2020-09-09T16:28:14Z",
        "merged_at": null,
        "body": "Spelling mistakes -\n`eviciton` > `eviction`\n`familar` > `familiar`\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2016-02-01T04:50:47Z",
        "closed_at": "2020-09-05T20:15:47Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2016-01-30T21:23:26Z",
        "closed_at": "2016-01-31T07:57:14Z",
        "merged_at": "2016-01-31T07:57:14Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 1,
        "changed_files": 6,
        "created_at": "2016-01-29T07:17:25Z",
        "closed_at": "2018-03-14T16:02:20Z",
        "merged_at": null,
        "body": "Related to [#3021](https://github.com/antirez/redis/issues/3021)\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-01-28T19:47:35Z",
        "closed_at": "2016-05-05T08:15:17Z",
        "merged_at": "2016-05-05T08:15:17Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-01-24T04:34:55Z",
        "closed_at": "2021-06-28T13:50:47Z",
        "merged_at": null,
        "body": "if sh==null & init==0 memset() will crash\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-01-22T01:56:06Z",
        "closed_at": "2021-06-29T10:13:47Z",
        "merged_at": null,
        "body": "\u0096Perfect annotation:Change \"ot\" to \"not\" .\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2016-01-21T04:14:13Z",
        "closed_at": "2016-05-05T08:05:49Z",
        "merged_at": null,
        "body": "Fixed following issue :\nhttps://github.com/antirez/redis/issues/3020 (With no bindaddr given, server should give error if it cannot bind both IPv4 and IPv6)\n\nEarlier behavior \nlistenToPort function was returning success if it can bind any one of IPv4 or IPv6 on not setting the bind option. This is confusing for the user, since there's no indication anywhere that the server tried and failed to bind on one of the interfaces, in case the port is free on IPv6 interface, but busy on IPv4 interface.\n\nReason\nThe \"listenToPort\" was deciding if the connection is successful by checking for the value of \"count\" to be non-zero. Both IPv4 & IPv6 connection attempts increment this value, and so even if one failed, the count is greater than 0, and hence connection is shown to be successful, and no error message is given.\n\nChanged behavior\nNow after both the connection attempts, if \"ANET_ERR\" is returned, a check is added to give an error message and to return a \"C_ERR\" to the calling function, the same behavior that is displayed when the server is started by specifying 0.0.0.0 or :: (defaults) as bind address.\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2016-01-18T11:38:15Z",
        "closed_at": "2021-07-26T08:42:38Z",
        "merged_at": null,
        "body": "I have read the source code of Redis for a mouth,and I found that the \"dict\u201c structure field which called privdata didn't use in the source code.So I remove the structure filed(privdata),and modify some macros and functions related in \"dict.h\" and \"dict.c\".The modify has pass the \"make test\".\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-01-13T17:23:24Z",
        "closed_at": "2016-05-05T14:21:21Z",
        "merged_at": "2016-05-05T14:21:21Z",
        "body": "1 microsecond = 1000 nanoseconds\n1e3 = 1000\n10e3 = 10000\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2016-01-13T08:53:10Z",
        "closed_at": "2016-05-05T14:23:48Z",
        "merged_at": "2016-05-05T14:23:48Z",
        "body": "solution:\nwhen using pthread mutex to implement atomic counters\n    in src/atomicvar.h:line:75\n        pthread_mutex_lock(&mutex); \\\n        dstvar = var; \\\n        pthread_mutex_unlock(&mutex); \\\n    } while(0)\n    but in src/zmalloc.c:line:75\n        size_t _n = (__n); \\\n        if (_n&(sizeof(long)-1)) _n += sizeof(long)-(_n&(sizeof(long)-1)); \\\n        if (zmalloc_thread_safe) { \\\n            atomicIncr(used_memory,__n,&used_memory_mutex); \\\n        } else { \\\n            used_memory += _n; \\\n        } \\\n    } while(0)\n    so there is a more symbol '&' when call then atomic\\* function;\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-01-12T09:32:08Z",
        "closed_at": "2016-05-05T15:16:58Z",
        "merged_at": "2016-05-05T15:16:58Z",
        "body": "If sentinel's connection to master/slave/sentinel became disconnected just after the last PONG and before the next PING, `sentinelSendPeriodicCommands()` might just return without providing `sentinelSendPing()`, thus `ri->link->act_ping_time` might still stay at `0` and in `sentinelCheckSubjectivelyDown()`, the variable `elapsed` might not be updated for `sdown` event detection. In this case the sentinel might be no longer to set the `sdown` event for the connection that disconnected from and the further operations (e.g. `switch-master`) might not be executed.\n\nSo the provided fix is to give it a chance to check if the sentinel's connection status is disconnected, set the elapsed time as the time difference between the last available time and the current time, and have it to compare with the predefined `down_after_period`, to decide if the connection that connects to or disconnected from fits `sdown` event.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-01-08T14:53:32Z",
        "closed_at": "2017-03-31T06:44:52Z",
        "merged_at": null,
        "body": "redis-trib.rb import cmd, when --from node is cluster-enabled, program should exit.\nredis-trib.rb addnode cmd, when master-id is error, program should exit.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2016-01-08T11:55:32Z",
        "closed_at": "2018-08-26T09:25:44Z",
        "merged_at": "2018-08-26T09:25:44Z",
        "body": "See https://reproducible-builds.org/specs/source-date-epoch/ for more\ndetails.\n\nSigned-off-by: Chris Lamb chris@chris-lamb.co.uk\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2016-01-04T10:49:18Z",
        "closed_at": "2016-05-05T15:20:38Z",
        "merged_at": null,
        "body": "The commands arity should always include the command name itself. So the commandCommand should also have a arity of 1 instead of 0.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-31T21:05:05Z",
        "closed_at": "2020-07-27T14:56:36Z",
        "merged_at": null,
        "body": "ran into an undefined variable\n\nbacktrace:\n\n``` ruby\nredis-trib.rb:878:in `rescue in move_slot': undefined local variable or method `key' for #<RedisTrib:0x000000014e2978> (NameError)\nfrom redis-trib.rb:872:in `move_slot'\nfrom redis-trib.rb:542:in `fix_open_slot'\nfrom redis-trib.rb:421:in `block in check_open_slots'\nfrom redis-trib.rb:421:in `each'\nfrom redis-trib.rb:421:in `check_open_slots'\nfrom redis-trib.rb:360:in `check_cluster'\nfrom redis-trib.rb:1055:in `fix_cluster_cmd'\nfrom redis-trib.rb:1611:in `<main>'\n```\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-30T06:27:47Z",
        "closed_at": "2020-09-09T16:28:18Z",
        "merged_at": null,
        "body": "Small typo fix in tests/unit/expire.tcl!\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-29T17:01:15Z",
        "closed_at": "2021-11-18T09:31:29Z",
        "merged_at": null,
        "body": "`quit` is treated explicitely in `processCommand`.\nThe `COMMAND` command gives a listing of all available commands, _except_ `QUIT` because that one is missing from the large table.\n\nAdding it should be the most unobtrusive way to handle it.\n\nThe only breaking thing I can think of: With `QUIT` being in the command table, it's now valid to have `rename-command quit foo` in the config, except it doesn't have a real effect (except that `COMMAND` will report the wrong things now)\n\n@BridgeAR discovered it. \n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-12-29T10:05:43Z",
        "closed_at": "2021-10-27T06:56:57Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1480,
        "deletions": 124,
        "changed_files": 11,
        "created_at": "2015-12-28T22:25:48Z",
        "closed_at": "2021-03-24T10:45:52Z",
        "merged_at": null,
        "body": "Ported the Interval sets from http://blog.togo.io/how-to/adding-interval-sets-to-redis/  to 3.2\nAdded commands iadd,irem,irembystab,istab to allow for easier interval set processing than a manual processing, maintenance and overhead in doing it with ZSETs\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-12-28T10:07:07Z",
        "closed_at": "2020-09-09T16:28:21Z",
        "merged_at": null,
        "body": "This mentions that \"expect stop\" is required for supervised upstart to work correctly.\n\nSee http://upstart.ubuntu.com/cookbook/#expect-stop for an explanation.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2015-12-28T07:43:44Z",
        "closed_at": "2021-06-27T08:52:41Z",
        "merged_at": null,
        "body": "Signed-off-by: dongxinghuo guocheng_123456@126.com\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 17,
        "changed_files": 1,
        "created_at": "2015-12-24T16:19:33Z",
        "closed_at": "2021-07-15T08:32:56Z",
        "merged_at": null,
        "body": "Fix strict-aliasing rules warning on CentOS 6 with gcc 4.4\n<img width=\"471\" alt=\"centos6warning\" src=\"https://cloud.githubusercontent.com/assets/3982329/11996415/1ec7749e-aa9d-11e5-87fb-a57771ffa6dd.png\">\n<img width=\"1425\" alt=\"gcc4 4\" src=\"https://cloud.githubusercontent.com/assets/3982329/11996416/1fb91d76-aa9d-11e5-8daf-de7f96973c6a.png\">\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2015-12-24T05:29:00Z",
        "closed_at": "2022-04-28T07:22:48Z",
        "merged_at": null,
        "body": "We found a issue about chained replication in our product environment. \n\nLet's say A -> B -> C three nodes chained replication. When network trembled, B disconnect with A and C disconnected with B, B and C reconnect and request resync with their master at the same time. C sent 'psync' and wait B's reply, if B is in it's handshake status, B will reply \"-ERR Can't SYNC while not connected with my master\". In this condition,  C will give up psync, free it's cached_master and perform full sync with B.\n\nThe attachment show how to reproduce the issue.\n[redis-fix.txt](https://github.com/antirez/redis/files/71462/redis-fix.txt)\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 634,
        "deletions": 200,
        "changed_files": 10,
        "created_at": "2015-12-23T15:56:34Z",
        "closed_at": "2016-06-29T16:21:17Z",
        "merged_at": null,
        "body": "adding cmd acls for redis. but It needs more implementations. so I need some advice to finish this pr. just WIP.\n\n---\n\nFuture Work \n1. changing sentinel to connect acls - Done\n## 2. checking passwd (only check with plain text)\n\n---\n\nTODO\n## 1. adding acl for cmd\n\nI ignore specifing some command groups(string, set, zset, list, hash, hyperloglog) becase I think we don't need to divide them. so there are some groups below.\n\n-readonly\n-write\n-slow\n-admin\n-pubsub\n-scripting\n-all\n\nbut adding or removing each command with +, - opeartor\ncharsyam \"mypassword\" +#all -keys -config\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2015-12-21T07:31:48Z",
        "closed_at": "2020-09-09T16:28:24Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-19T22:31:30Z",
        "closed_at": "2020-04-05T20:48:31Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-12-19T15:43:58Z",
        "closed_at": "2020-09-09T16:28:27Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-18T11:48:28Z",
        "closed_at": "2021-06-24T06:43:31Z",
        "merged_at": null,
        "body": "Hi, just found a minor fix\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-16T16:29:12Z",
        "closed_at": "2021-03-14T18:08:47Z",
        "merged_at": null,
        "body": "minor correction\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 47,
        "deletions": 4,
        "changed_files": 5,
        "created_at": "2015-12-16T08:25:15Z",
        "closed_at": "2016-05-05T15:25:30Z",
        "merged_at": null,
        "body": "The syntax is GETBIS key offset [range].\nThe result are;\n\n```\n127.0.0.1:6379> getbit hoge 1 7\n1) (integer) 0\n2) (integer) 1\n3) (integer) 1\n4) (integer) 0\n5) (integer) 0\n6) (integer) 1\n7) (integer) 0\n```\n\nPlease give me feedbacks.\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-12-16T05:56:06Z",
        "closed_at": "2015-12-22T07:59:48Z",
        "merged_at": "2015-12-22T07:59:48Z",
        "body": "This helps with processing of the pretty printed results for Lua datatypes (similar to what #2954 does for tables).\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-16T03:41:53Z",
        "closed_at": "2016-05-05T15:26:35Z",
        "merged_at": "2016-05-05T15:26:35Z",
        "body": "`Nonexistent` is a better word in this context. `Undeclared` may work as well.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-16T02:22:56Z",
        "closed_at": "2015-12-22T08:00:36Z",
        "merged_at": "2015-12-22T08:00:36Z",
        "body": "It's only slightly more verbose, but is valid Lua code, which simplifies integration with other tools that need to evaluate this information to access variable values.\n\nThis patch changes values like `{{3; 4}; \"b\"={1; 2}}` to `{{3; 4}; [\"b\"]={1; 2}}`\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2015-12-14T23:52:44Z",
        "closed_at": "2021-07-18T11:11:10Z",
        "merged_at": "2021-07-18T11:11:10Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2015-12-13T06:56:16Z",
        "closed_at": "2015-12-14T16:57:43Z",
        "merged_at": "2015-12-14T16:57:43Z",
        "body": "Fix issue #2855\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2015-12-11T07:35:43Z",
        "closed_at": "2021-07-15T22:39:30Z",
        "merged_at": null,
        "body": "I think this is not a good approach.\nbut before adding ACL feature, I think this is need to prevent some crack.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2015-12-11T03:45:10Z",
        "closed_at": "2015-12-11T14:55:46Z",
        "merged_at": null,
        "body": "Signed-off-by: Hongze Zhu hongze.zhu@gmail.com\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 42,
        "changed_files": 20,
        "created_at": "2015-12-10T13:13:10Z",
        "closed_at": "2020-09-09T16:29:23Z",
        "merged_at": null,
        "body": "While reading source code, I found a typo in source code comment.\nBut through confirming similar issue, I found many issue, 'a' vs 'an'.\n\nThis patch fixes them.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-09T23:01:09Z",
        "closed_at": "2016-02-02T21:45:54Z",
        "merged_at": null,
        "body": "Reads the requirepass line from the config file, munges\nit into a flag/param if found, and adds it to the call to\nredis-cli on service stop.\n\nsed command explained:\n-n              do not print each line after processing it\n/^requirepass /{...}    on lines starting with requirepass, do:\n  s/[^ ]*/-a/;      substitute up to the first space with -a flag\n  p;            print the processed line\n  q;            quit sed\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-12-09T06:09:18Z",
        "closed_at": "2021-07-18T12:27:42Z",
        "merged_at": "2021-07-18T12:27:42Z",
        "body": "See https://github.com/antirez/disque/commit/97ae9fbbf302da5d1c358bda666a76a5e79f5cdc and https://github.com/antirez/disque/pull/129/files\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-09T06:02:40Z",
        "closed_at": "2021-06-29T03:14:54Z",
        "merged_at": null,
        "body": "The other mention of this string in rdb.c is spelled correctly already: https://github.com/antirez/redis/blob/acc2336fd189ddf9e97b0fc589c43ae26a1fd153/src/rdb.c#L1419\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 71,
        "deletions": 77,
        "changed_files": 13,
        "created_at": "2015-12-09T06:02:35Z",
        "closed_at": "2021-07-15T11:57:07Z",
        "merged_at": null,
        "body": "This commit renames more things to the general server name instead of redis.\nThis is the next step for a common core between redis based applications such as [disque](https://github.com/antirez/disque) and [discnt](https://github.com/erikdubbelboer/discnt)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-12-08T03:36:13Z",
        "closed_at": "2016-12-28T03:15:42Z",
        "merged_at": null,
        "body": "I'd like to know if there is any way to [prevent different redis clusters from mixing via CLUSTER MEET](http://stackoverflow.com/questions/33093456/prevent-different-redis-clusters-from-mixing-via-cluster-meet) but got no answer so I decide to make one.\n\nUPDATED: I didn't notice there is a `state` member in `clusterMsg`. It seems fine to use this field, so that no need to change protocol anymore.\n\nWhen a node receives a MEET message, it would not response if both the cluster of sender and the cluster of itself are in the OK state, thus keeping two difference cluster from accidentally mixing.\n\nThanks for reviewing. From a department running dozens of clusters...\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2015-12-06T21:51:06Z",
        "closed_at": "2015-12-11T09:57:05Z",
        "merged_at": "2015-12-11T09:57:05Z",
        "body": "The default of 15 seconds isn't enough if you have to migrate big enough keys. Being able to set a higher timeout when needed makes migrating some datasets much easier.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-12-04T02:22:25Z",
        "closed_at": "2020-09-09T16:28:30Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-12-03T05:30:37Z",
        "closed_at": "2015-12-04T02:27:17Z",
        "merged_at": null,
        "body": "When \"slaveof no one\" command is requested by user while the redis is still waiting for the replication stream from its master, redis cancels replication process by just closing the connection with master.\nBut without aeDeleteFileEvent(), that fd number still remain in the epoll, so this will make redis keep failing to accept new clients because epoll add will fail.\nThis fix is not just for 2.8 branch but also can be applied to every redis branches.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2015-12-03T00:03:54Z",
        "closed_at": "2016-05-05T15:27:50Z",
        "merged_at": null,
        "body": "  A suggestion to compile entire statements and expressions, as suggested by code style guidelines from the Linux Kernel and practitioners.\n\n```\nhttps://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/Documentation/CodingStyle#n892\nhttps://www.cqse.eu/en/blog/living-in-the-ifdef-hell/\n```\n\nIt might improve code understanding, maintainability and error-proneness.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2015-12-02T09:50:44Z",
        "closed_at": "2020-09-09T16:28:34Z",
        "merged_at": null,
        "body": "modify error word keyevente\nthe sdstrim example output shoule be HelloWorld\ndelete redundancy color judge in sdscatcolor\nSigned-off-by: binyan.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-02T09:30:09Z",
        "closed_at": "2021-07-20T06:13:21Z",
        "merged_at": null,
        "body": "Add that the time to live is in seconds for TTL help.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-12-02T06:45:08Z",
        "closed_at": "2020-09-05T20:30:44Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 76023,
        "deletions": 2249,
        "changed_files": 380,
        "created_at": "2015-11-26T05:53:05Z",
        "closed_at": "2016-05-05T07:10:13Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-11-23T03:28:16Z",
        "closed_at": "2017-01-16T18:06:01Z",
        "merged_at": null,
        "body": "Self-explanatory. Thanks for the awesome work :)\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-11-20T18:38:21Z",
        "closed_at": "2021-04-06T09:16:32Z",
        "merged_at": null,
        "body": "old url http://redis.io/topics/keyspace-events\nleads to 'Sorry, I can't find that page :-/'\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2015-11-19T19:06:40Z",
        "closed_at": "2020-08-05T14:53:02Z",
        "merged_at": null,
        "body": "While trying to do some profiling using MONITOR I realized it can be improved to provide better tracking of clients.  I experimented with this and here's what I can up with:\n1. If CLIENT SETNAME is used, that name is reported instead of the connection information.\n2. Commands sent by Lua are attributed to the original client connection (more useful than a global Lua client context) but have a `/lua` suffix to signify they were generated by Lua.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 30,
        "changed_files": 2,
        "created_at": "2015-11-15T14:34:24Z",
        "closed_at": "2016-06-10T07:07:42Z",
        "merged_at": "2016-06-10T07:07:42Z",
        "body": "",
        "comments": 16
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-11-13T15:51:15Z",
        "closed_at": "2016-10-21T21:46:27Z",
        "merged_at": null,
        "body": "Fixes typo\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-11-12T11:12:41Z",
        "closed_at": "2015-11-27T11:31:25Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-11-09T14:50:29Z",
        "closed_at": "2015-11-09T16:58:37Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-11-04T11:37:56Z",
        "closed_at": "2020-09-09T16:28:37Z",
        "merged_at": null,
        "body": "just a small typo\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-11-04T10:49:21Z",
        "closed_at": "2015-11-09T17:06:41Z",
        "merged_at": "2015-11-09T17:06:41Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2015-10-31T17:59:29Z",
        "closed_at": "2016-02-10T14:56:21Z",
        "merged_at": null,
        "body": ":rabbit: \n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2015-10-31T07:38:40Z",
        "closed_at": "2018-04-08T05:30:48Z",
        "merged_at": null,
        "body": "redis bgsave process close listen sockets at fork, but didn't close other connected sockets. and it may cause client block util bgsave exit, as those sockets were inherits by bgsave process.\n\nAnd it may cause client timeout  if bgsave take too long.\n### You can reproduce it easily\n#### server\n1. set timetout = 1s in redis.conf.\n2. sleep N(for exmple 30) seconds in `rdbSaveBackground`.\n#### client\n1. send bgsave command to server.\n2. after 1s, as we set timeout 1s, and it will be evicted.\n3. send command to server again, and it will be blocked util bgsave process exit, and get a rst package.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 168417,
        "deletions": 22868,
        "changed_files": 713,
        "created_at": "2015-10-30T16:40:26Z",
        "closed_at": "2015-11-09T17:08:37Z",
        "merged_at": null,
        "body": "redis\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2015-10-30T14:06:41Z",
        "closed_at": "2017-12-06T07:19:03Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2015-10-29T13:51:08Z",
        "closed_at": "2021-06-30T16:11:28Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-10-28T20:40:50Z",
        "closed_at": "2016-02-29T16:42:10Z",
        "merged_at": null,
        "body": "Fixing a few typos\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2015-10-27T21:21:17Z",
        "closed_at": "2016-02-29T22:20:29Z",
        "merged_at": null,
        "body": "Similar to my prior pull request I found a few more structures that will have padding resulting in a few extra bytes on an 64 bit machine. I couldn't find any instances in which changing the ordering would have any harmful consequences. I also probably could have added this to my first pull request, but I prefer separate requests. Let me know if this is not the \"redis way of doing things\".\n\n<!-- Reviewable:start -->\n\n[<img src=\"https://reviewable.io/review_button.svg\" height=\"40\" alt=\"Review on Reviewable\"/>](https://reviewable.io/reviews/antirez/redis/2829)\n\n<!-- Reviewable:end -->\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2015-10-26T21:19:47Z",
        "closed_at": "2016-02-29T22:20:40Z",
        "merged_at": null,
        "body": "By re-ordering a few variables in clusterState in [cluster.h](src/cluster.h) and config in [redis-benchmark.c](src/redis-benchmark.c), we can utilize structure packing to reduce the memory footprint. If I'm not mistaken the first commit should reduce clusterState by 16 bytes and config by 8 bytes. Tables of portions of the original layouts of the structures are listed below. By re-arranging the variables we should be able to eliminate the padding.\n\n**clusterState** ( x = offset of `failover_auth_count`)\n\n<table>\n<tr>\n<td><b>Variable</b></td><td><b>Offset</b></td>\n</tr>\n<tr>\n<td>failover_auth_count</td><td>x + 0</td>\n</tr>\n<tr>\n<td>failover_auth_sent</td><td>x + 4</td>\n</tr>\n<tr>\n<td>failover_auth_rank</td><td>x + 8</td>\n</tr>\n<tr>\n<td><i>padding</i></td><td>x + 12</td>\n</tr>\n<tr>\n<td>failover_auth_epoch</td><td>x + 16</td>\n</tr>\n<tr>\n<td>cant_failover_reason</td><td>x + 24</td>\n</tr>\n<tr>\n<td><i>padding</i></td><td>x + 28</td>\n</tr>\n</table>\n\n\n**config** ( x = offset of `hostport`)\n\n<table>\n<tr>\n<td><b>Variable</b></td><td><b>Offset</b></td>\n</tr>\n<tr>\n<td>hostport</td><td>x + 0</td>\n</tr>\n<tr>\n<td><i>padding</i></td><td>x + 4</td>\n</tr>\n<tr>\n<td>hosocket</td><td>x + 8</td>\n</tr>\n</table>\n\n\nIf the actual byte ordering of the variables is important for some reason, I'm sorry and this patch is not appropriate, but I didn't find anything to suggest this is so. Any comments or criticisms are appreciated, as this is my first patch submitted to this project, so I'm definitely a noobie here :smiley: \n\nI wrote the patch thinking about an x86 64bit machine, so in instances in which a single integer was adjacent to a pointer I considered that to be a 4 byte aligned value with 4 bytes of padding adjacent to a 8 byte aligned value.\n\n<!-- Reviewable:start -->\n\n[<img src=\"https://reviewable.io/review_button.svg\" height=\"40\" alt=\"Review on Reviewable\"/>](https://reviewable.io/reviews/antirez/redis/2825)\n\n<!-- Reviewable:end -->\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 77313,
        "deletions": 22418,
        "changed_files": 438,
        "created_at": "2015-10-25T11:09:47Z",
        "closed_at": "2015-11-09T12:02:52Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2015-10-24T12:03:37Z",
        "closed_at": "2017-01-04T08:03:14Z",
        "merged_at": null,
        "body": "This is just the result of fixing some of the warnings that CppCheck generates for codes in src directory.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2015-10-21T05:31:07Z",
        "closed_at": "2019-11-12T07:17:35Z",
        "merged_at": null,
        "body": "Fix several typos in deps/README.md\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-10-14T19:19:59Z",
        "closed_at": "2015-10-15T07:51:28Z",
        "merged_at": "2015-10-15T07:51:28Z",
        "body": "This change allows a slave to properly time out a dead master during the extended asynchronous synchronization state machine.  Now, slaves will record their last interaction with the master and apply the replication timeout before a response to the PSYNC request is received.\n\nYou can reproduce this issue by using diskless sync and a very long repl-diskless-sync-delay on the master.  If you control-Z the master after it has started the BGSAVE delay timer, the replication will get stuck indefinitely.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2015-10-14T18:25:34Z",
        "closed_at": "2021-07-16T16:56:36Z",
        "merged_at": null,
        "body": "Sometimes, we need set the first dict hashtable size when using at scale.\nso I think this is useful.\n\ndict-ht-initial-size is only changed in redis.conf(not config set command)\n\nin redis.conf\n\n``` c\ndict-ht-initial-size 102476800\n```\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-10-14T15:18:40Z",
        "closed_at": "2017-05-07T18:03:42Z",
        "merged_at": null,
        "body": "For GitHub Licenses API: https://developer.github.com/v3/licenses/\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-10-14T06:00:00Z",
        "closed_at": "2015-10-15T11:02:36Z",
        "merged_at": "2015-10-15T11:02:35Z",
        "body": "Adds `--copy` and `--replace` options to the redis-trib's `import` command.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2015-10-07T15:08:26Z",
        "closed_at": "2017-03-12T11:00:29Z",
        "merged_at": null,
        "body": "Fixes the slightly odd English sentence \"Because the commad second\nargument may be relative or absolute...\" by removing the word \"commad\".\n\nReformatted for 80 characters.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-10-06T10:38:38Z",
        "closed_at": "2017-02-13T18:03:52Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 229,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2015-10-03T02:39:35Z",
        "closed_at": "2021-08-02T05:57:50Z",
        "merged_at": "2021-08-02T05:57:49Z",
        "body": "Add NX, XX, GT, and LT flags to EXPIRE, PEXPIRE, EXPIREAT, PEXAPIREAT.\r\n- NX - only modify the TTL if no TTL is currently set \r\n- XX - only modify the TTL if there is a TTL currently set \r\n- GT - only increase the TTL (considering non-volatile keys as infinite expire time)\r\n- LT - only decrease the TTL (considering non-volatile keys as infinite expire time)\r\nreturn value of the command is 0 when the operation was skipped due to one of these flags.\r\n\r\n---------\r\n\r\nJust some quick work to add an `NX` option to expire commands. This allows expiration to be set for only once, just like setnx. In our application, we have some zsets and hashes that update frequently, but expire at a fixed time defined at its creation.\r\n\r\nSearched issue history, I also found some others who had similar requirements: #1840 \r\n\r\nThis is just the first commit. If you think it's OK I will add docs and tests for this feature. Thanks!\r\n\r\nEDIT: spelling\r\n",
        "comments": 16
    },
    {
        "merged": false,
        "additions": 31044,
        "deletions": 3682,
        "changed_files": 243,
        "created_at": "2015-10-02T12:49:33Z",
        "closed_at": "2015-10-05T15:49:04Z",
        "merged_at": null,
        "body": "Working on the accounce-ip command for a sentinel\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 45,
        "deletions": 11,
        "changed_files": 5,
        "created_at": "2015-09-28T05:34:56Z",
        "closed_at": "2021-07-15T10:37:52Z",
        "merged_at": null,
        "body": "Fixed AIX build using xlC compiler. Tested on aix 6.1. This is minor fix.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-09-23T22:10:58Z",
        "closed_at": "2017-07-07T14:17:48Z",
        "merged_at": null,
        "body": "I found that redis currently is running all integration test on http://ci.redis.io/ \n\nI guess this runs only after the code is merged into branch.\n\nAdding travis, will help to make sure all pull requests minimum compile at clang and gcc and basic tests are working . \n\nI can add support for osx build too as a followup to this patch request once approved.\n\nThis makes changes for the other developer to test easier too.\n\nLook at https://travis-ci.org/PradheepShrinivasan/redis/builds/81863361 for an example.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-09-23T17:09:46Z",
        "closed_at": "2015-09-24T00:35:44Z",
        "merged_at": null,
        "body": "some memory used should consider\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2015-09-15T23:08:24Z",
        "closed_at": "2019-03-12T17:10:29Z",
        "merged_at": "2019-03-12T17:10:29Z",
        "body": "This pull request allows blpop and friends to block for less than a second.\n",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2015-09-15T20:00:27Z",
        "closed_at": "2018-06-03T01:13:12Z",
        "merged_at": null,
        "body": "Just a small change to generate-command-help.rb to include cluster and geo command groups, and an update to src/help.h generated with the new commands included.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 351,
        "deletions": 176,
        "changed_files": 7,
        "created_at": "2015-09-15T19:09:16Z",
        "closed_at": "2017-07-03T11:29:37Z",
        "merged_at": null,
        "body": "Sample execution times of all commands, and expose percentiles through 'INFO commandstats'.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 317,
        "deletions": 254,
        "changed_files": 33,
        "created_at": "2015-09-10T04:25:22Z",
        "closed_at": "2017-07-03T11:16:01Z",
        "merged_at": null,
        "body": "I guess this isn't really a big \"code\" change so much as build integration \"fixes\" to satisfy an embedded development request for shared lib async interface.  It's all merged to latest 2.4 so take a look when you have time.  This works nicely on embedded Linux camera using redis as comm/event bus.  There's also an \"easy\" redis-ipc interface over in VCTLabs if you're interested, although I had to bug my boss a bit about the readme, she did a pretty nice one even (describes above use case, etc).  Thanks!\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-09-08T11:00:48Z",
        "closed_at": "2021-03-18T14:55:55Z",
        "merged_at": null,
        "body": "Full random field name tests for HSET and HMSET. HMSET also uses a\nrandom hash name too for more accurate testing.\n\nTested and data checked using the redis-cli. Performance seems within the likely range when compared to other commands.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 40,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-09-04T00:03:32Z",
        "closed_at": "2021-05-17T18:11:39Z",
        "merged_at": null,
        "body": "redis-trib.rb rebalance seednode:7000\nwith optional --slots\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2015-08-28T01:57:51Z",
        "closed_at": "2021-04-21T06:36:01Z",
        "merged_at": null,
        "body": "use defined constant instead of digital -1\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 18944,
        "deletions": 9958,
        "changed_files": 135,
        "created_at": "2015-08-25T14:47:45Z",
        "closed_at": "2018-03-22T16:11:04Z",
        "merged_at": null,
        "body": "update jemalloc to 4.0.0\nand I also update LG_QUANTUM as 3  for redis sds header optimization.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-08-24T09:08:49Z",
        "closed_at": "2018-10-05T13:26:35Z",
        "merged_at": null,
        "body": "Allow to use SO_REUSEPORT socket option. This allows to exploit more CPU cores and allows to do upgrades, restarts with having zero downtime. \nTo enable this just compile with: `make CFLAGS=-DHAVE_REUSEPORT=1`.\ncc @antirez @badboy \n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-08-23T08:48:48Z",
        "closed_at": "2017-11-08T16:06:10Z",
        "merged_at": "2017-11-08T16:06:10Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-08-19T13:15:58Z",
        "closed_at": "2018-07-19T19:06:52Z",
        "merged_at": null,
        "body": "About the first commit is easy to spot and fix.  \n\nThe second commit is an \"uneducated\" guess, I'm not sure what the error value should be.  Tests pass after both changes, though.\n\n```\nreplication.c: In function \u2018startBgsaveForReplication\u2019:\nreplication.c:495:14: error: \u2018REIDS_NOTICE\u2019 undeclared (first use in this function)\n     redisLog(REIDS_NOTICE,\"Starting BGSAVE for SYNC with target: %s\",\n              ^\nreplication.c:495:14: note: each undeclared identifier is reported only once for each function it appears in\nreplication.c: In function \u2018syncCommand\u2019:\nreplication.c:622:61: error: \u2018C_OK\u2019 undeclared (first use in this function)\n             if (startBgsaveForReplication(c->slave_capa) != C_OK) {\n                                                             ^\nmake[1]: *** [replication.o] Erreur 1\nmake[1]: quittant le r\u00e9pertoire \u00ab /home/manu/src/github.com/db/redis/redis/src \u00bb\nmake: *** [all] Erreur 2\n```\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-08-19T05:28:16Z",
        "closed_at": "2021-07-22T04:56:13Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-08-17T09:16:15Z",
        "closed_at": "2016-11-01T01:41:56Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-08-12T05:57:05Z",
        "closed_at": "2016-01-15T15:24:06Z",
        "merged_at": "2016-01-15T15:24:06Z",
        "body": "Make sure monitor is attached in one connection before issuing commands to be monitored in another one\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-08-08T20:09:20Z",
        "closed_at": "2015-08-08T21:16:20Z",
        "merged_at": null,
        "body": "HGETALL returns all keys and values in any order. When consistency is important, it should be sorted.\n\nMost people will use the keys to identify the values so it is not a big deal, but it seems incorrect not to sort since it is not required.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-08-08T19:44:07Z",
        "closed_at": "2016-08-12T12:09:41Z",
        "merged_at": null,
        "body": "`BRPOP` is variadic, all its arguments except 0 and -1 are keys. `BLPOP` was well defined.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-08-04T23:24:42Z",
        "closed_at": "2018-05-09T18:43:12Z",
        "merged_at": null,
        "body": "pubsub unsubscribe proc when it receives no argument calls unsubscribe\nwith no argument, which will unsubscribe from all channels. However it\ndoes no read, thus allows the test to keep going and send a publish\ncommand in another socket before the client actually had unsubscribed.\n\nWhen no channel is provided, the command should read to make\nsure the command was executed before continuing.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2015-08-04T01:09:49Z",
        "closed_at": "2021-06-24T15:34:21Z",
        "merged_at": null,
        "body": "- Fix bug in `maxmemoryToString` function\n- Change function name to `maxmemoryPolicyToString`\n- Change var name from `evict_policy` to `maxmemory_policy`\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 18,
        "changed_files": 1,
        "created_at": "2015-08-03T09:01:47Z",
        "closed_at": "2015-12-11T17:16:11Z",
        "merged_at": null,
        "body": "Closes #2710\n- \"keysinslot\" now retrieves more keys (10 -> 10000) for every iteration\n- Use pipeline to run \"migrate\" commands much faster then current\n\nTest environment is below,\n- Tested on my MacBook Pro Retina. (15 inch, Late 2013)\n  - OSX 10.10.3\n  - i7 2.3GHz, 16GB DDR3 1600 MHz, SSD 256GB\n- total 6 processes of redis-server (cluster mode)\n  - 3 masters, 3 slaves\n- 1000001 keys in one slot (slot number 13784)\n  - \u201c{hashtag}0\u201d to \u201c{hashtag}1000000\u201d\n- check elapsed seconds for only moving slot 13784\n\nTest result is here,\na) verbose = true\n- origin (keysinslot 10, not using pipeline) : elapsed: 142.083674 secs\n- keysinslot 1000, using pipeline : elapsed: 59.248071 secs\n- keysinslot 10000, using pipeline : elapsed: 57.582782 secs\n\nb) verbose = false\n- origin (keysinslot 10, not using pipeline) : elapsed: 115.745819 secs\n- keysinslot 1000, using pipeline : elapsed: 59.676552 secs\n- keysinslot 10000, using pipeline : elapsed: 57.819602 secs\n\nIncreasing fetch size of keysinslot and using pipeline seems to be 2 ~ 2.5 times faster than current.\nI couldn't test it with multiple machines since I don't have three or more physical machines.\n\nBtw, just modifying verbose option to 'false' also reduces more than 25 secs.\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-08-02T18:43:19Z",
        "closed_at": "2020-09-09T16:21:56Z",
        "merged_at": null,
        "body": "Just a typo fix.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 259,
        "deletions": 20,
        "changed_files": 8,
        "created_at": "2015-07-29T06:51:06Z",
        "closed_at": "2017-11-14T14:48:50Z",
        "merged_at": null,
        "body": "Adds 2 commands: `EVALTXN` and `EVALSHATXN` - runs Lua scripts in a transaction\nAdds 1 script sub-comand: `SCRIPT ROLLBACK` - kills scripts that have written if they are execued as a transaction\nAdds 1 configuration option: `lua-all-transactions` - for specifying that all `EVAL`/`EVALSHA` should be transactions too\n",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 38471,
        "deletions": 8895,
        "changed_files": 339,
        "created_at": "2015-07-27T06:13:28Z",
        "closed_at": "2016-06-10T07:49:36Z",
        "merged_at": null,
        "body": "build redis on hp-ux.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-07-26T14:08:29Z",
        "closed_at": "2021-06-30T09:23:31Z",
        "merged_at": null,
        "body": "For the sdscatfmt function in sds.c, when the parameter fmt ended up with '%', the behavior is undefined. This commit fix this bug.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 432,
        "deletions": 130,
        "changed_files": 2,
        "created_at": "2015-07-24T16:30:52Z",
        "closed_at": "2018-03-22T16:19:14Z",
        "merged_at": null,
        "body": "After patching  sdshdr size optimization.\nSentinel is broken. \nbecause sentinel uses hiredis and it's sds is old(just fixed size sdshdr)\nso, we should apply new sdshdr into hiredis also.\n",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-07-24T12:12:01Z",
        "closed_at": "2015-09-08T07:24:45Z",
        "merged_at": "2015-09-08T07:24:45Z",
        "body": "Sentinel die if you run ckquorum without <master> arg. I included one step to verify number of args sent. \n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-07-22T20:24:55Z",
        "closed_at": "2017-07-15T15:15:07Z",
        "merged_at": null,
        "body": "Use of `waiting` is long gone, so it can be removed.\nIsn't the `lobj ? ... :` check superfluous as well? Either `lobj` points to a previously existing key or it will be created anyway.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 49,
        "deletions": 15,
        "changed_files": 3,
        "created_at": "2015-07-22T16:11:36Z",
        "closed_at": "2015-08-06T07:44:04Z",
        "merged_at": null,
        "body": "adding \"REPLCONF capa eof\" used by slave to declare that it is capable of diskless EOF formatted stream.\nmaster now sends that new format only if the enabled in the config file and slave seems capable of understanding it.\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-07-16T11:12:10Z",
        "closed_at": "2015-07-17T08:39:02Z",
        "merged_at": "2015-07-17T08:39:02Z",
        "body": "aof_delayed_fsync was not set to 0 when calling CONFIG RESETSTAT.\nThis might also apply to other branches than 2.8\nfixes #2677\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2015-07-16T08:20:01Z",
        "closed_at": "2015-07-17T08:34:43Z",
        "merged_at": "2015-07-17T08:34:43Z",
        "body": "I think it's should be numeric properly..\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-07-15T15:20:10Z",
        "closed_at": "2021-08-03T18:40:34Z",
        "merged_at": null,
        "body": "geoadd is like zadd. and georem is just wrapper of zrem.\nbut I think georem is need for pair.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2015-07-10T19:39:14Z",
        "closed_at": "2015-07-13T16:33:00Z",
        "merged_at": null,
        "body": "This PR adds a `EXISTSANY` command.\n\n`Usage: EXISTSANY key [key ...]`\n\nreturns 1 if any of the keys exist\nreturns 0 if none of the keys exist\n\nPerformance is O(N), but breaks when finding first key that exists since we are not returning a count.\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2015-07-10T07:33:31Z",
        "closed_at": "2015-07-17T08:44:19Z",
        "merged_at": "2015-07-17T08:44:19Z",
        "body": "bugfix: errno might change before logging\n\nSigned-off-by: Yongyue Sun abioy.sun@gmail.com\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 80,
        "deletions": 28,
        "changed_files": 3,
        "created_at": "2015-07-07T17:40:41Z",
        "closed_at": "2020-02-24T18:39:54Z",
        "merged_at": null,
        "body": "These new commands are symmetrical to the rpoplpush command, and they've been implemented using a generic method with the existing pattern of a flag being REDIS_HEAD or REDIS_TAIL to indicate the start or end of a list. But I still need help resolving the blocking methods (blpoprpush an so on) and creating tests, because I'm don't know anything about tcl.\n\nI implemented the blocking commands, but I still need to adapt the `serveClientBlockedOnList` function so that it knows the difference between all methods (it currently supports only the rpoplpush command).\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2015-07-07T08:42:45Z",
        "closed_at": "2021-06-17T09:40:21Z",
        "merged_at": null,
        "body": "handle signale interrupt,then continue read and write\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2015-07-06T19:17:23Z",
        "closed_at": "2020-09-09T16:29:35Z",
        "merged_at": null,
        "body": "The `saveconf` variable was only used in one spot which didn't make much sense.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2015-07-05T14:55:09Z",
        "closed_at": "2021-06-24T06:12:57Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 306,
        "deletions": 18,
        "changed_files": 6,
        "created_at": "2015-06-27T16:02:01Z",
        "closed_at": "2021-07-15T10:10:09Z",
        "merged_at": null,
        "body": "Pretty much the same as SRANDMEMBER, including its handling of the optional COUNT parameter.  Implemented in response to issue #1050.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-06-26T10:01:59Z",
        "closed_at": "2015-07-17T08:55:58Z",
        "merged_at": "2015-07-17T08:55:58Z",
        "body": "fix command info, pfcount support multi keys\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-06-26T06:47:35Z",
        "closed_at": "2017-07-02T12:43:07Z",
        "merged_at": null,
        "body": "You can now specify an optional parameter for latency and it runs a certain amount of latency poll cycles (there is only output after it's done). Useful for monitoring latencies automatically.\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2015-06-24T12:58:38Z",
        "closed_at": "2015-07-17T09:00:44Z",
        "merged_at": "2015-07-17T09:00:44Z",
        "body": "Fixes #2506, #1594 and probably others.\n\nThis also removes including the sysctl header, as it is not needed anymore and does not exist on Solaris (added in ec5a0c548b0afbb1bd584b5761bf740460fd20a2, but `getMemorySize` was moved after that to zmalloc.c).\n\nThe only thing that remains is a warning about the now unused variable:\n\n```\ncluster.c:364:29: warning: unused parameter \u2018filename\u2019 [-Wunused-parameter]\n```\n\nI could fix that as well if wanted.\n\nTested on Solaris 11.2 and SmartOS\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-06-18T07:41:41Z",
        "closed_at": "2015-06-24T13:49:10Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 33307,
        "deletions": 45857,
        "changed_files": 261,
        "created_at": "2015-06-18T06:35:29Z",
        "closed_at": "2017-07-03T11:14:53Z",
        "merged_at": null,
        "body": "We are using the port for Windows based on Redis 2.6. (2.6.12) with aof file. Redis is used as temporary message storage and sometimes when the load is heavy and the messages are big the redis crashes with this message:\n\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\n[10516] 17 Jun 11:59:56.027 # ------------------------------------------------\n[10516] 17 Jun 11:59:56.027 # !!! Software Failure. Press left mouse button to continue\n[10516] 17 Jun 11:59:56.028 # Guru Meditation: \"Unknown list encoding\" #..\\src\\t_list.c:120\n[10516] 17 Jun 11:59:56.028 # ------------------------------------------------\n\nI debugged this and the reason is that the method listTypeLength does not support the encoding type REDIS_ENCODING_LINKEDLISTARRAY. This seems to be a\nwindows specific type. I fixed this like this (new code is the 2nd else branch):\n\nunsigned long listTypeLength(robj _subject) {\n    if (subject->encoding == REDIS_ENCODING_ZIPLIST) {\n        return ziplistLen(subject->ptr);\n    }\n    else if (subject->encoding == REDIS_ENCODING_LINKEDLIST) {\n        return listLength((list_)subject->ptr);\n# ifdef _WIN32\n\n```\n}\nelse if (subject->encoding == REDIS_ENCODING_LINKEDLISTARRAY) {\n    /* read only array */\n    cowListArray *ar = (cowListArray *)subject->ptr;\n    return (unsigned long)ar->numele;\n```\n# endif\n\n```\n} else {\n    redisPanic(\"Unknown list encoding\");\n}\n```\n\n}\n\nQuestions:\n1. Is the fix valid. If not what would be?\n2. If this is ok, could you add this to the official releases?\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 97,
        "deletions": 75,
        "changed_files": 7,
        "created_at": "2015-06-16T00:04:54Z",
        "closed_at": "2015-06-17T15:43:51Z",
        "merged_at": null,
        "body": "I found various spots where we may dereference a pointer and it could be NULL.\nNot sure what the behaviour should be in the case of it being NULL, feel free to modify or comment.\n\nAlso found  `lf.f`potentially being leaked in `deps/lua/src/lauxlib.c`\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-06-12T05:13:31Z",
        "closed_at": "2015-06-24T18:52:51Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-06-11T11:53:30Z",
        "closed_at": "2015-06-11T13:15:22Z",
        "merged_at": "2015-06-11T13:15:22Z",
        "body": "fix\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-05-31T15:31:14Z",
        "closed_at": "2017-07-15T15:16:34Z",
        "merged_at": null,
        "body": "Thanks, @adriano-di-giovanni.\nCloses #2598\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-05-28T16:20:51Z",
        "closed_at": "2021-06-03T11:53:04Z",
        "merged_at": null,
        "body": "If the value of ANET_ERR were to change, anetTcpAccept and anetUnixAccept would conceal errors.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-05-21T10:25:08Z",
        "closed_at": "2015-05-28T13:09:51Z",
        "merged_at": "2015-05-28T13:09:51Z",
        "body": "DEL/INCR/DECR and others could be NTH but apparently never made it to the implementation of SORT\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-05-21T09:40:36Z",
        "closed_at": "2015-05-28T13:10:26Z",
        "merged_at": "2015-05-28T13:10:26Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2015-05-15T11:02:16Z",
        "closed_at": "2015-05-15T13:09:55Z",
        "merged_at": null,
        "body": "update\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2015-05-12T09:15:31Z",
        "closed_at": "2022-05-23T13:19:15Z",
        "merged_at": null,
        "body": "This prevents a false warning on systems that have huge page tables configured with the value of `madvise`.\n\nThe kernel documentation mentions that the huge pages setting has three possible values:\n\n```\nTransparent Hugepage Support can be entirely disabled (mostly for\ndebugging purposes) or only enabled inside MADV_HUGEPAGE regions (to\navoid the risk of consuming more memory resources) or enabled system\nwide. This can be achieved with one of:\n\necho always >/sys/kernel/mm/transparent_hugepage/enabled\necho madvise >/sys/kernel/mm/transparent_hugepage/enabled\necho never >/sys/kernel/mm/transparent_hugepage/enabled\n```\n\nSince Redis doesn't mark any memory as `MADV_HUGEPAGE` with `madvise`, this setting is safe for Redis and still would allow other processes to use huge page tables if they want without impacting Redis.\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-05-12T01:05:57Z",
        "closed_at": "2015-05-15T15:06:18Z",
        "merged_at": "2015-05-15T15:06:18Z",
        "body": "It closes #2572.\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-05-11T19:32:57Z",
        "closed_at": "2015-05-25T10:06:25Z",
        "merged_at": "2015-05-25T10:06:25Z",
        "body": "This new command triggers a config flush to save the in-memory config to\ndisk. This is useful for cases of a configuration management system or a\npackage manager wiping out your sentinel config while the process is\nstill running - and has not yet been restarted. It can also be useful\nfor scripting a backup and migrate or clone of a running sentinel.\n\nFor additional reasons and details see: https://github.com/therealbill/redis-rcp/blob/master/RCP4.md as it implements RCP4.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 117,
        "deletions": 99,
        "changed_files": 13,
        "created_at": "2015-05-09T06:10:56Z",
        "closed_at": "2021-04-26T23:23:04Z",
        "merged_at": null,
        "body": "This patch series does two things:\n- Unify the numerous declarations of millisecond/microsecond resolution timestamp functions into a single location (adding rtime.c and rtime.h)\n- Use CLOCK_MONOTONIC when it's available (see justification in commit message). As far as I'm aware, this is a Linux-only thing. All other platforms will continue to use gettimeofday().\n\nAny call sites that are actually _intending_ to get wall clock time (e.g. for log timestamps) are unmodified.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2015-05-08T09:13:32Z",
        "closed_at": "2020-09-09T16:29:26Z",
        "merged_at": null,
        "body": "readme.md: 2 grammar and typo changes\nredis.conf: changed \"uncomment\"  to \"comment out\", as the word used was opposite of the intended meaning.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-05-07T19:28:50Z",
        "closed_at": "2015-05-15T15:22:22Z",
        "merged_at": "2015-05-15T15:22:22Z",
        "body": "Fix typo in 00-RELEASENOTES.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-05-05T13:52:56Z",
        "closed_at": "2015-05-05T16:44:47Z",
        "merged_at": "2015-05-05T16:44:46Z",
        "body": "fix compile error with struct msghdr  in FreeBSD 10\n\nI test build and run ./runtest, it works well.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-05-05T12:37:44Z",
        "closed_at": "2021-06-03T11:49:48Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 27722,
        "deletions": 2457,
        "changed_files": 194,
        "created_at": "2015-04-30T11:19:53Z",
        "closed_at": "2015-05-04T10:35:49Z",
        "merged_at": null,
        "body": "study\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-04-28T15:06:44Z",
        "closed_at": "2015-05-04T10:17:42Z",
        "merged_at": "2015-05-04T10:17:41Z",
        "body": "I found simple memory leak in sentinel.\n\njust fixed it.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-04-28T09:06:51Z",
        "closed_at": "2015-04-29T08:06:46Z",
        "merged_at": "2015-04-29T08:06:46Z",
        "body": "Closes #2549\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-04-26T09:10:30Z",
        "closed_at": "2015-05-04T10:19:45Z",
        "merged_at": "2015-05-04T10:19:45Z",
        "body": "The problem seems to exist in all versions (although the crash is a bit different).  A quick script to reproduce the issue:\n\n``` bash\nredis-cli CONFIG SET client-output-buffer-limit \"normal 8388608 8388608 3\"\nredis-cli EVAL \"redis.call('set', 'bigkey', string.rep('xxxxxxxxxx',2000000))\" 0\nredis-cli EVAL \"redis.call('get', 'bigkey')\" 0\necho 'Redis will crash any moment now...'\nwhile true; do\n  redis-cli EVAL \"redis.call('ping')\" 0\n  sleep 1\ndone\n```\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-04-23T22:34:09Z",
        "closed_at": "2015-04-24T07:05:14Z",
        "merged_at": "2015-04-24T07:05:14Z",
        "body": "Just so it's extra official :smile:\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-04-23T17:03:36Z",
        "closed_at": "2015-05-04T10:55:41Z",
        "merged_at": null,
        "body": "Originally, only the +slave event which occurs when a slave is\nreconfigured during sentinelResetMasterAndChangeAddress triggers a flush\nof the config to disk.  However, newly discovered slaves don't\napparently trigger this flush but do trigger the +slave event issuance.\n\nSo if you start up a sentinel, add a master, then add a slave to the\nmaster (as a way to reproduce it) you'll see the +slave event issued,\nbut the sentinel config won't be updated with the known-slave entry.\n\nThis change makes sentinel do the flush of the config if a new slave is\ndetected in sentinelRefreshInstanceInfo.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-04-21T15:54:59Z",
        "closed_at": "2015-05-04T10:56:02Z",
        "merged_at": "2015-05-04T10:56:02Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-04-20T07:01:42Z",
        "closed_at": "2015-05-04T11:06:57Z",
        "merged_at": null,
        "body": "_dictNextPower() can be an expensive operation when size is big number, so try to delay it as much as possible.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-04-19T15:47:08Z",
        "closed_at": "2015-05-04T11:00:02Z",
        "merged_at": "2015-05-04T11:00:02Z",
        "body": "- fix typo\n- fix doc example\n- free unused sds(x and y )\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2015-04-17T13:31:00Z",
        "closed_at": "2015-05-15T15:36:18Z",
        "merged_at": "2015-05-15T15:36:18Z",
        "body": "Uphold the [smove contract](http://redis.io/commands/smove) to return 0 when the element is not a member of the source set, even if source=dest.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-04-15T15:22:08Z",
        "closed_at": "2020-09-05T20:43:31Z",
        "merged_at": null,
        "body": "sentinel should be changed to cluster\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 18,
        "changed_files": 1,
        "created_at": "2015-04-14T04:13:31Z",
        "closed_at": "2015-04-14T16:53:23Z",
        "merged_at": null,
        "body": "RESP was introduced in redis 1.2, yet the benchmarks run using the old protocol, which partially defeats the purpose of the benchmark.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-04-13T21:23:37Z",
        "closed_at": "2020-09-05T20:44:51Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-04-10T20:41:29Z",
        "closed_at": "2021-11-27T11:36:25Z",
        "merged_at": null,
        "body": "  list was outdated.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-04-09T08:54:53Z",
        "closed_at": "2015-07-24T08:26:35Z",
        "merged_at": null,
        "body": "dictEntry struct, listNode struct, and maybe others, consume 24 bytes each.\nbut since jemalloc doesn't have a pool for 24 bytes allocations they are taken from the 32 bytes pool, and produce about 33% internal fragmentation overhead.\n\nThis patch adds a 24 bytes pool size class to jemalloc.\n\n**test (with relatively short strings)**\ndebug populate 10000000\noriginal code's used_memory: 1254709872\nwith patch used_memory: 1094714048\nmemory optimization: **14%**\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 364,
        "deletions": 139,
        "changed_files": 8,
        "created_at": "2015-04-09T08:32:54Z",
        "closed_at": "2015-07-24T06:54:00Z",
        "merged_at": null,
        "body": "sds strings were using 8 byte header, this has two implications:\n1. wasting memory - even on short strings (of less than 256 chars) we have a full 4 bytes len field.\n2. we are not able to hold strings that are larger than 2^31 chars. (see issues with MIGRAGE https://github.com/antirez/redis/issues/757)\n\nWith this patch, each sds instance can be one of 4 different internal types:\n- strings with 8bit length - 3 bytes header\n- strings with 16bit length - 5 bytes header\n- strings with 32bit length - 9 bytes header\n- strings with 64bit length - 17 bytes header\n\n**Test (on relatively short strings)**\ndebug populate 10000000\nused_memory of original code: 1254709872\nused_memory with new code: 1078723024\nmemory optimization: **16%**\n\n*Notice that considering internal fragmentation, there are some string lengths which will now fit into a smaller jemalloc bin, for these the result will be more 2x factor of memory reduction.\n",
        "comments": 23
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-04-08T14:33:14Z",
        "closed_at": "2021-10-27T09:14:37Z",
        "merged_at": null,
        "body": "A little oversight. :-)\n### Test Code\n\n``` c\nint main()\n{                         \n    int retry = 10;       \n    while(retry--);                 \n    printf(\"%d\\n\", retry);                                                \n}\n```\n### Output\n\n``` c\n-1\n```\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 45,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2015-04-08T11:19:21Z",
        "closed_at": "2018-01-09T16:36:04Z",
        "merged_at": "2018-01-09T16:36:04Z",
        "body": "#The format of migrate command is \"MIGRATE host port key destination-db timeout [COPY] [REPLACE]\". If the destination server is protected by a paassword, it will not work. This obstacle has confused so many people and so long time (for example:http://redis.io/commands/migrate, the question asked by Marc-Olivier LaBarre \"Does it work if the destination redis instance is protected by a password?\") that I decided to add this feature by myself.\r\n",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-04-07T02:51:52Z",
        "closed_at": "2017-07-03T11:13:34Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 36028,
        "deletions": 8390,
        "changed_files": 330,
        "created_at": "2015-04-06T08:45:49Z",
        "closed_at": "2015-04-08T12:34:58Z",
        "merged_at": null,
        "body": "/\\* MIGRATE host port key dbid timeout [COPY | REPLACE | PASSWORD] /\nvoid migrateCommand(redisClient *c) {\nmigrateCachedSocket *cs;\nint copy, replace, auth, j;\nchar password;\nlong timeout;\nlong dbid;\nlong long ttl, expireat;\nrobj *o;\nrio cmd, payload;\nint retry_num = 0;\n\ntry_again:\n/\\* Initialization */\ncopy = 0;\nreplace = 0;\nttl = 0;\nauth = 0;\npassword = NULL;\n\n/\\* Parse additional options */\nfor (j = 6; j < c->argc; j++) {\n    if (!strcasecmp(c->argv[j]->ptr,\"copy\")) {\n        copy = 1;\n    } else if (!strcasecmp(c->argv[j]->ptr,\"replace\")) {\n        replace = 1;\n    } else if (!auth) {\n        auth = 1;\n        password = c->argv[j]->ptr;\n    } else {\n        addReply(c,shared.syntaxerr);\n        return;\n    }\n}\n\n/\\* Sanity check */\nif (getLongFromObjectOrReply(c,c->argv[5],&timeout,NULL) != REDIS_OK)\n    return;\nif (getLongFromObjectOrReply(c,c->argv[4],&dbid,NULL) != REDIS_OK)\n    return;\nif (timeout <= 0) timeout = 1000;\n\n/\\* Check if the key is here. If not we reply with success as there is\n- nothing to migrate (for instance the key expired in the meantime), but\n- we include such information in the reply string. */\n  if ((o = lookupKeyRead(c->db,c->argv[3])) == NULL) {\n  addReplySds(c,sdsnew(\"+NOKEY\\r\\n\"));\n  return;\n  }\n\n/\\* Connect _/\ncs = migrateGetSocket(c,c->argv[1],c->argv[2],timeout);\nif (cs == NULL) return; /_ error sent to the client by migrateGetSocket() */\n\nrioInitWithBuffer(&cmd,sdsempty());\n\n/\\* Authentication _/\nif (auth) {\n    redisAssertWithInfo(c,NULL,rioWriteBulkCount(&cmd,'_',2));\n    redisAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,\"AUTH\",4));\n    redisAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,password,strlen(password)));\n}\n\n/\\* Send the SELECT command if the current DB is not already selected. _/\nint select = cs->last_dbid != dbid; /_ Should we emit SELECT? _/\nif (select) {\n    redisAssertWithInfo(c,NULL,rioWriteBulkCount(&cmd,'_',2));\n    redisAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,\"SELECT\",6));\n    redisAssertWithInfo(c,NULL,rioWriteBulkLongLong(&cmd,dbid));\n}\n\n/\\* Create RESTORE payload and generate the protocol to call the command. _/\nexpireat = getExpire(c->db,c->argv[3]);\nif (expireat != -1) {\n    ttl = expireat-mstime();\n    if (ttl < 1) ttl = 1;\n}\nredisAssertWithInfo(c,NULL,rioWriteBulkCount(&cmd,'_',replace ? 5 : 4));\nif (server.cluster_enabled)\n    redisAssertWithInfo(c,NULL,\n        rioWriteBulkString(&cmd,\"RESTORE-ASKING\",14));\nelse\n    redisAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,\"RESTORE\",7));\nredisAssertWithInfo(c,NULL,sdsEncodedObject(c->argv[3]));\nredisAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,c->argv[3]->ptr,\n        sdslen(c->argv[3]->ptr)));\nredisAssertWithInfo(c,NULL,rioWriteBulkLongLong(&cmd,ttl));\n\n/\\* Emit the payload argument, that is the serialized object using\n- the DUMP format. */\n  createDumpPayload(&payload,o);\n  redisAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,payload.io.buffer.ptr,\n                          sdslen(payload.io.buffer.ptr)));\n  sdsfree(payload.io.buffer.ptr);\n\n/\\* Add the REPLACE option to the RESTORE command if it was specified\n- as a MIGRATE option. */\n  if (replace)\n  redisAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,\"REPLACE\",7));\n\n/\\* Transfer the query to the other node in 64K chunks. */\nerrno = 0;\n{\n    sds buf = cmd.io.buffer.ptr;\n    size_t pos = 0, towrite;\n    int nwritten = 0;\n\n```\nwhile ((towrite = sdslen(buf)-pos) > 0) {\n    towrite = (towrite > (64*1024) ? (64*1024) : towrite);\n    nwritten = syncWrite(cs->fd,buf+pos,towrite,timeout);\n    if (nwritten != (signed)towrite) goto socket_wr_err;\n    pos += nwritten;\n}\n```\n\n}\n\n/\\* Read back the reply. */\n{\n    char buf1[1024];\n    char buf2[1024];\n    char buf3[1024];\n\n```\n/* Read the three replies */\nif (auth && syncReadLine(cs->fd, buf1, sizeof(buf1), timeout) <= 0)\n    goto socket_rd_err;\nif (select && syncReadLine(cs->fd, buf2, sizeof(buf2), timeout) <= 0)\n    goto socket_rd_err;\nif (syncReadLine(cs->fd, buf3, sizeof(buf3), timeout) <= 0)\n    goto socket_rd_err;\n\n// Do not need to return error about server password.\n//\n// If the server (c->argv[1]:c->argv[2]) have not set the password while the client\n// sent its command with password, the following operations will continue to sucess.\n//\n// if the server have set the password while the client sent its command without any\n// any password, the following operations will also fail and return the alert\n// \"NOAUTH Authentication required\".\n//\n// if ((auth && buf1[0] == '-') || (select && buf2[0] == '-') || buf3[0] == '-') {\n//     addReplyErrorFormat(c,\"Target instance replied with error: %s\",\n//         (auth && buf1[0] == '-') ? buf1+1 : ((select && buf2[0] == '-') ? buf2+1 : buf3+1));\n//     goto socket_rd_err;\n// }\nif ((select && buf2[0] == '-') || buf3[0] == '-') {\n    /* On error assume that last_dbid is no longer valid. */\n    cs->last_dbid = -1;\n    addReplyErrorFormat(c,\"Target instance replied with error: %s\",\n        (select && buf2[0] == '-') ? buf2+1 : buf3+1);\n} else {\n    /* Update the last_dbid in migrateCachedSocket */\n    cs->last_dbid = dbid;\n    robj *aux;\n\n    addReply(c,shared.ok);\n\n    if (!copy) {\n        /* No COPY option: remove the local key, signal the change. */\n        dbDelete(c->db,c->argv[3]);\n        signalModifiedKey(c->db,c->argv[3]);\n        server.dirty++;\n\n        /* Translate MIGRATE as DEL for replication/AOF. */\n        aux = createStringObject(\"DEL\",3);\n        rewriteClientCommandVector(c,2,aux,c->argv[3]);\n        decrRefCount(aux);\n    }\n}\n```\n\n}\n\nsdsfree(cmd.io.buffer.ptr);\nreturn;\nsocket_wr_err:\nsdsfree(cmd.io.buffer.ptr);\nmigrateCloseSocket(c->argv[1],c->argv[2]);\nif (errno != ETIMEDOUT && retry_num++ == 0) goto try_again;\naddReplySds(c,\nsdsnew(\"-IOERR error or timeout writing to target instance\\r\\n\"));\nreturn;\n\nsocket_rd_err:\nsdsfree(cmd.io.buffer.ptr);\nmigrateCloseSocket(c->argv[1],c->argv[2]);\nif (errno != ETIMEDOUT && retry_num++ == 0) goto try_again;\naddReplySds(c,\nsdsnew(\"-IOERR error or timeout reading from target node\\r\\n\"));\nreturn;\n}\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2015-04-04T21:14:43Z",
        "closed_at": "2018-10-01T17:44:58Z",
        "merged_at": null,
        "body": "Changed links to hyperlinks.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2015-04-04T21:00:29Z",
        "closed_at": "2020-09-09T16:28:40Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2015-04-02T06:36:43Z",
        "closed_at": "2021-06-08T09:27:15Z",
        "merged_at": null,
        "body": "Use the C99 standard uint32_t type instead of the implementation\nspecific u_int32_t.\n\nThis fixes the following compile error when building with musl libc:\n\n  In file included from sha1.c:28:0:\n  sha1.h:11:5: error: unknown type name 'u_int32_t'\n       u_int32_t state[5];\n       ^\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-04-02T04:19:59Z",
        "closed_at": "2016-12-03T20:56:55Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 22,
        "changed_files": 2,
        "created_at": "2015-04-01T16:35:18Z",
        "closed_at": "2017-07-03T11:29:11Z",
        "merged_at": null,
        "body": "I've read the sds.h & sds.c completely, and I find there are some places need to be slightly modified.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-04-01T15:30:06Z",
        "closed_at": "2020-09-05T20:48:00Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-04-01T01:44:23Z",
        "closed_at": "2021-06-10T07:08:56Z",
        "merged_at": null,
        "body": "if fp is a NULL pointer,why do we waste time computing strlen(field)\nsorry for my bad English\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2015-03-31T20:48:55Z",
        "closed_at": "2015-04-01T07:43:34Z",
        "merged_at": "2015-04-01T07:43:34Z",
        "body": "master was closing the connection if the RDB transfer took long time.\nand also sent PINGs to the slave before it got the initial ACK, in which case the slave wouldn't be able to find the EOF marker.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2015-03-31T10:28:43Z",
        "closed_at": "2021-06-22T05:56:09Z",
        "merged_at": null,
        "body": "**dup** succeed, but **listAddNodeTail** failed, memory may leak.\nneed release **_value_.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-03-31T02:12:08Z",
        "closed_at": "2020-06-26T21:04:44Z",
        "merged_at": null,
        "body": "Add maintenance mode to sentinel.. kind of like we talked about at redis con @antirez \n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2015-03-30T19:26:21Z",
        "closed_at": "2020-06-26T21:05:10Z",
        "merged_at": null,
        "body": "Example runs below:\n\n```\n[root@redis-cluster-node1 ~]# ./redis-trib.rb show-key-distribution 10.184.13.42:6379\nConnecting to node 10.184.13.42:6379: OK\nConnecting to node 10.184.13.41:6379: OK\nConnecting to node 10.184.13.38:6379: OK\nConnecting to node 10.184.13.34:6379: OK\nConnecting to node 10.184.13.40:6379: OK\nConnecting to node 10.184.13.4:6379: OK\n\n\nMaster IP       Keys On Shard\n10.184.13.42:6379   1827621\n10.184.13.41:6379   1659226\n10.184.13.40:6379   1835822\n```\n\n```\n[root@redis-cluster-node1 ~]# ./redis-trib.rb call --slaveonly 10.184.13.42:6379 dbsize\nConnecting to node 10.184.13.42:6379: OK\nConnecting to node 10.184.13.41:6379: OK\nConnecting to node 10.184.13.38:6379: OK\nConnecting to node 10.184.13.34:6379: OK\nConnecting to node 10.184.13.40:6379: OK\nConnecting to node 10.184.13.4:6379: OK\n>>> Calling DBSIZE\n10.184.13.38:6379: 1827621\n10.184.13.34:6379: 1835822\n10.184.13.4:6379: 1659226\n```\n\n```\n[root@redis-cluster-node1 ~]# ./redis-trib.rb call --masteronly 10.184.13.42:6379 dbsize\nConnecting to node 10.184.13.42:6379: OK\nConnecting to node 10.184.13.41:6379: OK\nConnecting to node 10.184.13.38:6379: OK\nConnecting to node 10.184.13.34:6379: OK\nConnecting to node 10.184.13.40:6379: OK\nConnecting to node 10.184.13.4:6379: OK\n>>> Calling DBSIZE\n10.184.13.42:6379: 1827621\n10.184.13.41:6379: 1659226\n10.184.13.40:6379: 1835822\n```\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-03-26T23:51:07Z",
        "closed_at": "2020-09-09T16:28:44Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-03-26T20:41:26Z",
        "closed_at": "2015-04-01T13:32:28Z",
        "merged_at": "2015-04-01T13:32:28Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2015-03-24T18:06:18Z",
        "closed_at": "2017-11-02T14:34:15Z",
        "merged_at": null,
        "body": "See #2110\n\nWhen binding redis to an IP address, using a network interface with multiple IP aliases, Redis picks up the wrong IP, so the sentinels are not able to connect to the slave and they mark it as `sdown`.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-03-23T20:45:56Z",
        "closed_at": "2015-03-24T08:07:22Z",
        "merged_at": "2015-03-24T08:07:22Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-03-23T08:23:56Z",
        "closed_at": "2020-09-09T16:29:29Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 36073,
        "deletions": 8390,
        "changed_files": 330,
        "created_at": "2015-03-21T00:06:48Z",
        "closed_at": "2015-04-01T13:34:02Z",
        "merged_at": null,
        "body": "Trying to dowload\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 251,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2015-03-16T13:46:49Z",
        "closed_at": "2020-12-30T14:05:02Z",
        "merged_at": null,
        "body": "See Issue https://github.com/antirez/redis/issues/678\nThe new command uses this way:\n  `z[rev]rangebyscorestore dstkey key min max [LIMIT offset count]`\n\nThis implementation has 2 optimization:\n1. avoid appending elements one by one to ziplist\n2. check if skiplist can be casted to ziplist in the end.\n\n---\n### TEST CASE\n\nadd 32 elements to ztest\n\n``` c\n127.0.0.1:6379> zadd ztest 1 a1 2 a2 3 a3 4 a4 5 a5 6 a6 7 a7 8 a8 9 a9 10 a10 11 a11 12 a12 13 a13 14 a14 15 a15 16 a16 17 a17 18 a18 19 a19 20 a20 21 a21 22 a22 23 a23 24 a24 25 a25 26 a26 27 a27 28 a28 29 a29 30 a30 31 a31 32 a32\n```\n\nget and output element from 2 to 16\n\n``` c\n127.0.0.1:6379> zrangebyscorestore helo ztest 2 16\n(integer) 15\n127.0.0.1:6379> zrange helo 0 -1\n 1) \"a2\"\n 2) \"a3\"\n 3) \"a4\"\n 4) \"a5\"\n 5) \"a6\"\n 6) \"a7\"\n 7) \"a8\"\n 8) \"a9\"\n 9) \"a10\"\n10) \"a11\"\n11) \"a12\"\n12) \"a13\"\n13) \"a14\"\n14) \"a15\"\n15) \"a16\"\n```\n\nget and output element from 2+2 to 2+2+4-1\n\n``` c\n127.0.0.1:6379> zrangebyscorestore helo ztest 2 16 limit 2 4\n(integer) 4\n127.0.0.1:6379> zrange helo 0 -1 \n1) \"a4\"\n2) \"a5\"\n3) \"a6\"\n4) \"a7\"\n```\n\nget and output element from 4 to 16\n\n``` c\n127.0.0.1:6379> zrevrangebyscorestore helo ztest 16 4\n(integer) 13\n127.0.0.1:6379> zrange helo 0 -1 \n 1) \"a4\"\n 2) \"a5\"\n 3) \"a6\"\n 4) \"a7\"\n 5) \"a8\"\n 6) \"a9\"\n 7) \"a10\"\n 8) \"a11\"\n 9) \"a12\"\n10) \"a13\"\n11) \"a14\"\n12) \"a15\"\n13) \"a16\"\n```\n\nget and output element from 16-5-3+1 to 16-5\n\n``` c\n127.0.0.1:6379> zrevrangebyscorestore helo ztest 16 4 limit  5 3\n(integer) 3\n127.0.0.1:6379> zrange helo 0 -1 \n1) \"a9\"\n2) \"a10\"\n3) \"a11\"\n```\n\n---\n### MORE TEST  TO show convert between ziplist and skiplist\n\n``` c\nset zset-max-ziplist-entries 4\nset zset-max-ziplist-value 4\n```\n\n``` c\n127.0.0.1:6379> zadd hello 123 hello 321 world 1 a1 2 a2 3 a3 4 a4 5 a5\n(integer) 7\n127.0.0.1:6379> OBJECT ENCODING hello\n\"skiplist\"\n```\n\nentries > 4 && max-value <= 4  ---- skiplist\n\n``` c\n127.0.0.1:6379> zrangebyscorestore world hello 1 5\n(integer) 5\n127.0.0.1:6379> OBJECT ENCODING world\n\"skiplist\"\n```\n\nentries <= 4 && max-value <= 4  ---- ziplist\n\n``` c\n127.0.0.1:6379> zrangebyscorestore world hello 1 4\n(integer) 4\n127.0.0.1:6379> OBJECT ENCODING world\n\"ziplist\"\n```\n\nentries <= 4 && max-value > 4  ---- skiplist\n\n``` c\n127.0.0.1:6379> zrangebyscorestore world hello 4 200\n(integer) 3\n127.0.0.1:6379> OBJECT ENCODING world\n\"skiplist\"\n127.0.0.1:6379> zrange world 0 -1\n1) \"a4\"\n2) \"a5\"\n3) \"hello\"\n```\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2015-03-12T21:35:17Z",
        "closed_at": "2021-06-27T09:09:13Z",
        "merged_at": null,
        "body": "This is useful for adding various security flags at build time\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2015-03-12T01:14:10Z",
        "closed_at": "2021-05-25T12:40:34Z",
        "merged_at": null,
        "body": "- correct size for EMBSTR\n  \n  The amount of memory used by the sds string at object->ptr for a REDIS_ENCODING_EMBSTR object is:\n  sdslen(o->ptr) + sizeof(struct sdshdr) + 1;\n- replace a code block with existing sdigits10\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-03-10T22:26:18Z",
        "closed_at": "2020-09-05T20:52:42Z",
        "merged_at": null,
        "body": "To listen on all interfaces one has to _comment_ the `bind` directive.\n\nThis fixes a typo introduced in 0aa5acc8f31a45ba4ee625227bae80e125fd8bdb\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2015-03-06T07:18:38Z",
        "closed_at": "2021-03-16T14:54:57Z",
        "merged_at": null,
        "body": "- anetUnixGenericConnect: close fd when anetNonBlock failed\n  - anetNonBlock doesn't close fd when error happens.\n-  _anetTcpServer: close fd when anetSetReuseAddr failed\n  - anetV6Only and anetListen close fd inside when failed, anetSetReuseAddr only reports error but doesn't close it.\n- anetSockName: check ip and ip_len before write to ip\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2015-03-04T08:58:20Z",
        "closed_at": "2015-03-04T15:45:26Z",
        "merged_at": null,
        "body": "Unstable\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-03-03T13:01:07Z",
        "closed_at": "2015-03-10T02:35:59Z",
        "merged_at": null,
        "body": "After we delete an event, we should also update the events' mask,\nbecause next add and delete operation will depend on events' mask.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2015-03-03T08:02:10Z",
        "closed_at": "2020-09-09T16:28:46Z",
        "merged_at": null,
        "body": "Fix 'salve' typos to 'slave'\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-03-03T03:39:30Z",
        "closed_at": "2020-09-09T16:29:32Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-03-03T03:32:10Z",
        "closed_at": "2015-03-03T16:03:57Z",
        "merged_at": null,
        "body": "Makes it easier to work on the Mac (hope it's all right!) \n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 39,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2015-03-03T01:51:57Z",
        "closed_at": "2022-01-17T08:02:58Z",
        "merged_at": null,
        "body": "- In kqueue, {`ident`, `filter`} identify an unique event.\n  As we now care for EVFILT_READ and EVFIT_WRITE, the event kind is AE_KIND == 2. \n  We should alloc \"sizeof(kevent) \\* setsize \\* AE_KIND\" bytes for \"events\" of aeApiState to satisfy the ceil of unique event amount.\n- `kqueue` may return events `{fd1, EVFILT_READ},{fd2, EVFIT_WRITE}...{*.*}`, and fd1 can equals to fd2.\n  To keep consistency with other implements of `aeApiPoll`, we should merge them into one event in `eventLoop->fired`\n  ---\n  ## TEST CODE\n\n``` c\n#include<sys/types.h>\n#include<sys/event.h>\n#include<sys/time.h>\n#include<fcntl.h>\n#include<unistd.h>\n#include<stdio.h>\n\nint main()\n{\n    struct kevent events[4];\n    int kq = kqueue();\n    int fd = open(\"hw\", O_RDWR);\n    EV_SET(events, fd, EVFILT_READ, EV_ADD, 0, 0, NULL);\n    kevent(kq, events, 1, NULL, 0, NULL);\n    EV_SET(events, fd, EVFILT_WRITE, EV_ADD, 0, 0, NULL);\n    kevent(kq, events, 1, NULL, 0, NULL);\n    printf(\"%d\\n\", kevent(kq, NULL, 0, events, 4, NULL));\n    return 0;\n}\n```\n\n---\n\n``` c\n# echo \"hello world\" > hw\n# gcc kqueue.c\n# ./a.out\n```\n## RESULT\n\n`printf(\"%d\\n\", kevent(kq, NULL, 0, events, 4, NULL));` outputs 2.\nWe care EVFILT_READ and EVFIT_WRITE on a single fd, and it finally return 2 events.\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 483,
        "deletions": 246,
        "changed_files": 17,
        "created_at": "2015-03-02T16:10:59Z",
        "closed_at": "2017-04-25T11:17:28Z",
        "merged_at": null,
        "body": "See discussion:\nhttps://groups.google.com/forum/#!topic/redis-dev/ky7Je45_vVU\n",
        "comments": 16
    },
    {
        "merged": true,
        "additions": 36,
        "deletions": 23,
        "changed_files": 1,
        "created_at": "2015-02-25T18:40:07Z",
        "closed_at": "2015-02-26T09:17:13Z",
        "merged_at": "2015-02-26T09:17:13Z",
        "body": "Avoid redundant SELECT calls when continuously migrating keys to\nthe same dbid within a target Redis instance.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 91,
        "deletions": 20,
        "changed_files": 4,
        "created_at": "2015-02-22T21:23:59Z",
        "closed_at": "2015-02-26T15:00:51Z",
        "merged_at": null,
        "body": "[Issue #2416]\n\nINFO command was designed to take only a single parameter or \"all\" as\ninput. Changes to INFO done in this PR can make it more useful by\nenabling multiple options.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 47,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2015-02-21T15:03:23Z",
        "closed_at": "2015-02-27T14:24:04Z",
        "merged_at": "2015-02-27T14:24:04Z",
        "body": "In response to https://github.com/antirez/redis/issues/2065\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-02-20T17:10:46Z",
        "closed_at": "2021-03-18T14:52:25Z",
        "merged_at": null,
        "body": "Fixes the https://github.com/antirez/redis/issues/2412\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-02-19T17:19:09Z",
        "closed_at": "2021-07-15T09:44:47Z",
        "merged_at": null,
        "body": "The RDB version is updated to 7 by commit 101b3a6e42e84e5cb423ef413225d8b8d8cc0bbc, \nbut Redis has processed version 7 as invalid version by now.\nIn unstable branch, redis-check-rdb always failed as follows.\n\n$ bin/redis-check-rdb data/dump.rdb\n48433:C 20 Feb 01:57:33.767 # Checking RDB file data/dump.rdb\n48433:C 20 Feb 01:57:33.767 # Unknown RDB format version: 7\n\nPlease give me comments.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2015-02-19T15:13:41Z",
        "closed_at": "2015-02-27T13:19:02Z",
        "merged_at": null,
        "body": "Found this one this morning; the code in here was dead because `nread` is checked for `0` right above. This was causing the `server.current_client` from not being reset to `NULL` when the client closed the connection.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-02-18T13:19:09Z",
        "closed_at": "2015-02-25T11:52:23Z",
        "merged_at": "2015-02-25T11:52:23Z",
        "body": "rehashing.c fails to compile as dictGetRandomKeys was removed in the recent SPOP rewrite. This patch uses dictGetSomeKeys to fix the compile error.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 7457,
        "deletions": 5939,
        "changed_files": 33,
        "created_at": "2015-02-17T19:12:23Z",
        "closed_at": "2020-08-24T14:23:41Z",
        "merged_at": null,
        "body": "Initial PR for adding SSL support natively to Redis.\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-02-17T16:43:45Z",
        "closed_at": "2015-02-24T16:13:39Z",
        "merged_at": "2015-02-24T16:13:39Z",
        "body": "The built-in quicklist Pop() is completely busted and needs this fix from emperor1523.  This PR also includes accurate testing instead of dumb testing.\n\nNote: this problem doesn't show up in Redis usage because Redis uses quicklistPopCustom() to provide a custom non-busted allocator.  Perhaps emperor1523 was using quicklist in another codebase and found the problem (or maybe was just reading the source and noticed the memcpy error)?\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-02-16T16:15:03Z",
        "closed_at": "2015-02-24T16:14:02Z",
        "merged_at": null,
        "body": "Argument position was mistake\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-02-12T12:33:02Z",
        "closed_at": "2015-02-12T16:07:04Z",
        "merged_at": "2015-02-12T16:07:04Z",
        "body": "All other headline are uppercase, but \"Event notification\" is not. \n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2015-02-12T03:34:28Z",
        "closed_at": "2015-02-25T11:09:57Z",
        "merged_at": null,
        "body": "For maintainance, I think setting read-only mode is useful.\n(not for seperate client with readonlyCommand)\n\nfor example, when change master server to another. To ensure, master data is not changed.\n\n``` c\nconfig set read-only yes\n\nset a abc\n-READONLY You can't write against a read only server.\n\nconfig set read-only no\n\nset a abc\n```\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 38,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2015-02-12T02:11:51Z",
        "closed_at": "2020-10-02T12:11:47Z",
        "merged_at": null,
        "body": "Closes #2349\n\nParameter added in order to retrieve the value before setting the new value.\n\nWith this option we can replace the `GETSET` command because we can do the same with only the `SET` command.\n\n@antirez @mattsta  I'm closing the #2351 after our discussion because we changed the implementation completely. Can you review it please?\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-02-12T00:08:35Z",
        "closed_at": "2015-03-13T17:23:36Z",
        "merged_at": "2015-03-13T17:23:36Z",
        "body": "When trying to debug sentinel connections or max connections errors it would be very useful to have the ability to see the list of connected clients to a running sentinel. At the same time it would be very helpful to be able to name each sentinel connection or kill offending clients.\n\nThis commits adds the already defined `CLIENT` commands back to Redis Sentinel.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 116,
        "deletions": 47,
        "changed_files": 4,
        "created_at": "2015-02-11T14:04:23Z",
        "closed_at": "2016-05-12T17:47:39Z",
        "merged_at": null,
        "body": "Efficiency improvements in dict.c\n- add dictMove can be used to move an entry from one dict to the other\n  without double key-hashing and memory allocation overheads.\n- avoid repeated key-hashing like find followed by add or by delete\n  (see previous implementation of dictReplace)\n- responsibilities and scope of _dictKeyIndex and dictGenericDelete\n  changed so that they are flexible and better serve wrapper functions.\n- dictDeleteNoFree can optionally return a copy of the dictEntry it deleted\n  (someone can use it instead of doing dictFind before calling dictDelete)\n- dictAddRaw optionally returns the pre-existing entry in case the add failed\n  (so someone doesn't need to call find when it fails if he needs the entry)\n- comment improvements on function names:\n  dictReplace = \"Add or Overwrite\", dictReplaceRaw = \"Add or Find\", etc.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2015-02-11T13:37:35Z",
        "closed_at": "2016-05-09T19:56:56Z",
        "merged_at": null,
        "body": "- change (d->ht[0].size == 0) with (d->ht[0].used + d->ht[1].used == 0) to abort a lookup when the dict is empty (not only when it is not yet initialized)\n- avoid calling dictCompareKeys on two identical pointers (two strings sitting in the same memory address are identical)\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 440,
        "deletions": 91,
        "changed_files": 8,
        "created_at": "2015-02-08T14:47:50Z",
        "closed_at": "2015-02-11T12:59:53Z",
        "merged_at": null,
        "body": "- add dictMove can be used to move an entry from one dict to the other\n  without double key-hashing and memory allocation overheads.\n- avoid repeated key-hashing like find followed by add or by delete\n  (see previous implementation of dictReplace)\n- responsibilities and scope of _dictKeyIndex and dictGenericDelete\n  changed so that they are flexible and better serve wrapper functions.\n- dictDeleteNoFree can optionally return a copy of the dictEntry it deleted\n  (someone can use it instead of doing dictFind before calling dictDelete)\n- dictAddRaw optionally returns the pre-existing entry in case the add failed\n  (so someone doesn't need to call find when it fails if he needs the entry)\n- _dictNextPower uses bit trick rather than loop\n- change (d->ht[0].size == 0) with (d->ht[0].used + d->ht[1].used == 0)\n  to abort a lookup when the dict is empty (not only when it is not yet initialized)\n- avoid calling dictCompareKeys on two identical pointers\n  (two strings sitting in the same memory address are identical)\n- comment improvements on function names:\n  dictReplace = \"Add or Overwrite\", dictReplaceRaw = \"Add or Find\", etc.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2015-02-07T17:02:53Z",
        "closed_at": "2015-02-10T08:27:46Z",
        "merged_at": "2015-02-10T08:27:46Z",
        "body": "Fixing #2371 as per @mattsta's suggestion\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-02-06T03:21:07Z",
        "closed_at": "2015-02-06T09:46:54Z",
        "merged_at": "2015-02-06T09:46:54Z",
        "body": "(In dictc branch) As we directly touch rehashidx, it's better we keep it in synchronization.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 179,
        "deletions": 125,
        "changed_files": 1,
        "created_at": "2015-02-05T14:00:05Z",
        "closed_at": "2021-08-06T10:34:48Z",
        "merged_at": null,
        "body": "Added new mixed `SET/GET` test with configurable SET/GET requests ratio `-t mixed [-a <ratio>]`.\nDefault ratio is 50. `-t mixed -a 0` is equivalent to `-t get`, and `-t mixed -a 100` is equivalent to `-t set`.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 38,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2015-02-05T13:53:11Z",
        "closed_at": "2021-03-18T15:34:47Z",
        "merged_at": null,
        "body": "Added new benchmark option to support user-defined data payload `-z <filename>`.\n\nInstead of using _\"xxx....\"_ as command data (object values), data payload can be loaded from the specified file. File must be large enough to hold the data of the configured data size (`-d <size>`).\nThis feature supports binary data (including '\\0' bytes), so it can be used with some (pseudo-)random streams, like `/dev/urandom`.\nNew option applies to set-like commands only (`SET/MSET/LPUSH/RPUSH/...`).\n\nUsing the predefined data payload does not affect Redis performance directly, but may be used to exercise the RDB compression algorithm, and thus, by increasing the snapshot size and snapshot saving time, indirectly influence the overall server performance.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 48,
        "deletions": 13,
        "changed_files": 4,
        "created_at": "2015-02-05T00:53:06Z",
        "closed_at": "2020-08-04T14:50:25Z",
        "merged_at": null,
        "body": "Closes #2344\n\n@antirez @mattsta can you guys review please?\n\nI have one doubt about my implementation, do I need to implement a `addReplyBulkDouble` function or can I call `addReplyDouble`?\n\nCalling `addReplyDouble`, when I ran the tests, it freeze at `[35/36]`. Can you guys help me with this?\n",
        "comments": 23
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-02-04T18:38:57Z",
        "closed_at": "2018-08-26T09:23:41Z",
        "merged_at": "2018-08-26T09:23:41Z",
        "body": "Signed-off-by: Chris Lamb chris@chris-lamb.co.uk\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-02-04T18:38:53Z",
        "closed_at": "2015-02-12T15:42:10Z",
        "merged_at": "2015-02-12T15:42:10Z",
        "body": "Signed-off-by: Chris Lamb chris@chris-lamb.co.uk\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-02-04T11:14:29Z",
        "closed_at": "2020-09-05T20:59:32Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 64,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2015-02-02T19:59:04Z",
        "closed_at": "2020-10-02T12:09:27Z",
        "merged_at": null,
        "body": "This closes #2349\n\nCan you guys review, please? @mattsta @antirez \n\n(This PR was made with help from @Tavio)\n",
        "comments": 20
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-02-02T13:22:53Z",
        "closed_at": "2015-02-03T13:23:28Z",
        "merged_at": null,
        "body": "These changes keeps consistency output of redis-cli.\n1.  Remove disused new line\n2. Before (num:1 key has a value, but hoge key doesn't)\n   $ redis-cli --csv mget num:1 hoge\n   \"1\",NIL\n   $ redis-cli --csv mget hoge num;1\n   NIL\n   ,\"1\"\n- After \n  $ redis-cli --csv mget num:1 hoge\n  \"1\",NIL\n  $ redis-cli --csv mget hoge num;1\n  NIL,\"1\"\n1. Conform format of error output to another output type(raw, no-raw, csv)\n2. Before (Each output format are unsynchronized)\n   $ redis-cli --raw gget\n   ERR unknown command 'gget'\n\n$ redis-cli --no-raw gget\n(error) ERR unknown command 'gget'\n$ redis-cli --csv gget\nERROR,\"ERR unknown command 'gget'\"\n\n-After (All output type show error in same format)\n$ redis-cli --csv gget\n(error) ERR unknown command 'gget'\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-02-02T06:29:18Z",
        "closed_at": "2015-02-03T14:32:38Z",
        "merged_at": "2015-02-03T14:32:38Z",
        "body": "- Little optimization\n  \n  Just like the case of `sortval->type == REDIS_ZSET && dontsort`,\n  We can use the same trick for `sortval->type == REDIS_LIST && dontsort`.\n- Bug fix\n  \n  The little bug is in the situation with `dontsort DESC` options as below:\n\n``` c\n    The result is:\n\n    127.0.0.1:6379> RPUSH rank 1 3 5 7 9 2 4 6 8 10\n    (integer) 10\n    127.0.0.1:6379> sort rank by rank desc limit 0 5\n    1) \"1\"\n    2) \"3\"\n    3) \"5\"\n    4) \"7\"\n    5) \"9\"\n\n\n    The expected result should be:\n\n    127.0.0.1:6379> RPUSH rank 1 3 5 7 9 2 4 6 8 10\n    (integer) 10\n    127.0.0.1:6379> sort rank by rank desc limit 0 5\n    1) \"10\"\n    2) \"8\"\n    3) \"6\"\n    4) \"4\"\n    5) \"2\"\n```\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 17,
        "changed_files": 1,
        "created_at": "2015-02-02T03:29:41Z",
        "closed_at": "2015-02-02T15:23:18Z",
        "merged_at": null,
        "body": "The file is already (mostly) formatted using MarkDown-style indentation for code blocks,\nheadings, etc. so renaming the file will make the GitHub preview much nicer.\n\nFor a demo of what this will look like, see:\nhttps://github.com/mbrukman/redis/tree/readme-md\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-01-30T21:07:38Z",
        "closed_at": "2017-07-03T11:21:12Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-01-30T11:11:43Z",
        "closed_at": "2021-04-27T15:05:13Z",
        "merged_at": null,
        "body": "fixes the possible of memory leak in redis-benchmark\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-01-29T17:27:18Z",
        "closed_at": "2021-04-28T09:07:45Z",
        "merged_at": null,
        "body": "Fixes #2337\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-01-28T22:38:31Z",
        "closed_at": "2015-01-30T08:20:47Z",
        "merged_at": null,
        "body": "processMultibulkBuffer and readQueryFromClient assume that sdsmakeroom\nwill never return NULL. This causes Access Violation crash when it tries\nto reference its members (Crashing the process.)\n\ntried these in my windows version and they fixed the crashes I was seeing. But I'm not very familiar with this code so would really appreciate if someone can take a look.\n\nThe way I reproed the crash was when setting maxmemory to something smallish (10mb) and then try to insert large values (>4mb).\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2015-01-28T14:09:09Z",
        "closed_at": "2015-01-29T14:20:20Z",
        "merged_at": null,
        "body": "(github is having problems)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 36,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2015-01-27T18:15:59Z",
        "closed_at": "2021-07-18T06:18:51Z",
        "merged_at": null,
        "body": "Added a new command:\nHMERGEBITMASK key field value\n\nthe command executes bitwise OR operation between the current value of the hash field and the one passed in the call.\nNot existing Keys are automatically added and hash fields are considered as zeroed string. Hash field are automatically expanded to match the size of the value when necessary\n\nNote: the patch was made by a friend (Vincenzo), I am not taking credit for his work just sharing in his place.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-01-27T17:19:16Z",
        "closed_at": "2020-02-20T16:42:54Z",
        "merged_at": null,
        "body": "Currently, redis-trib can't resolve hostname to IP address, e.g., when it starts to create redis cluster.\nRelated to issue #2186.\n",
        "comments": 17
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-01-25T19:02:21Z",
        "closed_at": "2015-02-02T21:47:57Z",
        "merged_at": "2015-02-02T21:47:57Z",
        "body": "Code was adding '\\n'  (line 521) to the end of NIL values exlusively making csv output inconsistent.  Removed '\\n'\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-01-25T18:56:28Z",
        "closed_at": "2015-01-26T16:15:32Z",
        "merged_at": null,
        "body": "Printing a newline here breaks the expectations one might have on the CSV output\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-01-23T09:03:50Z",
        "closed_at": "2021-04-06T09:16:50Z",
        "merged_at": null,
        "body": "Sometimes you want to check in one shot if multiple keys exists. For my needs, this change is both non-breaking to existing command / protocol, but maybe if someone needs, a command that returns which one exists can be made as well (since this should be faster then using KEYS).\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-01-22T23:06:21Z",
        "closed_at": "2015-01-23T18:49:34Z",
        "merged_at": null,
        "body": "....)\"\n\nThe output of `sha1sum /path/to/script.lua` and `redis-cli SCRIPT LOAD\n\"$(cat /path/to/script.lua)\"` will differ if script.lua ends with a\ntrailing newline (surely the majority of cases) as the latter invocation\nstrips the final newline character. The resulting mismatched SHA then\ncauses sysadmin confusion when EVALSHA does not find the corresponding\nscript.\n\nThis patch adds a --script-load argument which loads the file \"internally\",\nbypassing any shell nonsense. eg.\n\n  $ redis-cli --script-load /path/to/script.lua\n  \"add40959283772668fbb821092b603b41b2c6aec\"\n\nOf course, whilst something like:\n\n  $ SHA=\"$(redis-cli SCRIPT LOAD \"$(cat /path/to/script.lua)\")\"\n  $ # .. Now use $SHA, not sha1sum /path/to/script.lua\n\n.. is certainly possible, it is hardly very elegant. :)\n\n(Another alternative would be to change the contract of SCRIPT LOAD such\nthat it does not guarantee to return the SHA1 per-se, merely that it\nreturns /some/ unique identifier...which remains a SHA1 right now. However,\nI bet this would break some existing installations.)\n\nSigned-off-by: Chris Lamb chris@chris-lamb.co.uk\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 24,
        "changed_files": 1,
        "created_at": "2015-01-20T14:56:54Z",
        "closed_at": "2015-02-11T09:52:16Z",
        "merged_at": null,
        "body": "When LRU based eviction happens during rehashing of a very large dictionary, a call to dictGetRandomKeys will usually hang for a VERY long time since it jumps to an empty area in the hash table and then scans the next index of the table until it finds something.\n\nSince the dict rehashing (in my case from 2^24 to 2^25) consumes a good deal of memory, there's a good chance it will start a massive eviction, repeated calls to a very slow function will make the server hang for a VERY VERY long time.\n\nWith this fix the random function is coupled with the rehashing mechanism, it knows which indexes should be taken from the first hash table and which from the second.\n",
        "comments": 40
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-01-20T11:55:10Z",
        "closed_at": "2018-08-26T12:50:38Z",
        "merged_at": null,
        "body": "Useful for scripts.\n\nSigned-off-by: Chris Lamb chris@chris-lamb.co.uk\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2015-01-19T19:43:57Z",
        "closed_at": "2015-01-20T18:21:27Z",
        "merged_at": null,
        "body": "The following Lua script will crash the Redis instance. It uses the [lua-redis-debugger](https://github.com/RedisLabs/redis-lua-debugger), but the crash is not due to the way this hacks into the Lua sandbox, I just didn't reduce the test case for now.\n\n``` lua\nrld.start()\nredis.call(\"KEYS\", \"*\")\nrld.stop()\n```\n\nTesting:\n\n```\n$ redis-cli --eval rld.lua\n\"rld v0.1.0 loaded to Redis\"\n$ redis-cli --eval iter.lua\nError: Server closed the connection(gdb) bt\n```\n\nFull crash log is [in a Gist](https://gist.github.com/badboy/1cdcc3380c50dd30a5c3).\n\nBacktrace:\n\n```\n(gdb) bt\n#0  0x00007eff16a01dc7 in kill () from /usr/lib/libc.so.6\n#1  0x0000000000461ae9 in sigsegvHandler (sig=11, info=0x7ffff9198f70, secret=0x7ffff9198e40) at debug.c:919\n#2  <signal handler called>\n#3  0x00007eff16af53ba in __memcpy_avx_unaligned () from /usr/lib/libc.so.6\n#4  0x000000000046fa2f in luaRedisGenericCommand (lua=0x170f6e0, raise_error=1) at scripting.c:257\n#5  0x00000000004700f7 in luaRedisCallCommand (lua=0x170f6e0) at scripting.c:435\n#6  0x000000000048e1c8 in luaD_precall ()\n#7  0x000000000049719a in luaV_execute ()\n#8  0x000000000048e61d in luaD_call ()\n#9  0x000000000048bf95 in lua_call ()\n#10 0x000000000048de20 in luaD_callhook ()\n#11 0x000000000048df46 in luaD_poscall ()\n#12 0x000000000048e1ec in luaD_precall ()\n#13 0x000000000049719a in luaV_execute ()\n#14 0x000000000048e61d in luaD_call ()\n#15 0x000000000048d968 in luaD_rawrunprotected ()\n#16 0x000000000048e77b in luaD_pcall ()\n#17 0x000000000048c01a in lua_pcall ()\n#18 0x00000000004717c1 in evalGenericCommand (c=0x171eda8, evalsha=0) at scripting.c:1006\n#19 0x00000000004719ee in evalCommand (c=0x171eda8) at scripting.c:1075\n#20 0x0000000000421f08 in call (c=0x171eda8, flags=7) at redis.c:2049\n#21 0x0000000000422a57 in processCommand (c=0x171eda8) at redis.c:2309\n#22 0x000000000043162a in processInputBuffer (c=0x171eda8) at networking.c:1143\n#23 0x0000000000431923 in readQueryFromClient (el=0x16f6e48, fd=7, privdata=0x171eda8, mask=1) at networking.c:1208\n#24 0x000000000041ab7e in aeProcessEvents (eventLoop=0x16f6e48, flags=3) at ae.c:412\n#25 0x000000000041ad08 in aeMain (eventLoop=0x16f6e48) at ae.c:455\n#26 0x00000000004265f3 in main (argc=3, argv=0x7ffff9199d68) at redis.c:3832\n```\n\nTurns out if the cached object is unchanged in the client's args, it might first free it, then re-assign it.\nI'm not sure the fix is actually the best one (I'm not even a 100% sure why this argument caching might be necessary), but atleast it prevents the crash.\n\nvalgrind still complains about a memory leak in the scripting code (either line 255 or 261), so there might still be something wrong.\n\nThanks to @markuman who actually triggered the crash, I just tracked it down.\n",
        "comments": 24
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 30,
        "changed_files": 8,
        "created_at": "2015-01-19T19:11:53Z",
        "closed_at": "2015-02-24T16:22:53Z",
        "merged_at": "2015-02-24T16:22:53Z",
        "body": "#### Before:\n\n``` haskell\n    127.0.0.1:6379> debug object hihihi\n    ... encoding:quicklist serializedlength:-2147483559 ...\n```\n#### After:\n\n``` haskell\n    127.0.0.1:6379> debug object hihihi\n    ... encoding:quicklist serializedlength:2147483737 ...\n```\n\nAlso includes fixes for correct return types in a few places.\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-01-16T09:03:24Z",
        "closed_at": "2018-08-26T09:19:52Z",
        "merged_at": "2018-08-26T09:19:52Z",
        "body": "This may look a little pointless (and it is a complete no-op change here)\nbut as package maintainers need to modify these lines to actually\ndaemonize (etc. etc) but it's far preferable if the diff is restricted to\nactually changing just that bit, not adding docs, etc. The less diff the\nbetter, in general.\n\nSigned-off-by: Chris Lamb chris@chris-lamb.co.uk\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-01-16T00:30:40Z",
        "closed_at": "2018-08-26T12:49:39Z",
        "merged_at": null,
        "body": "Whilst misleading if run from Jenkins or from a general regression point of\nview (where knowing about all failures is invariably more useful), bailing\nout on the first failure is more appropriate behaviour in a \"REPL\", TDD or\nsimply a quick \"hack it until the tests pass\" mode of coding.\n\nSigned-off-by: Chris Lamb chris@chris-lamb.co.uk\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 50,
        "deletions": 25,
        "changed_files": 3,
        "created_at": "2015-01-15T19:16:13Z",
        "closed_at": "2015-02-11T14:21:50Z",
        "merged_at": null,
        "body": "These commits are suggestions about how to fix failures found by valgrind.  They are only suggestions because Redis Cluster is a 4,700 line file and I'm not sure I know what's going on with everything there.\n\nThe biggest problem we have: a cluster 'node' is represented by a pointer to a node struct.  But, this single pointer gets stored in up to _5+_ different places (global nodes dict, `master->slaves` array, `slave->masterof` reference back to the slave's master (one copy of the pointer on each slave), importing slots, migrating slots, (and maybe other places!)).  We must remove _all_ references to the node when the node is free'd or else we get combinations of double free and/or segfaults when another function tries to access node data again.  These double free and segfault problems were happening in _multiple places_ throughout the cluster code because our cleanup functions aren't centralized.\n\nI'd suggest adding _one_ place to attach a node reference and and _one_ place to clear all of them if possible.  Right now, 'node' pointers are being directly set throughout the 4,700 line file and I can't really figure out how to account for everything. (an alternate fix: reference nodes by _name_ everywhere instead of pointer values.  Then if we try to use a node later, but it doesn't exist, we get a \"not found\" value instead of accessing a dead pointer.)\n\nRelated fixes for the \"too many copies of a node pointer\" are:\n- https://github.com/mattsta/redis/commit/0286d7a4e2c3b87dea591cb7f30bd7ea8edf9b94 tries to fix slaves retaining a bad 'slaveof' pointer after the master is deleted.  I'm not sure if `clusterSetNodeAsMaster()` is the correct thing to do, but  when the master is deleted, the slave can't remain a slave.\n- https://github.com/mattsta/redis/commit/ba8888a4a709d0a64d76f81888a1bf2b27fb9700 is a fix because our node pointer cleanup functions were 50% in the reset function and 50% in the free function.\n- https://github.com/mattsta/redis/commit/e5e5ccd3a85557d7aa1b104a2960565aaca2ba82 is me giving up and just deleting everything because something was still being left behind.\n\nI'm not confident[1] I've found all the 'stale node pointer' cleanup locations yet.  We probably need better API infrastructure around setting and clearing these pointers.\n\n[1]: continued testing shows I _haven't_ found all the places where stale pointers are being kept, but I'm out of idea on how to track them down.  We are still reading free'd pointers in places (shows up when running valgrind against tests in `06-slave-stop-cond.tcl` and `08-update-msg.tcl`). \u2014 the remaining bad reads all happen when accessing a master node pointer from a `slaveof` pointer in `clusterNodeRemoveSlave()`, except the `slaveof` isn't valid anymore.  For some reason, the master node gets free'd and the replicas don't get their `->slaveof` reset to NULL.  This results in reading and writing to free'd memory since we have no indication the pointer is now invalid.\n\nOther fixes here:\n- https://github.com/mattsta/redis/commit/b2c4ebd6be728d56f08764a0cc1a276e7767cf29 was actually fixed by initializing `server.cluster->mf_end = 0;` in 6274a6789deaeea35951e5409e94e4ff77de645a, but I had this fix already committed locally.  Can be thrown away if not wanted.\n- https://github.com/mattsta/redis/commit/1cab67a52f919cb339a5f4a01782cfdc08f52dfd fixes more potential sending of uninitialized bytes.\n- ~~https://github.com/mattsta/redis/commit/0f9d99dc695cff1b49a932eaaf736ba3acf29a51 fixes a memory leak~~ Fixed in https://github.com/antirez/redis/commit/4433f5a7f24350cb398ae448fca691a53a51a155.\n- https://github.com/mattsta/redis/commit/7e74a66674db60a8931bf04f73d5f7bc44d27ef8 fixes a double free.\n- https://github.com/mattsta/redis/commit/51ed0b90707ea7f18550f0b11af89930971ae05f fixes slightly incorrect pointer math and a missing bounds check. (verify this is actually correct now)\n- https://github.com/mattsta/redis/commit/18b8b88f6e02ceeb5edccbeab9b1eebe13e66c74 attempts to fix an interesting problem: the time delay between a `CLUSTER RESET HARD` and `CLUSTER SET-CONFIG-EPOCH` can allow the reset node to re-discover other nodes.  This commit adds an option to set the config epoch immediately after an atomic reset hard so no other nodes can re-join between.  (I'm not sure if this is the _actual_ fix, but it seems likely.)\n\nTo get this testing working, I had to increase timeouts all throughout the testing code (timeouts of \"retry 100 times, 50 ms apart\" isn't long enough.  A better retry value is up to 500 times at 300 ms delay).  All those changes are in commits at https://github.com/mattsta/redis/commits/add-travis-ci, but they aren't committed back here because they are mostly useful for running valgrind tests where the Redis interaction slow down considerably.\n\nThe valgrind tests _also_ introduce more slowness where the Sentinel tests can fail with `NOGOODSLAVE No suitable slave to promote` errors, but I haven't been able to track down if that's a sentinel timeout issue, a test timeout issue, or a sentinel code issue.\n\nI think that's all for now.\n\n**update**  of course, after I posted this, my test server showed another error!\n- https://github.com/mattsta/redis/commit/dcbffbccf27b76bf13edc8d6dc7cf7a4d03ea251 fixes a memory leak due to not releasing an iterator.\n\nMore suggestions:\n- It would be nice if we could run each of the cluster tests individually (`--single 01, 02, 03, ...`) because then we can run them in parallel (the tests don't depend on each other).  Some of our errors are hiding because an early test fails then other tests don't run at all.\n",
        "comments": 18
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2015-01-14T03:17:52Z",
        "closed_at": "2021-05-09T10:27:46Z",
        "merged_at": null,
        "body": "It's related to https://github.com/antirez/redis/issues/1812#issuecomment-69802198\n\nFrom current unstable branch, there's display issue from redis-cli.\n\n```\n127.0.0.1:6379> del foo\n(integer) 0\n127.0.0.1:6379> select 4\nOK\n127.0.0.1:6379[4]> del foo\n(integer) 1\n127.0.0.1:6379[4]> set foo bar\nOK\n127.0.0.1:6379[4]> select 1000\n(error) ERR invalid DB index\n127.0.0.1:6379> get foo\n\"bar\"\n127.0.0.1:6379> select 0\nOK\n127.0.0.1:6379> get foo\n(nil)\n```\n\nAfter patched, it will be changed to below,\n\n```\n127.0.0.1:6379> auth foobared\nOK\n127.0.0.1:6379> del foo\n(integer) 0\n127.0.0.1:6379> select 4\nOK\n127.0.0.1:6379[4]> del foo\n(integer) 1\n127.0.0.1:6379[4]> set foo bar\nOK\n127.0.0.1:6379[4]> select 1000\n(error) ERR invalid DB index\n127.0.0.1:6379[4]> get foo\n\"bar\"\n127.0.0.1:6379[4]> select 0\nOK\n127.0.0.1:6379> get foo\n(nil)\n```\n\n<!-- Reviewable:start -->\n---\nThis change is\u2002[<img src=\"https://reviewable.io/review_button.svg\" height=\"34\" align=\"absmiddle\" alt=\"Reviewable\"/>](https://reviewable.io/reviews/antirez/redis/2283)\n<!-- Reviewable:end -->\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2015-01-09T22:19:52Z",
        "closed_at": "2015-02-24T16:19:17Z",
        "merged_at": "2015-02-24T16:19:17Z",
        "body": "Let's be consistent and add `*_human` to all memory fields.\n\nAlso, since we have `maxmemory_policy`, let's add the actual `maxmemory` value itself too.  The actual `maxmemory` value will for better error reports (and removes one more thing we have to ask people to manually provide when filing reports).\n\nNew output looks like:\n\n``` haskell\n127.0.0.1:6379> INFO memory\n# Memory\nused_memory:1008816\nused_memory_human:985.17K\nused_memory_rss:1818624\nused_memory_rss_human:1.73M\nused_memory_peak:1008816\nused_memory_peak_human:985.17K\ntotal_system_memory:17179869184\ntotal_system_memory_human:16.00G\nused_memory_lua:35840\nused_memory_lua_human:35.00K\nmaxmemory:0\nmaxmemory_human:0B\nmaxmemory_policy:noeviction\nmem_fragmentation_ratio:1.80\nmem_allocator:libc\n```\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-01-09T17:33:02Z",
        "closed_at": "2015-01-09T20:25:58Z",
        "merged_at": null,
        "body": "`dofile` is a potential vector for attack. Also discussed in https://github.com/antirez/redis/issues/1725 and http://www.agarri.fr/kom/archives/2014/09/11/trying_to_hack_redis_via_http_requests/index.htm\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 129,
        "deletions": 28,
        "changed_files": 5,
        "created_at": "2015-01-08T21:42:33Z",
        "closed_at": "2015-01-12T14:56:36Z",
        "merged_at": "2015-01-12T14:56:36Z",
        "body": "For related issues, see:\n- Closes #463\n- Closes #1967\n- Fixes #2076\n- Fixes #2264\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-01-08T17:39:04Z",
        "closed_at": "2015-01-21T14:13:05Z",
        "merged_at": null,
        "body": "Fixes the following warning:\n\n```\nrdb.c: In function \u2018rdbLoad\u2019:\nrdb.c:1319:17: warning: format \u2018%s\u2019 expects argument of type \u2018char *\u2019, but argument 3 has type \u2018void *\u2019 [-Wformat=]\n                 redisLog(REDIS_NOTICE,\"RDB '%s': %s\", auxkey->ptr, auxval->ptr);\n                 ^\nrdb.c:1319:17: warning: format \u2018%s\u2019 expects argument of type \u2018char *\u2019, but argument 4 has type \u2018void *\u2019 [-Wformat=]\nrdb.c:1324:21: warning: format \u2018%s\u2019 expects argument of type \u2018char *\u2019, but argument 3 has type \u2018void *\u2019 [-Wformat=]\n                     auxkey->ptr);             \n```\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2015-01-08T12:18:37Z",
        "closed_at": "2015-01-09T15:17:42Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2015-01-07T21:31:53Z",
        "closed_at": "2017-07-24T13:19:54Z",
        "merged_at": "2017-07-24T13:19:53Z",
        "body": "Fixes #2258\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2014-12-30T14:28:00Z",
        "closed_at": "2015-01-10T07:17:50Z",
        "merged_at": null,
        "body": "dictReleaseIterator() does not set di to NULL\nSo,  the line `if (di) dictReleaseIterator(di);` may free the di twice, which may cause memory error.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 47,
        "deletions": 16,
        "changed_files": 12,
        "created_at": "2014-12-23T16:38:22Z",
        "closed_at": "2021-04-26T23:39:13Z",
        "merged_at": null,
        "body": "Problem:\n    gettimeofday(2) on FreeBSD synchronizes all the CPU clocks to ensure a\n    monotonically increasing result. This causes pipeline flushes and delays\n    on some CPUs which can be very damaging to performance on busy systems.\n\nAnalysis:\n    Instead of gettimeofday(2), call clock_gettime(2) using the\n    CLOCK_REALTIME_FAST option. This results in the same behavior as Linux's\n    gettimeofday(2) and avoids the issues with clock synchronization.\n\nUnit Tests: Pass.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-12-23T02:18:23Z",
        "closed_at": "2014-12-23T09:58:43Z",
        "merged_at": "2014-12-23T09:58:43Z",
        "body": "This is from the original INFO-CACHE PR, but didn't make it into the merged PR.\n\nAdds cached age of info (in ms) to INFO-CACHE reply.\n\nSee https://github.com/antirez/redis/pull/2148#issuecomment-66716013 for details and example output.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 77,
        "deletions": 79,
        "changed_files": 3,
        "created_at": "2014-12-21T13:39:26Z",
        "closed_at": "2014-12-22T10:00:39Z",
        "merged_at": "2014-12-22T10:00:39Z",
        "body": "a. memory leak in t_set.c has been fixed\nb. memory leaks in rdbSaveToSlavesSockets()  have been fixed\nc. end-of-line spaces has been removed, for loops have been ordered up to match existing Redis style, comments format has been fixed (added \\* in the beggining of every comment line)\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 25,
        "changed_files": 1,
        "created_at": "2014-12-20T02:55:06Z",
        "closed_at": "2014-12-23T10:00:01Z",
        "merged_at": "2014-12-23T10:00:01Z",
        "body": "This tiny bit of code has gone through so many revisions.  Hopefully\nit's more correct now.\n\nFixes #2204\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2014-12-18T12:45:53Z",
        "closed_at": "2014-12-18T14:11:44Z",
        "merged_at": "2014-12-18T14:11:44Z",
        "body": "...negative count argument due to signed/unsigned mismatch.\n\nsetTypeRandomElements() now returns unsigned long, and also uses unsigned long for anything related to count of members.\nspopWithCountCommand() now uses unsigned long elements_returned instead of int, for values returned from setTypeRandomElements()\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-12-17T10:08:00Z",
        "closed_at": "2014-12-17T11:27:23Z",
        "merged_at": "2014-12-17T11:27:22Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 34204,
        "deletions": 8198,
        "changed_files": 319,
        "created_at": "2014-12-17T09:34:09Z",
        "closed_at": "2014-12-22T19:34:51Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 82,
        "deletions": 15,
        "changed_files": 3,
        "created_at": "2014-12-16T20:34:10Z",
        "closed_at": "2014-12-17T16:16:42Z",
        "merged_at": "2014-12-17T16:16:41Z",
        "body": "This was https://github.com/antirez/redis/pull/1820, but I broke GitHub by renaming the branch and I can't re-open the old one.\n\nAnother feature we should include with this PR: create `used_memory_rss_human` field in INFO since it's _really_ easy to overlook that metric in `info memory`.  Every other memory field has a `_human` output (except lua memory too)\n\nThis PR will help us resolve issues like https://github.com/antirez/redis/issues/2217 much easier without having to ask everybody \"how much memory total?  which eviction policy?\"\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 414,
        "deletions": 12,
        "changed_files": 9,
        "created_at": "2014-12-14T10:08:10Z",
        "closed_at": "2014-12-17T16:59:59Z",
        "merged_at": "2014-12-17T16:59:59Z",
        "body": "More details of implementation in issue https://github.com/antirez/redis/issues/1793.\nPrevious pull request #1803, based on antirez:3.0 is now deprecated. This pull request supersedes it.\n\n---\n\nCommit message:\n\nAdded <count> parameter to SPOP:\n- spopCommand() now runs spopWithCountCommand() in case the <count> param is found.\n- Added intsetRandomMembers() to Intset: Copies N random members from the set into inputted 'values' array. Uses either the Knuth or Floyd sample algos depending on ratio count/size.\n- Added setTypeRandomElements() to SET type: Returns a number of random elements from a non empty set. This is a version of setTypeRandomElement() that is modified in order to return multiple entries, using dictGetRandomKeys() and intsetRandomMembers().\n- Added tests for SPOP with <count>: unit/type/set, unit/scripting, integration/aof\n## \n\nCleaned up code a bit to match with required Redis coding style\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 23,
        "changed_files": 13,
        "created_at": "2014-12-13T09:22:48Z",
        "closed_at": "2015-02-10T16:09:30Z",
        "merged_at": null,
        "body": "just fix typos errors :)\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-12-13T03:14:00Z",
        "closed_at": "2019-02-20T16:21:56Z",
        "merged_at": null,
        "body": "Mind for the case:\n\n```\nint main(int argc, char *argv[])\n{\n    char *test1 = \"ababb\";\n    printf(\"%d\\n\", memcmp(test1+1, test1, 1));\n    printf(\"%d\\n\", memcmp(test1+2, test1, 3));\n    return 0;\n}\n```\n\n---\n\nOutput:\n\n```\n1\n1\n```\n\n---\n\nAs `compareStringObjectsForLexRange()` calls `memcmp()` to compare two string.\nThe special case `memcpy(\"abb\",\"aba\",3) == 1` .\nSo the edge case may cause potential bugs. :-)\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-12-12T12:57:13Z",
        "closed_at": "2014-12-12T17:42:44Z",
        "merged_at": "2014-12-12T17:42:44Z",
        "body": "just remove some compile warning in CentOS 5.x(gcc version 4.1.2 20080704)\n\nbefore\n\n``` c\ndb.c: In function \u2018scanGenericCommand\u2019:\ndb.c:419: warning: \u2018patlen\u2019 may be used uninitialized in this function\ndb.c:418: warning: \u2018pat\u2019 may be used uninitialized in this function\n```\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2014-12-12T03:40:08Z",
        "closed_at": "2014-12-12T17:43:33Z",
        "merged_at": null,
        "body": "When deleting the last one of a list of ziplist type.\nIf the direction is REDIS_TAIL the next, li->zi is set to ZIP_END, but logically it should be NULL.\n\nThis cause the li run another more time for ZIP_END. \nIn t_list.c the problem doesn't appear, because listTypeDelete deal with the ZIP_END condition. :-)\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-12-11T03:21:37Z",
        "closed_at": "2014-12-11T06:33:03Z",
        "merged_at": null,
        "body": "in the method dictRehash  , the check \"if rehashed the whole table...\" is only in the begin of the while loop,\nso there is a case that the while loop ended,   ht[0].size = 0, ht[1].size > 0, d->rehashidx != -1, \nin this case, ht[0].size =0 but the dict is not empty.\n\nin the api method \"dictFind\",\"dictGenericDelete\", there are two lines beloew:\nA:  if (d->ht[0].size == 0) return DICT_ERR;\nB:  if (dictIsRehashing(d)) _dictRehashStep(d);\nin that case it should not be return DICT_ERR in the line A.\n\nslove methods:\n1. in the method dictRehash:  add the check \"if rehashed the whole table...\" not only in the begin of the while loop, but also in the end of the loop\n2. change the order of lines A and B in the method \"dictFind\",\"dictGenericDelete\"\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-12-10T03:21:07Z",
        "closed_at": "2014-12-10T08:50:36Z",
        "merged_at": "2014-12-10T08:50:36Z",
        "body": "in the case (all chars of the string s found in 'cset' ),\nline[573] will no more do the same thing line[572] did.\nthis will be more faster especially in the case that the string s is very long and all chars of string s found in 'cset'\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-12-09T00:00:10Z",
        "closed_at": "2014-12-09T07:59:49Z",
        "merged_at": "2014-12-09T07:59:49Z",
        "body": "This allows shell pipes to correctly end redis-cli.\n\nTested with `redis-cli monitor | head -n20` and redis-benchmark.\nBefore: It hang.\nAfter: It correctly exits after 20 lines.\n\nRef #2066 \n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2014-12-08T03:24:38Z",
        "closed_at": "2014-12-09T10:27:40Z",
        "merged_at": "2014-12-09T10:27:40Z",
        "body": "It may be better to initialize `label` inside `sparklineSequenceAddSample()` \nto keep consistence with `freeSparklineSequence` which release `label` inside.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-12-05T12:50:51Z",
        "closed_at": "2014-12-09T11:09:08Z",
        "merged_at": "2014-12-09T11:09:08Z",
        "body": "``` sh\n$ ~ pidof redis-server\n# nothing\n\n$ ~ ps aux | grep [r]edis\nredis      593  0.0  0.0  36900  5564 ?        Ssl  Dec02   1:37 /usr/bin/redis-server 127.0.0.1:6379\nklay     15927  0.0  0.0  16772  6068 pts/6    S+   13:58   0:00 redis-cli\n\n$ ~ uname -a\nLinux edge 3.17.4-1-ARCH #1 SMP PREEMPT Fri Nov 21 21:14:42 CET 2014 x86_64 GNU/Linux\n```\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-12-05T01:44:10Z",
        "closed_at": "2014-12-09T11:24:04Z",
        "merged_at": "2014-12-09T11:24:04Z",
        "body": "See https://sourceware.org/glibc/wiki/Release/2.20#Packaging_Changes\n\nThis change should probably go into hiredis as well.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 11,
        "changed_files": 3,
        "created_at": "2014-12-02T16:21:46Z",
        "closed_at": "2014-12-03T09:42:30Z",
        "merged_at": null,
        "body": "As discovered in #2175, ZSCAN + bigger-than-ziplist-zset can print the wrong values since we are trying to print a double as a long double.  Let's not do that.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2014-12-02T06:58:27Z",
        "closed_at": "2014-12-09T11:46:15Z",
        "merged_at": "2014-12-09T11:46:15Z",
        "body": "### This new redisPopcount reduce the execution time by 10% than the old one.\n- The old version counts 16 bytes in a loop. \n  Every loop it adds the pop(uint32_t) to the highest byte of a uint32_t, then shift 24.\n  Finally add up the 4 uint32_t. \n  We can add all the 4 pop(uint32_t) to a highest byte of a uint32_t and shift 24.\n  This will cause 5% reduce.\n- The ceil of a byte is 255, and the ceil of pop(a byte) is 8\n  floor(255 / 8) = 31;\n  floor(31 / 4) = 7;\n  So we can count 7*4=28 bytes in a loop.\n  This will cause %5 reduce.\n#### The test for short length is not stable, because loop will cause great influence.\n#### benchmark for execution time.\n\nStart:\nlength: 128\nNew version speed up 1.75%\nNew New version speed up 6.26%\nlength: 256\nNew version speed up 11.11%\nNew New version speed up 17.33%\nlength: 512\nNew version speed up 7.67%\nNew New version speed up 10.23%\nlength: 1024\nNew version speed up 4.04%\nNew New version speed up 10.79%\nlength: 2048\nNew version speed up 6.35%\nNew New version speed up 11.98%\nlength: 4096\nNew version speed up 6.22%\nNew New version speed up 11.61%\nlength: 8192\nNew version speed up 5.74%\nNew New version speed up 11.03%\nlength: 16384\nNew version speed up 6.02%\nNew New version speed up 11.44%\nlength: 32768\nNew version speed up 5.74%\nNew New version speed up 11.28%\nlength: 65536\nNew version speed up 5.71%\nNew New version speed up 11.36%\nlength: 131072\nNew version speed up 5.73%\nNew New version speed up 11.40%\nlength: 262144\nNew version speed up 5.79%\nNew New version speed up 11.44%\nlength: 524288\nNew version speed up 5.74%\nNew New version speed up 11.44%\nlength: 1048576\nNew version speed up 5.69%\nNew New version speed up 11.41%\nlength: 2097152\nNew version speed up 5.69%\nNew New version speed up 11.46%\nlength: 4194304\nNew version speed up 5.72%\nNew New version speed up 11.43%\nlength: 8388608\nNew version speed up 5.70%\nNew New version speed up 11.40%\n.\n.\n.\n\n---\n## TEST CODE\n\n```\n#include <stdio.h>\n#include <inttypes.h>\n#include <sys/time.h>\n\nsize_t newnewRedisPopcount(void *s, long count) {\n    size_t bits = 0;\n    unsigned char *p = s;\n    uint32_t *p4;\n    static const unsigned char bitsinbyte[256] = {0,1,1,2,1,2,2,3,1,2,2,3,2,3,3,4,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,4,5,5,6,5,6,6,7,5,6,6,7,6,7,7,8};\n\n    /* Count initial bytes not aligned to 32 bit. */\n    while((unsigned long)p & 3 && count) {\n        bits += bitsinbyte[*p++];\n        count--;\n    }\n\n    /* Count bits 28 bytes at a time */\n    p4 = (uint32_t*)p;\n    while(count>=28) {\n        uint32_t aux1, aux2, aux3, aux4, aux5, aux6, aux7;\n\n        aux1 = *p4++;\n        aux2 = *p4++;\n        aux3 = *p4++;\n        aux4 = *p4++;\n        aux5 = *p4++;\n        aux6 = *p4++;\n        aux7 = *p4++;\n        count -= 28;\n\n        aux1 = aux1 - ((aux1 >> 1) & 0x55555555);\n        aux1 = (aux1 & 0x33333333) + ((aux1 >> 2) & 0x33333333);\n        aux2 = aux2 - ((aux2 >> 1) & 0x55555555);\n        aux2 = (aux2 & 0x33333333) + ((aux2 >> 2) & 0x33333333);\n        aux3 = aux3 - ((aux3 >> 1) & 0x55555555);\n        aux3 = (aux3 & 0x33333333) + ((aux3 >> 2) & 0x33333333);\n        aux4 = aux4 - ((aux4 >> 1) & 0x55555555);\n        aux4 = (aux4 & 0x33333333) + ((aux4 >> 2) & 0x33333333);\n        aux5 = aux5 - ((aux5 >> 1) & 0x55555555);\n        aux5 = (aux5 & 0x33333333) + ((aux5 >> 2) & 0x33333333);\n        aux6 = aux6 - ((aux6 >> 1) & 0x55555555);\n        aux6 = (aux6 & 0x33333333) + ((aux6 >> 2) & 0x33333333);\n        aux7 = aux7 - ((aux7 >> 1) & 0x55555555);\n        aux7 = (aux7 & 0x33333333) + ((aux7 >> 2) & 0x33333333);\n        bits += ((((aux1 + (aux1 >> 4)) & 0x0F0F0F0F) +\n                    ((aux2 + (aux2 >> 4)) & 0x0F0F0F0F) +\n                    ((aux3 + (aux3 >> 4)) & 0x0F0F0F0F) +\n                    ((aux4 + (aux4 >> 4)) & 0x0F0F0F0F) +\n                    ((aux5 + (aux5 >> 4)) & 0x0F0F0F0F) +\n                    ((aux6 + (aux6 >> 4)) & 0x0F0F0F0F) +\n                    ((aux7 + (aux7 >> 4)) & 0x0F0F0F0F))* 0x01010101) >> 24;\n    }\n    /* Count the remaining bytes. */\n    p = (unsigned char*)p4;\n    while(count--) bits += bitsinbyte[*p++];\n    return bits;\n}\nsize_t newRedisPopcount(void *s, long count) {\n    size_t bits = 0;\n    unsigned char *p = s;\n    uint32_t *p4;\n    static const unsigned char bitsinbyte[256] = {0,1,1,2,1,2,2,3,1,2,2,3,2,3,3,4,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,4,5,5,6,5,6,6,7,5,6,6,7,6,7,7,8};\n\n    /* Count initial bytes not aligned to 32 bit. */\n    while((unsigned long)p & 3 && count) {\n        bits += bitsinbyte[*p++];\n        count--;\n    }\n\n    /* Count bits 16 bytes at a time */\n    p4 = (uint32_t*)p;\n    while(count>=16) {\n        uint32_t aux1, aux2, aux3, aux4;\n\n        aux1 = *p4++;\n        aux2 = *p4++;\n        aux3 = *p4++;\n        aux4 = *p4++;\n        count -= 16;\n\n        aux1 = aux1 - ((aux1 >> 1) & 0x55555555);\n        aux1 = (aux1 & 0x33333333) + ((aux1 >> 2) & 0x33333333);\n        aux2 = aux2 - ((aux2 >> 1) & 0x55555555);\n        aux2 = (aux2 & 0x33333333) + ((aux2 >> 2) & 0x33333333);\n        aux3 = aux3 - ((aux3 >> 1) & 0x55555555);\n        aux3 = (aux3 & 0x33333333) + ((aux3 >> 2) & 0x33333333);\n        aux4 = aux4 - ((aux4 >> 1) & 0x55555555);\n        aux4 = (aux4 & 0x33333333) + ((aux4 >> 2) & 0x33333333);\n        bits += ((((aux1 + (aux1 >> 4)) & 0x0F0F0F0F) +\n                    ((aux2 + (aux2 >> 4)) & 0x0F0F0F0F) +\n                    ((aux3 + (aux3 >> 4)) & 0x0F0F0F0F) +\n                    ((aux4 + (aux4 >> 4)) & 0x0F0F0F0F))* 0x01010101) >> 24;\n    }\n    /* Count the remaining bytes. */\n    p = (unsigned char*)p4;\n    while(count--) bits += bitsinbyte[*p++];\n    return bits;\n}\n\nsize_t redisPopcount(void *s, long count) {\n    size_t bits = 0;\n    unsigned char *p = s;\n    uint32_t *p4;\n    static const unsigned char bitsinbyte[256] = {0,1,1,2,1,2,2,3,1,2,2,3,2,3,3,4,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,4,5,5,6,5,6,6,7,5,6,6,7,6,7,7,8};\n\n    /* Count initial bytes not aligned to 32 bit. */\n    while((unsigned long)p & 3 && count) {\n        bits += bitsinbyte[*p++];\n        count--;\n    }\n\n    /* Count bits 16 bytes at a time */\n    p4 = (uint32_t*)p;\n    while(count>=16) {\n        uint32_t aux1, aux2, aux3, aux4;\n\n        aux1 = *p4++;\n        aux2 = *p4++;\n        aux3 = *p4++;\n        aux4 = *p4++;\n        count -= 16;\n\n        aux1 = aux1 - ((aux1 >> 1) & 0x55555555);\n        aux1 = (aux1 & 0x33333333) + ((aux1 >> 2) & 0x33333333);\n        aux2 = aux2 - ((aux2 >> 1) & 0x55555555);\n        aux2 = (aux2 & 0x33333333) + ((aux2 >> 2) & 0x33333333);\n        aux3 = aux3 - ((aux3 >> 1) & 0x55555555);\n        aux3 = (aux3 & 0x33333333) + ((aux3 >> 2) & 0x33333333);\n        aux4 = aux4 - ((aux4 >> 1) & 0x55555555);\n        aux4 = (aux4 & 0x33333333) + ((aux4 >> 2) & 0x33333333);\n        bits += ((((aux1 + (aux1 >> 4)) & 0x0F0F0F0F) * 0x01010101) >> 24) +\n            ((((aux2 + (aux2 >> 4)) & 0x0F0F0F0F) * 0x01010101) >> 24) +\n            ((((aux3 + (aux3 >> 4)) & 0x0F0F0F0F) * 0x01010101) >> 24) +\n            ((((aux4 + (aux4 >> 4)) & 0x0F0F0F0F) * 0x01010101) >> 24);\n    }\n    /* Count the remaining bytes. */\n    p = (unsigned char*)p4;\n    while(count--) bits += bitsinbyte[*p++];\n    return bits;\n}\n\nlong long ustime(void) {\n    struct timeval tv;\n    long long ust;\n\n    gettimeofday(&tv, NULL);\n    ust = ((long long)tv.tv_sec) * 1000000;\n    ust += tv.tv_usec;\n    return ust;\n}\n\nchar tmp[1<<29];\n\nvoid initialize()\n{\n    int i;\n    for (i = 0; i < 1<<29; i++)\n        tmp[i] = rand();\n}\nvoid runTest(int length)\n{\n    int i;\n    long long timePre, timeCur;\n    size_t res1, res2, res3;\n    long long timePreTotal, timeNewTotal, timeNewNewTotal;\n    timePreTotal = timeNewTotal = timeNewNewTotal = 0;\n    for (i = 0; i < 4096; i++) {\n        int start;\n        start = rand() % sizeof(tmp - length);\n\n        timePre = ustime();\n        res1 = redisPopcount(start + tmp, length);\n        timeCur = ustime();\n        timePreTotal += timeCur - timePre;\n\n        timePre = ustime();\n        res2 = newRedisPopcount(start + tmp, length);\n        timeCur = ustime();\n        timeNewTotal += timeCur - timePre;\n\n        timePre = ustime();\n        res3 = newnewRedisPopcount(start + tmp, length);\n        timeCur = ustime();\n        timeNewNewTotal += timeCur - timePre;\n\n        if (res1 != res2 || res2 != res3)\n            printf(\"Panic!\\n\");\n    }\n    /*printf(\"Old version time: %lld\\n\", timePreTotal);*/\n    /*printf(\"New version time: %lld\\n\", timeNewTotal);*/\n    /*printf(\"New New version time: %lld\\n\", timeNewNewTotal);*/\n\n    printf(\"length: %d\\n\", length);\n    printf(\"New version speed up %.2lf%%\\n\", 100.0*(timePreTotal - timeNewTotal)/timePreTotal);\n    printf(\"New New version speed up %.2lf%%\\n\", 100.0*(timePreTotal - timeNewNewTotal)/timePreTotal);\n}\nint main(int argc, char *argv[])\n{\n    int i;\n    initialize();\n    printf(\"Start:\\n\");\n    for(i = 128; i < 1<<29; i <<= 1)\n        runTest(i);\n    return 0;\n}\n```\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-12-01T22:24:13Z",
        "closed_at": "2014-12-09T11:57:22Z",
        "merged_at": "2014-12-09T11:57:22Z",
        "body": "`ioctl` is defined in either `unistd.h` or `stropts.h` on Solaris\nand in either `sys/ioctl.h` or `stropts.h` on Linux.\n\nBy simply including `stropts.h` we are warning-free on both systems.\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 28,
        "changed_files": 1,
        "created_at": "2014-12-01T21:56:04Z",
        "closed_at": "2014-12-09T14:24:44Z",
        "merged_at": "2014-12-09T14:24:44Z",
        "body": "Before change:\n  \u200b1416263718.077019 [0 127.0.0.1:57023] \"SET\" \"key:rand_int\" \"xxx\"\n      1416263718.077019 [0 127.0.0.1:57023] \"AUTH\" \"test\"\n      1416263718.077019 [0 127.0.0.1:57023] \"SET\" \"key:rand_int\" \"xxx\"\n      1416263718.077019 [0 127.0.0.1:57023] \"AUTH\" \"test\"\n      1416263718.077019 [0 127.0.0.1:57023] \"SET\" \"key:rand_int\" \"xxx\"\n      1416263718.077019 [0 127.0.0.1:57023] \"AUTH\" \"test\"\n      1416263718.077019 [0 127.0.0.1:57023] \"SET\" \"key:rand_int\" \"xxx\"\n      1416263718.077019 [0 127.0.0.1:57023] \"AUTH\" \"test\"\n      1416263718.077019 [0 127.0.0.1:57023] \"SET\" \"key:rand_int\" \"xxx\"\n      1416263718.077019 [0 127.0.0.1:57023] \"AUTH\" \"test\"\n      1416263718.077019 [0 127.0.0.1:57023] \"SET\" \"key:rand_int\" \"xxx\"\n\nAfter the change:\n\n 1416266483.678809 [0 127.0.0.1:23778] \"AUTH\" \"test\"\n      1416266483.678809 [0 127.0.0.1:23778] \"SELECT\" \"1\"\n      1416266483.678809 [1 127.0.0.1:23778] \"GET\" \"key:**rand_int**\"\n      1416266483.678809 [1 127.0.0.1:23778] \"GET\" \"key:**rand_int**\"\n      1416266483.678809 [1 127.0.0.1:23778] \"GET\" \"key:**rand_int**\"\n      1416266483.678809 [1 127.0.0.1:23778] \"GET\" \"key:**rand_int**\"\n      1416266483.678809 [1 127.0.0.1:23778] \"GET\" \"key:**rand_int**\"\n      1416266483.678809 [1 127.0.0.1:23778] \"GET\" \"key:**rand_int**\"\n      1416266483.678809 [1 127.0.0.1:23778] \"GET\" \"key:**rand_int**\"\n      1416266483.678809 [1 127.0.0.1:23778] \"GET\" \"key:**rand_int**\"\n      1416266483.678809 [1 127.0.0.1:23778] \"GET\" \"key:**rand_int**\"\n      1416266483.679809 [1 127.0.0.1:23778] \"GET\" \"key:**rand_int**\"\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-11-28T01:59:11Z",
        "closed_at": "2014-11-28T08:21:31Z",
        "merged_at": "2014-11-28T08:21:31Z",
        "body": "Heya,\n\nThere's no need to reallocate the data buffer each loop run, in particular because it's never freed and leaks otherwise.\n- razzle\n\nP.S.: Even though freeing after the infinite loop would be redundant, it might be a good idea in case a loop iterations limit is introduced later.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2014-11-25T14:01:50Z",
        "closed_at": "2014-12-09T14:57:17Z",
        "merged_at": "2014-12-09T14:57:17Z",
        "body": "Comments above may be the old version's. :-)\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2014-11-24T18:11:02Z",
        "closed_at": "2014-11-24T21:31:52Z",
        "merged_at": "2014-11-24T21:31:52Z",
        "body": "This syncs lua-cmsgpack with the mattsta/lua-cmsgpack upstream maintenance branch.\n\nFixes #2161\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-11-23T09:33:49Z",
        "closed_at": "2014-12-10T09:04:16Z",
        "merged_at": null,
        "body": "--latency-history now shows 50th & 99th Percentile latency distribution.\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2014-11-22T03:40:44Z",
        "closed_at": "2015-02-03T13:17:39Z",
        "merged_at": "2015-02-03T13:17:39Z",
        "body": "Issue: #2157 \nAs the SET command is parsed, it remembers which options are already set\nand if a duplicate option is found, raises an error because it is\nessentially an invalid syntax.\n\nIt still allows mutually exclusive options like EX and PX because taking\nan option over another (precedence) is not essentially a syntactic\nerror.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 72,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2014-11-21T13:30:59Z",
        "closed_at": "2014-11-22T02:12:26Z",
        "merged_at": null,
        "body": "Add ziplistMerge function.\nIt uses __ziplistCascadeUpdate() which is more faster than appending one by one.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2014-11-21T00:33:51Z",
        "closed_at": "2014-12-10T09:14:51Z",
        "merged_at": null,
        "body": "Return the sha1 hash of the value of a key, which can be used to check the content of a key without transferring the entire value over the network.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2014-11-20T14:45:51Z",
        "closed_at": "2014-11-20T16:51:40Z",
        "merged_at": null,
        "body": "Thanks to @mattsta 's [commit](https://github.com/mattsta/redis/commit/d4f1cf00fde28bd2dd3bca7d903d620bd40a0be7).\nFind some little bugs in ziplist's test code(ziplist.c/pop).\npop() used ziplistDelete() which realloc zl.\nSo pop should return the new zl.\n\nThis P.R. will cause conflicts after mattsta's commit being merged.\nI will fix the inconsistency if this little bug is worth being fixed. :)\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 69,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-11-19T18:34:07Z",
        "closed_at": "2014-12-10T10:02:43Z",
        "merged_at": "2014-12-10T10:02:43Z",
        "body": "Sentinel queries the INFO from every master and from every replica of\nevery master.\n\nWe can cache the INFO results in Sentinel so Sentinel can be a single\nplace to quickly get all INFO output for an entire Sentinel monitoring\ngroup.\n\nThis commit gives us SENTINEL INFO-CACHE in two forms:\n- SENTINEL INFO-CACHE \u2014 returns all masters and all replicas\n- SENTINEL INFO-CACHE master0 master1 ... masterN \u2014\u00a0vararg specific masters\n\nResults are returned as a multibulk reply with two top-level entries\nfor each master.  The first entry for each master is the name of the master.\nThe second entry is a nested multibulk reply with the contents of INFO,\nfirst for the master, then an additional entry for each of the\nreplicas.\n\nNames are up for debate.  I also considered calling it `SENTINEL INFO` but that could be confused with regular `INFO` too easily.\n## Example Output\n### Query single master\n\n``` haskell\nmatt@ununoctium:~/repos/redis/src% ./redis-cli -p 26379 sentinel info-cache foo\n1) \"foo\"\n2) 1) \"# Server\\r\\nredis_version:2.9.999\\r\\nredis_git_sha1:0ed2c601\\r\\nredis_git_dirty:1\\r\\nredis_build_id:249db2940eda69a4\\r\\nredis_mode:standalone\\r\\nos:Darwin 14.0.0 x86_64\\r\\narch_bits:64\\r\\nmultiplexing_api:kqueue\\r\\ngcc_version:4.2.1\\r\\nprocess_id:30382\\r\\nrun_id:ff52291af6738e6b39dfa86f1bc023880c820b60\\r\\ntcp_port:3333\\r\\nuptime_in_seconds:807\\r\\nuptime_in_days:0\\r\\nhz:10\\r\\nlru_clock:7134590\\r\\nconfig_file:\\r\\n\\r\\n# Clients\\r\\nconnected_clients:2\\r\\nclient_longest_output_list:0\\r\\nclient_biggest_input_buf:0\\r\\nblocked_clients:0\\r\\n\\r\\n# Memory\\r\\nused_memory:57748768\\r\\nused_memory_human:55.07M\\r\\nused_memory_rss:61313024\\r\\nused_memory_peak:57782032\\r\\nused_memory_peak_human:55.11M\\r\\nused_memory_lua:35840\\r\\nmem_fragmentation_ratio:1.06\\r\\nmem_allocator:libc\\r\\n\\r\\n# Persistence\\r\\nloading:0\\r\\nrdb_changes_since_last_save:0\\r\\nrdb_bgsave_in_progress:0\\r\\nrdb_last_save_time:1416419940\\r\\nrdb_last_bgsave_status:ok\\r\\nrdb_last_bgsave_time_sec:1\\r\\nrdb_current_bgsave_time_sec:-1\\r\\naof_enabled:0\\r\\naof_rewrite_in_progress:0\\r\\naof_rewrite_scheduled:0\\r\\naof_last_rewrite_time_sec:-1\\r\\naof_current_rewrite_time_sec:-1\\r\\naof_last_bgrewrite_status:ok\\r\\naof_last_write_status:ok\\r\\n\\r\\n# Stats\\r\\ntotal_connections_received:3\\r\\ntotal_commands_processed:1887\\r\\ninstantaneous_ops_per_sec:2\\r\\nrejected_connections:0\\r\\nsync_full:1\\r\\nsync_partial_ok:0\\r\\nsync_partial_err:0\\r\\nexpired_keys:0\\r\\nevicted_keys:0\\r\\nkeyspace_hits:0\\r\\nkeyspace_misses:0\\r\\npubsub_channels:1\\r\\npubsub_patterns:0\\r\\nlatest_fork_usec:952\\r\\nmigrate_cached_sockets:0\\r\\n\\r\\n# Replication\\r\\nrole:master\\r\\nconnected_slaves:1\\r\\nslave0:ip=127.0.0.1,port=6379,state=online,offset=45888,lag=0\\r\\nmaster_repl_offset:45888\\r\\nrepl_backlog_active:1\\r\\nrepl_backlog_size:1048576\\r\\nrepl_backlog_first_byte_offset:2\\r\\nrepl_backlog_histlen:45887\\r\\n\\r\\n# CPU\\r\\nused_cpu_sys:0.42\\r\\nused_cpu_user:0.52\\r\\nused_cpu_sys_children:0.04\\r\\nused_cpu_user_children:0.12\\r\\n\\r\\n# Cluster\\r\\ncluster_enabled:0\\r\\n\\r\\n# Keyspace\\r\\ndb0:keys=616,expires=0,avg_ttl=0\\r\\n\"\n   2) \"# Server\\r\\nredis_version:2.9.999\\r\\nredis_git_sha1:0ed2c601\\r\\nredis_git_dirty:1\\r\\nredis_build_id:249db2940eda69a4\\r\\nredis_mode:standalone\\r\\nos:Darwin 14.0.0 x86_64\\r\\narch_bits:64\\r\\nmultiplexing_api:kqueue\\r\\ngcc_version:4.2.1\\r\\nprocess_id:30382\\r\\nrun_id:ff52291af6738e6b39dfa86f1bc023880c820b60\\r\\ntcp_port:3333\\r\\nuptime_in_seconds:807\\r\\nuptime_in_days:0\\r\\nhz:10\\r\\nlru_clock:7134590\\r\\nconfig_file:\\r\\n\\r\\n# Clients\\r\\nconnected_clients:2\\r\\nclient_longest_output_list:0\\r\\nclient_biggest_input_buf:0\\r\\nblocked_clients:0\\r\\n\\r\\n# Memory\\r\\nused_memory:57748768\\r\\nused_memory_human:55.07M\\r\\nused_memory_rss:61313024\\r\\nused_memory_peak:57782032\\r\\nused_memory_peak_human:55.11M\\r\\nused_memory_lua:35840\\r\\nmem_fragmentation_ratio:1.06\\r\\nmem_allocator:libc\\r\\n\\r\\n# Persistence\\r\\nloading:0\\r\\nrdb_changes_since_last_save:0\\r\\nrdb_bgsave_in_progress:0\\r\\nrdb_last_save_time:1416419940\\r\\nrdb_last_bgsave_status:ok\\r\\nrdb_last_bgsave_time_sec:1\\r\\nrdb_current_bgsave_time_sec:-1\\r\\naof_enabled:0\\r\\naof_rewrite_in_progress:0\\r\\naof_rewrite_scheduled:0\\r\\naof_last_rewrite_time_sec:-1\\r\\naof_current_rewrite_time_sec:-1\\r\\naof_last_bgrewrite_status:ok\\r\\naof_last_write_status:ok\\r\\n\\r\\n# Stats\\r\\ntotal_connections_received:3\\r\\ntotal_commands_processed:1887\\r\\ninstantaneous_ops_per_sec:2\\r\\nrejected_connections:0\\r\\nsync_full:1\\r\\nsync_partial_ok:0\\r\\nsync_partial_err:0\\r\\nexpired_keys:0\\r\\nevicted_keys:0\\r\\nkeyspace_hits:0\\r\\nkeyspace_misses:0\\r\\npubsub_channels:1\\r\\npubsub_patterns:0\\r\\nlatest_fork_usec:952\\r\\nmigrate_cached_sockets:0\\r\\n\\r\\n# Replication\\r\\nrole:master\\r\\nconnected_slaves:1\\r\\nslave0:ip=127.0.0.1,port=6379,state=online,offset=45888,lag=0\\r\\nmaster_repl_offset:45888\\r\\nrepl_backlog_active:1\\r\\nrepl_backlog_size:1048576\\r\\nrepl_backlog_first_byte_offset:2\\r\\nrepl_backlog_histlen:45887\\r\\n\\r\\n# CPU\\r\\nused_cpu_sys:0.42\\r\\nused_cpu_user:0.52\\r\\nused_cpu_sys_children:0.04\\r\\nused_cpu_user_children:0.12\\r\\n\\r\\n# Cluster\\r\\ncluster_enabled:0\\r\\n\\r\\n# Keyspace\\r\\ndb0:keys=616,expires=0,avg_ttl=0\\r\\n\"\n```\n### Query all masters\n\n(kinda boring because I only have one pair)\n\n``` haskell\nmatt@ununoctium:~/repos/redis/src% ./redis-cli -p 26379 sentinel info-cache\n1) \"foo\"\n2) 1) \"# Server\\r\\nredis_version:2.9.999\\r\\nredis_git_sha1:0ed2c601\\r\\nredis_git_dirty:1\\r\\nredis_build_id:249db2940eda69a4\\r\\nredis_mode:standalone\\r\\nos:Darwin 14.0.0 x86_64\\r\\narch_bits:64\\r\\nmultiplexing_api:kqueue\\r\\ngcc_version:4.2.1\\r\\nprocess_id:30382\\r\\nrun_id:ff52291af6738e6b39dfa86f1bc023880c820b60\\r\\ntcp_port:3333\\r\\nuptime_in_seconds:988\\r\\nuptime_in_days:0\\r\\nhz:10\\r\\nlru_clock:7134771\\r\\nconfig_file:\\r\\n\\r\\n# Clients\\r\\nconnected_clients:2\\r\\nclient_longest_output_list:0\\r\\nclient_biggest_input_buf:0\\r\\nblocked_clients:0\\r\\n\\r\\n# Memory\\r\\nused_memory:57748768\\r\\nused_memory_human:55.07M\\r\\nused_memory_rss:61321216\\r\\nused_memory_peak:57782032\\r\\nused_memory_peak_human:55.11M\\r\\nused_memory_lua:35840\\r\\nmem_fragmentation_ratio:1.06\\r\\nmem_allocator:libc\\r\\n\\r\\n# Persistence\\r\\nloading:0\\r\\nrdb_changes_since_last_save:0\\r\\nrdb_bgsave_in_progress:0\\r\\nrdb_last_save_time:1416419940\\r\\nrdb_last_bgsave_status:ok\\r\\nrdb_last_bgsave_time_sec:1\\r\\nrdb_current_bgsave_time_sec:-1\\r\\naof_enabled:0\\r\\naof_rewrite_in_progress:0\\r\\naof_rewrite_scheduled:0\\r\\naof_last_rewrite_time_sec:-1\\r\\naof_current_rewrite_time_sec:-1\\r\\naof_last_bgrewrite_status:ok\\r\\naof_last_write_status:ok\\r\\n\\r\\n# Stats\\r\\ntotal_connections_received:3\\r\\ntotal_commands_processed:2338\\r\\ninstantaneous_ops_per_sec:1\\r\\nrejected_connections:0\\r\\nsync_full:1\\r\\nsync_partial_ok:0\\r\\nsync_partial_err:0\\r\\nexpired_keys:0\\r\\nevicted_keys:0\\r\\nkeyspace_hits:0\\r\\nkeyspace_misses:0\\r\\npubsub_channels:1\\r\\npubsub_patterns:0\\r\\nlatest_fork_usec:952\\r\\nmigrate_cached_sockets:0\\r\\n\\r\\n# Replication\\r\\nrole:master\\r\\nconnected_slaves:1\\r\\nslave0:ip=127.0.0.1,port=6379,state=online,offset=57262,lag=1\\r\\nmaster_repl_offset:57262\\r\\nrepl_backlog_active:1\\r\\nrepl_backlog_size:1048576\\r\\nrepl_backlog_first_byte_offset:2\\r\\nrepl_backlog_histlen:57261\\r\\n\\r\\n# CPU\\r\\nused_cpu_sys:0.51\\r\\nused_cpu_user:0.57\\r\\nused_cpu_sys_children:0.04\\r\\nused_cpu_user_children:0.12\\r\\n\\r\\n# Cluster\\r\\ncluster_enabled:0\\r\\n\\r\\n# Keyspace\\r\\ndb0:keys=616,expires=0,avg_ttl=0\\r\\n\"\n   2) \"# Server\\r\\nredis_version:2.9.999\\r\\nredis_git_sha1:0ed2c601\\r\\nredis_git_dirty:1\\r\\nredis_build_id:249db2940eda69a4\\r\\nredis_mode:standalone\\r\\nos:Darwin 14.0.0 x86_64\\r\\narch_bits:64\\r\\nmultiplexing_api:kqueue\\r\\ngcc_version:4.2.1\\r\\nprocess_id:30382\\r\\nrun_id:ff52291af6738e6b39dfa86f1bc023880c820b60\\r\\ntcp_port:3333\\r\\nuptime_in_seconds:988\\r\\nuptime_in_days:0\\r\\nhz:10\\r\\nlru_clock:7134771\\r\\nconfig_file:\\r\\n\\r\\n# Clients\\r\\nconnected_clients:2\\r\\nclient_longest_output_list:0\\r\\nclient_biggest_input_buf:0\\r\\nblocked_clients:0\\r\\n\\r\\n# Memory\\r\\nused_memory:57748768\\r\\nused_memory_human:55.07M\\r\\nused_memory_rss:61321216\\r\\nused_memory_peak:57782032\\r\\nused_memory_peak_human:55.11M\\r\\nused_memory_lua:35840\\r\\nmem_fragmentation_ratio:1.06\\r\\nmem_allocator:libc\\r\\n\\r\\n# Persistence\\r\\nloading:0\\r\\nrdb_changes_since_last_save:0\\r\\nrdb_bgsave_in_progress:0\\r\\nrdb_last_save_time:1416419940\\r\\nrdb_last_bgsave_status:ok\\r\\nrdb_last_bgsave_time_sec:1\\r\\nrdb_current_bgsave_time_sec:-1\\r\\naof_enabled:0\\r\\naof_rewrite_in_progress:0\\r\\naof_rewrite_scheduled:0\\r\\naof_last_rewrite_time_sec:-1\\r\\naof_current_rewrite_time_sec:-1\\r\\naof_last_bgrewrite_status:ok\\r\\naof_last_write_status:ok\\r\\n\\r\\n# Stats\\r\\ntotal_connections_received:3\\r\\ntotal_commands_processed:2338\\r\\ninstantaneous_ops_per_sec:1\\r\\nrejected_connections:0\\r\\nsync_full:1\\r\\nsync_partial_ok:0\\r\\nsync_partial_err:0\\r\\nexpired_keys:0\\r\\nevicted_keys:0\\r\\nkeyspace_hits:0\\r\\nkeyspace_misses:0\\r\\npubsub_channels:1\\r\\npubsub_patterns:0\\r\\nlatest_fork_usec:952\\r\\nmigrate_cached_sockets:0\\r\\n\\r\\n# Replication\\r\\nrole:master\\r\\nconnected_slaves:1\\r\\nslave0:ip=127.0.0.1,port=6379,state=online,offset=57262,lag=1\\r\\nmaster_repl_offset:57262\\r\\nrepl_backlog_active:1\\r\\nrepl_backlog_size:1048576\\r\\nrepl_backlog_first_byte_offset:2\\r\\nrepl_backlog_histlen:57261\\r\\n\\r\\n# CPU\\r\\nused_cpu_sys:0.51\\r\\nused_cpu_user:0.57\\r\\nused_cpu_sys_children:0.04\\r\\nused_cpu_user_children:0.12\\r\\n\\r\\n# Cluster\\r\\ncluster_enabled:0\\r\\n\\r\\n# Keyspace\\r\\ndb0:keys=616,expires=0,avg_ttl=0\\r\\n\"\n```\n#### Better formatting\n\n``` haskell\nmatt@ununoctium:~/repos/redis/src% echo \"$(./redis-cli -p 26379 sentinel info-cache)\"\nfoo\n# Server\nredis_version:2.9.999\nredis_git_sha1:0ed2c601\nredis_git_dirty:1\nredis_build_id:249db2940eda69a4\nredis_mode:standalone\nos:Darwin 14.0.0 x86_64\narch_bits:64\nmultiplexing_api:kqueue\ngcc_version:4.2.1\nprocess_id:30382\nrun_id:ff52291af6738e6b39dfa86f1bc023880c820b60\ntcp_port:3333\nuptime_in_seconds:1801\nuptime_in_days:0\nhz:10\nlru_clock:7135584\nconfig_file:\n\n# Clients\nconnected_clients:2\nclient_longest_output_list:0\nclient_biggest_input_buf:0\nblocked_clients:0\n\n# Memory\nused_memory:57748784\nused_memory_human:55.07M\nused_memory_rss:61378560\nused_memory_peak:57782048\nused_memory_peak_human:55.11M\nused_memory_lua:35840\nmem_fragmentation_ratio:1.06\nmem_allocator:libc\n\n# Persistence\nloading:0\nrdb_changes_since_last_save:0\nrdb_bgsave_in_progress:0\nrdb_last_save_time:1416419940\nrdb_last_bgsave_status:ok\nrdb_last_bgsave_time_sec:1\nrdb_current_bgsave_time_sec:-1\naof_enabled:0\naof_rewrite_in_progress:0\naof_rewrite_scheduled:0\naof_last_rewrite_time_sec:-1\naof_current_rewrite_time_sec:-1\naof_last_bgrewrite_status:ok\naof_last_write_status:ok\n\n# Stats\ntotal_connections_received:3\ntotal_commands_processed:4376\ninstantaneous_ops_per_sec:2\nrejected_connections:0\nsync_full:1\nsync_partial_ok:0\nsync_partial_err:0\nexpired_keys:0\nevicted_keys:0\nkeyspace_hits:0\nkeyspace_misses:0\npubsub_channels:1\npubsub_patterns:0\nlatest_fork_usec:952\nmigrate_cached_sockets:0\n\n# Replication\nrole:master\nconnected_slaves:1\nslave0:ip=127.0.0.1,port=6379,state=online,offset=108672,lag=1\nmaster_repl_offset:108672\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:2\nrepl_backlog_histlen:108671\n\n# CPU\nused_cpu_sys:0.90\nused_cpu_user:0.76\nused_cpu_sys_children:0.04\nused_cpu_user_children:0.12\n\n# Cluster\ncluster_enabled:0\n\n# Keyspace\ndb0:keys=616,expires=0,avg_ttl=0\n\n# Server\nredis_version:2.9.999\nredis_git_sha1:0ed2c601\nredis_git_dirty:1\nredis_build_id:249db2940eda69a4\nredis_mode:standalone\nos:Darwin 14.0.0 x86_64\narch_bits:64\nmultiplexing_api:kqueue\ngcc_version:4.2.1\nprocess_id:30382\nrun_id:ff52291af6738e6b39dfa86f1bc023880c820b60\ntcp_port:3333\nuptime_in_seconds:1801\nuptime_in_days:0\nhz:10\nlru_clock:7135584\nconfig_file:\n\n# Clients\nconnected_clients:2\nclient_longest_output_list:0\nclient_biggest_input_buf:0\nblocked_clients:0\n\n# Memory\nused_memory:57748784\nused_memory_human:55.07M\nused_memory_rss:61378560\nused_memory_peak:57782048\nused_memory_peak_human:55.11M\nused_memory_lua:35840\nmem_fragmentation_ratio:1.06\nmem_allocator:libc\n\n# Persistence\nloading:0\nrdb_changes_since_last_save:0\nrdb_bgsave_in_progress:0\nrdb_last_save_time:1416419940\nrdb_last_bgsave_status:ok\nrdb_last_bgsave_time_sec:1\nrdb_current_bgsave_time_sec:-1\naof_enabled:0\naof_rewrite_in_progress:0\naof_rewrite_scheduled:0\naof_last_rewrite_time_sec:-1\naof_current_rewrite_time_sec:-1\naof_last_bgrewrite_status:ok\naof_last_write_status:ok\n\n# Stats\ntotal_connections_received:3\ntotal_commands_processed:4376\ninstantaneous_ops_per_sec:2\nrejected_connections:0\nsync_full:1\nsync_partial_ok:0\nsync_partial_err:0\nexpired_keys:0\nevicted_keys:0\nkeyspace_hits:0\nkeyspace_misses:0\npubsub_channels:1\npubsub_patterns:0\nlatest_fork_usec:952\nmigrate_cached_sockets:0\n\n# Replication\nrole:master\nconnected_slaves:1\nslave0:ip=127.0.0.1,port=6379,state=online,offset=108672,lag=1\nmaster_repl_offset:108672\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:2\nrepl_backlog_histlen:108671\n\n# CPU\nused_cpu_sys:0.90\nused_cpu_user:0.76\nused_cpu_sys_children:0.04\nused_cpu_user_children:0.12\n\n# Cluster\ncluster_enabled:0\n\n# Keyspace\ndb0:keys=616,expires=0,avg_ttl=0\n```\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2014-11-19T16:44:23Z",
        "closed_at": "2014-12-09T14:25:24Z",
        "merged_at": null,
        "body": "Previous benchmark AUTH implementation had a bug due to how redis-benchmark\ngenerates queries.\n\nThe same problem applies to SELECT, and there was already a fix\nin place for SELECT, so this just copies the SELECT methodology and\nuses it for AUTH too.\n\nProblem first reported at\nhttps://github.com/antirez/redis/commit/8fcce90b259f85a4c9fb45301a1c1b67db8ebdb3#commitcomment-8606641\n\nOutput from `% ./redis-benchmark -c 1 -a bob -n 3 -t set -r 99999999` is now:\n\n``` haskell\n1416415268.893890 [0 127.0.0.1:56169] \"AUTH\" \"bob\"\n1416415268.893905 [0 127.0.0.1:56169] \"SET\" \"key:000048417175\" \"xxx\"\n1416415268.894251 [0 127.0.0.1:56169] \"SET\" \"key:000055642065\" \"xxx\"\n1416415268.894338 [0 127.0.0.1:56169] \"SET\" \"key:000054091795\" \"xxx\"\n```\n\nPrior to this commit, redis-benchmark would prepend AUTH to every command sent.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3795,
        "deletions": 693,
        "changed_files": 47,
        "created_at": "2014-11-18T18:45:15Z",
        "closed_at": "2015-01-08T08:51:55Z",
        "merged_at": "2015-01-08T08:51:55Z",
        "body": "# What\n\nMy implementation of what Twitter has previously described as ziplists in a linked list for better memory efficiency.\n\nLonger writeup with improvement graphs at https://matt.sh/redis-quicklist\n# How\n\nRedis currently uses two list encodings.  This replace both.  The code in this PR/branch removes the two current list encodings of `REDIS_ENCODING_ZIPLIST` and `REDIS_ENCODING_LINKEDLIST` and replaces them with `REDIS_ENCODING_QUICKLIST`.\n\nThe only parameter for the new list type is the existing `list-max-ziplist-entries` field which tells Redis how many elements to allow per ziplist node before creating a new node.  The other param of `list-max-ziplist-value` is now deleted.\n\nThe RDB format remains the same.  A quicklist is written to the RDB as a linked list, so the RDB is still usable by prior Redis versions.  The existing RDB format for saved ziplists and linkedlists both get converted to quicklists when the RDB is loaded.  Quicklists are saved as RDB linkedlist types so on reload, Redis will re-create a new compact quicklist with full interior ziplists even if the saved quicklist had not-full interior ziplists.\n# Tests\n\nAll test pass for me.  You should try to break it too.  There is one spurious timing error in Redis tests (waiting too long for RDB loading), but I don't think it's new. (?)\n\nThe quicklist code also includes [~1,000 lines of internal tests](https://github.com/mattsta/redis/blob/quicklist/src/quicklist.c#L1160-L2013).\n\nI added a new compile parameter of \"REDIS_TEST\" \u2014\u00a0if you compile with `-DREDIS_TEST` you can run Redis code tests with `redis-server test <module>`.  So, to test quicklist with code-level tests, compile with that define and run `redis-server test quicklist`.  Also supported: ziplist, util, intset, [and more](https://github.com/mattsta/redis/blob/quicklist/src/redis.c#L3522-L3542).\n# Extras\n\nAlso includes [`DEBUG JEMALLOC INFO`](https://github.com/mattsta/redis/commit/ab7c6492e883e097be25f7970cf328dd852a4828) to get [internal jemalloc stats](https://gist.github.com/mattsta/c2d3db5aa19f34355d69).\n\nFiguring out what any of that means is left as an exercise for the reader.\n# Future Work\n\nThis approach could be applied to some other areas too.  Redis currently converts hashes from zipmaps to full hash tables when they get too big, but we could make a hash table of zipmaps to save a lot of space too.\n",
        "comments": 36
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-11-11T10:10:31Z",
        "closed_at": "2014-12-10T07:59:10Z",
        "merged_at": null,
        "body": "Recent versions of GNU libc will emit warnings when _BSD_SOURCE\nis defined without _DEFAULT_SOURCE is not.\nThe rationale from libc's standpoint here is that _BSD_SOURCE and\n_SVID_SOURCE will now be merged in the single _DEFAULT_SOURCE\nmacros.\nWithout this, the build breaks at -Werror.\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 72,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2014-11-11T10:07:02Z",
        "closed_at": "2014-12-11T13:39:09Z",
        "merged_at": "2014-12-11T13:39:09Z",
        "body": "Both upstart and systemd provide a way for daemons to\nbe supervised, as well as a mechanism for them to\nsignal their readyness status.\n\nThis patch provides compatibility with this functionality while\nnot interfering with other methods.\n\nWith this, it will be possible to use `expect stop` with upstart\nand `Type=notify` with systemd.\n\nA more detailed explanation of the mechanism can be found here:\nhttp://spootnik.org/entries/2014/11/09_pid-tracking-in-modern-init-systems.html\n",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2014-11-11T06:43:52Z",
        "closed_at": "2014-12-10T14:28:41Z",
        "merged_at": "2014-12-10T14:28:41Z",
        "body": "This is a PR for approved feature #831 which is addressed by copying @charsyam's original PR #833 in a way that's able to be merged.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-11-10T09:09:00Z",
        "closed_at": "2014-12-10T15:15:45Z",
        "merged_at": null,
        "body": "when we look up a key, we will check the key's expire first,\nif the key exceed the time, we don't look up key again, return NULL directly\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-11-09T14:42:49Z",
        "closed_at": "2014-11-10T06:43:12Z",
        "merged_at": null,
        "body": "Without checking if the rehash operation ends at last, protential inconsistency may occur.\ne.g. `if (d->ht[0].size == 0)  return *;` (line 409 488 882)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2014-11-04T05:07:41Z",
        "closed_at": "2014-12-10T15:19:55Z",
        "merged_at": null,
        "body": "The optimized method may be more efficient:)\n\nsize & (size-1) /\\* if size is a power of two, the value is  0 _/\nsize |= size-1; /_ make all the bit 0 after the lastest bit 1 to be 1, e.g. 001100->001111 */\n",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 25,
        "changed_files": 20,
        "created_at": "2014-11-03T22:57:31Z",
        "closed_at": "2014-12-10T15:20:44Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2014-11-02T02:52:55Z",
        "closed_at": "2014-12-09T14:13:05Z",
        "merged_at": "2014-12-09T14:13:05Z",
        "body": "A protential bug.\n\nTest on Ubuntu 12.04\nvoid hello(const char _fmt, ...)\n{\n    va_list ap;\n    printf(\"0x%08x\\n\",_ap);\n    va_start(ap, fmt);\n    printf(\"0x%08x\\n\",_ap);\n    va_end(ap);\n    printf(\"0x%08x\\n\",_ap);\n}\nva_end do nothing with ap\n\nbut, should this be a protential bug with other platforms?\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2014-10-31T20:36:01Z",
        "closed_at": "2021-01-28T10:24:36Z",
        "merged_at": null,
        "body": "## Background\n\nThis problem has been reported multiple times on the mailing list and here in GitHub Issues.\n\nPeople sometimes configure replicas using \"SLAVEOF masterhost.internal.corp 6379\", then Sentinel shows up, compares the _hostname_ to the IP address on the master, then re-runs SLAVEOF IP PORT even though the slave is _already_ connected to the correct server.\n## Potential Fixes\n\nThe bad \"comparing hostname to IP address\" issue can be fixed in two places:\n- redis-server: Force SLAVEOF to resolve hostname to IP address\n  - potentially causes a server stall because DNS resolution isn't async\n  - I decided not to use this approach, but it's still at https://github.com/mattsta/redis/commit/4d522c374b85b53c59e37303d89db7e6a30b8c49\n- sentinel: Force Sentniel to resolve all `master_host` addresses\n  - Still non-async resolution, but not performance impacting since it's in Sentinel and not Redis itself.\n  - This approach seems the best, and it's what this PR is all about.\n\nAlternatively, we could just deny _all_ hostnames by forcing IP-only in the SLAVEOF command.  That seems like an extreme fix because hostnames are useful things to have.\n## This Fix\n\nIf we don't resolve hosts to IP addresses, Sentniel compares\na slave's masterhost (potentially a hostname) against the\nmaster's slave list (only IPs).\n\nThis commit resolves hostnames provided by replicas so\nwe can string compare IP->IP instead of Hostname->IP.\n\nDuring any failures, Sentinel will use IP addresses to\nre-parent replicas or rewrite the main config, so the\nhostnames are for convenience only and never get persisted\nin the Sentinel config.\n\nIf a replica goes away and comes back, Sentinel will re-attach it to the master using the master's IP address and not the previous hostname that was configured.  Sentinel internally still operates 100% on IP addresses _except_ for comparing the `master_host` field on replicas against master IP addresses.\n\nIf the resolve fails, we emit a \"-noresolve\" event and\nset the local IP field equal to the non-existing hostname\nanyway.  (Note: since the Redis server is _already_ connected\nto the hostname it reports in INFO, it is _very_ unlikely that Sentniel can't also\nresolve the same hostname because Redis already had to resolve\nit when somebody configured `SLAVEOF hostname port`.)\n\nAlso, now `SENTNIEL SLAVES mymaster` reports `master-host` as the hostname and `master-host-ip` as the resolved IP address of the hostname.\nExample:\n\n``` haskell\nlocalhost:26379> sentinel slaves mymaster\n.\n.\n.\n   33) \"master-host\"\n   34) \"mememe.vcap.me\"\n   35) \"master-host-ip\"\n   36) \"127.0.0.1\"\n.\n.\n.\n```\n\nFixes #2075\nFixes #2032\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-10-31T17:18:00Z",
        "closed_at": "2014-12-11T13:40:18Z",
        "merged_at": "2014-12-11T13:40:18Z",
        "body": "Fix two typos in redis.conf:\n- \"trnasfers\" --> \"transfers\"\n- \"enalbed\" --> \"enabled\"\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 66,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-10-30T20:03:43Z",
        "closed_at": "2014-12-11T14:24:10Z",
        "merged_at": null,
        "body": "Manually creating a redis-cluster is ultra painful without this commands...\n\nCLUSTER ADDSLOTRANGE 0 5\nis equalient for CLUSTER ADDSLOTS 0 1 2 3 4 5\n\nCLUSTER ADDSLOTRANGE &lt;from> &lt;to>\nCLUSTER DELSLOTRANGE &lt;from> &lt;to>\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 24,
        "changed_files": 1,
        "created_at": "2014-10-30T15:44:26Z",
        "closed_at": "2014-12-11T13:58:37Z",
        "merged_at": "2014-12-11T13:58:37Z",
        "body": "if redis works in cluster-mode and redis-cli was run with argv, reconnect if needs.\nexample:\n./redis-cli -c -p 7001 set foo bar\n\nif return is MOVED redis-cli just do nothing.\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2014-10-30T06:18:42Z",
        "closed_at": "2014-10-30T21:31:32Z",
        "merged_at": null,
        "body": "Hi all,\n\nWhen redis-cli re-connects (for example, reboot server and then reconnect) to redis-server without explicit ip and port, redis-cli will run cliConnect(1) and then send a \"connect\" command to the server (see redis-cli.c:942).\nThen, redis-cli will show an error prompt \"(error) ERR unknown command 'connect'\", even if the connection is successful.\nI think the prompt is misleading, and suggest to modify the codes around redis-cli.c:917, which is straightforward.\n\nBest regards\nMin Fu\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 690,
        "deletions": 40,
        "changed_files": 15,
        "created_at": "2014-10-29T05:37:18Z",
        "closed_at": "2014-12-11T14:04:32Z",
        "merged_at": null,
        "body": "A multiple-reader queue.\n\nCommands: \n- qpush key values ... \n- qpop key\n- qpos key offset\n- qinfo key\n- qget key offset\n- qrange key start end\n- qdel key\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2014-10-28T16:31:18Z",
        "closed_at": "2014-10-31T09:01:59Z",
        "merged_at": "2014-10-31T09:01:59Z",
        "body": "Same as the original bind fixes (we just missed these the\nfirst time around).\n\nThis helps Redis not automatically send\nconnections from the first IP on an interface if we are bound\nto a specific IP address (e.g. with multiple IP aliases on one\ninterface, you want to send from _your_ IP, not from the first IP\non the interface).\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 25570,
        "deletions": 16766,
        "changed_files": 251,
        "created_at": "2014-10-27T14:40:27Z",
        "closed_at": "2014-10-27T16:31:23Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-10-27T14:10:50Z",
        "closed_at": "2014-12-11T14:05:38Z",
        "merged_at": "2014-12-11T14:05:38Z",
        "body": "Syntax error against redis-db client caused imports to fail.\n\nReported at https://groups.google.com/forum/#!topic/redis-db/W2na8mYS_t0\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-10-27T09:55:06Z",
        "closed_at": "2014-12-11T14:08:51Z",
        "merged_at": "2014-12-11T14:08:51Z",
        "body": "Fix function prototype in redis-cli.c.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2014-10-27T06:13:32Z",
        "closed_at": "2014-12-11T14:11:22Z",
        "merged_at": "2014-12-11T14:11:22Z",
        "body": "when i use redis-benchmark's idlemode, \n redis-benchmark -p 6379 -h ip -I -c 5000\n\nthe result is \n\n> Creating 5000 idle connections and waiting forever (Ctrl+C when done)\n> All clients disconnected... aborting.\n\nI think the result isn't correct.\nlook into the detail code.I think  In idle mode,the connection shouldn't create write event\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-10-26T18:11:29Z",
        "closed_at": "2014-12-11T14:20:53Z",
        "merged_at": "2014-12-11T14:20:53Z",
        "body": "status command currently reports success when redis has crashed and the pid file still exists. Changing to check the actual process is running.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 28,
        "changed_files": 1,
        "created_at": "2014-10-25T20:43:35Z",
        "closed_at": "2020-10-08T11:05:44Z",
        "merged_at": null,
        "body": "In accordance with the man page of read(2) syscall:\nhttp://man7.org/linux/man-pages/man2/read.2.html\n\nwhen it returns -1 there are three possible issues:\n1. errno == EINTR. This indicates that a signal was received before any\n    bytes were read and read() can be reissued.\n2. errno == EAGAIN. File descriptor from which we read is marked as\n    nonblocking and the read() would block because no data is currently\n    available.\n3. Otherwise there is a more serious error. Simply reissuing the read is\n    unlikely to succeed.\n\nAccording with the man page of signals(7):\nhttp://man7.org/linux/man-pages/man7/signal.7.html\n\nthere are two possible solutions to properly handle interuptions of syscalls:\n1. Provide behavior compatible with BSD signal semantics by using\n    SA_RESTART flag with signal handlers.\n2. Check errno each time after syscall returns error and reissue it if needed.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2014-10-23T20:11:41Z",
        "closed_at": "2014-10-23T21:57:00Z",
        "merged_at": null,
        "body": "This will address the request made here, https://groups.google.com/forum/#!topic/redis-db/kyMN-bN1MSI, and can address issue #2067 by providing 2 additional configuration options.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2014-10-23T18:43:36Z",
        "closed_at": "2014-10-31T09:38:22Z",
        "merged_at": "2014-10-31T09:38:22Z",
        "body": "So... apparently nobody has ever tried to start a cluster with IPv6 addresses?\n\nThis PR is just two simple fixes to allow IPv6+Redis Cluster.\n\n:iphone: \n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 49,
        "deletions": 33,
        "changed_files": 9,
        "created_at": "2014-10-23T18:40:13Z",
        "closed_at": "2014-12-11T17:34:24Z",
        "merged_at": "2014-12-11T17:34:35Z",
        "body": "If you have multiple replicas sync at the same time, you can't tell which replica is causing which action.\n\nSee the \"Before\" box below \u2014 two cluster replicas are attached and sync'ing at once, but you can't tell which is which.\n\nBefore:\n\n``` haskell\n17204:M 23 Oct 13:31:11.256 # IP address for this node updated to ::1\n17204:M 23 Oct 13:31:15.958 # Cluster state changed: ok\n17204:M 23 Oct 13:31:18.421 * Slave asks for synchronization \n17204:M 23 Oct 13:31:18.421 * Full resync requested by slave.\n17204:M 23 Oct 13:31:18.421 * Starting BGSAVE for SYNC\n17204:M 23 Oct 13:31:18.421 * Background saving started by pid 17333\n17333:C 23 Oct 13:31:18.422 * DB saved on disk\n17204:M 23 Oct 13:31:18.439 * Slave asks for synchronization \n17204:M 23 Oct 13:31:18.439 * Full resync requested by slave.\n17204:M 23 Oct 13:31:18.439 * Waiting for end of BGSAVE for SYNC\n17204:M 23 Oct 13:31:18.475 * Background saving terminated with success\n17204:M 23 Oct 13:31:18.475 * Synchronization with slave succeeded\n17204:M 23 Oct 13:31:18.475 * Synchronization with slave succeeded\n```\n\nAfter:\n\n``` haskell\n22172:M 23 Oct 14:18:27.602 # IP address for this node updated to [::1]\n22172:M 23 Oct 14:18:27.645 # Cluster state changed: ok\n22172:M 23 Oct 14:18:30.619 * Slave asks for synchronization from [::1]:59696\n22172:M 23 Oct 14:18:30.619 * Full resync requested by slave from [::1]:59696\n22172:M 23 Oct 14:18:30.619 * Starting BGSAVE for SYNC\n22172:M 23 Oct 14:18:30.619 * Background saving started by pid 22238\n22238:C 23 Oct 14:18:30.620 * DB saved on disk\n22172:M 23 Oct 14:18:30.637 * Slave asks for synchronization from [::1]:59698\n22172:M 23 Oct 14:18:30.637 * Full resync requested by slave from [::1]:59698\n22172:M 23 Oct 14:18:30.637 * Waiting for end of BGSAVE for SYNC\n22172:M 23 Oct 14:18:30.675 * Background saving terminated with success\n22172:M 23 Oct 14:18:30.675 * Synchronization with slave succeeded from [::1]:59696\n22172:M 23 Oct 14:18:30.675 * Synchronization with slave succeeded from [::1]:59698\n```\n\nI thought this would be a quick one or two line change, but I ended up refactoring all the IP formatting code.  Some of the \"format an IP properly\" code was getting copy/pasted around (since it's just one or two lines), but we can make a nicer interface for it.\n\nNow we have achieved the goal of eradicating any copy/paste use of `strchr(ip, ':')` outside of formatting functions and config parsing.\n",
        "comments": 14
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2014-10-23T15:56:22Z",
        "closed_at": "2014-12-11T14:54:39Z",
        "merged_at": "2014-12-11T14:54:39Z",
        "body": "People mostly use SORT against lists, but our prior\nbehavior was pretending lists were an unordered bag\nrequiring a forced-sort when no sort was requested.\n\nWe can just use the native list ordering to ensure\nconsistency across replicaion and scripting calls.\n\nTests added too (they fail before this commit).\n\nCloses #2079\nCloses #545 (again)\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-10-23T09:07:55Z",
        "closed_at": "2014-12-12T21:58:56Z",
        "merged_at": "2014-12-12T21:58:56Z",
        "body": "closes #2062\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 73,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2014-10-22T11:11:25Z",
        "closed_at": "2016-06-15T09:18:48Z",
        "merged_at": null,
        "body": "The regular TTL command sets the idletime to zero. This command returns the TTL in seconds without modifying the idletime.\nThis can be useful when you are fetching information about objects but don't want to touch them at the same time.\n",
        "comments": 28
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-10-22T05:03:07Z",
        "closed_at": "2021-03-16T14:52:06Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-10-22T02:20:41Z",
        "closed_at": "2021-10-20T09:01:53Z",
        "merged_at": null,
        "body": "closes #2074\n",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-10-21T12:14:41Z",
        "closed_at": "2022-01-09T14:41:03Z",
        "merged_at": null,
        "body": "the `tvp->tv_usec` was always not less than zero, and if `tvp->sec`\nless than zero, `tvp->tv_usec` must be zero.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 333,
        "deletions": 24,
        "changed_files": 7,
        "created_at": "2014-10-20T18:57:57Z",
        "closed_at": "2021-01-28T16:18:40Z",
        "merged_at": null,
        "body": "This implements a dynamically updating process title based on the state of the Redis server.\n\nThe format of the new process title is:\n\n```\n{{executable}} {{name}} {{config}} {{ip0}}:{{port}} {{mode}} {{role}} {{state}}\n```\n\nwhere:\n\n``` haskell\nexecutable is: argv[0]\nname is: [name] | <empty>\nconfig is : config file path | <empty>\n[Note: the name and config policy is (name xor config) - If a name is set, the config path is absent.]\nmode is: cluster | sentinel | <empty> (empty if regular master/replica or master-with-no-replicas setup)\nrole is: master | replica | <empty> (empty if standalone master with no replicas)\nstate is: live cluster node description | live replica description | live sentinel description | <empty> (empty if no replica _or_ if a forked background process)\n```\n\nYou can see example output based on different scenarios at https://gist.github.com/mattsta/7512d430b3cc43bd669e\n\nCreation History:\n- In commit 6356cf6808be9ea88ab97be33ebf1576eeb57da6, Redis changed the way it shows up in process lists.  Previously, the format was the launch command line.  After commit 6356cf6808be9ea88ab97be33ebf1576eeb57da6, then format became \"redis-server ip:port\" \u2014\u00a0which is _largely_ not useful in many scenarios!\n  - personal note: When I have 10 Redis processes running on my server, I like to see them by logical name (typically the name of the config file shows is the name of the server), but having 10 servers with only port identifiers makes me consult _another_ lookup table to figure out what things are doing (or grep through pid files then match pid file contents to pid file names... it's not pretty).\n- We've had people complaining about the change for a while.  See https://github.com/antirez/redis/issues/694 and https://github.com/antirez/redis/issues/1979\n- So, after first suggesting this [back in April](https://github.com/antirez/redis/issues/694#issuecomment-41324583), I finally got around to adding a dynamic updating proctitle interface.\n\nImplementation History:\n- I started out by allowing a user-specified format string (double curly bracket formatting).  But, the format parsing code was [too ugly](https://github.com/mattsta/redis/blob/proctitle/src/redis.c#L3668-L3722), so I scrapped it in favor of [static content positions](https://github.com/mattsta/redis/blob/proctitle-noformat/src/redis.c#L3684-L3712).\n- I added (or refactored out) unique state information reporters for each major mode of Redis ([master/replica](https://github.com/mattsta/redis/blob/proctitle-noformat/src/replication.c#L1514-L1526), [sentinel](https://github.com/mattsta/redis/blob/proctitle-noformat/src/sentinel.c#L2919-L2967), [cluster](https://github.com/mattsta/redis/blob/proctitle-noformat/src/cluster.c#L3560-L3607)).  I tried to select the best information to show for each mode, but we can always add or remove information as necessary.  (For example, I included cluster epoch in the cluster instance output, but not really sure why... it just looks neat.)\n- The dynamic title updating is [triggered in serverCron()](https://github.com/mattsta/redis/blob/proctitle-noformat/src/redis.c#L1251-L1256) meaning:\n  - to update the process title, functions call `PROCTITLE_UPDATE()` which just sets `server.update_proctitle = 1`\n  - the server title update can lag server state by a second or two\n  - if a process updates state multiple times, the state will only be read at most once per second (this prevents Redis from flapping the title unnecessarily).\n  - Cluster and Sentinel both write out all state changes to config files \u2014\u00a0and it was very simple to just add the process title update flag to one place in the config writing code to trigger title updates too (since we know the state changed when the config updater is called).\n  - Replication state tracking is a bit more tricky; it required adding `PROCTITLE_UPDATE()` triggers in a few different places since there's no centralized \"replication state updated\" hook.\n  - Sentinel state tracking required some additional non-config-file-update-based triggers because internal per-node state (especially transitional states) aren't written out to the state/config files.\n- I added a [very simple \"server instance name\" config option](https://github.com/mattsta/redis/commit/d9daeb778a1457867c39efd25e9d66eb894b6002) before people [started complaining about it](https://groups.google.com/forum/#!searchin/redis-db/instance$20name/redis-db/kEDe3IixFoo/moMjEOF6VFgJ).  We can either keep it, remove it, or refactor it into something more extensible.  I'd like to go further and have arbitrary \"config metadata\" per-instance, but that's out of the scope of what's needed for a simple process title name.\n\nSo, here it is.\n\nIf you've got free time and want to help double check everything here: try it out, try to make it crash, try to find any remaining edge cases where state updates don't get reflected in the title (meaning we need more `PROCTITLE_UPDATE()` calls somewhere), and try to think of any pathological conditions where the title updates could cause problems.\n\n:shipit: \n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-10-20T01:12:54Z",
        "closed_at": "2021-06-30T13:46:07Z",
        "merged_at": "2021-06-30T13:46:06Z",
        "body": "Adds call to intrev16ifbe to ensure ZIPLIST_LENGTH is compared correctly\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-10-18T18:48:24Z",
        "closed_at": "2014-10-30T11:01:51Z",
        "merged_at": "2014-10-30T11:01:51Z",
        "body": "Closes #2066\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 68,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2014-10-14T22:12:36Z",
        "closed_at": "2018-09-17T13:24:31Z",
        "merged_at": null,
        "body": "This can address issue https://github.com/antirez/redis/issues/2067 by providing 2 additional configuration options. \n",
        "comments": 48
    },
    {
        "merged": false,
        "additions": 39,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2014-10-13T23:35:35Z",
        "closed_at": "2022-01-10T10:00:34Z",
        "merged_at": null,
        "body": "Suppose a client periodically receives 8MB messages, but rarely anything smaller, and has a soft limit of 8mb in 60 seconds. obuf_soft_limit_reached_time currently only gets checked when adding to the buffer, not removing from it.  Even if the client reads each message in well under 60 seconds, the next time a message gets published obuf_soft_limit_reached_time is still set. checkClientOutputBufferLimits thinks the soft limit's been exceeded ever since the first message was published and disconnects the client.\n\nThis adds a check after sending data, too, recognizing when the buffer shrinks between writes.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 75,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2014-10-13T07:08:39Z",
        "closed_at": "2017-07-15T15:39:44Z",
        "merged_at": null,
        "body": "Create a new list command, linfo get list addtional information(history size, created time & updated time)\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 87,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2014-10-12T03:31:51Z",
        "closed_at": "2021-10-04T09:06:03Z",
        "merged_at": null,
        "body": "Fix for ##1861\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-10-10T14:42:38Z",
        "closed_at": "2015-02-25T09:09:40Z",
        "merged_at": "2015-02-25T09:09:40Z",
        "body": "Without this commit, Redis can generate an invalid config:\n\n``` haskell\n127.0.0.1:26379> sentinel monitor bob-master 127.0.0.1 6379 0\n```\n\nSentinel accepts that command, but then after a restart, we get...\n\n```\n*** FATAL CONFIG FILE ERROR ***\nReading the configuration file, at line 4\n>>> 'sentinel monitor bob-master 127.0.0.1 6379 0'\nQuorum must be 1 or greater.\n```\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-10-10T00:30:17Z",
        "closed_at": "2021-06-21T04:47:29Z",
        "merged_at": null,
        "body": "Refs #2051\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2014-10-09T16:04:21Z",
        "closed_at": "2015-02-25T09:18:07Z",
        "merged_at": "2015-02-25T09:18:07Z",
        "body": "Previously the string was created empty then re-sized\nto fit the offset, but sds resize causes the sds to\nover-allocate by at least 1 MB (which is a lot when\nyou are operating at bit-level access).\n\nThis also improves the speed of initial sets by 2% to 6%\nbased on quick testing.\n\nPatch logic provided by @oranagra\n\nFixes #1918\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 66,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2014-10-07T12:57:43Z",
        "closed_at": "2022-01-10T07:00:10Z",
        "merged_at": null,
        "body": "I suggest to add this usefull(at least for us) and simple command. It just returns a sum of a bunch of numeric keys. All non-integer (non-numeric for sumfloat) and non-existing keys are ignored. This behaviour is discussable (for example we can return an error if there is at least one non-numeric or non-integer key), but I personally tried to be consistent with other commands (for example mget command will return nil for non-string key). I would be glad to make a pull request to documentation if you find it usefull too. Many thanks.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2014-10-06T14:37:21Z",
        "closed_at": "2022-04-07T09:39:27Z",
        "merged_at": null,
        "body": "Don't change `dirty` or notify for a change when we set the same bit twice with `setbit`.\n\n`setbit` should work like [`sadd`](https://github.com/antirez/redis/blob/73a809b1591378e1042a1028d0b8e10217e6e7c7/src/t_set.c#L250) which also doesn't mark anything dirty or send any notification when nothing changes.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-10-01T11:44:20Z",
        "closed_at": "2014-10-29T19:49:32Z",
        "merged_at": null,
        "body": "Adds a \"`(char *)`\" cast to avoid the warning:\n\n```\ndebug.c: In function \u2018debugCommand\u2019:\ndebug.c:339:17: warning: format \u2018%s\u2019 expects argument of type \u2018char *\u2019, but argument 4 has type \u2018void *\u2019 [-Wformat=]\n                 (c->argc == 3) ? \"key\" : c->argv[3]->ptr, j);\n                 ^\ndebug.c:339:17: warning: format \u2018%s\u2019 expects argument of type \u2018char *\u2019, but argument 4 has type \u2018void *\u2019 [-Wformat=]\n```\n\nSilly merge.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2014-10-01T02:08:08Z",
        "closed_at": "2014-10-06T13:30:41Z",
        "merged_at": null,
        "body": "print error when you do -> \n\n```\nredis-cli info invalidsection\n```\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-09-30T13:46:20Z",
        "closed_at": "2017-06-23T16:09:50Z",
        "merged_at": null,
        "body": "EVAL and EVALSHA descriptions are currently equal. This update adds the information that for EVALSHA the script should already be cached and that it's called by its SHA-1 digest.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-09-29T10:47:00Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": ":smile: \n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-09-27T22:15:52Z",
        "closed_at": "2017-07-15T15:38:54Z",
        "merged_at": null,
        "body": "I think it can be easilier to change redis.sock permissions by this way\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-09-25T20:41:55Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-09-19T15:43:24Z",
        "closed_at": "2021-04-27T12:00:02Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-09-18T18:49:00Z",
        "closed_at": "2014-09-19T10:49:35Z",
        "merged_at": "2014-09-19T10:49:35Z",
        "body": "Fixed in Redis by 1a5e5b6, but since that part of the hiredis code is largely copy/paste'd from Redis, the fix needs to be ported over too.\n\nFix confirmed in #2012\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-09-18T14:17:43Z",
        "closed_at": "2014-09-18T15:21:22Z",
        "merged_at": "2014-09-18T15:21:22Z",
        "body": "This came up when we got junk IPv6 addresses on 'info replication' when replicating through a Unix domain socket.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-09-16T15:27:56Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "Fixed a typo - \"secuentially\" is actually spelled as \"sequentially\".\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2014-09-15T17:45:15Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-09-11T14:22:34Z",
        "closed_at": "2017-07-24T13:18:08Z",
        "merged_at": "2017-07-24T13:18:08Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-09-09T06:51:34Z",
        "closed_at": "2014-09-10T07:57:54Z",
        "merged_at": "2014-09-10T07:57:54Z",
        "body": "*SCAN will cause redis server to hang for seconds\nafter millions of keys was deleted by SCAN/DEL pairs\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-09-06T16:53:56Z",
        "closed_at": "2014-09-06T21:30:15Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-09-03T02:56:25Z",
        "closed_at": "2017-07-03T11:19:13Z",
        "merged_at": null,
        "body": "Fixes sprintf reading beyond the end of a buffer if a key gets truncated.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2014-09-02T22:05:28Z",
        "closed_at": "2014-09-03T13:10:35Z",
        "merged_at": "2014-09-03T13:10:35Z",
        "body": "32 bit builds don't have a big enough long to capture\nthe same range as a 64 bit build.  If we use \"long long\"\nwe get proper size limits everywhere.\n\nThe GETRANGE doc says large requests don't fail, they just return the entire contents of the string.\n\nFixes #1981\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 57,
        "deletions": 40,
        "changed_files": 1,
        "created_at": "2014-09-02T00:27:44Z",
        "closed_at": "2017-07-15T15:33:21Z",
        "merged_at": null,
        "body": "- Wrote missing documentation for `listInsertNode()` and `listRewindTail()`.\n- Various grammatical improvements to clarify documentation.\n- Reordered segments to make them more consistent between functions - _On error..._, _On success..._ etc.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 40,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2014-09-01T00:38:25Z",
        "closed_at": "2014-09-03T13:15:34Z",
        "merged_at": null,
        "body": "same as #1968 but PRing against unstable.\n## Problem\n\nI've observed redis servers eat up all the machines memory and get into swapping causing issues on the clients, when a bug or just careless usage introduces very large keys (100s of MBs).\n## What is here?\n\nThis adds the `max-key-size` config option, when the value of that option is not null keys longer than the number in bytes defined will be rejected.\n\nI'm PRing against 2.8, I can move it to 3.x if there is interest.\n\nTopic on the google group: https://groups.google.com/forum/#!topic/redis-db/aLxpHURnA8Q\n\n/cc @csfrancis, @fbogsany\n\nwhat do you think ? @antirez @pietern @badboy\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2014-08-27T18:20:43Z",
        "closed_at": "2014-09-03T13:18:47Z",
        "merged_at": null,
        "body": "Add a configuration option for slaves to refuse syncing to a master if there is not enough data in the master. Based on 2.8, basically a rebase of https://github.com/antirez/redis/pull/1247\n\nie:\nrepl-min-transfer-size 1g\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 40,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2014-08-25T22:30:33Z",
        "closed_at": "2014-09-02T15:16:06Z",
        "merged_at": null,
        "body": "## Problem\n\nI've observed redis servers eat up all the machines memory and get into swapping causing issues on the clients, when a bug or just careless usage introduces very large keys (100s of MBs).\n## What is here?\n\nThis adds the `max-key-size` config option, when the value of that option is not null keys longer than the number in bytes defined will be rejected.\n\nI'm PRing against 2.8, I can move it to 3.x if there is interest.\n\nTopic on the google group: https://groups.google.com/forum/#!topic/redis-db/aLxpHURnA8Q\n\n/cc @csfrancis, @fbosany\n\nwhat do you think ? @antirez @pietern \n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2014-08-25T21:14:02Z",
        "closed_at": "2015-01-12T14:56:36Z",
        "merged_at": null,
        "body": "This is a continuation of #463 \n\nIt's useful to have a pidfile be created even when Redis is in the foreground for various reasons.\n\nThese commits will always write the pidfile if `pidfile` is given in the configuration.\n\nTwo modes:\n- foreground: If `pidfile` is not given in the configuration, then no pidfile is written.\n- daemonize: If no `pidfile` in configuration, the default location is used (`#define REDIS_DEFAULT_PID_FILE \"/var/run/redis.pid\"`).  This is the current behavior.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2014-08-25T20:56:47Z",
        "closed_at": "2015-02-24T16:20:09Z",
        "merged_at": "2015-02-24T16:20:09Z",
        "body": "Improved original fix to also \"return an empty string like in Redis Server\" for non-existing sections (instead of returning a syntax error).\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-08-25T18:24:25Z",
        "closed_at": "2014-08-26T08:03:54Z",
        "merged_at": "2014-08-26T08:03:54Z",
        "body": "Companion for 8eeb1802ec42682a614a5ebca318a0ba44ca7c03, but dealing with hiredis.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-08-23T16:18:13Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "ofter -> often\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-08-23T14:50:45Z",
        "closed_at": "2021-02-01T14:01:43Z",
        "merged_at": null,
        "body": "Added else if clause to sentinelHandleConfiguration to check for a single-argument statement which is also a valid file-path to catch improper placement of the --sentinel option and return an appropriate error string. Nothin' too fancy.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 137,
        "deletions": 2,
        "changed_files": 8,
        "created_at": "2014-08-21T19:32:37Z",
        "closed_at": "2014-08-25T19:56:33Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2014-08-15T21:03:16Z",
        "closed_at": "2014-09-10T13:46:50Z",
        "merged_at": null,
        "body": "This fixes a bug introduced by this commit:\n\n8237d88\n\nThe length of the static argv is not stored when it is first created; only when extended. On the first occasion this is relatively harmless (since the initialised length was 0), but on any subsequent \"first\" allocation - eg after having been cleared here: https://github.com/antirez/redis/blob/8237d88f48e0843839eb530c408686d66bcae6ab/src/scripting.c#L352\n\nthen a new allocated argv implicitly inherits the length of the last freed one. Where the new argv is smaller than the last freed one, and then later reused without extension at any length greater than its actual length, memory is overwritten.\n\nThis fixes issue #1939.\n\nSee also related issue #1943.\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-08-14T15:26:16Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "Fun fact: this typo seems to have been in the [first commit](https://github.com/antirez/redis/commit/ed9b544e10b84cd43348ddfab7068b610a5df1f7).\n\nOur long-lived nightmare will finally come to an end.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2014-08-13T17:34:34Z",
        "closed_at": "2017-07-24T13:20:31Z",
        "merged_at": "2017-07-24T13:20:31Z",
        "body": "It's not POSIX (BSD systems have -E instead) and we don't actually need it.\n\nCloses #1922\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2014-08-12T07:10:20Z",
        "closed_at": "2014-08-26T14:21:42Z",
        "merged_at": null,
        "body": "For reasoning, see #1929.\n\nAll tests pass, and everything runs as expected.\n\n`int -> long` change needs to be made in `deps/hiredis/dict.h` also, but a PR will me made to it's repo.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2014-08-09T15:36:30Z",
        "closed_at": "2017-07-15T15:32:17Z",
        "merged_at": null,
        "body": "Firstly, I'm not a C developer so this might not be the best way to add this feature.  It's not yet ready for merging, I've got a few questions below that I'd like to sort out before it's in a mergeable state.\n\nThis adds the ability to name databases:\n\n```\nset a 1\ndbname site-cache\nselect 1\nset a 1\ndbname sessions\ninfo\n```\n\nleads to\n\n```\ndb0:keys=1,expires=0,avg_ttl=0,name=site-cache\ndb1:keys=1,expires=0,avg_ttl=0,name=sessions\n```\n\nThe reason for adding this was so I could quickly see at a glance what a database is being used for.\n\nQuestions:\n1. Would this feature be wanted?\n2. How do I go about getting the name to save and persist between restarts?\n3. How do I get the name to sync to slaves?\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-08-06T06:18:08Z",
        "closed_at": "2014-08-25T08:28:22Z",
        "merged_at": null,
        "body": "As `char[32]` is not sufficient for an IPv6 address, I changed it to `char[REDIS_IP_STR_LEN]`, which makes sentinels to run on IPv6 network.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-08-05T12:51:03Z",
        "closed_at": "2014-08-25T08:28:22Z",
        "merged_at": null,
        "body": "This one-line correction fixes issue #1649, a dumb one, but my OCD was killing me.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-08-05T07:14:54Z",
        "closed_at": "2014-08-25T08:28:22Z",
        "merged_at": null,
        "body": "Once this is merged and the latest changes to `commands.json` in antirez/redis-doc are merged, `utils/generate-command-help.rb > src/help.h` should be re-run, so we get nice and easy tab-completed help in `redis-cli` again for all available commands.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-08-04T17:11:14Z",
        "closed_at": "2014-09-04T09:51:38Z",
        "merged_at": null,
        "body": "I previously added source binding to sentinelReconnectInstance(),\nbut I missed adding it to the Hello flow.\n\nAll tests pass.\n\nThanks to @dkong for tracking down the problem in\nhttps://github.com/dkong/redis/commit/9b6eb0e7d4b879873d46e6d0fac2d9cb4fab4868\n",
        "comments": 29
    },
    {
        "merged": false,
        "additions": 534,
        "deletions": 4,
        "changed_files": 8,
        "created_at": "2014-08-04T10:08:22Z",
        "closed_at": "2017-07-03T11:18:38Z",
        "merged_at": null,
        "body": "Redis white black IP list feature\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 348,
        "deletions": 163,
        "changed_files": 42,
        "created_at": "2014-08-02T02:41:57Z",
        "closed_at": "2014-08-25T21:35:32Z",
        "merged_at": null,
        "body": ":alien: DON'T PANIC :alien: \n\nThis is every _simple_, non-controversial, single-issue fix I found in the issue/PR backlog.\n\nYes, this branch has ~~54~~ ~~57~~ 63 commits that will auto-close over 50 issues when merged (#463, #513, #537, #811, #857, #858, #878, #952, #1188, #1187, #997, #1092, #1105, #1129, #1161, #1186, #1191, #1300, #1327, #1376, #1428, #1450, #1456, #1543, #1519, #1561, #1597, #1610, #1614, #1615, #1631, #1637, #1660, #1681, #1696, #1512, #1728, #1741, #1602, #1741, #1774, #1801, #1842, #1843, #1847, #1844, #1883, #1645, #1647, #1900, #760, #566, #1313, #521, #1649, #1912, #1909, #1914, #1097, #1915)\n\nYes, this branch has ~~31~~ ~~32~~ 34 authors (HI COMMUNITY!  We haven't forgotten you!).\n\nYes, I manually reviewed, edited, and/or fixed every commit here.\n\nFor quick browsing of all the changes, use this diff view: https://github.com/antirez/redis/pull/1906/files\n\nEverything works, all tests pass (https://travis-ci.org/mattsta/redis/builds/31819638), and this Branch of Awesome Correctness is ready for merging into Actual Redis.\n\nNotable changes include:\n- increased maximum sds size to 4 GB from 2 GB  (just an `int` to `unsigned int` replacement; everything still works)\n- if Redis is in the foreground and you Ctrl-C, it will now `SHUTDOWN` cleanly instead of just killing the server.\n- Sentinel's INFO command now accepts \"all\" to act like the Redis INFO command.\n- adds `-a` option to `redis-benchmark` to allow auth when testing\n- stops Redis tests from polluting background colors if you have a non-black-background terminal\n- many code clarifications and movement towards being more correct everywhere possible\n- if you request a PID file, it will now always be written instead of only showing up in daemon mode.\n- many more technical fixes, tiny refactorings, and logic improvements throughout the code\n",
        "comments": 22
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-08-01T21:35:00Z",
        "closed_at": "2014-10-05T13:51:37Z",
        "merged_at": null,
        "body": "This appears to address issue #1904 by simply adding \"auth\" as a known sentinel command.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 210,
        "deletions": 165,
        "changed_files": 44,
        "created_at": "2014-07-31T19:02:58Z",
        "closed_at": "2014-10-06T07:44:54Z",
        "merged_at": "2014-10-06T07:44:54Z",
        "body": "This is every typo or comment fix I could find in the issues backlog (plus a few more I manually fixed by browsing source and tests).\n\nThis single PR will automatically close the following issues when merged: #1243, #1242, #1507, #1351, #1373, #1386, #1441, #1513, #1537, #1544, #1673, #1682, #1713, #1806, #1871, #1872, #1877, #1897\n\nThese commits _only_ modify spelling and trailing whitespace.  There are no code functionality changes introduced.\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 3,
        "changed_files": 6,
        "created_at": "2014-07-29T21:49:02Z",
        "closed_at": "2014-08-25T08:28:20Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2014-07-29T13:01:21Z",
        "closed_at": "2021-10-14T11:02:59Z",
        "merged_at": null,
        "body": "There are cases where a failed rdbLoadObject doesn't lead to a program abort. In such cases we should cleanly free objects allocated during the de-serialization.\nThe cleanup in this PR might be a bit ugly. Let me know if you have suggestions how to make it nicer.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-07-28T20:09:18Z",
        "closed_at": "2014-10-06T07:45:19Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-07-28T03:16:14Z",
        "closed_at": "2014-07-28T07:38:33Z",
        "merged_at": "2014-07-28T07:38:33Z",
        "body": "URL for info on notifications events was wrong\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-07-22T10:43:48Z",
        "closed_at": "2014-07-22T14:54:16Z",
        "merged_at": null,
        "body": "just hello\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2014-07-21T21:34:47Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "We only want to use the last STORE key, but we have to record\nwe actually found a STORE key so we can increment the final return\nkey count.\n\nTest added to prevent further regression.\n\n(As noted in antirez/redis-doc#392)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-07-21T08:41:28Z",
        "closed_at": "2014-07-31T15:48:25Z",
        "merged_at": null,
        "body": "Fix typo.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-07-17T07:25:33Z",
        "closed_at": "2014-10-06T07:45:19Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2014-07-15T06:42:24Z",
        "closed_at": "2015-01-09T10:55:41Z",
        "merged_at": null,
        "body": "Personally, \"is\" is better than \"was\".\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-07-14T15:43:22Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "Spelling fix.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2014-07-12T16:03:42Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "Just a few typos I discovered while reading the code (and a few corrected using aspell, too).\n\nThe overall code looks good to me and it seems to work as expected.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2014-07-11T20:05:47Z",
        "closed_at": "2014-07-14T13:37:33Z",
        "merged_at": "2014-07-14T13:37:33Z",
        "body": "Previously, the command definition for the OBJECT command specified\na minimum of two args (and that it was variadic), which meant that\nif you sent this:\n\nOBJECT foo\n\nWhen cluster was enabled, it would result in an assertion/SEGFAULT\nwhen Redis was attempting to extract keys.\n\nIt appears that OBJECT is not variadic, and only ever takes 3 args.\n\nhttps://gist.github.com/michael-grunder/25960ce1508396d0d36a\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 57,
        "deletions": 8,
        "changed_files": 3,
        "created_at": "2014-07-07T09:45:35Z",
        "closed_at": "2017-07-15T15:37:14Z",
        "merged_at": null,
        "body": "we are suffering `Latency due to AOF and disk I/O` (see: http://redis.io/topics/latency)\n\nredis has already use a single thread to do `fsync`, but `write` still would block, we use::\n\n```\nsudo strace -f -p $(pidof redis-server) -T -e trace=fdatasync,write 2>&1 | grep -v '0.0' | grep -v unfinished\n```\n\nwatch a `write` block more than **2 seconds**.\n\nso I think can we use async-thread to write aof to avoid the main working thread block,\n\nthis patch do so, here is the test script and result::\n\n```\n$ cat 2.8.12.conf\nappendfilename appendonly.2.8.12\nappendonly yes\nport 12000\n$ cat async.conf\nappendfilename appendonly.async\nappendonly yes\nport 12001\n\n$ cat run-test.sh\npkill -f 1200\nrm appendonly.*\nrm *.latency\nrm *.qps\n\nsleep  1\n\n../src/redis-server.2.8.12 ./2.8.12.conf &\n../src/redis-server.async  ./async.conf &\n\nsleep  1\n\n../src/redis-cli -p 12001 set k1 v1\n../src/redis-cli -p 12000 set k0 v0\n\n../src/redis-benchmark --csv -n 10000000 -p 12000 -t set > 2.8.12.qps&\n../src/redis-benchmark --csv -n 10000000 -p 12001 -t set > async.qps&\n\ndd if=/dev/zero of=xxxxx bs=1M count=10000 &\n\n../src/redis-cli --latency-history -p 12000 >2.8.12.latency &\n../src/redis-cli --latency-history -p 12001 >async.latency  &\n```\n\nand here is the result::\n\n```\n$ tail -f 2.8.12.latency\nmin: 0, max: 35, avg: 0.18 (1447 samples) -- 15.00 seconds range\nmin: 0, max: 49, avg: 0.15 (1458 samples) -- 15.01 seconds range\nmin: 0, max: 20, avg: 0.14 (1468 samples) -- 15.01 seconds range\nmin: 0, max: 1, avg: 0.12 (1473 samples) -- 15.01 seconds range\nmin: 0, max: 91, avg: 0.19 (1416 samples) -- 15.00 seconds range\nmin: 0, max: 495, avg: 0.62 (1388 samples) -- 15.00 seconds range\nmin: 0, max: 20, avg: 0.23 (1416 samples) -- 15.00 seconds range\nmin: 0, max: 1885, avg: 3.61 (1087 samples) -- 15.01 seconds range\nmin: 0, max: 772, avg: 0.66 (1435 samples) -- 15.76 seconds range\nmin: 0, max: 602, avg: 0.80 (1359 samples) -- 15.01 seconds range\nmin: 0, max: 1, avg: 0.16 (1465 samples) -- 15.01 seconds range\nmin: 0, max: 2, avg: 0.37 (1432 samples) -- 15.01 seconds range\n\n$ tail -f async.latency\nmin: 0, max: 2, avg: 0.10 (1461 samples) -- 15.00 seconds range\nmin: 0, max: 1, avg: 0.10 (1461 samples) -- 15.01 seconds range\nmin: 0, max: 12, avg: 0.13 (1467 samples) -- 15.00 seconds range\nmin: 0, max: 1, avg: 0.12 (1472 samples) -- 15.00 seconds range\nmin: 0, max: 1, avg: 0.10 (1409 samples) -- 15.01 seconds range\nmin: 0, max: 3, avg: 0.17 (1443 samples) -- 15.01 seconds range\nmin: 0, max: 1, avg: 0.10 (1460 samples) -- 15.00 seconds range\nmin: 0, max: 1, avg: 0.11 (1470 samples) -- 15.01 seconds range\nmin: 0, max: 11, avg: 0.13 (1441 samples) -- 15.01 seconds range\nmin: 0, max: 6, avg: 0.16 (1435 samples) -- 15.00 seconds range\n\n$ cat 2.8.12.qps\n\"SET\",\"56891.23\"\n$ cat async.qps\n\"SET\",\"61486.62\"\n```\n\nso the async-aof patch give us stable latency, and higher qps.\n\nthe test is run on a 8 cpu machine with traditional hdd.\n",
        "comments": 23
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2014-07-05T21:44:49Z",
        "closed_at": "2015-02-13T16:06:25Z",
        "merged_at": null,
        "body": "Adding the .md extension makes GitHub render the README and CONTRIBUTING as Markdown instead of plain text.\n\nBefore:\n![screen shot 2014-07-05 at 2 42 36 pm](https://cloud.githubusercontent.com/assets/703628/3487605/7cb409bc-048d-11e4-9ce3-c7d7cdad9932.png)\n\nAfter:\n![screen shot 2014-07-05 at 2 43 00 pm](https://cloud.githubusercontent.com/assets/703628/3487606/7cb57ce8-048d-11e4-9a36-531930162f64.png)\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 461,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2014-07-04T07:28:55Z",
        "closed_at": "2017-07-15T15:31:40Z",
        "merged_at": null,
        "body": "@antire,\n\nI added int64 support for redis lua in this patch. \n\nCurrently, \u201cnumber\" is the only type in lua to define numerical object, which is actually a \"double\". The integer returned from redis will be conversed to \"number\"; the return value of \"tonumber\" lua function is also a \"number\". However, the conversion from a 64bit \"int\" to a \"double\" may cause loss of data.\n\nIn some cases, we'd like to use \"int64\" type, and do some calculation in lua script(i.e. incremental id). So I added this support for redis, and want to merge it to redis, since it is a common requirement. \n\nPls let me know if you have any comments, thx.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2014-06-29T10:34:01Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "Previously the end was casted to a smaller type which resulted in a wrong check and failed with values larger than handled by unsigned.\n\nFixes #1844\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2014-06-29T02:22:49Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "Negative key count causes segfault in Lua functions.\n\nFixes #1842\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2014-06-28T11:27:39Z",
        "closed_at": "2015-02-13T16:07:58Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-06-21T13:59:23Z",
        "closed_at": "2014-06-23T08:07:13Z",
        "merged_at": "2014-06-23T08:07:13Z",
        "body": "If we're writing the initial AOF, we shouldn't exit because Redis will load the empty (or partial) AOF on restart.\n",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-06-18T05:00:02Z",
        "closed_at": "2014-06-18T07:31:02Z",
        "merged_at": "2014-06-18T07:31:02Z",
        "body": "see also #1658\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 81,
        "deletions": 15,
        "changed_files": 3,
        "created_at": "2014-06-17T15:23:49Z",
        "closed_at": "2014-11-20T21:09:23Z",
        "merged_at": null,
        "body": "Add some useful config and system data we can use in crash output.\n\nMy first attempt at getting system memory involved the very clean OS X `sysctl` interface.  After I committed it, it seemed a bit _too_ easy.  On further inspection, of course, there's no cross-platform stable way of getting system memory.  \n\nI ended up tweaking a [very comprehensive memory discovery function](http://nadeausoftware.com/articles/2012/09/c_c_tip_how_get_physical_memory_size_system) and it works great.\n\nNew output sample:\n\n``` haskell\n# Memory\nused_memory:1009056\nused_memory_human:985.41K\nused_memory_rss:1712128\nused_memory_peak:1009056\nused_memory_peak_human:985.41K\ntotal_system_memory:17179869184\ntotal_system_memory_human:16.00G\nused_memory_lua:33792\nmem_fragmentation_ratio:1.70\nmem_allocator:libc\nmaxmemory_policy:noeviction\n```\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2014-06-16T15:00:41Z",
        "closed_at": "2014-07-31T16:03:01Z",
        "merged_at": null,
        "body": "update\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2014-06-11T00:37:34Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "Change comment from \"On error REDIS_OK is returned\"  to \"On success REDIS_OK is returned\" to fix issue #1805\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2014-06-10T18:27:26Z",
        "closed_at": "2014-06-11T08:09:02Z",
        "merged_at": "2014-06-11T08:09:02Z",
        "body": "The new check-for-number behavior for Lua arguments broke\nusers who use large strings of just integers.\n\nThe Lua number check would convert the string to a number, but\nthat breaks user data because\nLua numbers have limited precision compared to an arbitrarily\nprecise number wrapped in a string.\n\nRegression fixed and new test added.\n\nFixes #1118 again.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 34545,
        "deletions": 8258,
        "changed_files": 326,
        "created_at": "2014-06-10T16:01:16Z",
        "closed_at": "2014-12-17T16:56:37Z",
        "merged_at": null,
        "body": "More details of implementation in issue https://github.com/antirez/redis/issues/1793.\n\n---\n\nCommit message:\n\nAdded <count> parameter to SPOP:\n- spopCommand() now runs spopWithCountCommand() in case the <count> param is found.\n- Added intsetRandomMembers() to Intset: Copies N random members from the set into inputted 'values' array. Uses either the Knuth or Floyd sample algos depending on ratio count/size.\n- Added setTypeRandomElements() to SET type: Returns a number of random elements from a non empty set. This is a version of setTypeRandomElement() that is modified in order to return multiple entries, using dictGetRandomKeys() and intsetRandomMembers().\n- Added tests for SPOP with <count>: unit/type/set, unit/scripting, integration/aof\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-06-09T18:45:13Z",
        "closed_at": "2014-08-25T08:28:20Z",
        "merged_at": null,
        "body": "redis cluster is leaking memory while connecting to nodes due to missing freeaddrinfo().\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2014-06-06T06:09:49Z",
        "closed_at": "2014-06-06T07:32:17Z",
        "merged_at": null,
        "body": "I think it is more suitable to change function name, replicationCacheMaster to replicationCachedMaster\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 29,
        "changed_files": 2,
        "created_at": "2014-06-01T13:45:20Z",
        "closed_at": "2014-06-06T08:37:57Z",
        "merged_at": "2014-06-06T08:37:57Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-05-31T18:25:42Z",
        "closed_at": "2014-06-06T08:33:11Z",
        "merged_at": "2014-06-06T08:33:11Z",
        "body": "fix the bug described in issue #1787\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 59,
        "deletions": 23,
        "changed_files": 1,
        "created_at": "2014-05-30T18:45:29Z",
        "closed_at": "2014-07-22T16:01:01Z",
        "merged_at": null,
        "body": "When attempting to union many keys, ZUNIONSTORE was taking 14s. With these modifications, it is down to ~200ms. However, I guarantee I don't understand how refcnts are used in Redis. The reason I need refcnts is because when aggregating, I use the destination ZSET as the accumulated aggregate. Then, because ZSETs sort on insert, I need to sort the destination. I did so by creating a new ZSET and inserting all the values/scores into it. The method described here is more cache friendly by iterating through each key, rather than scanning forward through all keys for the aggregate. It also visits each value once, rather than multiple times.\n",
        "comments": 33
    },
    {
        "merged": false,
        "additions": 159,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2014-05-28T19:38:27Z",
        "closed_at": "2014-06-27T10:15:27Z",
        "merged_at": null,
        "body": "Yesterday I wrote a quick note about adding cluster support to clients, but it raised some interesting issues.\n\nHere's an attempt at fixing some cluster implementation difficulties.\n\nFour new things:\n- Use the local bind IP (if available) instead of reporting `:0` for the `myself` IP\n- Provide a `CLUSTER SLOTS` command with Redis-formatted output of the relevant parts of `CLUSTER NODES`\n- Add `COMMANDS` so clients can discover key position information for each command, allowing better client-side key to slot mapping.\n- `PUBLISH` to `__cluster__:state` when the cluster state changes so clients can automatically re-fetch their node map without waiting for timeouts or failures.  Channel name was chosen arbitrarily.  Open to suggestions for proper naming.\n\nFor more details, see each individual commit message/essay.\n",
        "comments": 25
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-05-28T07:46:03Z",
        "closed_at": "2014-06-06T08:45:00Z",
        "merged_at": "2014-06-06T08:45:00Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 65,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2014-05-27T18:42:08Z",
        "closed_at": "2015-02-25T09:38:32Z",
        "merged_at": null,
        "body": "Let's help tag releases from clean checkout sources. \n\nNew scripts:\n- utils/lint.sh [branch]\n  - checks out clean repo from current dir\n  - builds\n  - runs tests (runtest, runtest-cluster, runtest-sentinel)\n  - aborts on _any_ error\n- utils/tag.sh [branch] [tag]\n  - runs lint.sh on branch\n  - if all tests pass, tags the branch\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-05-27T03:46:56Z",
        "closed_at": "2014-10-29T20:20:45Z",
        "merged_at": null,
        "body": "Just a small change to proctitle, showing which configuration file was used to start the server, or when it is using the default configuration.\nThe results will be like that\n\n```\n./redis-server *:6379 /tmp/redis.conf\n./redis-server *:6379 [default configuration]\n./redis-server *:6379 [cluster] /tmp/redis.conf\n./redis-server *:6379 [sentinel] [default configuration]\n```\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-05-23T17:57:53Z",
        "closed_at": "2014-05-26T15:56:58Z",
        "merged_at": "2014-05-26T15:56:58Z",
        "body": "Previously the PID format was:\n`[PID] Timestamp`\n\nBut it recently changed to:\n`PID:X Timestamp`\n\nThe tcl testing framework was grabbing the PID from `\\[\\d+\\]`, but\nthat's not valid anymore.\n\nNow we grab the pid from `\"PID: <PID>\"` in the part of Redis startup\noutput to the right of the ASCII logo.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2014-05-22T22:27:42Z",
        "closed_at": "2014-08-25T08:28:20Z",
        "merged_at": null,
        "body": "Ref #1773\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-05-22T18:04:07Z",
        "closed_at": "2014-06-06T09:19:21Z",
        "merged_at": "2014-06-06T09:19:21Z",
        "body": "If you run the --intrinsic-latency analyzer you will get a output like:\n\n```\n$ redis-cli -p 6380 --intrinsic-latency 60\nMax latency so far: 5 microseconds.\nMax latency so far: 9 microseconds.\nMax latency so far: 121 microseconds.\nMax latency so far: 122 microseconds.\n\n24586821 total runs (avg 2 microseconds per run).\nWorst run took 61.00x times the avarege.\n```\n\nIn this message the word \"avarege\" seems to got a typo. The correct word seems to be \"average\".\nThis PR will fix this.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-05-22T01:53:12Z",
        "closed_at": "2014-12-10T15:16:02Z",
        "merged_at": null,
        "body": "fixes https://github.com/antirez/redis/issues/1768\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 74,
        "deletions": 14,
        "changed_files": 3,
        "created_at": "2014-05-21T14:21:38Z",
        "closed_at": "2021-07-20T10:24:59Z",
        "merged_at": null,
        "body": "When several consecutive nodes are removed from the tail or head of a given list, the new algorithm reduces the overhead of removing nodes one by one and the intermediary operations (set new head/tail, update links...).\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2014-05-21T09:08:32Z",
        "closed_at": "2020-08-20T05:42:25Z",
        "merged_at": null,
        "body": "Hi,\n\nI have committed small changes for adding `key-hit` and `key-miss` notification.\n\nRedis has keyspace notification feature, but doesn't contain key hit (read) miss event. I think, this change can provide to track that on server side.\n\n```\nk : Key hit event. (events generated when a data of the key is read)\nm : Key miss events (events generated when a key is not exists in db)\n```\n\nI didn't put above 2 event into `A` alias, due to avoid side effects to current users. These probably flow many events enough.\n\nThanks.\nJinoos\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-05-19T20:22:40Z",
        "closed_at": "2014-05-20T14:14:34Z",
        "merged_at": "2014-05-20T14:14:34Z",
        "body": "When scanning the argument list inside of a redis.call() invocation\nfor pre-cached values, there was no check being done that the\nargument we were on was in fact within the bounds of the cache size.\n\nSo if a redis.call() command was ever executed with more than 32\narguments (current cache size #define setting) redis-server could\nsegfault.\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2014-05-18T14:32:43Z",
        "closed_at": "2014-05-19T13:39:30Z",
        "merged_at": "2014-05-19T13:39:30Z",
        "body": "...ions.\n\nSet the MSB as documented.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2014-05-16T07:43:31Z",
        "closed_at": "2021-10-04T16:54:22Z",
        "merged_at": null,
        "body": "This commit implements the PFADDXX command, which has the same\nsemantics as PFADD but will return an error if the key doesn't\nalready exist.\n\nReferences #1735\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2014-05-15T16:28:33Z",
        "closed_at": "2014-06-12T23:40:18Z",
        "merged_at": null,
        "body": "Linux uses port 0 to get this behavior.\nRedis already used port 0 to mean \"disable TCP\"\nSo this uses port -42 under the hood as a flag.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 140,
        "deletions": 81,
        "changed_files": 2,
        "created_at": "2014-05-15T09:20:56Z",
        "closed_at": "2015-12-02T21:02:20Z",
        "merged_at": null,
        "body": "As mentioned may be worth doing in #1725.\n\nThis does impose a larger performance penalty on scripting calls, though - a new environment is copied for each call, and that's fairly expensive. Are there any thoughts on this approach?\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 159,
        "deletions": 2,
        "changed_files": 7,
        "created_at": "2014-05-12T19:38:20Z",
        "closed_at": "2021-04-27T20:07:50Z",
        "merged_at": null,
        "body": "This is a first attempt at fixing issue #1704.\n\nSome security applications require the ability to disable all disk persistence.  Redis makes globally disabling persistence difficult since multiple places enable/disable persistence and persistence can always be manipulated by `CONFIG SET` during runtime.  This commit removes the ability to enable persistence after the server starts.\n\nNew configuration options (can be set at start only, no live changes):\n- `disable-replication` - removes `SYNC`, `PSYNC`, `SLAVEOF`, `MIGRATE`, `CLUSTER`, `REPLCONF` commands.\n  - Adds guards to replication functions making sure they can't be forced to run.\n- `disable-all-persistence` - removes `SAVE`, `BGSAVE`, `BGREWRITEAOF` commands.  Disables reading AOF or RDB on startup.  Disables `CONFIG REWRITE` so people can't try to change configuration options to hold data then save it to disk.  Disables `SHUTDOWN SAVE`.  Avoids spawning the background I/O threads.\n  - Adds guards to write functions making sure they can't be circumvented through other entry points.\n  - This config option has a long name and includes an annoying-to-type word like \"persistence\" to hopefully stop people from enabling it by mistake.\n\n`disable-all-persistence` automatically enables `disable-replication` because a.) we don't want replicas to persist our data and b.) replication requires serializing the DB to disk to send to the replica.\n\n`disable-replication` automatically sets `cluster-enabled` to `no`.\n\nIf you're on Linux, `disable-all-persistence` also attempts to lock all memory into RAM to stop anything from entering your swap space.  That can only happen if you either run as root or have explicit capability `CAP_IPC_LOCK`.\n\nThe implementation is very paranoid.  It disables commands then, in the commands themselves, each potential write command aborts if nopersist is set, then in the functions performing writes or network connections, the function immediately returns if nopersist (or noreplication) is set.\n\nThe reason for the multi-layered approach is: some operations (like saving an RDB) have multiple entry points (`SAVE`, `BGSAVE`, the `save` config option, `SHUTDOWN SAVE`, ...) and I wanted to make sure, even if we miss disabling an entry point, that write functions are still disabled.  Multi-layered blocks will make security audit people happier too.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2167,
        "deletions": 6,
        "changed_files": 17,
        "created_at": "2014-05-12T18:51:17Z",
        "closed_at": "2015-07-17T09:34:13Z",
        "merged_at": null,
        "body": "This commit integrates geo commands (https://matt.sh/redis-geo) into Redis.\n\nThe only major remaining issue I'm aware of is: we need to clean up my copy/pasted zset access functions in `zset.{c,h}` with a correct zset access API.\n\nOnce we get the zset interface fixed, we can squash\nrelevant commits in this branch and have one nice commit\nto merge into unstable.\n\nThis commit adds:\n- Geo commands\n- Tests; runnable with: ./runtest --single unit/geo\n- Geo helpers in deps/geohash-int/\n- src/geo.{c,h} and src/geojson.{c,h} implementing geo commands\n- Updated build configurations to get everything working\n- TEMPORARY: src/zset.{c,h} implementing zset score and zset\n  range reading without writing to client output buffers.\n- Modified linkage of one t_zset.c function for use in zset.c\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 137,
        "deletions": 172,
        "changed_files": 5,
        "created_at": "2014-05-12T17:50:38Z",
        "closed_at": "2015-01-28T16:30:27Z",
        "merged_at": "2015-01-28T16:30:27Z",
        "body": "This is a first attempt at automatically running `redis-check-rdb` when RDB fails to load.\n\nCurrently, when an RDB fails to load, Redis prints a stack trace telling the user to report the issue on GitHub, but we know the problem is corrupt data, not corrupt code.\n\nI've attempted a fix with some changes:\n- rename `redis-check-dump` to `redis-check-rdb` (matches `redis-check-aof` better now too)\n- convert `redis-check-rdb` into a mode of Redis (like how Sentinel is a mode of Redis)\n  - If RDB is defined in your config, you can run `redis-server redis.conf --check-rdb` and it will read the RDB defined in redis.conf\n  - This changes the usage of `redis-check-rdb` to no longer accept a filename though.  The filename must be passed with `--dbfilename <dbname>` if you are checking something not defined in your config (or something other than the default name of `dump.rdb`).\n  - Also, removes all redundancy from the old `redis-check-dump` that copy/pasted many Redis defines. Some of the copy/pasted defines didn't match the defines in Redis anymore, so wrong values were being compared.\n- with `redis-check-rdb` living inside Redis, now we can automatically run `redis-check-rdb` when the RDB fails to load (for duplicate keys or bad encodings).  The user will see immediate details about why their RDB can't load instead of just seeing a scary stack trace.\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2014-05-12T15:13:16Z",
        "closed_at": "2014-06-09T09:42:14Z",
        "merged_at": "2014-06-09T09:42:14Z",
        "body": "Included:\n- Fix very old issue #232\n- Fix lack of `strtold` on cygwin (as reported in various web pages about compiling Redis under cygwin)\n\nRedis can now compile under Cygwin without any end-user patches.\n\n(Also see everybody having the same problems at https://www.google.com/search?q=redis+cygwin+SA_ONSTACK)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2014-05-12T13:34:56Z",
        "closed_at": "2014-08-25T08:28:20Z",
        "merged_at": null,
        "body": "fix:  potential overflow in intset.c\n\nclean: mismatch comment and redundant code\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-05-12T09:20:50Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2014-05-09T09:10:04Z",
        "closed_at": "2014-05-12T13:35:26Z",
        "merged_at": null,
        "body": "Signed-off-by: Wei Jin wjin.cn@gmail.com\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-05-07T16:26:12Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "This was discovered by _bodya and reported in the IRC channel.\nEverything worked fine as these scripts are always executed as shell\nscripts.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 44,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2014-05-04T20:59:16Z",
        "closed_at": "2017-07-15T15:20:39Z",
        "merged_at": null,
        "body": "I have added the status and the restart option for the service, also i have fixed the problem that was with the config file because the port number was included in the file name.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-05-04T15:23:35Z",
        "closed_at": "2015-01-07T10:14:09Z",
        "merged_at": null,
        "body": "when saving rdb, save db dict size also.\nwhen loading rdb, load db dict size and expand db dict with this dict\nsize value.\nso avoid several db dict table expansion & rehash while loading datas.\nbecause of rdb file compatibility, added new rdb opcode RDB_OPCODE_DB_DICT_SIZE\n\ntest result\n\ndata count : 9,463,389   (with 'set' command only)\nrdb file size : 312,382,550 bytes\n\nbefore\naverage loading time : 9.924 seconds\ntable expansion count : 23\n\nafter\naverage loading time : 6.998 seconds.  (29.4% reduction)\ntable expansion count : 1\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-05-04T15:23:00Z",
        "closed_at": "2014-07-31T16:08:22Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-04-30T12:53:34Z",
        "closed_at": "2014-10-15T14:52:23Z",
        "merged_at": null,
        "body": "This fix adds authentication to the stop verb (if configured in the conf file). Without this, the server (i.e. my VM) won't shutdown as it keeps waiting for Redis to stop. \n\nAn alternative approach would be to sigterm the process but I didn't want to change the current logic too much.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 24,
        "changed_files": 1,
        "created_at": "2014-04-30T03:56:18Z",
        "closed_at": "2019-12-16T13:07:02Z",
        "merged_at": null,
        "body": "I think  `hex_digit_to_int` will be faster with 3 if statements and simple math, and saving one variable in `sdsjoin` may improve performance.  \n\nCorrect me if I am wrong.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2014-04-30T01:49:46Z",
        "closed_at": "2014-05-04T14:27:37Z",
        "merged_at": null,
        "body": "in order to to -> in order to\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-04-29T10:58:19Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 761,
        "deletions": 153,
        "changed_files": 24,
        "created_at": "2014-04-24T19:31:39Z",
        "closed_at": "2014-06-23T09:55:19Z",
        "merged_at": null,
        "body": "We've had reports of multiple deployments needing Sentinel to send\nfrom a non-default address (See #1667).\n\nNow users can specify their source address as their first 'bind' entry\nand it'll be used for all outbound Sentinel communication too.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-04-22T16:22:49Z",
        "closed_at": "2014-04-23T09:37:47Z",
        "merged_at": "2014-04-23T09:37:47Z",
        "body": "The 'cluster nodes' command shows wrong slot allocation if we have the following slot distribution:\nmaster-a: 0-16382\nmaster-b: 16383\nCurrent output:\nmaster-a: 0-16383\nmaster-b: 16383\n(both nodes showing 16383!)\n\nSee also: https://gist.github.com/kingsumos/11185353\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-04-22T14:51:22Z",
        "closed_at": "2014-04-23T09:55:50Z",
        "merged_at": "2014-04-23T09:55:50Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-04-21T18:59:39Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "According to manuals \"Connecting to the socket object requires\nread/write permission.\"\nhttp://man7.org/linux/man-pages/man7/unix.7.html\n\nPermissions 0755 are confusing.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-04-17T13:52:54Z",
        "closed_at": "2014-04-23T09:59:59Z",
        "merged_at": null,
        "body": "Added a command option to make a Slave become a Master: 'CLUSTER REPLICATE NONE'\nToday the only option to make a Slave become a Master is by using the 'CLUSTER FAILOVER' command, however a switch-over is triggered and the old Master becomes a Slave.\nThis is useful for changing the master/slave allocation without adding/removing nodes.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-04-16T15:57:51Z",
        "closed_at": "2014-08-01T17:16:07Z",
        "merged_at": null,
        "body": "Similar issue to https://github.com/antirez/redis/issues/1058\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 306,
        "deletions": 0,
        "changed_files": 8,
        "created_at": "2014-04-15T18:51:52Z",
        "closed_at": "2020-08-12T14:58:57Z",
        "merged_at": "2020-08-12T14:58:57Z",
        "body": "This is a Linux-specific feature, sorry, but it's still a useful one.\n\nWhen Linux runs out of memory the kernel's OOM killer chooses which processes are going to be killed based on various parameters.  By tuning the oom_score_adj parameter (in /proc), it's possible to hint the kernel what processes should be killed first.\n\nThe oom-score-adj configuration parameter allows Redis processes to tune their own oom_score_adj value depending on their current state/role and thus implement policies such as:\n- Kill bgsave/bgrewriteaof childs before anything else\n- Kill slaves before masters\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2014-04-15T18:16:59Z",
        "closed_at": "2014-04-16T12:48:25Z",
        "merged_at": null,
        "body": "There is an inconsistency in the way read-only scripts are replicated:\n- On EVAL, a script won't be replicated if it's read-only.\n- On EVALSHA it will always be replicated unless it's already in the replication cache (i.e. known to be replicated).\n\nIn the following sequence, the EVALSHA on the slave fails:\n\n```\nmaster> EVAL <some readonly script>\nslave> EVALSHA <sha1 of script>\n```\n\nIn the following sequence, it will succeed:\n\n```\nmaster> EVAL <some readonly script>\nmaster> EVALSHA <sha1 of script>\nslave> EVALSHA <sha1 of script>\n```\n\nThe proposed solution is to simply use SCRIPT LOAD on read-only EVAL if the script was not previously cached.\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 70,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2014-04-15T17:03:17Z",
        "closed_at": "2014-04-15T21:15:02Z",
        "merged_at": null,
        "body": "I know the issue of script replication was discussed a lot in the past, but this may be another point of view and possibly a significant improvement in a few cases.\n\nWe came across the following scenario:\n- One Redis master with multiple slaves (1-10), used for data availability and potentially load balancing of read-only operations.\n- Clients update data on master Redis every second.\n- LUA needs to perform potentially intensive aggregation of that data and produce summary records.\n\nWe want to optimize:\n- Avoid replicating the per-second data updates to all slaves, as we simply don't need them (nobody reads them from slaves, and they get re-written every second anyway so nothing to lose).\n- Avoid running the intensive LUA code on every slave, run it once in the master and just make the resulting aggregated records available to all slaves.\n\nIt seems like this can be a relatively common real-world optimization case, so I came up with a proposed mechanism for volatile scripts.  It has two parts:\n\nVEVAL and VEVALSHA commands which allow executing LUA scripts in Volatile Mode.  When scripts execute in this mode, they are not replicated to slaves as we assume nothing they do is worth replication.\n\nEssentially this introduces a potentially interesting concept of \"volatile keys\", but for now we don't mark them in any way and simply don't replicate these changes.\n\nWhen our script finally needs to create the aggregated record, it calls a new redis.nvcall() LUA function which is identical to call/pcall in all aspects but one - commands performed through it are replicated to slaves (as discrete Redis commands) if data was changed.\n",
        "comments": 16
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-04-15T11:17:00Z",
        "closed_at": "2014-04-16T12:48:56Z",
        "merged_at": null,
        "body": "testtesttesttest\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-04-14T22:46:59Z",
        "closed_at": "2014-04-23T13:58:27Z",
        "merged_at": null,
        "body": "Spent several hours researching this and finally got a great tip from TheRealBill_here on the irc channel.\n",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-04-13T21:45:06Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-04-12T18:23:47Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "This is a suggestion that does not make any major change in the code.\nBy accepting this, the code is more readable for future maintainers.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2014-04-10T21:12:21Z",
        "closed_at": "2014-04-23T14:13:49Z",
        "merged_at": "2014-04-23T14:13:49Z",
        "body": "Deleting an expired key should return 0, not success.\n\nFixes #1648\n\nWith active expiration, most keys on a lightly used server _will_ expire soon after their expiration time.  Only on loaded servers or servers with manually disabled active expiration (via debug) will the problem of successfully deleting keys that shouldn't exist show itself.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-04-09T10:10:49Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-04-08T19:40:34Z",
        "closed_at": "2014-04-23T10:07:46Z",
        "merged_at": null,
        "body": "I happen to be working on a system that lacks urandom. While the code does try\nto handle this case and artificially create some bytes if the file pointer is\nempty, it does try to close it unconditionally, leading to a segfault.\n\nCloses #1671\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 11,
        "changed_files": 4,
        "created_at": "2014-04-08T00:05:16Z",
        "closed_at": "2014-06-09T09:37:29Z",
        "merged_at": "2014-06-09T09:37:29Z",
        "body": "Josiah found an error condition: if you're blocked waiting\nfor a list to exist, but the list is created with a non-user\ncommand, the blocked client never gets notified.\n\nThis commit adds notification of blocked clients into\nthe DB layer and away from individual commands.\n\nLists can be created by [LR]PUSH, SORT..STORE, RENAME, MOVE,\nand RESTORE.  Previously, blocked client notifications were\nonly triggered by [LR]PUSH.  Your client would never get\nnotified if a list were created by SORT..STORE or RENAME or\na RESTORE, etc.\n\nBlocked client notification now happens in one unified place:\n- dbAdd() triggers notification when adding a list to the DB\n\nTwo new tests are added that fail prior to this commit.\n\nAll test pass.\n\nFixes #1668\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 29,
        "changed_files": 2,
        "created_at": "2014-04-06T14:21:25Z",
        "closed_at": "2014-06-01T13:30:15Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 427,
        "deletions": 4,
        "changed_files": 14,
        "created_at": "2014-04-06T02:39:33Z",
        "closed_at": "2017-07-07T14:51:37Z",
        "merged_at": null,
        "body": ":warning: Work in progress\n- Adds support for n-dimensional double arrays\n- Supports RDB persistence\n- Adds commands\n  - XSET KEY INDEX[ INDEX[ ...]] VALUE - sets values in subregion selected by index, creates matrix at KEY with dimensions based on INDEX if doesn't exist\n  - XGET KEY INDEX[ INDEX[ ...]] - returns subregion selected by INDEX, -1 for any dimension index means \"entire dimension\"\n  - XZEROS KEY SHAPE [SHAPE [...]] - creates a zero-valued nd array\n  - XONES KEY SHAPE [SHAPE [...]] - creates a one-valued nd array\n  - XEYE KEY SIZE - creates a 2d identity matrix of size SIZE\n- Would like to add basic linear algebra commands\n  - XSCALE matrixkey SCALAR [newmatrix]\n  - XDOT matrixkeya matrixkeyb [newmatrix]\n  - etc\u2026\n\nExamples:\n\n```\nredis 127.0.0.1:6379> xzeros mat 3 3\n1) \"2\"\n2) \"3\"\n3) \"3\"\nredis 127.0.0.1:6379> xget mat -1 -1\n 1) \"2\"\n 2) \"3\"\n 3) \"3\"\n 4) \"0\"\n 5) \"0\"\n 6) \"0\"\n 7) \"0\"\n 8) \"0\"\n 9) \"0\"\n10) \"0\"\n11) \"0\"\n12) \"0\"\nredis 127.0.0.1:6379> xset mat -1 0 1\n1) \"2\"\n2) \"3\"\n3) \"3\"\nredis 127.0.0.1:6379> xget mat -1 -1\n 1) \"2\"\n 2) \"3\"\n 3) \"3\"\n 4) \"1\"\n 5) \"0\"\n 6) \"0\"\n 7) \"1\"\n 8) \"0\"\n 9) \"0\"\n10) \"1\"\n11) \"0\"\n12) \"0\"\nredis 127.0.0.1:6379> xget mat -1 0\n1) \"1\"\n2) \"3\"\n3) \"1\"\n4) \"1\"\n5) \"1\"\nredis 127.0.0.1:6379> xget mat 0 -1\n1) \"1\"\n2) \"3\"\n3) \"1\"\n4) \"0\"\n5) \"0\"\n```\n\n```\nredis 127.0.0.1:6379> xeye identitymat 2 2\n1) \"2\"\n2) \"2\"\n3) \"2\"\nredis 127.0.0.1:6379> xget identitymat -1 -1\n1) \"2\"\n2) \"2\"\n3) \"2\"\n4) \"1\"\n5) \"0\"\n6) \"0\"\n7) \"1\"\n```\n\nCloses #370\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 1051,
        "deletions": 400,
        "changed_files": 9,
        "created_at": "2014-04-04T21:39:15Z",
        "closed_at": "2014-11-14T16:17:16Z",
        "merged_at": null,
        "body": "Today turned into Lua maintenance day.\n\nHere we have:\n- upgrade cjson to latest version\n- upgrade cmsgpack to latest version (also removes a build-time warning)\n- add tests for cjson and cmsgpack to verify they are properly installed and bound to global tables\n- fix a lingering Redis build issue on some versions of Solaris\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 212,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2014-04-04T15:58:38Z",
        "closed_at": "2014-11-14T16:05:28Z",
        "merged_at": "2014-11-14T16:05:28Z",
        "body": "A few people have written custom C commands because bit\nmanipulation isn't exposed through Lua.  Let's give\nthem Mike Pall's bitop.\n\nThis adds bitop 1.0.2 (2012-05-08) from http://bitop.luajit.org/\n\nbitop is imported as \"bit\" into the global namespace.\n\nNew Lua commands: bit.tobit, bit.tohex, bit.bnot, bit.band, bit.bor, bit.bxor,\nbit.lshift, bit.rshift, bit.arshift, bit.rol, bit.ror, bit.bswap\n\nVerification of working (the asserts would abort on error, so (nil) is correct):\n\n``` haskell\n127.0.0.1:6379> eval \"assert(bit.tobit(1) == 1); assert(bit.band(1) == 1); assert(bit.bxor(1,2) == 3); assert(bit.bor(1,2,4,8,16,32,64,128) == 255)\" 0\n(nil)\n127.0.0.1:6379> eval 'assert(0x7fffffff == 2147483647, \"broken hex literals\"); assert(0xffffffff == -1 or 0xffffffff == 2^32-1, \"broken hex literals\"); assert(tostring(-1) == \"-1\", \"broken tostring()\"); assert(tostring(0xffffffff) == \"-1\" or tostring(0xffffffff) == \"4294967295\", \"broken tostring()\")' 0\n(nil)\n```\n\nTests also integrated into the scripting tests and can be run with:\n./runtest --single unit/scripting\n\nTests are excerpted from `bittest.lua` included in the bitop distribution.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2014-04-04T09:06:34Z",
        "closed_at": "2020-07-29T12:26:37Z",
        "merged_at": null,
        "body": "LUA VM memory is not allocated by zmalloc() and thus not counted by Redis.\nWhen many scripts are used, this can become significant and affect the calculated fragmentation ratio.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2014-04-04T08:27:50Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-04-03T16:38:02Z",
        "closed_at": "2014-06-17T21:38:02Z",
        "merged_at": "2014-06-17T21:38:02Z",
        "body": "Minor addition to make `redis-cli --intrinsic-latency` 10% more user friendly.\n\nIf we run a long latency session and want to Ctrl-C out of it,\nit's nice to still get the summary output at the end.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2014-04-03T11:25:30Z",
        "closed_at": "2014-07-31T16:12:15Z",
        "merged_at": null,
        "body": "Unstable\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 30153,
        "deletions": 6970,
        "changed_files": 289,
        "created_at": "2014-04-03T11:21:45Z",
        "closed_at": "2014-07-31T16:12:36Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-04-02T02:29:26Z",
        "closed_at": "2014-08-25T21:25:05Z",
        "merged_at": null,
        "body": "It fixes #1645.\n\nActually it's more natural to throw CROSSSLOT if store parameter's key should stored to other node.\nBut I respect previous thing, \"deny sort parameters related to multi key\", and it's very easy to implement.\n\nPlease review and comment! Thanks!\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2014-04-02T00:03:14Z",
        "closed_at": "2014-04-02T06:12:27Z",
        "merged_at": "2014-04-02T06:12:27Z",
        "body": "Both the error messages and the keyspace notifications were still using the HLL prefix for the HyperLogLog commands, so this fixes that. \n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2014-03-30T21:18:46Z",
        "closed_at": "2014-07-31T16:13:18Z",
        "merged_at": null,
        "body": "Under some 64-bit Linux, 64-bit types are unsigned long.\nUnder 64-bit OS X and some other Linux, 64-bit types are unsigned long long.\n\nThis makes manually specifying format types impossible unless you\ncast or use system-provided specifiers.\n\nLet's use the system provided C99 format type specifier abstraction so\nthe system provides `%lu` or `%llu` depending on what works on that system.\n\nBefore this patch, `unstable` fails `gcc` and `clang` `-Werror` compiles: https://travis-ci.org/mattsta/redis/builds/21894346\n\nUsing this patch, the warnings go away and all tests pass now: https://travis-ci.org/mattsta/redis/builds/21896098\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-03-29T03:34:23Z",
        "closed_at": "2014-09-28T17:15:30Z",
        "merged_at": null,
        "body": "Fixes previous incorrect behavior:\n\n``` haskell\nredis-cli -n 9\nAUTH\n[operations act on DB 0 because SELECT didn't get re-run]\n```\n\nAlso fixes error where selecting an invalid DB (`-1`, `9999999`) showed up in the CLI (#1313)\n\nFixes antirez#1639 and #1313\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2014-03-27T02:35:15Z",
        "closed_at": "2014-09-28T17:12:43Z",
        "merged_at": null,
        "body": "When we CTRL-C from a foreground redis-server, it should shutdown cleanly instead of just exiting immediately.\n\nThe slightly obtuse switch statement in the signal handler preserves the original log output.\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-03-24T18:49:31Z",
        "closed_at": "2014-03-24T20:04:03Z",
        "merged_at": "2014-03-24T20:04:03Z",
        "body": "lookupKey() uses LRU_CLOCK(), so it seems object\ncreation should use LRU_CLOCK() too.\n\n(Just an update for consistency across usage.)\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 398,
        "deletions": 1,
        "changed_files": 8,
        "created_at": "2014-03-24T13:41:45Z",
        "closed_at": "2014-05-30T23:06:12Z",
        "merged_at": null,
        "body": "Hi Antirez,\n\nI implemented the ZLEX extension on redis-server, described [here](https://github.com/tw-bert/redis-zlex/wiki/ZLEX-redis-server-extension).\n\nZLEX is a read-only function that queries the lexicographical index of a Sorted Set.\nTCL unit tests are included.\n\nSupersedes the ZRANGEBYLEX [pending request](https://github.com/antirez/redis/issues/324)\nDevelopment of ZLEX started before seeing this pending request, hence the slight name change. However, shorter might be better. I'll change it if needed.\nZLEX also has some minor differences in functionality, compared to ZRANGEBYLEX. This is due to use case requirements by my company. I tried to put as less restrictions as possible on querying the lexicographical sorted set index (while keeping it sound).\n\nSome background info in this [SO thread](http://stackoverflow.com/questions/21311863/alphabetical-index-with-millions-of-rows-in-redis)\n\nKind regards, TW\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 20,
        "changed_files": 6,
        "created_at": "2014-03-23T07:43:16Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "According to the C standard, it is desirable to give the type 'void' to functions have no argument.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 15,
        "changed_files": 3,
        "created_at": "2014-03-23T00:08:55Z",
        "closed_at": "2014-03-25T07:42:39Z",
        "merged_at": "2014-03-25T07:42:39Z",
        "body": "This series of patches fixes:\n- two unsigned integer wraparounds\n- one infinite loop\n- one configuration parsing error\n- one automatic reconfiguration error edge case\n- error messages, hopefully making them more useful\n\nI'm unsure about one change: instead of reserving `server.maxclients+32` fds, I changed it to  `server.maxclients+REDIS_EVENTLOOP_FDSET_INCR` to equal how the server event loop is configured at https://github.com/antirez/redis/blob/unstable/src/redis.c#L1639\n\nThanks to @bs3g for spontaneously closing #1227 which made me look at #356 too.  Sorry I couldn't use the patches in #1227, but all of those fixes eventually got implemented in these commits.\n## New ulimit Error Message Output\n\n``` haskell\nmatt@ununoctium:~/repos/redis/src% ./redis-server\n[9326] 23 Mar 00:04:16.675 # Failed to set max file limit. You requested maxclients of 10000 but your 'ulimit -n' is set to 3. Please increase your 'ulimit -n' to at least 10128.\n\nmatt@ununoctium:~/repos/redis/src% ./redis-server \n[9719] 22 Mar 20:05:01.768 # Your current 'ulimit -n' of 32 is not enough for Redis to start. Please increase your open file limit to at least 10128. Exiting.\n\nmatt@ununoctium:~/repos/redis/src% ulimit -n 129\nmatt@ununoctium:~/repos/redis/src% ./redis-server \n[9991] 22 Mar 20:05:31.896 # You requested maxclients of 10000 requiring at least 10128 max file descriptors.\n[9991] 22 Mar 20:05:31.896 # Redis can't set maximum open files to 10128 because of OS error: Operation not permitted.\n[9991] 22 Mar 20:05:31.896 # Current maximum open files is 129. maxclients has been reduced to 1 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n\nmatt@ununoctium:~/repos/redis/src% ulimit -n 3000\nmatt@ununoctium:~/repos/redis/src% ./redis-server \n[10234] 22 Mar 20:05:58.653 # You requested maxclients of 10000 requiring at least 10128 max file descriptors.\n[10234] 22 Mar 20:05:58.653 # Redis can't set maximum open files to 10128 because of OS error: Operation not permitted.\n[10234] 22 Mar 20:05:58.653 # Current maximum open files is 3000. maxclients has been reduced to 2872 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n\numatt@ununoctium:~/repos/redis/src% ulimit -n 12000\nmatt@ununoctium:~/repos/redis/src% ./redis-server \n[starts normally]\n```\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-03-21T18:48:50Z",
        "closed_at": "2014-03-24T17:32:51Z",
        "merged_at": null,
        "body": "This got removed in 2e5c394 during a new feature addition.\n\nThe prior commit had \"break if masters.length == masters_count\"\nbut we are guaranteed to aready have that condition met since\notherwise we would haven't gotten this far.\n\nWithout this break statement, it's possible some masters may\nbe forgotten and have zero replicas while other masters have\nmore than their requested number of replicas.\n\nThanks to carlos for pointing out this regression at:\nhttps://groups.google.com/forum/#!topic/redis-db/_WVVqDw5B7c\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2014-03-21T17:25:06Z",
        "closed_at": "2014-03-24T17:27:21Z",
        "merged_at": null,
        "body": "This bug was introduced in 2e5c394f during a refactor.\n\nIt took me a while to understand what was going on with\nthe code, so I've refactored it further by:\n- Replacing boolean values with meaningful symbols\n- Replacing 'i' with a meaningful variable name\n- Adding the proper abort check\n- Factoring out now duplicated conditionals\n- Adding optional verbose logging (we're inside _four_\n  different looping constructs, so it takes a while to\n  figure out where all the moving parts are)\n- Updating comment for the section\n\nThis fixes a problem when the number of master instances\nequaled the number of replica instances.  Before, when\nthere were equal numbers of both, nodes_count would go to\nzero, but the while loop would spin in i < @replicas because\ni would never be updated (because the nodes_list of each ip\nwas length == 0, which triggered an endless loop of\nnext -> i = 0 -> 0 < 1? -> true -> next -> i = 0 ...)\n\nThanks to carlo who found this problem at:\nhttps://groups.google.com/forum/#!topic/redis-db/_WVVqDw5B7c\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2014-03-21T08:59:39Z",
        "closed_at": "2014-03-24T17:15:31Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2014-03-20T22:17:28Z",
        "closed_at": "2014-03-21T08:46:50Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 144,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2014-03-18T21:34:20Z",
        "closed_at": "2014-03-24T16:40:49Z",
        "merged_at": null,
        "body": "As much as I enjoy being a human continuous integration system, perhaps we can automate it for better and quicker coverage?\n\nBy adding this one tiny YAML file (yes, it's sad to pollute the repository with an ugly external configuration file, but that's just how it works), we can have six VMs launched and running Redis unit tests after every push.  Why six VMs?  {gcc,clang}{make test,make test-sentinel} + {gcc,clang}{make REDIS_CFLAGS=-Werror} = 6 VMs.\n\nFor example output, see https://travis-ci.org/mattsta/redis/builds/21050931 \u2014\u00a0you can see we currently have one failing test because of the `gcc -Werror` fixed in antirez/redis#1617\n\nTravis CI will run on each push to any branch.  Just sign up at https://travis-ci.org/, login with github credentials, then enable it for your Redis repository.  By default, it will also build and report on pull requests too.\n",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 43,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2014-03-18T19:53:20Z",
        "closed_at": "2014-03-24T08:09:31Z",
        "merged_at": null,
        "body": "This may be more of a philosophical difference than an objective argument, but should we assume `maxmemory` is primarily our dataset size or all Redis memory usage even for transient allocations (temporary buffers)?\n\nIf maxmemory is _all_ memory usage, then read-only clients can forcibly evict keys by growing large pipeline output buffers (large output buffer = large total memory usage = over maxmemory limit = evict keys).\n\nThe patches in this PR:\n- allow us to see the aggregate output buffer size in INFO output\n- ignore output buffer size when determining if we are over memory limits (just like we do with Replica buffers and AOF buffers).\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2014-03-18T19:43:38Z",
        "closed_at": "2014-03-24T17:33:46Z",
        "merged_at": null,
        "body": "GCC-4.9 warned about this, but clang didn't.\n\nThis commit fixes warning:\nsentinel.c: In function 'sentinelReceiveHelloMessages':\nsentinel.c:2156:43: warning: variable 'master' set but not used [-Wunused-but-set-variable]\n     sentinelRedisInstance *ri = c->data, *master;\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2014-03-18T11:47:25Z",
        "closed_at": "2014-08-25T20:39:35Z",
        "merged_at": null,
        "body": "I believe this change improves scripting.c evalGenericCommand readability.\n\nInstead of pushing the error handler before the function lookup (that needs to clean the stack if it fails), we can get the error handler just before the pcall, using lua_insert to move the error handler function to the proper position in the stack.\n\nBesides avoiding the lua_pop when lookup fails, I believe it's easier to understand why the error handler is now being pushed into the stack, since it is closer to pcall function call and it also explains the -2 pcall argument naturally.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-03-18T03:57:48Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "I think we should free the memory here.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-03-17T03:23:59Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "I thought the hint message was phrased strangely . If you would like the message to be phrased differently let me know.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 140,
        "deletions": 73,
        "changed_files": 3,
        "created_at": "2014-03-15T13:45:11Z",
        "closed_at": "2014-03-24T17:12:43Z",
        "merged_at": null,
        "body": "I finally found the time to fix most of the outstanding issues regarding the `install_server.sh` script.\n\nI used [bats](https://github.com/sstephenson/bats) and Vagrant to quickly test atleast some aspects of the script (on ubuntu, debian and centos). My test files can be found here: [gist](https://gist.github.com/badboy/6f81b2e0035c6dc8c110)\nThe `install_server.sh` script should now be as POSIX-compatible as I know.\n\nThe following pull requests were used as reference and thus thanks to all contributors:\n- https://github.com/antirez/redis/pull/960, thanks @chilts (install script name)\n- https://github.com/antirez/redis/pull/909, thanks @jimothyGator (missing space)\n- https://github.com/antirez/redis/pull/781, thanks @calebperkins (`id -u`)\n- https://github.com/antirez/redis/pull/724, thanks @quiver (missing space)\n- https://github.com/antirez/redis/pull/1498, thanks @jw2013 (missing spaces)\n- https://github.com/antirez/redis/pull/1486, thanks @alonisser (correct paths when not inside utils dir, fixes #1485 #1486)\n- https://github.com/antirez/redis/pull/1345, thanks @GoldenWings91 (missing space)\n- https://github.com/antirez/redis/pull/1148, thanks @nsabine (missing spaces)\n- https://github.com/antirez/redis/pull/1146, thanks @TrafeX (removed relative paths, enabled backslash escapes in echo)\n- https://github.com/antirez/redis/pull/1038, thanks @jumping (added double quotation marks)\n- https://github.com/antirez/redis/pull/1066, thanks @melvyn-sopacua (always include sysv init info and avoid ugly \"echo -e\")\n\nAll of the following issues can be closed as well, they reported bugs the above pull requests solved:\n- https://github.com/antirez/redis/issues/864\n- https://github.com/antirez/redis/issues/840\n- https://github.com/antirez/redis/issues/816\n- https://github.com/antirez/redis/issues/505\n- https://github.com/antirez/redis/issues/363\n- https://github.com/antirez/redis/issues/259\n- https://github.com/antirez/redis/issues/1539\n- https://github.com/antirez/redis/issues/1485\n- https://github.com/antirez/redis/issues/1399\n- https://github.com/antirez/redis/issues/1228\n- https://github.com/antirez/redis/issues/1099\n- https://github.com/antirez/redis/pull/1037\n\nThere is one pull request left: https://github.com/antirez/redis/pull/1224\nIt mostly changes the init script a little bit, I'm not sure if this is really needed here.\n",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 15,
        "changed_files": 3,
        "created_at": "2014-03-13T18:54:13Z",
        "closed_at": "2014-03-24T18:26:25Z",
        "merged_at": "2014-03-24T18:26:25Z",
        "body": "As reported in https://github.com/antirez/redis/issues/1604,\nRedis was exiting and emptying out datasets.  This commit\nwill greatly reduce such occurrences.\n\nIf you are running an old (non-3.0+) Redis, this patch should\napply cleanly to anything newer than Redis 2.1.2.\n\nPreviously, the (!fp) would only catch lack of free space\nunder OS X.  Linux waits to discover it can't write until\nit actually writes contents to disk.\n\n(fwrite() returns success even if the underlying file\nhas no free space to write into.  All the errors\nonly show up at flush/sync/close time.)\n\nFixes antirez/redis#1604\n",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-03-12T18:20:58Z",
        "closed_at": "2014-08-25T21:24:21Z",
        "merged_at": null,
        "body": "Fixes calculating the binary search pivot index calculation for large values, c.f. [Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken](http://googleresearch.blogspot.de/2006/06/extra-extra-read-all-about-it-nearly.html)\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 316,
        "deletions": 254,
        "changed_files": 33,
        "created_at": "2014-03-12T04:54:48Z",
        "closed_at": "2015-09-10T04:13:16Z",
        "merged_at": null,
        "body": "These are some enhancements I needed to support some developer needs at work (embedded Linux domain).  Feel free if interested.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-03-12T00:55:55Z",
        "closed_at": "2014-08-25T08:28:22Z",
        "merged_at": null,
        "body": "Fix two cluster config file related segfaults.  One assert-generated segfault can be ignored and just worked through.  The other now generates a proper error message instead of just crashing.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2014-03-11T19:16:33Z",
        "closed_at": "2016-05-25T16:20:12Z",
        "merged_at": null,
        "body": "I just used a similar approach as in the interactive repl, just reconnecting to the newly set host/port. \nI'm not sure about the \"Redirected ...\" message. I think it's ok to print this to stdout if cluster mode is enabled.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 73,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2014-03-11T18:23:05Z",
        "closed_at": "2014-04-24T14:09:11Z",
        "merged_at": null,
        "body": "This is a first attempt at fixing the ongoing problem of people\nlaunching multiple Redis Cluster instances in the same directory\nwith default options (which makes all instances use the same\n\"nodes.conf\" which breaks their cluster).\n\nThis commit uses a `flock` approach to the problem.  Another\nvalid solution would be to use an external lock file.  Another\nvalid pattern would be to obtain the lock in `clusterInit`\ninstead of where we read/create the config in `clusterLoadConfig`. \n\nDo we allow runtime moving of cluster-config-file?  If so,\nthis commit doesn't handle that case yet.  Actually, I just\nchecked, and the 'set' command doesn't work at all in\na cluster because:\n\n``` haskell\n127.0.0.1:37446> set cluster-config-file poo.conf\n(error) MOVED 13508 127.0.0.1:19029\n127.0.0.1:37446> debug cmdkeys set cluster-config-file poo.conf\n1) \"cluster-config-file\"\n```\n",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-03-11T14:23:45Z",
        "closed_at": "2014-03-12T12:30:48Z",
        "merged_at": null,
        "body": "The redis-rb client takes the additional info as an options hash, not as additional arguments.\n\nI'm using redis-rb v3.0.7, which takes only two arguments, see https://github.com/redis/redis-rb/blob/master/lib/redis.rb#L399\nI couldn't find the code that it ever took 5 arguments, so I think this code in redis-trib.rb should have never worked.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-03-11T13:36:35Z",
        "closed_at": "2017-07-15T15:21:52Z",
        "merged_at": null,
        "body": "Syntax correction of 2 lines of bash code.\n\nProblem: \n    The install_server.sh does not create a valid init script according to chkconfig on centos 6.5\n\nHow to reproduce/debug: \n    sudo bash -x ./install_server.sh\n\noutput:\nWelcome to the redis service installer\nThis script will help you easily set up a running redis server\nPlease select the redis port for this instance: [6379] \nSelecting default: 6379\nPlease select the redis config file name [/etc/redis/6379.conf] /etc/redis.conf\nSelected default - /etc/redis/6379.conf\nPlease select the redis log file name [/var/log/redis_6379.log] /var/log/redis/redis.conf\nSelected default - /var/log/redis_6379.log\nPlease select the data directory for this instance [/var/lib/redis/6379] /var/lib/redis\nSelected default - /var/lib/redis/6379\nwhich: no redis-server in (/sbin:/bin:/usr/sbin:/usr/bin)\nPlease select the redis executable path [] /usr/local/bin/redis-server\nwhich: no redis-cli in (/sbin:/bin:/usr/sbin:/usr/bin)\ns#^port [0-9]{4}$#port 6379#;s#^logfile .+$#logfile /var/log/redis_6379.log#;s#^dir .+$#dir /var/lib/redis/6379#;s#^pidfile .+$#pidfile /var/run/redis_6379.pid#;s#^daemonize no$#daemonize yes#;\nCopied /tmp/6379.conf => /etc/init.d/redis_6379\nInstalling service...\nservice redis_6379 does not support chkconfig\nservice redis_6379 does not support chkconfig\n exists, process is already running or crashed\nInstallation successful!\n\nProof of problem:\n      the chkconfig headers are not included in the init script.\n\nRoot cause:\n    Invalid syntax\n     if [ !`which chkconfig` ];\n    -bash: !`which: event not found\n\nSuggested fix:\n    change syntax to\n    if [ ! $(which chkconfig) ];\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-03-10T04:03:31Z",
        "closed_at": "2014-03-10T09:10:48Z",
        "merged_at": null,
        "body": "https://github.com/antirez/redis/issues/722\n\nThis fix proposes to make the `install` rule to depends on a new one,\nnamed `deps`, that builds first all dependencies.\n\nTested on Ubuntu 12.0.4.3 LTS.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-03-07T21:34:52Z",
        "closed_at": "2014-03-10T10:39:45Z",
        "merged_at": "2014-03-10T10:39:45Z",
        "body": "The previous implementation wasn't taking into account\nthe storage key in position 1 being a requirement (it\nwas only counting the source keys in positions 3 to N).\n\nFixes antirez/redis#1581\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-03-06T22:09:21Z",
        "closed_at": "2017-07-15T15:58:02Z",
        "merged_at": null,
        "body": "Fixes issue where update.rc was called every time (even on centos systems)\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-03-06T17:56:37Z",
        "closed_at": "2014-12-17T16:48:18Z",
        "merged_at": "2014-12-17T16:48:18Z",
        "body": "(Only update the error message this time, not the log level)\n\nIf we woke up to accept a connection, but we can't\naccept it, inform the user of the error going on\nwith their networking.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 15388,
        "deletions": 11134,
        "changed_files": 113,
        "created_at": "2014-03-05T21:43:52Z",
        "closed_at": "2017-06-23T16:10:15Z",
        "merged_at": null,
        "body": "This was already discussed before (#253), but since Lua 5.2 has been available for more than two years now, I believe it makes sense to update it.\n\nAll tests are passing and only a few modifications were needed. And it works: \\o/\n\n```\n127.0.0.1:6379> EVAL \"return _VERSION\" 0\n\"Lua 5.2\"\n```\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-03-04T23:41:09Z",
        "closed_at": "2014-03-06T16:40:04Z",
        "merged_at": "2014-03-06T16:40:04Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2014-03-04T04:37:45Z",
        "closed_at": "2014-03-06T17:14:36Z",
        "merged_at": "2014-03-06T17:14:36Z",
        "body": "refer to updateLRUClock's comment REDIS_LRU_CLOCK_MAX is 22 bits,but #define REDIS_LRU_CLOCK_MAX ((1<<21)-1) only 21 bits\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-02-28T22:57:11Z",
        "closed_at": "2014-03-03T10:16:51Z",
        "merged_at": "2014-03-03T10:16:51Z",
        "body": "This is a fix for antirez/redis#1570\n\nused_memory_peak only updates in serverCron every server.hz,\nbut Redis can use more memory and a user can request memory\nINFO before used_memory_peak gets updated in the next\ncron run.\n\nThis patch updates used_memory_peak to the current\nmemory usage if the current memory usage is higher\nthan the recorded used_memory_peak value.\n\n(And it only calls zmalloc_used_memory() once instead of\ntwice as it was doing before.)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-02-28T22:42:17Z",
        "closed_at": "2014-07-31T16:14:28Z",
        "merged_at": null,
        "body": "[Continuing the march of Tiny Non-Feature-Impacting Improvements.]\n\nccache helps when switching between many\nbranches.  It just caches the previously\nbuilt object and puts it in place on an\nunchanged re-compile.\n\nYou can even do the clever thing of having\na post-checkout hook to automatically re-build\nyour current directory (the build should happen\nin less than a second if you've previously\nbuild the branch you're switching to).\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 244,
        "deletions": 81,
        "changed_files": 1,
        "created_at": "2014-02-27T20:05:36Z",
        "closed_at": "2014-03-03T10:24:25Z",
        "merged_at": "2014-03-03T10:24:25Z",
        "body": "This commit reworks the redis-cli --bigkeys command to provide more\ninformation about our progress as well as output summary information\nwhen we're done.\n- We now show an approximate percentage completion as we go\n- Hiredis pipelining is used for TYPE and SIZE retreival\n- A summary of keyspace distribution and overall breakout at the end\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 20,
        "changed_files": 3,
        "created_at": "2014-02-26T23:18:51Z",
        "closed_at": "2014-03-06T16:58:23Z",
        "merged_at": null,
        "body": "Here are a few miscellaneous fixes/cleanup patches I came across while fixing other issues.\n\n(Whoops \u2014\u00a0I based this branch off my other branch also that also has PR now.  The duplicate shared commit will vanish when either of the PRs get applied.)\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2014-02-26T23:15:40Z",
        "closed_at": "2014-03-10T09:28:32Z",
        "merged_at": "2014-03-10T09:28:32Z",
        "body": "This patch allows multiple Redis Cluster instances\nto communicate when running on the same interface\nof the same host.\n\nThe first address given to Redis as a bind parameter\n(server.bindaddr[0]) gets used as the source IP\nfor cluster communication.\n\nIf no bind address is specified by the user, the\nbehavior remains unchanged (if the user doesn't\nspecify a bind address, server.bindaddr[0] is NULL\nand the corresponding bind-to-ip code will be skipped).\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 44,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2014-02-23T14:19:04Z",
        "closed_at": "2017-07-15T15:59:50Z",
        "merged_at": null,
        "body": "Systemd use a regular message on a socket to see if a process is alive,\nand restart it if needed. In order to achieve that, a timer is added to\nthe main loop to notify systemd that we are still alive. This doesn't\nadd any overhead on a non-systemd enabled system, and requires to be\nconfigured by the sysadmin, in the systemd unit file to be used.\nSee http://0pointer.de/blog/projects/watchdog.html for details.\n\nThis requires a version newer than 209, due to usage of sd_watchdog_enabled\nfunction, and require to pass HAVE_SYSTEMD=1 to the Makefile.\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-02-21T04:50:15Z",
        "closed_at": "2014-02-21T08:32:13Z",
        "merged_at": "2014-02-21T08:32:13Z",
        "body": "If you launch redis with `redis-server --sentinel` then\nin a ps, your output only says \"redis-server IP:Port\" \u2014 this\npatch changes the proc title to include [sentinel] or\n[cluster] depending on the current server mode:\ne.g.  \"redis-server IP:Port [sentinel]\"\n      \"redis-server IP:Port [cluster]\"\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2014-02-19T22:38:58Z",
        "closed_at": "2014-03-10T09:37:50Z",
        "merged_at": "2014-03-10T09:37:50Z",
        "body": "I spent a few hours thinking my system had gone crazy before diving into the code and finding the hard-coded cluster port offset.  (I was starting Redis on really high ports.)\n\nIdeally we should use randomly-assigned cluster ports (just pass 0 as the port on `listenToPort` for the OS to assign a free port).\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-02-18T14:30:23Z",
        "closed_at": "2014-07-31T16:15:55Z",
        "merged_at": null,
        "body": "hi @antirez.\n\nI found a bug when I read redis sentinel code.\nSituation:\n Sentinel has multiple ip, and bind only one of them.\n\nfor exmaple.\nSentinel has ip [192.168.1.13, 192.168.1.14]\nand it binds 192.168.1.13\n\nand it will broadcast its ip with Hello Message.\nBut at that time, we don't know what ip is connected with Redis Master.\nso it can send other ip 192.168.1.14 with Hello Message. like below.\n\n``` c\n192.168.1.14,26379,runid,0,mymaster,192.168.1.8,6379,0\n```\n\nso It caues connection failed.\n\n1] condition 1, A has multiple ip, B has just a ip\n -> A can connect B.\n -> But b can't\n\n2] condition 2, A has multiple ip, B has multiple ip\n -> A can't, B can't\n\nso it can cause some unexpectable situation.\nso sentinel should return bined ip in conf with Hello Message.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-02-10T04:21:20Z",
        "closed_at": "2014-02-23T16:47:28Z",
        "merged_at": "2014-02-23T16:47:28Z",
        "body": "Running SYNC or PSYNC in the cli returns the\nRedis replication stream as output, then upon encountering\nthe next command, redis-cli aborts because it's receiving\na replication stream instead of command output.\n\nTrying to read the replication stream from redis-cli isn't useful (as it breaks the CLI and doesn't feed you ongoing output anyway).\n\nIf you want to view/debug a live replication stream, you should do `cat <(echo sync) - | nc localhost 6379` instead.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-02-08T22:11:14Z",
        "closed_at": "2014-08-20T13:09:19Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-02-08T14:11:23Z",
        "closed_at": "2014-08-25T20:40:41Z",
        "merged_at": null,
        "body": "Because slave will send sync command to Master.\nand that time It will load RDB.\nso Slave doesn't need to load RDB when it start.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2014-02-08T07:20:04Z",
        "closed_at": "2020-12-13T20:37:27Z",
        "merged_at": null,
        "body": "RSS used by the process can be obtained from /proc/self/psinfo on\nSolaris and Illumos based systems.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2014-02-06T00:00:04Z",
        "closed_at": "2017-07-15T15:57:54Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2014-02-04T11:48:30Z",
        "closed_at": "2018-03-22T16:28:45Z",
        "merged_at": null,
        "body": "In scanGenericCommand, There are two buf values to just convert int to string.\n\nand rv and len also are just used for same purpose.\n\nso, We don't need to use them seperately. \n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-02-03T20:50:26Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1085,
        "deletions": 2,
        "changed_files": 8,
        "created_at": "2014-01-29T16:52:12Z",
        "closed_at": "2017-07-15T15:22:32Z",
        "merged_at": null,
        "body": "This is a patch to support Interval sets to Redis.  it adds 1 datatype and 4 new commands, RDB and AOF support.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2014-01-23T13:58:23Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2014-01-23T00:32:18Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "Using macro ZIP_DECODE_PREVLEN instead of zipEntry in ziplist.c to avoid unnecessary decoding.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2014-01-21T05:38:52Z",
        "closed_at": "2014-07-31T16:20:21Z",
        "merged_at": null,
        "body": "Fix two spell bugs about \"rerwite\" to \"rewrite\" in aof.c\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 8,
        "changed_files": 5,
        "created_at": "2014-01-17T04:04:15Z",
        "closed_at": "2014-08-28T15:22:30Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-01-16T20:11:42Z",
        "closed_at": "2014-04-11T16:16:41Z",
        "merged_at": null,
        "body": "dictAdd returns DICT_OK (value: 1), but there was an assert\nchecking for REDIS_OK (value: 0) instead.\n\nI'm not sure what was breaking because of this.  Is it just on a seldom-used code path?\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 17,
        "changed_files": 2,
        "created_at": "2014-01-15T12:16:52Z",
        "closed_at": "2014-10-06T09:27:12Z",
        "merged_at": null,
        "body": "In line 664 of cluster.c I'm assuming it's past tense of 're-add' and not 'read'.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2014-01-09T13:14:20Z",
        "closed_at": "2014-03-15T20:17:34Z",
        "merged_at": null,
        "body": "changed utils/install_server.sh to fix issue #840, #864 and #1099\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1310,
        "deletions": 1152,
        "changed_files": 61,
        "created_at": "2014-01-07T22:00:34Z",
        "closed_at": "2017-05-07T21:56:34Z",
        "merged_at": null,
        "body": "To make it compilable with a c++ compiler this is what I did:\n\nFixed all the casts from void\\* to T*\nRenamed variables using c++ keywords: new, throw, class\nAdded extern \"C\" where needed (lua lib headers)\nFixed some inconsistencies between signatures in headers file and .c implementation\n\nNow to compile it against g++ is enough to issue:\n\nCXX=1 make\n\nfirst benchmarks (posted on mailing list) are promising.\n\nRegards\nGaetano Mendola\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2014-01-07T02:27:48Z",
        "closed_at": "2021-06-30T15:23:47Z",
        "merged_at": null,
        "body": "I encountered redis instance hanged.\n\nMy application had been executing `RANDOMKEY` to slave instance continuously.\nSuddenly the instance did not response to any command using CPU 100%.\n\nI attached redis process and found the instance reiterated in `dbRandomKey` loop.\n\nIn fact, the `db` has only 40 keys and those are all expired.\nThe logic in `expireIfNeeded` did not delete any expired key in case of instance is slave\nand `dbRandomKey` tried to find other key again.\n\nI fixed it to return NULL if the instance is slave and the size of db and expired is equal.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2013-12-28T21:02:39Z",
        "closed_at": "2014-05-30T22:59:50Z",
        "merged_at": null,
        "body": "Fixing an issue with install_server.sh script failing if not run from inside the utils folder\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-12-27T14:02:03Z",
        "closed_at": "2014-01-28T13:10:14Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-12-23T04:38:44Z",
        "closed_at": "2015-01-27T09:13:50Z",
        "merged_at": null,
        "body": "According to context,the size should be 16 rather than 64\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-12-20T10:05:37Z",
        "closed_at": "2013-12-20T11:33:12Z",
        "merged_at": null,
        "body": "This is a fix for issue 1406 - https://github.com/antirez/redis/issues/1406 \n\nChecking if the server is slave and the command is read only, in that case allowing it to proceed with returning data\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2013-12-18T07:00:10Z",
        "closed_at": "2017-10-17T23:35:18Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2013-12-15T04:59:36Z",
        "closed_at": "2013-12-18T06:33:43Z",
        "merged_at": null,
        "body": "The 'querylen+2' in 'sdsrange(c->querybuf,querylen+2,-1)' is for the \\r\\n case.\nAnd it will be  'querylen+1' for \\n case.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-12-13T14:32:37Z",
        "closed_at": "2014-04-29T09:58:31Z",
        "merged_at": null,
        "body": "how to reproduce:\n1. config-file use relative path\n2. option 'dir' set to another path\n3. execute cmd CONFIG REWRITE\n4. report \"ERR Rewriting config file: No such file or directory\"\n\nthe code:\n\n``` c\nresetServerSaveParams();\nloadServerConfig(configfile,options); /* cwd is changed here */\nsdsfree(options);\n/* get the wrong absolute path */\nif (configfile) server.configfile = getAbsolutePath(configfile); \n```\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-12-12T08:34:44Z",
        "closed_at": "2013-12-12T10:23:45Z",
        "merged_at": "2013-12-12T10:23:45Z",
        "body": "No description given.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2013-12-10T02:49:08Z",
        "closed_at": "2013-12-12T08:35:13Z",
        "merged_at": null,
        "body": "It is impossible to run in this \"if (te->id > maxId)\", unless te->id exceeds the range of long long. Even if it exceeds and run in, it is buggy.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-12-09T18:23:42Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "We have an environment where GCC > 4.0.1 (4.1.2) but glibc < 2.6 (2.5) (I know, weird).  Thus linking would fail because even though HAVE_ATOMIC was set to true, there is no 'sync_add_and_fetch' .  The simple one-line change is to also require glibc 2.6.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2013-12-08T11:05:05Z",
        "closed_at": "2013-12-11T11:06:18Z",
        "merged_at": "2013-12-11T11:06:18Z",
        "body": "Currently redis simply ignores such lines (as if they were blank). For example:\n\nset \"\"\"test-key\"\"\" value\n\nWould just be ignored.\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 30801,
        "deletions": 15999,
        "changed_files": 252,
        "created_at": "2013-12-07T03:27:21Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "modified aeDeleteFileEvent() function:\n\n``` c\nvoid aeDeleteFileEvent(aeEventLoop *eventLoop, int fd, int mask)\n{\n    if (fd >= eventLoop->setsize) return;\n    aeFileEvent *fe = &eventLoop->events[fd];\n\n    if (fe->mask == AE_NONE) return;\n\n    aeApiDelEvent(eventLoop, fd, mask);\n\n    fe->mask = fe->mask & (~mask);\n    if (fd == eventLoop->maxfd && fe->mask == AE_NONE) {\n        /* Update the max fd */\n        int j;\n\n        for (j = eventLoop->maxfd-1; j >= 0; j--)\n            if (eventLoop->events[j].mask != AE_NONE) break;\n        eventLoop->maxfd = j;\n    }\n}\n```\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 19,
        "changed_files": 1,
        "created_at": "2013-12-05T20:44:45Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "A handful of small typo and grammar fixes in redis.conf.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-12-05T19:18:13Z",
        "closed_at": "2014-04-23T00:05:33Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2013-12-04T22:39:58Z",
        "closed_at": "2014-07-31T16:32:11Z",
        "merged_at": null,
        "body": "Compilation warnings:\n\n```\n    CC sentinel.o\nsentinel.c: In function \u2018sentinelRefreshInstanceInfo\u2019:\nsentinel.c:1640:9: warning: variable \u2018first_runid\u2019 set but not used [-Wunused-but-set-variable]\n     int first_runid = 0;    /* true if this is the first runid we receive. */\n         ^\nsentinel.c:1639:9: warning: variable \u2018runid_changed\u2019 set but not used [-Wunused-but-set-variable]\n     int runid_changed = 0;  /* true if runid changed. */\n         ^\nsentinel.c: In function \u2018sentinelFailoverSwitchToPromotedSlave\u2019:\nsentinel.c:3236:9: warning: variable \u2018old_master_port\u2019 set but not used [-Wunused-but-set-variable]\n     int old_master_port;\n         ^\n```\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2013-12-02T20:39:34Z",
        "closed_at": "2014-07-31T16:39:36Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-12-02T11:24:22Z",
        "closed_at": "2014-07-31T16:40:33Z",
        "merged_at": null,
        "body": "I saw this little mistake in some recent bug reports. :)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-11-30T22:28:40Z",
        "closed_at": "2016-07-29T09:30:19Z",
        "merged_at": null,
        "body": "the redis-cli history file (in linenoise) is created with the default OS umask value which makes it world readable in most systems and could potentially expose authentication credentials to other users.\n\nPS: I think AUTH commands shouldn't be logged in the history file at all\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 25,
        "changed_files": 4,
        "created_at": "2013-11-30T16:07:59Z",
        "closed_at": "2013-12-24T03:38:06Z",
        "merged_at": null,
        "body": "Those options will be thrown without this patch:\n  include, rename-command, min-slaves-to-write, min-slaves-max-lag,\nappendfilename.\n\nI found Redis throws \"appendfilename\" on config rewrite during playing Redis sentinel,\nthen I write a script to figure out other thrown options:\n\n```\n$ cat ~/check-redis-config.c.sh\nperl -ne 'print if /void loadServerConfigFromString/ .. /^}/' config.c | perl -lne 'print $1 if /^ {8}\\S.*strcasecmp.*,\\s*\"([^\"]+)/' | sort > loadServerConfigFromString.txt\n\nperl -ne 'print if /void configSetCommand/ .. /^}/' config.c | perl -lne 'print $1 if /^ {4}\\S.*strcasecmp.*,\\s*\"([^\"]+)/'  | sort > configSetCommand.txt\n\nperl -ne 'print if /void configGetCommand/ .. /^}/' config.c | perl -lne 'print $1 if /^ {4}(?:\\S*config_get_\\w+\\(|\\S.*stringmatch[^\"]+)\\s*\"([^\"]+)/' | sort > configGetCommand.txt\n\n{\nperl -ne 'print if /int rewriteConfig\\(/ .. /^}/' config.c | perl -lne 'print $1 if /^ {4}\\S*rewriteConfig[^\"]+\"([^\"]+)/'\nperl -ne 'print if /int rewriteConfig\\(/ .. /^}/' config.c | perl -lne 'print lc($1) if !/\"/ && /^ {4}(?:\\S.* )?rewriteConfig(\\S+)Option/'\n} | sort > rewriteConfig.txt\n```\n",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2013-11-30T06:46:56Z",
        "closed_at": "2013-12-12T10:30:11Z",
        "merged_at": "2013-12-12T10:30:11Z",
        "body": "checked with aspell, also reviewed all related pull requests in antirez/redis about redis.conf and sentinel.conf,  this pull request covers all of them.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-11-29T03:03:23Z",
        "closed_at": "2013-11-29T16:28:55Z",
        "merged_at": null,
        "body": null,
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-11-26T23:31:24Z",
        "closed_at": "2013-12-12T22:23:48Z",
        "merged_at": null,
        "body": "This line is described as mentioning a slave, when it's actually describing a\nmaster\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2013-11-26T12:10:54Z",
        "closed_at": "2013-11-26T14:15:25Z",
        "merged_at": "2013-11-26T14:15:25Z",
        "body": "New sentinel will create pub/sub link (`ri->pc`) for both master instance and slave instance, so we should check pub/sub link for both master instance and slave instance, not just master instance (which old sentinel does).\n\nAll testes passed after fix.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-11-20T06:19:20Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "Fixed a tiny typo in the comments of redis.h.\n\n@jbergstroem, I don't know how to remove the unintentional redis.conf in the original PR, so I removed that PR and  started this one.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 721,
        "changed_files": 2,
        "created_at": "2013-11-20T03:09:11Z",
        "closed_at": "2013-11-20T06:11:08Z",
        "merged_at": null,
        "body": "Hi, just corrected a tiny typo in the comment of redis.h.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-11-16T10:11:29Z",
        "closed_at": "2017-07-15T15:54:26Z",
        "merged_at": null,
        "body": "Show slave info in master log.\n\nbefore \n\n``` c\n[3878] 16 Nov 19:06:18.900 * Slave asks for synchronization\n```\n\nafter\n\n``` c\n[3878] 16 Nov 19:06:18.900 * Slave(127.0.0.1:6380) asks for synchronization\n```\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2013-11-14T16:14:54Z",
        "closed_at": "2018-03-22T16:29:06Z",
        "merged_at": null,
        "body": "Allowing sort command in read-only-slave(sorry. some of my mistake. I broke my former branch. so I recreate.)\n\n1] removing \"w\" attribute from sort Command\n2] and prohibit writing with store option in sortCommand when it is read only slave.\n\nbut. I might think this is not a good approach. because it is weak that someone add new option to new option.\n\nso. how about add some other command only to store it's result to list.(ex. sortAndStore)\n\n``` c\n127.0.0.1:6380> sort key desc\n1) \"20\"\n2) \"10\"\n3) \"5\"\n4) \"4\"\n5) \"3\"\n6) \"2\"\n7) \"1\"\n127.0.0.1:6380> sort key desc store k\n(error) READONLY You can't write against a read only slave.\n127.0.0.1:6380> \n```\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-11-14T15:33:17Z",
        "closed_at": "2014-08-25T20:42:42Z",
        "merged_at": null,
        "body": "There is no need to check \"filter\" first time.\nbecause filter is always 0.\n\nso !filter is always 1 in first if statement.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-11-08T23:38:57Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "Fix up a typo.\n\ncc @antirez\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 21,
        "changed_files": 6,
        "created_at": "2013-11-08T20:00:11Z",
        "closed_at": "2014-02-03T09:36:44Z",
        "merged_at": null,
        "body": "In high RPS environments, the default listen backlog is not sufficient, so\ngiving users the power to configure it is the right approach, especially\nsince it requires only minor modifications to the code.\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 55,
        "changed_files": 1,
        "created_at": "2013-11-08T06:37:21Z",
        "closed_at": "2013-11-25T10:10:32Z",
        "merged_at": "2013-11-25T10:10:32Z",
        "body": "D'oh. I forgot to backport to 2.8 branch. Cherry-picked #1365.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 13,
        "changed_files": 6,
        "created_at": "2013-11-06T21:42:57Z",
        "closed_at": "2014-08-01T19:02:22Z",
        "merged_at": null,
        "body": "In high RPS environments, the default listen backlog is not sufficient, so\ngiving users the power to configure it is the right approach, especially\nsince it requires only minor modifications to the code.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 38,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-11-02T01:22:54Z",
        "closed_at": "2014-07-11T23:13:28Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2013-10-29T21:55:21Z",
        "closed_at": "2014-10-02T17:53:53Z",
        "merged_at": null,
        "body": "This commit introduces the SETNXEX command. Since SETNX is typically\nused as a lock, it is nice to be able to specify the expiry of the lock\nat the time of creation. The key and ttl should not be reset if the key\nexists.\n\nSigned-off-by: Aaron Bedra aaron@aaronbedra.com\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-10-29T18:29:05Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 65,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2013-10-28T23:46:59Z",
        "closed_at": "2017-07-03T11:10:24Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 37,
        "changed_files": 11,
        "created_at": "2013-10-28T11:26:15Z",
        "closed_at": "2016-09-26T08:37:43Z",
        "merged_at": null,
        "body": "Hi Antirez,\n\nThe following is a set of patches to fix and enforce function prototypes, there is also a patch to remove dead code from src/dict.c:dictScan() . You will find the diff stat below.\n\nI've made these changes since I'm using redis in an internal project, and I'm enforcing some CFLAGS. These changes will also improve the code quality of redis.\n\nThanks for redis!\n\ndiff stat against my branch redis-fixes-28102013\n\ngit diff --stat origin/unstable..redis-fixes-28102013 \n deps/hiredis/sds.h     |  2 --\n src/cluster.c          |  2 +-\n src/config.c           |  2 +-\n src/db.c               |  2 +-\n src/dict.c             |  3 ---\n src/rand.c             |  8 +++++---\n src/rand.h             |  2 +-\n src/redis-check-dump.c | 14 +++++++-------\n src/redis-cli.c        | 20 ++++++++++----------\n src/redis.c            |  8 ++++----\n src/redis.h            |  8 ++++----\n 11 files changed, 34 insertions(+), 37 deletions(-)\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-10-26T18:28:22Z",
        "closed_at": "2014-07-31T16:44:28Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2013-10-26T05:25:47Z",
        "closed_at": "2014-05-30T22:59:53Z",
        "merged_at": null,
        "body": "Add space between ! and variable otherwise user input is always discarded.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-10-25T20:05:28Z",
        "closed_at": "2014-07-31T16:51:29Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2013-10-21T23:11:44Z",
        "closed_at": "2017-07-15T15:56:44Z",
        "merged_at": null,
        "body": "In short:\n\nCheck that all bytes are consumed out of the string by `stroll`.\n\nCheck that the empty string is not passed to `stroll`, just fail instead.\n\nIf you require any additional code formatting changes or tests to be written to accept this change, please let me know.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2013-10-21T21:53:57Z",
        "closed_at": "2021-06-30T12:18:49Z",
        "merged_at": null,
        "body": "Using accept4 on linux lets us skip the two calls to fcntl() to set a socket as non-blocking. This saves two system calls on linux. A corresponding issue was filed at #1332.\n\nA few other reasons to justify this change (as pointed by @moreaki):\n\nhttp://article.gmane.org/gmane.linux.kernel/758201\nhttp://lwn.net/Articles/281965/\nhttp://udrepper.livejournal.com/20407.html\nhttp://danwalsh.livejournal.com/53603.html\n\n Accept4 was in Linux 2.6.28 (Dec 2008) and support in GLIBC is available starting with version 2.10 (May 2009). We could also add support for older GLIBC versions if the maintainers think it would be useful.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-10-21T19:12:25Z",
        "closed_at": "2014-07-31T16:46:12Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-10-20T23:00:18Z",
        "closed_at": "2014-07-31T16:47:21Z",
        "merged_at": null,
        "body": "Just a minor removal of a stray line of whitespace, but every line counts.\n\ncc @antirez\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-10-15T11:48:38Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "Hi! \n\nthis is a couple of small fixes that i found during code review. please take a look. \n\nThanks.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-10-14T17:53:28Z",
        "closed_at": "2014-07-31T16:52:05Z",
        "merged_at": null,
        "body": "I spotted two typos in the comments of the redis.conf which I fixed in this commit\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-10-14T15:56:27Z",
        "closed_at": "2013-10-15T11:22:49Z",
        "merged_at": null,
        "body": "Added feature to free the \"slaves\".\n\nwhen called clusterNodeResetSlaves function before call clusterNodeAddSlaves function,\nif the \"slaves\" was free and do not set \"NULL\", will see error when called zrealloc().\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-10-14T15:41:48Z",
        "closed_at": "2013-10-15T11:22:42Z",
        "merged_at": null,
        "body": "Added feature to check whatever the \"fd\" is -1.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-10-14T15:20:19Z",
        "closed_at": "2013-10-15T11:22:35Z",
        "merged_at": null,
        "body": "Added feature to free memory in clusterLoadConfig function.\n(memory is \"line\")\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-10-14T15:06:14Z",
        "closed_at": "2013-10-15T11:21:54Z",
        "merged_at": null,
        "body": "Added feature to close config-file in clusterSaveConfig function.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2013-10-14T07:57:48Z",
        "closed_at": "2013-10-14T13:57:13Z",
        "merged_at": null,
        "body": "Added feature to memory free in clusterLoadConfig() function.\n\n(char\\* type's variable. line)\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2013-10-14T07:38:03Z",
        "closed_at": "2013-10-14T13:58:48Z",
        "merged_at": null,
        "body": "Add feature to closing config-file in clusterSaveConfig() function.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-10-12T18:20:16Z",
        "closed_at": "2014-06-30T04:16:39Z",
        "merged_at": null,
        "body": "When redis is started from php/apache it inherits all open file descriptors of apache/php, which creates a problem when stopping apache - the FDs, including connections to apache ports (80, 443,...),  remain in use by redis and apache can't get restarted without stopping redis.\nThis patch closes all open file descriptors inherited from any process when going into daemon mode.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-10-10T11:58:47Z",
        "closed_at": "2014-08-25T21:26:58Z",
        "merged_at": null,
        "body": "Select command with negative integer as a input makes redis-cli to\nrefresh its prompt wrongly.\n\nFor example:\n    [root@anup-centos-6 src]# ./redis-cli\n    127.0.0.1:6379> select -1\n    (error) ERR invalid DB index\n    127.0.0.1:6379[-1]>\n\nThe patch fixes this problem.\n\nSigned-off-by: Anup Shendkar anupshendkar@gmail.com\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-10-01T01:37:32Z",
        "closed_at": "2020-09-05T21:02:12Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2013-09-30T13:40:33Z",
        "closed_at": "2013-10-01T01:32:21Z",
        "merged_at": null,
        "body": "The return type of `sdsrange` now is void, hence the tests are not applicable anymore.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2013-09-27T12:49:44Z",
        "closed_at": "2014-08-25T20:42:14Z",
        "merged_at": null,
        "body": "The total length should be divided by 'keystep'.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 71,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2013-09-19T03:34:20Z",
        "closed_at": "2017-07-15T15:55:11Z",
        "merged_at": null,
        "body": "A mostly-useless patch that colorizes the Redis ASCII logo.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1369,
        "deletions": 630,
        "changed_files": 23,
        "created_at": "2013-09-15T15:40:07Z",
        "closed_at": "2013-11-14T16:11:37Z",
        "merged_at": null,
        "body": "Allowing sort command in read-only-slave\n\n1] removing \"w\" attribute from sort Command\n2] and prohibit writing with store option in sortCommand when it is read only slave.\n\nbut. I might think this is not a good approach. because it is weak that someone add new option to new option.\n\nso. how about add some other command only to store it's result to list.(ex. sortAndStore)\n\n```\n127.0.0.1:6380> sort key desc\n1) \"20\"\n2) \"10\"\n3) \"5\"\n4) \"4\"\n5) \"3\"\n6) \"2\"\n7) \"1\"\n127.0.0.1:6380> sort key desc store k\n(error) READONLY You can't write against a read only slave.\n127.0.0.1:6380> \n```\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-09-13T02:30:56Z",
        "closed_at": "2014-08-01T19:16:08Z",
        "merged_at": null,
        "body": "cc @antirez \n\nSmall fix, and hopefully no one has to see this debug message anyway.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 40,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2013-09-11T23:34:46Z",
        "closed_at": "2013-09-12T04:30:31Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2013-08-28T05:37:07Z",
        "closed_at": "2013-09-03T13:16:10Z",
        "merged_at": null,
        "body": "A mistype fixed.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2013-08-21T08:30:59Z",
        "closed_at": "2020-07-29T12:26:23Z",
        "merged_at": null,
        "body": "Apparently there are quite situations in the test suite where different connections race each other.  In a regular \"make test\" it probably never shows up, but once we leverage (portions of) the test suite and use remote servers things come up.\n\nNot sure it's such a high priority to fix but here are few recent fixes we had to do and some clarifications:\nIn pubsub.tcl:\n1. Consecutive pubsub tests using the same channels may fail, as the socket disconnection of a previous test may not be complete and thus the server may still count it as a live subscription.\n2. UNSUBSCRIBE on a deferring client races and may lose to commands that execute on the main connection.\n\nIn list.tcl:\nThere is an inherent race between the DEL, BRPOPLPUSH and LRANGE.  Unfortunately, there's no easy way to tell BRPOPLPUSH was received (unless we look at INFO - too complex), but the PING in the beginning reduces the chances significantly as we can at least be sure the 3 connections are established when the race starts.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2013-08-14T22:39:46Z",
        "closed_at": "2014-09-03T19:50:50Z",
        "merged_at": null,
        "body": "Add a configuration option for slaves to refuse syncing to a master if there is not enough data in the master. Based on 2.6, would likely need changes for 2.8/unstable with the partial-sync changes.\n\nie:\nrepl-min-transfer-size 1g\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-08-14T18:14:47Z",
        "closed_at": "2014-07-31T16:52:47Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-08-13T09:53:33Z",
        "closed_at": "2013-08-27T13:15:32Z",
        "merged_at": "2013-08-27T13:15:32Z",
        "body": "The bug is described here: https://github.com/antirez/lua-cmsgpack/issues/24\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-08-13T06:51:43Z",
        "closed_at": "2014-10-06T07:45:20Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-08-13T06:28:52Z",
        "closed_at": "2014-10-06T09:26:55Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2013-08-08T22:47:27Z",
        "closed_at": "2014-08-06T16:31:13Z",
        "merged_at": null,
        "body": "With upstart, the server will not shutdown if bind does not include 127.0.0.1. See https://groups.google.com/forum/#!topic/redis-db/DYLxoMbNzlQ (zero replies as of now).\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 122,
        "deletions": 55,
        "changed_files": 3,
        "created_at": "2013-08-06T00:48:31Z",
        "closed_at": "2016-06-15T10:50:02Z",
        "merged_at": null,
        "body": "Following [this thread](https://groups.google.com/forum/#!topic/redis-db/vhxibwvWHaQ) I set out to implement variadic LINSERT.\n\nI have started by separating the implementation of LINSERT from that of RPUSHX and LPUSHX since they were not so similar in the first place and would need to become even more different. Then I noticed making RPUSHX and LPUSHX variadic (like RPUSH / LPUSH) as well would be easy so I did that.\n\nI am rather confident in those first commits, the most challenging one is [the last one](https://github.com/catwell/redis/commit/65f2cc49e340bcc9c7a55f661ba9839e40475ea8): actually making LINSERT variadic.\n\nI had not expected that list iterators would break when used with ziplists (it is impossible to re-use an iterator after insertion). That means (unless I have missed something) that sometimes I need to iterate a ziplist up to the pivot as many times as there are elements to be inserted. I decided to try as hard as possible to avoid this case by converting to a regular list early if I could. This is why that code can look a bit complex and should probably be reviewed with attention...\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2013-07-30T17:28:30Z",
        "closed_at": "2014-03-22T20:40:11Z",
        "merged_at": null,
        "body": "## Introduction\n\nThis is my attempt to fix issue #356.\n## Tests\n\nUnfortunately, I have no idea how to set limit on fd through TCL script. I did look it up on Google, but haven't found any useful link. Anyway, I did manual testing -- \n\nrunning: `ulimit -n 1024 && ./redis-server`; expecting to get\n\n```\nUnable to set the max number of files limit to 10032 (Operation not permitted), \nsetting the max clients configuration to 912.\n```\n\nrunning: `ulimit -n 32 && ./redis-server`; expecting to get\n\n```\nUnable to set the max number of files limit to 10032 (Operation not permitted), \nsetting the max clients configuration to 0.\n```\n\nrunning: `ulimit -n 16 && ./redis-server`; expecting to exit Redis with this message\n\n```\nUnable to set the max number of files limit to the smallest files number (32) for\nrunning Redis (Operation not permitted)\n```\n## Notes\n- Feel free to share your thoughts.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2013-07-28T17:26:55Z",
        "closed_at": "2013-07-29T12:44:42Z",
        "merged_at": null,
        "body": "## Introduction\n\nI got a machine with `valgrind-3.6.0.SVN-Debian`. Unfortunately, `Valgrind-3.6.0` doesn't have the `show-possibly-lost` flag causing the unit-test to exit with the following error message:\n\n```\nERROR:valgrind: Bad option: --show-possibly-lost=no\nvalgrind: Use --help for more information or consult the user manual.\n```\n## Notes\n- I've assumed that any version older than 3.6.0 doesn't support the `show-possibly-lost` flag.\n- I've added a simple `attention` message in case the machine has old Valgrind version.\n- Feel free to share your thoughts.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 114,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2013-07-27T23:42:39Z",
        "closed_at": "2014-03-22T20:40:00Z",
        "merged_at": null,
        "body": "## Introduction\n\nMy patch to this feature request https://github.com/antirez/redis/issues/1044\n\n```\nSyntax: ZSUMSCORE key min max\n```\n\nWhat it does is computing the sum of scores of members, which their scores are between `min` and `max` values, in the sorted set `key`.\n\nTherefore, to compute the sum of scores of all the members in a specific sorted set, run:\n\n```\nZSUMSCORE sorted_set_name -inf +inf\n```\n## Notes\n- I have had the notion of adding a new variable, which holds the sum, to the skip-list & zip-list structures. By doing so, I probably achieve better performance (complexity O(1)). However, I guess that it is harder to implement and to test.\n- When running the `ZSUMSCORE double` stress test at the unit test, I get inaccurate results. Nevertheless, after replicating it manually, it seems fine. Even though, please note that when the score is with a floating point, the sum is accurate up to the 15th number after the point (or at least that is what I get when running the unit test).\n- Would like to hear you comments/ideas.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-07-24T22:05:10Z",
        "closed_at": "2021-04-27T08:55:30Z",
        "merged_at": null,
        "body": "On FreeBSD calling gettimeofday() causes all the cores on a multicore\nsystem to be synchronized. On a heavily loaded system with a\nsignificantly CPU bound application this can cause a 40% overall\nsystem degradation.\nA better option is to use clock_gettime() and pass in the\nCLOCK_REALTIME_FAST clock as the clock to use. This achieves the\nsame behavior as gettimeofday() on Linux.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2013-07-24T18:08:29Z",
        "closed_at": "2014-08-01T19:24:33Z",
        "merged_at": null,
        "body": "in sds.c\n\nmain function has invalid tests for sdsrange.\n\nso I fixed it to be able to build.\n\nThis is not important. but it might need for consistency for usage of sdsrange\n\n``` c\n1 - Create a string and obtain the length: PASSED\n2 - Create a string with specified length: PASSED\n3 - Strings concatenation: PASSED\n4 - sdscpy() against an originally longer string: PASSED\n5 - sdscpy() against an originally shorter string: PASSED\n6 - sdscatprintf() seems working in the base case: PASSED\n7 - sdstrim() correctly trims characters: PASSED\n8 - sdsrange(...,1,1): PASSED\n9 - sdsrange(...,1,-1): PASSED\n10 - sdsrange(...,-2,-1): PASSED\n11 - sdsrange(...,2,1): PASSED\n12 - sdsrange(...,1,100): PASSED\n13 - sdsrange(...,100,100): PASSED\n14 - sdscmp(foo,foa): PASSED\n15 - sdscmp(bar,bar): PASSED\n16 - sdscmp(bar,bar): PASSED\n17 - sdsnew() free/len buffers: PASSED\n18 - sdsMakeRoomFor(): PASSED\n19 - sdsIncrLen() -- content: PASSED\n20 - sdsIncrLen() -- len: PASSED\n21 - sdsIncrLen() -- free: PASSED\n21 tests, 21 passed, 0 failed\n```\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2013-07-24T14:14:33Z",
        "closed_at": "2013-08-21T09:18:42Z",
        "merged_at": "2013-08-21T09:18:42Z",
        "body": "fixed issue #1213  which says that when you start the server without assigned IP, it will failed if you do not have ipv6, and only have ipv4.  \n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2013-07-22T03:25:42Z",
        "closed_at": "2017-07-15T15:55:23Z",
        "merged_at": null,
        "body": "In commit 81e55ec0, the last return statement:\n\n   return bothsds ? sdscmp(astr,bstr) : strcmp(astr,bstr);\n\nwas deleted, which result in the variable 'bothsds' to be useless.\n\nThis patch clean up the useless variable and correct the compiler\nwarning.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-07-19T21:21:22Z",
        "closed_at": "2013-07-22T09:00:03Z",
        "merged_at": null,
        "body": "In 81e55ec0 the code here changed and now bothsds is not needed anymore.\n\nFixes the warning:\n\n```\n\u001bobject.c: In function \u2018compareStringObjectsWithFlags\u2019:\nobject.c:350:9: warning: variable \u2018bothsds\u2019 set but not used [-Wunused-but-set-variable]\n     int bothsds = 1;\n         ^\n```\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2013-07-18T07:21:55Z",
        "closed_at": "2013-07-27T05:08:40Z",
        "merged_at": null,
        "body": "Add an optional sampling probability parameter to MONITOR that determines the likelihood that an issued command will be received by the client.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-07-15T17:41:36Z",
        "closed_at": "2014-03-13T21:05:06Z",
        "merged_at": null,
        "body": "Currently the header guard incorrectly checks the variable _ZIMMAP_H instead of _ZIPMAP_H. This pull request fixes that.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2013-07-15T07:45:49Z",
        "closed_at": "2013-09-12T10:46:23Z",
        "merged_at": null,
        "body": "fixed an error of IP validating in \"cluster meet\" command\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-07-14T18:53:58Z",
        "closed_at": "2013-07-17T19:27:15Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-07-11T15:51:09Z",
        "closed_at": "2013-07-12T08:55:10Z",
        "merged_at": "2013-07-12T08:55:10Z",
        "body": "There are atleast two more places where `config.hostip` is printed (an error message and in `cliReadReply`). Should I fix them aswell or just the []-wrapping be a function?\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-07-11T11:46:44Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "Found by The Mayhem Team (Alexandre Rebert, Thanassis Avgerinos,\nSang Kil Cha, David Brumley, Manuel Egele) Cylab, Carnegie Mellon\nUniversity. See http://bugs.debian.org/716259 for more.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-07-11T06:20:23Z",
        "closed_at": "2014-08-25T20:44:20Z",
        "merged_at": null,
        "body": "@tnm tried to run KEYS against a slave of the GitHub production redis database with redis-cli today and we ran into a segfault in redis-cli after an sds buffer overflowed 2GB len. Seems reasonable to make it unsigned since the size should never be negative?\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-07-11T04:14:41Z",
        "closed_at": "2014-08-25T08:28:22Z",
        "merged_at": null,
        "body": "Hat tip to @rfuchs. See: https://github.com/redis/hiredis/pull/178.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-07-10T19:43:24Z",
        "closed_at": "2014-08-25T20:41:26Z",
        "merged_at": null,
        "body": "added publish command to benchmark utility\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 285,
        "deletions": 30,
        "changed_files": 9,
        "created_at": "2013-07-07T05:40:03Z",
        "closed_at": "2022-02-06T05:02:45Z",
        "merged_at": null,
        "body": "I can't believe I haven't submitted this before...\n\nThis patch implements a pair of commands to make RDB backups easier, without chewing up IOPS on the system unnecessarily.  Both commands emit an RDB file over a socket, rather than writing them to disk.  The differences are:\n- `PIPESAVE` invokes a system command (as specified by the new config variable `pipesavecommand`) and writes the RDB to that command's `stdin`.  Why would you want this?  Well, you can implement a pipeline to compress, encrypt, and send-to-S3 an RDB file, all without it touching disk.  This can, for example, reduce the time to do a backup from 3 hours to 15 minutes.\n- `DUMPSAVE` writes the RDB file straight back at the client who called the command.  The client should be grabbing that data and doing something with it (writing it to disk would be a good recommendation).  The purpose of this command is to allow a backup server to request an RDB file, without needing to either (a) become a slave (with the associated memory usage), or (b) Have the RDB written to disk locally.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-07-06T19:11:09Z",
        "closed_at": "2021-06-17T11:59:29Z",
        "merged_at": null,
        "body": "Hi,\n\nThe LSB tags are missing on redis init script. It's usefull to build daemons that need redis to be started before.\n\nOutput on debian :\n\n```\n  $ sudo update-rc.d redis defaults\n  insserv: warning: script 'redis' missing LSB tags and overrides\n```\n\nYou can find more infos here :\nhttp://wiki.debian.org/LSBInitScripts\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 9,
        "changed_files": 4,
        "created_at": "2013-07-04T19:56:08Z",
        "closed_at": "2014-03-23T20:40:48Z",
        "merged_at": null,
        "body": "Added `ziplistDeleteRangeFromOffset` allowing to remove more than one entry from ziplist starting from a given entry in the ziplist.\nChanged `ziplistDelete` to return result from `ziplistDeleteRangeFromOffset` call. \nIn addition, replaced every `ziplistDelete` double-call to a single `ziplistDeleteRangeFromOffset` call.\n\nThese changes were made due to a \"TODO\" request in the code.\n\nTests - I have run `make test` to approve that my changes haven't caused new problems.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2013-06-21T16:13:21Z",
        "closed_at": "2013-08-16T13:43:00Z",
        "merged_at": null,
        "body": "ZISMEMBER for zset are same as SISMEMBER for set: it determine if a given value is a member of a zset.\n\nFormat: `ZISMEMBER <key> <member>`\n\nReturn values:\n- If `key` not exists, return `0` .\n- If `key` exists, and `member` are `key`'s member, return `1` , else return `0` .\n\nTime complexity: O(1)\n\nExample:\n\n```\nredis 127.0.0.1:6379> FLUSHDB\nOK\nredis 127.0.0.1:6379> ZISMEMBER number pi\n(integer) 0\nredis 127.0.0.1:6379> ZADD number 3.14 pi\n(integer) 1\nredis 127.0.0.1:6379> ZISMEMBER number pi\n(integer) 1\nredis 127.0.0.1:6379> ZISMEMBER number e\n(integer) 0\n```\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2013-06-19T14:19:32Z",
        "closed_at": "2013-07-04T17:02:15Z",
        "merged_at": null,
        "body": "in https://github.com/antirez/redis/commit/37c29e037b08159e901227f0184973442bb86c2d#commitcomment-3458011\n\nit breaks sentinel INFO parsing routine in sentinelRefreshInstanceInfo.\nso I fixed it.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 179,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2013-06-15T09:23:00Z",
        "closed_at": "2021-03-18T14:51:15Z",
        "merged_at": null,
        "body": "I added a `BITPOS` operation to Redis, which accepts `[start end limit]` arguments. As the name suggests, it operates on bit strings and returns the positions (offsets) of all the set bits. My C is a little rusty, so it might need a little review/improvements.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2013-06-15T03:06:14Z",
        "closed_at": "2021-10-14T08:50:01Z",
        "merged_at": null,
        "body": "1] matching_name will be \"uninitialized value\" when value != eunm_val in rewriteConfigEnumOption\n2] name will be \"uninitialized value\" when validSyslogFacilities[j].value != value in rewriteConfigSyslogfacilityOption\n3] and removing compile error in clang, and current\n\nin linux.\n\n``` c\nconfig.c: In function \u2018rewriteConfigEnumOption\u2019:\nconfig.c:1352:10: warning: \u2018matching_name\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nconfig.c: In function \u2018rewriteConfigSyslogfacilityOption\u2019:\nconfig.c:1369:10: warning: \u2018name\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n```\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-06-15T02:41:29Z",
        "closed_at": "2018-03-22T16:21:14Z",
        "merged_at": null,
        "body": "Considering IPv6(because redis will support IPv6 also)\n\nIPv6 string address can represent to 45 bytes.\n\nref: http://stackoverflow.com/questions/166132/maximum-length-of-the-textual-representation-of-an-ipv6-address\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2013-06-14T11:32:36Z",
        "closed_at": "2015-02-12T16:31:07Z",
        "merged_at": null,
        "body": "Redis master recognizes slave's IP address by getpeername(2) so that address becomes an address assigned slave's eth0 even if IP aliased (eth0:1) in slave linux box and `bind <IP address assigned eth0:1)` in redis.conf.\n\nThis is not a promblem in simple master/slave replication except that `redis-clit -h MASTER info | grep slave` shows wrong slave's IP address.\n\nNow I introduced Redis Sentinel. Sentinel gets slaves address from result of `info` command to master, so sentinel recognizes not slave's eth0:1 but slave's eth0 as slave's IP address.\n\nSentinel's health check to slave will fail because slave only listen eth0:1. If redis master crashed, sentinel cannot failover because there is no healthy slave.\n\nTo fix it, slave calls bind(2) before connect(2) to master if there is `bind <address>` in redis.conf and that address is not local address (127.0.0.0/8).\n\nCould you give me any advice?\n\n```\nRedis master:\n  IP address:\n    eth0:   10.0.0.100\n\nRedis slave:\n  IP address:\n    eth0:   10.0.0.200\n    eth0:1: 10.0.0.201\n  redis.conf:\n    bind 10.0.0.201\n\nRedis sentinel:\n  IP address:\n    eth0:   10.0.0.50\n  sentinel.conf:\n    sentinel monitor mymaster 10.0.0.100 6379 1\n\n$ redis-cli -h 10.0.0.201 slaveof 10.0.0.100 6379\n\n$ redis-cli -h 10.0.0.100 info | grep slave\nslave0:10.0.0.200,6379,online  # not 10.0.0.201\n\n$ redis-cli -h 10.0.0.50 sentinel slaves mymaster | grep -A1 ip\nip\n10.0.0.200  # not 10.0.0.201\n\n```\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2013-06-07T16:34:18Z",
        "closed_at": "2016-01-21T14:49:06Z",
        "merged_at": null,
        "body": "So we can limit the maximum count of keys returned.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2013-06-07T16:14:22Z",
        "closed_at": "2014-05-30T22:59:55Z",
        "merged_at": null,
        "body": "Shell ! operator requires a space.  In the current script, all defaults are accepted automatically because the \"if\" statements silently error.\n\nRedHat Enterprise 6.4\n\n```\nsh-4.1$ which chkconfig\n/sbin/chkconfig\n```\n\nexample: \n\n```\nsh-4.1$  if [ !`which chkconfig` ]; then echo \"won't get here\"; fi\nsh: !`which: event not found\n```\n\ncorrected: \n\n```\nsh-4.1$  if [ ! `which chkconfig` ]; then echo \"not found\"; else echo \"found\"; fi\nfound\n```\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 20,
        "changed_files": 2,
        "created_at": "2013-06-04T09:34:45Z",
        "closed_at": "2014-05-30T22:59:58Z",
        "merged_at": null,
        "body": "This patch removes the relative paths, making it possible to run the install_server.sh script from every location without first have to step into the utils folder.\nI've also enabled backslash escapes for the echo statement used to write the init.d scripts on systems that have 'chkconfig' (e.g. Redhat).\nWithout this the init.d script failed.\n\nThe other changes are the remove of unnecessary whitespaces.\n\nTested on Redhat and Ubuntu.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2013-06-02T01:44:10Z",
        "closed_at": "2014-01-09T13:15:45Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2013-05-28T03:06:59Z",
        "closed_at": "2015-05-29T10:41:46Z",
        "merged_at": null,
        "body": "New Feature : ZADDNX\n\n1] spec\n- if data exists in ZSET, just skip.\n- if data doesn't exists in ZSET, add data into ZSET\n\n2] Usage\n\n``` c\nredis 127.0.0.1:6379> zaddnx mykey 1 one 2 two 3 three\n(integer) 3\nredis 127.0.0.1:6379> zaddnx mykey 3 one\n(integer) 0\nredis 127.0.0.1:6379> zaddnx mykey 2 one 4 four\n(integer) 1\n```\n",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 87,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2013-05-25T16:40:37Z",
        "closed_at": "2013-06-26T15:22:04Z",
        "merged_at": null,
        "body": "currently, sentinel doesn't have \"config rewrite\" feature.\n\nso I add \"config rewrite\" feature for sentinel.\n\nonly add \"config rewrite\" for compatibility with redis-server.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-05-25T10:25:48Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "removing useless LINE_BUFLEN definition in redis-cli.c\n\nit causes misunderstanding about changing redis-cli max line size.\n\nso. I might think it is better removing that line.\n\n(actually, I modified linenoise's LINENOISE_MAX_LINE.)\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-05-22T23:25:49Z",
        "closed_at": "2013-06-17T10:14:31Z",
        "merged_at": "2013-06-17T10:14:31Z",
        "body": "Okay. Sorry for spamming up the issues a little, I haven't contributed anything on github for a long time. This pull request will attempt to find the source of bad calls to the Redis Lua API, and should address issue #1121.\n\nFor instance, here is an example of it correctly reporting the source line of an error.\n\n> redis 127.0.0.1:6379> eval \"\\n\\n\\n\\nredis.call(\\\"del\\\")\" 0\n> (error) ERR Error running script (call to f_02e4b6c5f49751ef9096706ae82901bb51bf59f2): @user_script: 5: Wrong number of args calling Redis command From Lua script \n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-05-20T16:42:53Z",
        "closed_at": "2014-07-31T16:58:14Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 30,
        "changed_files": 6,
        "created_at": "2013-05-17T08:27:14Z",
        "closed_at": "2013-06-26T13:17:03Z",
        "merged_at": "2013-06-26T13:17:03Z",
        "body": "rebased version of https://github.com/antirez/redis/pull/735\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 47,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2013-05-15T23:31:44Z",
        "closed_at": "2013-05-16T13:14:06Z",
        "merged_at": null,
        "body": "",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2013-05-15T15:00:44Z",
        "closed_at": "2017-07-15T15:55:41Z",
        "merged_at": null,
        "body": "1] remove compile warning in clang\n2] add check condition when value is not in valid range.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2013-05-14T15:18:05Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "remove duplicated definition.\n\naddReply\naddReplySds\n\nand to move acceptHandler forward to sort.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-05-12T10:08:26Z",
        "closed_at": "2014-07-31T17:00:36Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2013-05-11T10:37:33Z",
        "closed_at": "2013-06-27T10:20:11Z",
        "merged_at": null,
        "body": "issue #1098: allowing shutdown nosave while server status is server.loading\n\n``` c\n1] allow shutdown command to pass to check server.loading\n2] and if it is not \"nosave\", then return \"server.loading error\"\n3] if argv is larger than 2, return \"syntax error\"\n```\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-05-09T14:34:15Z",
        "closed_at": "2014-08-25T08:28:22Z",
        "merged_at": null,
        "body": "Adding auth to redis-benchmark\n\n``` c\n-a <password>      Password for Redis Auth\n\nsrc/redis-benchmark -t set -n 1000000 -r 100000000 -a redis\n```\n\nI might think some people want this kind of functionality.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2013-05-07T12:06:40Z",
        "closed_at": "2013-05-08T07:49:33Z",
        "merged_at": "2013-05-08T07:49:33Z",
        "body": "...12 MB,\n\nbitcount commant may return negtive integer with string length more than 256 MB\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-05-07T07:00:07Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "For some Solaris flavours, the inet_aton in in resolv library. Unlink this library will introduce link error.\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2013-05-04T17:29:30Z",
        "closed_at": "2017-07-15T15:52:12Z",
        "merged_at": null,
        "body": "Changing some lines spaces - very trivial\n\n``` c\n         {\n                freshnodes--; /* otherwise we may loop forever. */\n                continue;\n         }\n```\n\n``` c\n         {\n            freshnodes--; /* otherwise we may loop forever. */\n            continue;\n         }\n```\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-04-30T03:13:54Z",
        "closed_at": "2013-05-02T14:33:10Z",
        "merged_at": null,
        "body": "``` c\nWhen key has expiretime which is smaller than now(), it just add \"*3\\r\\n$3\\r\\nSET\\r\\n\" and\nnot to add \"PEXPIREAT\"\n```\n\nThis patch is based on REDIS-DB group's topic: https://groups.google.com/forum/?fromgroups=#!topic/redis-db/Kvh2FAGK4Uk\n\nand I think it is a bug. Thank you.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2013-04-30T00:53:36Z",
        "closed_at": "2015-03-10T11:50:37Z",
        "merged_at": null,
        "body": "Follow conventions established in a6619562. This allows us to override Lua's linker (for instance while cross compiling)\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 64,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2013-04-27T16:11:06Z",
        "closed_at": "2016-09-05T15:22:13Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-04-25T01:09:00Z",
        "closed_at": "2021-05-20T08:21:53Z",
        "merged_at": null,
        "body": "For #issue-1069\n\nDisplaying aof and rdb absolute path in INFO Command\n\n``` c\n# Persistence\nloading:0\nrdb_path:/Users/charsyam/works/private/redis-orig/dump.rdb\nrdb_changes_since_last_save:0\nrdb_bgsave_in_progress:0\nrdb_last_save_time:1366851882\nrdb_last_bgsave_status:ok\nrdb_last_bgsave_time_sec:-1\nrdb_current_bgsave_time_sec:-1\naof_enabled:0\naof_path:/Users/charsyam/works/private/redis-orig/appendonly.aof\naof_rewrite_in_progress:0\naof_rewrite_scheduled:0\naof_last_rewrite_time_sec:-1\naof_current_rewrite_time_sec:-1\naof_last_bgrewrite_status:ok\n```\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 64,
        "deletions": 44,
        "changed_files": 2,
        "created_at": "2013-04-23T18:09:01Z",
        "closed_at": "2014-05-30T23:00:03Z",
        "merged_at": null,
        "body": "Fix various scripting errors, relating to [ !$string ] always evaluating\nto false.\nAlso fix an echo statement that missed the -e flag, so that \\n\ncharacters ended up in the file.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2013-04-19T05:04:41Z",
        "closed_at": "2013-04-19T07:55:36Z",
        "merged_at": null,
        "body": "the output \"Pinging node %40s\" may contains unprintable character.\n\nI just fix it like server.runid or server.repl_master_runid\nI think this may be better :-)\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-04-18T23:40:57Z",
        "closed_at": "2021-10-14T13:41:32Z",
        "merged_at": null,
        "body": "Hi, \n\nSpecifying a host name as opposed to an IP address in redis.conf, throws an invalid bind address error. This patch provides a simple mechanism (using tools already found in anet.c) to allow host names. \n\nIt would be probably better to use getaddrinfo, since gethostbyname is deprecated, but that would mean to rewrite a big part of anet.c.\n\nCheers,\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-04-18T03:41:31Z",
        "closed_at": "2013-12-25T11:33:02Z",
        "merged_at": null,
        "body": "In original code, the socket doesn't been closed after anetNonBlock failed.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2013-04-15T12:57:12Z",
        "closed_at": "2014-04-12T18:51:31Z",
        "merged_at": null,
        "body": "When compiling on Ubuntu 12.10 (GCC version 4.7.2 (Ubuntu/Linaro 4.7.2-2ubuntu1)) , getting the following warnings:\n\n``` gcc\nCC rdb.o\nrdb.c: In function \u2018rdbSaveBackground\u2019:\nrdb.c:742:21: warning: format \u2018%lu\u2019 expects argument of type \u2018long unsigned int\u2019, but argument 3 has type \u2018size_t\u2019 [-Wformat]\n```\n\n``` gcc\nCC aof.o\naof.c: In function \u2018rewriteAppendOnlyFileBackground\u2019:\naof.c:972:21: warning: format \u2018%lu\u2019 expects argument of type \u2018long unsigned int\u2019, but argument 3 has type \u2018size_t\u2019 [-Wformat]\n```\n\nIn order to fix it -- I have done a cast in the mentioned places (it has seemed to me the correct way to fix it).\n\n**EDIT** Although, in ANSI C, you do not have to declare a function prototype, it might cause some undefined behavior.\nIn `redis.h` we declare `prepareForShutdown` function without arguments, however, in `redis.c` (lines 954 and 1880) and `db.c` (line 365). we call it with one argument (e.g. `prepareShutdown(0)`).\nUnless we have some backward compatibility with very old code, I don't see a reason to keep it that way.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-04-11T23:43:52Z",
        "closed_at": "2013-06-01T11:45:36Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 779,
        "deletions": 262,
        "changed_files": 42,
        "created_at": "2013-04-11T18:47:24Z",
        "closed_at": "2013-06-15T08:57:30Z",
        "merged_at": null,
        "body": "I added a `BITPOS` operation to Redis, which accepts `[start end limit]` arguments. As the name suggests, it operates on bit strings and returns the positions (offsets) of all the set bits. My C is a little rusty, so it might need a little review/improvements.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-04-08T07:30:27Z",
        "closed_at": "2014-05-30T23:00:01Z",
        "merged_at": null,
        "body": "added the double quotation marks for the value of\u00a0PIDFILE\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-04-08T07:17:56Z",
        "closed_at": "2014-05-30T22:58:26Z",
        "merged_at": null,
        "body": "missing a blank.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2013-03-29T04:22:27Z",
        "closed_at": "2013-03-29T09:39:27Z",
        "merged_at": null,
        "body": "1] Problem: extend set options are not case unsensitive.\n- example)\n\n``` c\nredis 127.0.0.1:6379> set abc 100 nx \nOK\nredis 127.0.0.1:6379> set abc 100 Nx\n(error) ERR syntax error\n```\n\nredis Commands are case unsensitive, so I think redis command's options should be\ncase unsensitive.\n\n2]Patch: made them case unsensitive \n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-03-26T03:58:10Z",
        "closed_at": "2013-03-26T07:56:46Z",
        "merged_at": null,
        "body": "1] Reason: even though multi-exec has no command. server.dirty increases and this causes calling rdbSaveBackground()\n\n2] Content: this patch just return when c->mstate.count == 0 after some initialization. \n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 48,
        "deletions": 42,
        "changed_files": 3,
        "created_at": "2013-03-19T23:34:01Z",
        "closed_at": "2013-04-02T09:24:53Z",
        "merged_at": "2013-04-02T09:24:53Z",
        "body": "Here's a couple of minor changes to the build system. Besides spaces, tabs and nits it provides two benefits:\n- Avoid build warnings (-pthread, -rdynamic)\n- Pass CC to Lua build (CC=clang would still invoke gcc for lua)\n- use `install` instead of `cp -pf` where applicable\n\nPossibly adding this as well:\n- [ ] Fix silent \"make install\" output (currently says INSTALL install, since $@ is target)\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-03-19T11:26:40Z",
        "closed_at": "2014-07-31T17:03:16Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-03-18T23:39:21Z",
        "closed_at": "2014-05-30T23:39:12Z",
        "merged_at": null,
        "body": "Typo\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-03-18T22:18:28Z",
        "closed_at": "2021-03-14T18:11:09Z",
        "merged_at": null,
        "body": "This fixes issue #651 \"redis.call doesn't throw lua error when passed nil arguments\". I had some trouble debugging a script which came down to me inadvertently passing a boolean to a redis.call. I was confused as to why I was getting a table back from an HGET call. I understand if this is intentional, but it would be nice for the documentation to be updated in that case to be more clear.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-03-14T05:53:48Z",
        "closed_at": "2013-03-25T18:30:59Z",
        "merged_at": "2013-03-25T18:30:59Z",
        "body": "Fix bug in configGetCommand function: get correct masterauth value.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2013-03-12T23:38:15Z",
        "closed_at": "2013-03-13T11:13:37Z",
        "merged_at": null,
        "body": "when cluster_enabled is 1, redis just uses only 1 db(db idx = 0)\n\nso we don't need to set dbnum = 16.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-03-12T01:11:07Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "A simple Makefile fix to help fit redis into cross-compilation environments.  Thanks for your consideration.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2013-03-09T08:54:39Z",
        "closed_at": "2013-03-10T12:29:39Z",
        "merged_at": null,
        "body": "1. The function setKey() call dictFind() 3 times\u00a0before\u00a0actually write db->dict !! \n2. No need to call expireIfNeeded(), if the key is in db->expires, its value will be overwrite and it will be removed from db->expires eventually.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2013-03-09T07:33:59Z",
        "closed_at": "2013-03-09T08:49:43Z",
        "merged_at": null,
        "body": "1. The function setKey() call dictFind() 3 times before actually write db->dict !! \n2. No need to call expireIfNeeded(), if the key is in db->expires, its value will be overwrite and it will be removed from db->expires eventually.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2013-03-07T12:52:16Z",
        "closed_at": "2013-03-08T23:10:58Z",
        "merged_at": null,
        "body": "pqsort.c: replace the simple swap method (when swaptype is 0) with the XOR algorithm to save some memory, since it's used a lot (long temp variable).\nI know it's based on the NetBSD implementation, but why not optimize a little more?\n\nInspired by looking at #968 and pqsort.c\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-03-04T16:49:13Z",
        "closed_at": "2013-03-06T15:51:12Z",
        "merged_at": null,
        "body": "no need to \"return\"\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 3,
        "changed_files": 5,
        "created_at": "2013-03-02T09:23:12Z",
        "closed_at": "2013-03-04T09:49:54Z",
        "merged_at": null,
        "body": "point 2 of slave-serve-stale-data miss '-' between 'stale' and 'data'\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 377,
        "created_at": "2013-02-27T10:11:27Z",
        "closed_at": "2014-07-31T17:02:20Z",
        "merged_at": null,
        "body": "1),add a check for aeCreateTimeEvent in func initServer\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 239,
        "created_at": "2013-02-27T10:09:09Z",
        "closed_at": "2014-07-31T17:02:29Z",
        "merged_at": null,
        "body": "1),add a check for aeCreateTimeEvent in func initServer\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 239,
        "created_at": "2013-02-27T09:36:44Z",
        "closed_at": "2014-07-31T17:02:36Z",
        "merged_at": null,
        "body": "1),add a check for aeCreateTimeEvent  in func initServer\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2013-02-27T03:55:27Z",
        "closed_at": "2013-03-25T18:40:49Z",
        "merged_at": null,
        "body": "1),fix core dump of void spt_init(int argc, char *argv[], char *envp[]) **attribute**((constructor));\n setproctitle.c\n2),add a conf option \"proctitle\" in redis.conf\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-02-24T17:22:29Z",
        "closed_at": "2013-03-04T09:57:01Z",
        "merged_at": "2013-03-04T09:57:01Z",
        "body": "I had local changes and I use vimdiff as an external diff program. When I issued make it just hang in vimdiff. The switch suppresses external configured programs.  \n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-02-24T08:00:00Z",
        "closed_at": "2017-07-03T11:25:39Z",
        "merged_at": null,
        "body": "hi, antirez,\nIn src/redis.h, line 1151, has the code:\n\nvoid oom(const char *msg);\n\nbut I can't find its definition in redis' source code,\nso, I remove this useless function declaration.\n\nthanks\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-02-24T02:30:22Z",
        "closed_at": "2018-03-22T16:23:38Z",
        "merged_at": null,
        "body": "Don't need to call aeDeleteFileEvent()  twice.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-02-21T09:29:31Z",
        "closed_at": "2013-03-04T10:20:31Z",
        "merged_at": null,
        "body": "add NULL check code to clusterLoadConfig()\n\nThere are two cases \n1] zmalloc can return NULL\n2] sdssplitargs can return NULL\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-02-20T20:17:06Z",
        "closed_at": "2014-05-30T22:59:21Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2013-02-18T12:58:34Z",
        "closed_at": "2017-07-15T15:25:51Z",
        "merged_at": null,
        "body": "Patch for issue #954\n\nChanges slaveofCommand to set flag REDIS_CLOSE_AFTER_REPLY instead of freeing client\nimmediately when client is the replication master. Allows master to send 'SLAVEOF NO ONE'\nwithout slave crashing with a segfault.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 36,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2013-02-18T04:18:50Z",
        "closed_at": "2014-08-25T08:28:21Z",
        "merged_at": null,
        "body": "refactoring making flags string to add flags easily.\n\nand if flags == 0, it doesn't need to add \",\"\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-02-16T09:43:57Z",
        "closed_at": "2013-07-03T10:24:54Z",
        "merged_at": null,
        "body": "redis-cli (with the --pipe option) now exits with an error message instead of hanging forever. This fixes #681.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 62,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2013-02-14T20:50:44Z",
        "closed_at": "2013-02-23T09:47:40Z",
        "merged_at": null,
        "body": "It has (at least) two uglies that I'm unsure if it's worth finding the right lipstick,\nstatic sizes for the username and path buffers since it's brain damaging to\nfind those out reliably.\n\nThere is another PR for this: #803\n\nThe two main problems there is that wordexp() isn't very portable (even if POSIX)\ne.g. it isn't available in OpenBSD (yet, I've seen diffs float by though) and as far\nas I can tell it does not interact with sudo nicely.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 55,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2013-02-08T18:11:15Z",
        "closed_at": "2013-02-12T09:30:39Z",
        "merged_at": "2013-02-12T09:30:39Z",
        "body": "This makes it readable on GitHub and editors without auto wrapping.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-02-08T06:03:43Z",
        "closed_at": "2013-02-11T16:50:20Z",
        "merged_at": null,
        "body": "This is another way to remove compile warning for redis.c\n\nI think many times about this.\n\n``` c\nredis.c: In function \u2018genRedisInfoString\u2019:\nredis.c:1966:13: warning: format \u2018%llx\u2019 expects argument of type \u2018long long unsigned int\u2019, but argument 6 has type \u2018uint64_t\u2019 [-Wformat]\nredis.c: In function \u2018version\u2019:\nredis.c:2506:9: warning: format \u2018%llx\u2019 expects argument of type \u2018long long unsigned int\u2019, but argument 7 has type \u2018uint64_t\u2019 [-Wformat]\n\n```\n\nbut it looks bad. \n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 55,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2013-02-08T02:04:17Z",
        "closed_at": "2013-07-13T03:31:05Z",
        "merged_at": null,
        "body": "This change allows bind to accept a comma-delimited list of IP addresses. My motivation is to listen on localhost + an address on an internal network but not on a public network.\n\nI changed server.ipfd into a small fixed array rather than allocating dynamically since it's a bit simpler and it seems unlikely anyone will want to explicitly bind to more than a small number of addresses.\n\nThanks.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-02-06T05:50:36Z",
        "closed_at": "2013-03-06T11:41:59Z",
        "merged_at": null,
        "body": "issue: https://github.com/antirez/redis/issues/934\n\nbefore\n\n``` c\ncharsyam@ubuntu:~/projects/redis-charsyam$ !s\nsrc/redis-server redis.conf \nSegmentation fault (core dumped)\n```\n\nafter\n\n``` c\n*** FATAL CONFIG FILE ERROR ***\nReading the configuration file, at line 243\n>>> 'requirepass 'pass'\ncan't parse this line\n```\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-02-05T08:00:00Z",
        "closed_at": "2013-02-05T09:48:49Z",
        "merged_at": "2013-02-05T09:48:49Z",
        "body": "If each if conditions are all fail, variable retval will under uninitlized\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-02-04T06:16:06Z",
        "closed_at": "2013-02-04T09:21:18Z",
        "merged_at": "2013-02-04T09:21:18Z",
        "body": "In CASE 2, the call\u00a0sunionDiffGenericCommand will involve the string \"srandmember\" \n\n> sadd foo one\n> (integer 1)\n> sadd srandmember two\n> (integer 2)\n> srandmember foo 3\n> 1)\"one\"\n> 2)\"two\"\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2013-02-03T19:41:07Z",
        "closed_at": "2013-02-04T09:32:31Z",
        "merged_at": "2013-02-04T09:32:31Z",
        "body": "Make several edits to the example redis.conf configuration file for\nimproved flow and grammar.\n\nSigned-off-by: David Celis me@davidcel.is\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-02-01T17:17:37Z",
        "closed_at": "2013-02-02T02:07:20Z",
        "merged_at": null,
        "body": "This way it's much easier to go through the history when you are testing stuff.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 2,
        "changed_files": 6,
        "created_at": "2013-01-31T03:13:12Z",
        "closed_at": "2018-03-22T16:12:47Z",
        "merged_at": null,
        "body": "1] default: turn off slave connection tcp nodelay( set 1.\n\n``` c\nconfig set slave-tcp-nodelay-off 1 // turn it off\nconfig set slave-tcp-nodelay-off 0 // no action\nconfig get slave-tcp-nodelay-off\n```\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2013-01-28T10:09:44Z",
        "closed_at": "2013-01-31T11:37:49Z",
        "merged_at": "2013-01-31T11:37:49Z",
        "body": "Small hashes are now encoded as ziplists. I stumbled upon these leftover comments in the source this weekend and they confused me ;)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-01-24T15:45:34Z",
        "closed_at": "2014-05-30T22:59:29Z",
        "merged_at": null,
        "body": "The check to see if chkconfig exists is missing a space after `!`, so on CentOS/RHEL, the script will still try to use update-rc.d:\n\n```\nInstalling service...\n./install_server.sh: line 178: update-rc.d: command not found\n```\n\nThis patch changes two occurrences (lines 162 and 176) of :\n\n```\nif [ !`which chkconfig` ] ; then \n```\n\nto\n\n```\nif [ ! `which chkconfig` ] ; then \n```\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-01-24T05:16:50Z",
        "closed_at": "2013-01-24T10:20:38Z",
        "merged_at": null,
        "body": "Release channel objects in notifyKeyspaceEvent()\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-01-24T00:41:44Z",
        "closed_at": "2013-03-30T09:38:25Z",
        "merged_at": null,
        "body": "This patch just removes the following build warning messages:\n\n``` c\n    CC dict.o\n    CC redis.o\nredis.c: In function \u2018genRedisInfoString\u2019:\nredis.c:1967:13: warning: format \u2018%llx\u2019 expects argument of type \u2018long long unsigned int\u2019, but argument 6 has type \u2018uint64_t\u2019 [-Wformat]\nredis.c: In function \u2018version\u2019:\nredis.c:2505:9: warning: format \u2018%llx\u2019 expects argument of type \u2018long long unsigned int\u2019, but argument 7 has type \u2018uint64_t\u2019 [-Wformat]\n    CC sds.o\n    CC zmalloc.o\n```\n\nand now\n\n``` c\n    CC dict.o\n    CC redis.o\n    CC sds.o\n    CC zmalloc.o\n```\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2013-01-23T22:41:06Z",
        "closed_at": "2013-02-05T10:43:56Z",
        "merged_at": "2013-02-05T10:43:56Z",
        "body": "Updated pull request of https://github.com/antirez/redis/pull/877, with a different approach, using `info nameofexecutable` to get current running version of tcl.\n\nshell script changes tested on OS X, linux (gentoo), openbsd and freebsd.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-01-23T03:44:10Z",
        "closed_at": "2017-07-15T15:28:11Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 150,
        "deletions": 150,
        "changed_files": 49,
        "created_at": "2013-01-20T21:22:53Z",
        "closed_at": "2014-07-31T17:04:22Z",
        "merged_at": null,
        "body": "remove trailing white space in the whole project, which is separated from the pull request #880 suggested by @badboy. \n\nI have run \"make test\". The result shows everything is correct. \n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-01-19T05:31:22Z",
        "closed_at": "2013-01-19T09:25:01Z",
        "merged_at": "2013-01-19T09:25:01Z",
        "body": "Thank you for introducing nice `CLIENT GETNAME` and `CLIENT SETNAME` commands. \n\nThis pull request adds a description of the newly introduced commands to the error reply for syntax errors in `CLIENT` commands.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2013-01-18T09:16:31Z",
        "closed_at": "2013-01-19T09:27:56Z",
        "merged_at": "2013-01-19T09:27:56Z",
        "body": "This avoids unnecessary core dumps. Fixes antirez/redis#894\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-01-18T05:18:57Z",
        "closed_at": "2013-01-18T10:41:29Z",
        "merged_at": "2013-01-18T10:41:29Z",
        "body": "Redis pings slaves in \"pre-synchronization stage\" with newlines. (See\nhttps://github.com/antirez/redis/blob/2.6.9/src/replication.c#L814)\nHowever, redis-cli does not expect this - it sees the newline as the end\nof the bulk length line, and ends up returning 0 as bulk the length.\nThis manifests as the following when running redis-cli:\n\n```\n$ ./src/redis-cli --rdb some_file\nSYNC sent to master, writing 0 bytes to 'some_file'\nTransfer finished with success.\n```\n\nWith this commit, we just ignore leading newlines while reading the bulk\nlength line.\n\nTo reproduce the problem, load enough data into Redis so that the\npreparation of the RDB snapshot takes long enough for a ping to occur\nwhile redis-cli is waiting for the data.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-01-17T05:03:54Z",
        "closed_at": "2014-07-31T17:05:52Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-01-17T01:27:07Z",
        "closed_at": "2013-01-19T09:32:28Z",
        "merged_at": "2013-01-19T09:32:28Z",
        "body": "after failing connect to server through redis-cli \n\nand try to redis server using \"redis-server\"\n\nIt couldn't update the redis-cli prompt \n\nbefore:\n\n``` c\ncharsyam@ubuntu:~/projects/redis-charsyam/src$ redis-cli -p 3000\nCould not connect to Redis at 127.0.0.1:3000: Connection refused\nnot connected> connect 127.0.0.1 6379\nredis 127.0.0.1:3000> <-- it isn't updated.\n```\n\napplying this patch.\n\n``` c\ncharsyam@ubuntu:~/projects/redis-charsyam/src$ ./redis-cli -p 3000\nCould not connect to Redis at 127.0.0.1:3000: Connection refused\nnot connected> connect 127.0.0.1 6379\nredis 127.0.0.1:6379>\n```\n\nIt is updated well.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2013-01-16T17:49:31Z",
        "closed_at": "2013-01-19T09:46:22Z",
        "merged_at": null,
        "body": "Adding repl-syncio-timeout in config Command.\nregardless of any other issue. I think it is better to set timeout values.\n\n``` c\nredis 127.0.0.1:6379> config get repl-syncio-timeout\n1) \"repl-syncio-timeout\"\n2) \"5\"\n```\n\nand\n\n``` c\nredis 127.0.0.1:6379> config get repl-syncio-timeout\n1) \"repl-syncio-timeout\"\n2) \"3\"\n```\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-01-16T15:02:24Z",
        "closed_at": "2013-01-21T11:11:25Z",
        "merged_at": "2013-01-21T11:11:25Z",
        "body": "Fixes a minor typo in sentinel.conf\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2013-01-16T10:09:50Z",
        "closed_at": "2013-01-21T11:12:03Z",
        "merged_at": "2013-01-21T11:12:03Z",
        "body": "hi antirez, \n\nFixed some typos in Redis.c, please check. Thanks. \n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-01-15T14:49:20Z",
        "closed_at": "2013-01-19T14:28:09Z",
        "merged_at": null,
        "body": "Situation: \n    when Slave's maxmemory and maxmemory-policy are different from Master's\n    some datum are in Master but they aren't in Slave.\n\nHow:\n    for consistency between master and slave. it is better to turn off slave's freememoryIfNeed.\n\nWhy::\n    1] at the first time, I considered to copy master's maxmemory and maxmemory-policy setting \n    when they sync for the first time. but it can cause some troubles. because Master and Slave uses memory\n    differently. \n\n```\n2] even though, they uses same maxmemory-policy, slave can perform \"LRU or Expire\" itself.\n```\n\nIn conclusion, Skipping freememoryIfNeed in slave is better.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2013-01-14T22:42:09Z",
        "closed_at": "2013-01-21T11:18:03Z",
        "merged_at": null,
        "body": "Hi this pull request is to fix the memory leakage issues, while cleanup the trailing blank spaces in the source code. Thanks\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2013-01-13T22:23:37Z",
        "closed_at": "2014-08-25T08:28:22Z",
        "merged_at": null,
        "body": "Hi, the function dictIdentityHashFunction is not used in any of the project, so I am deleting it. Thanks\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2013-01-13T20:20:23Z",
        "closed_at": "2013-01-23T22:42:58Z",
        "merged_at": null,
        "body": "Just ran the test suite against tcl 8.6.0, which worked nicely. The test suite currently uses `tclsh8.5` everywhere, so I made calls to `tclsh` instead. Since there's currently a 8.5 requirement, I added a check for that.\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 60,
        "deletions": 47,
        "changed_files": 1,
        "created_at": "2013-01-13T12:06:03Z",
        "closed_at": "2017-07-03T11:16:59Z",
        "merged_at": null,
        "body": "The README file did not conform to github markdown. Essentially I just converted it to the github flavoured markdown syntax. Enjoy.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-01-09T16:17:33Z",
        "closed_at": "2013-01-21T11:19:02Z",
        "merged_at": "2013-01-21T11:19:02Z",
        "body": "fixed typo in a comment (step 2 memcheck)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-01-08T01:28:13Z",
        "closed_at": "2021-05-20T09:17:16Z",
        "merged_at": null,
        "body": "Without this echo wont interpret newline characters.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2013-01-06T04:57:30Z",
        "closed_at": "2017-07-03T11:24:18Z",
        "merged_at": null,
        "body": "hi antirez,\n\nI fix some typos in 00-RELEASENOTE.\n\nBest Regards.\n- caoxudong\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2013-01-02T10:01:16Z",
        "closed_at": "2014-08-25T08:28:22Z",
        "merged_at": null,
        "body": "We have seen an issue where an instance had a very large peak memory usage: `used_memory_peak:18446744073706301432`. The `bytesToHuman()` function can't stringify that properly (we get: `used_memory_peak_human:\\x90\\f\\x83\\xFB\\x83\\u007F`) so here's a version that resorts to simply printing the bytes for too-large values.\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2013-01-01T15:32:14Z",
        "closed_at": "2014-08-25T08:28:22Z",
        "merged_at": null,
        "body": "Check for io errors when writing the checksum to an rdb.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2012-12-27T22:57:49Z",
        "closed_at": "2014-05-30T10:18:55Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 2,
        "changed_files": 6,
        "created_at": "2012-12-27T19:11:57Z",
        "closed_at": "2017-07-15T15:28:26Z",
        "merged_at": null,
        "body": "This allows you to configure slaves so they WILL NOT auto-reconnect to their master if the connection is lost.  This is very useful if you're running many masters on one host and many slaves on a second host.  What you want to do is resync them one at a time, but redis has no facility for this.  But using this feature you can easily script a solution in Perl/Python/Ruby/whatever.\n\nWe've used this in production at Craigslist for a while now.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-12-26T20:14:20Z",
        "closed_at": "2014-10-16T01:56:13Z",
        "merged_at": null,
        "body": "Fix redis-cli prompt to state `not connected` after a `SHUTDOWN` command is sent.\n\nOriginal scenario (before fix):\n\n```\nredis 127.0.0.1:6379> KEYS *\n1) \"aaa\"\nredis 127.0.0.1:6379> SHUTDOWN\nredis 127.0.0.1:6379> KEYS *\nredis 127.0.0.1:6379> GET aaa\nredis 127.0.0.1:6379> \n```\n\nSame scenario after redis-cli fix:\n\n```\nredis 127.0.0.1:6379> KEYS *\n1) \"aaa\"\nredis 127.0.0.1:6379> SHUTDOWN\nnot connected> KEYS *\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\nnot connected> GET aaa\nCould not connect to Redis at 127.0.0.1:6379: Connection refused\nnot connected> \n```\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 85219,
        "deletions": 13,
        "changed_files": 219,
        "created_at": "2012-12-25T09:34:12Z",
        "closed_at": "2013-05-07T09:44:57Z",
        "merged_at": null,
        "body": "We have created a command `REVAL` to use [mruby](https://github.com/mruby/mruby) for scripting of Redis.\n\nmruby is the lightweight implementation of the Ruby language complying to (part of) the ISO standard.\nRuby is good at processing strings, arrays, hashes, etc. with its pretty designed libraries.\nWith mruby, we can do complex data operations in Redis with simple scripts.\n\nHow do you think about this idea?\nAs this patch is still rough cutting, some works will be needed to fit with Redis.\n(For example `REVALSHA` is not implemented. Not implemented features are listed later.)\n\nIf your feel positive about implementing scripting languages in Redis other than Lua, we would like to consider supporting JavaScript with [v8](http://code.google.com/p/v8/).\nJavaScript runs on both client-side and server-side.\nIf Redis supports JavaScript, we can write everything in JavaScript.\n\nThe usage of `REVAL` is as follows:\n\n---\n\n```\nREVAL script numkeys key [key ...] arg [arg \u2026]\n```\n## Features\n\nRun mruby in the context of Redis server.\n\n```\n> reval \"[KEYS[0],KEYS[1],ARGV[0],ARGV[1]]\" 2 key1 key2 first second\n1) \"key1\"\n2) \"key2\"\n3) \"first\"\n4) \"second\"\n```\n\nCall Redis commands from a mruby script. (same as `EVAL` command)\n\n```\n> reval \"REDIS.call('set','foo','bar')\" 0\nOK\n```\n## Conversion between mruby and Redis data types\n\n**Redis to mruby** conversion table.\n- Redis integer reply -> mruby Integer\n- Redis bulk reply -> mruby String\n- Redis multi bulk reply -> mruby Array\n- Redis error reply -> mruby Exception\n- Redis Nil bulk reply and Nil multi bulk reply -> mruby nil\n\n**mruby to Redis** conversion table.\n- mruby Integer -> Redis integer reply (the number is converted into an integer)\n- mruby String -> Redis bulk reply\n- mruby Array -> Redis multi bulk reply (truncated to the first nil inside the mruby array if any)\n- mruby Hash with a single ok key -> Redis status reply\n- mruby Hash with a single err key -> Redis error reply\n- mruby Error -> Redis error reply\n- mruby boolean false -> Redis Nil bulk reply\n- mruby boolean true -> Redis integer reply with value of 1.\n\n(examples)\n\n```\n> reval \"10\" 0\n(integer) 10\n\n> reval \"[1, 2, [3, 'Hello World!']]\" 0\n1) (integer) 1\n2) (integer) 2\n3) 1) (integer) 3\n   2) \"Hello World!\"\n\n> reval \"REDIS.call('get', 'foo')\" 0\n\"bar\"\n```\n## Not implemented yet\n- `REVALSHA`\n- `REDIS.log`\n- script timeout\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-12-24T21:18:34Z",
        "closed_at": "2013-12-11T10:36:16Z",
        "merged_at": "2013-12-11T10:36:16Z",
        "body": "Having a 'long' offset to an 'off_t' sized data block is wrong when size is greater than 2GB and less than 4GB, on a 32-bit process.  This results with replication being interrupted after transferring the first 2GB.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-12-22T06:46:41Z",
        "closed_at": "2014-08-01T20:59:36Z",
        "merged_at": null,
        "body": "hi,  antirez,\n\nI fixed a typo in dict.c,  for  \"more thank\"  to \"more than\"\n\nthanks\nRunzhen\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 16,
        "changed_files": 12,
        "created_at": "2012-12-21T22:24:29Z",
        "closed_at": "2015-01-06T19:42:18Z",
        "merged_at": null,
        "body": "On FreeBSD calling gettimeofday() causes all the cores on a multicore\nsystem to be synchronized. On a heavily loaded system with a\nsignificantly CPU bound application this can cause a 40% overall\nsystem degradation.\n\nA better option is to use clock_gettime() and pass in the\nCLOCK_REALTIME_FAST clock as the clock to use. This achieves the\nsame behavior as gettimeofday() on Linux.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 16,
        "changed_files": 12,
        "created_at": "2012-12-21T19:07:57Z",
        "closed_at": "2012-12-21T22:23:02Z",
        "merged_at": null,
        "body": "On FreeBSD calling gettimeofday() causes all the cores on a multicore\nsystem to be synchronized. On a heavily loaded system with a\nsignificantly CPU bound application this can cause a 40% overall\nsystem degradation.\n\nA better option is to use clock_gettime() and pass in the\nCLOCK_REALTIME_FAST clock as the clock to use. This achieves the\nsame behavior as gettimeofday() on Linux.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2012-12-20T22:03:00Z",
        "closed_at": "2013-01-19T14:28:57Z",
        "merged_at": null,
        "body": "This issue is based on \"redis doesn't close replication fd\", so it can call syncWithMaster with previous replication fd.\n\nThere are 2 approaches.\n\n1] when client requests \"slaveof\" command\n -> closeing replication fd.\n\n2] when syncWithMaster call\n-> checking fd is repl_transfer_s\n\nand if repl_state == REDIS_REPL_CONNECTED , then redis doesn't close replication fd.\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-12-19T21:38:18Z",
        "closed_at": "2014-05-30T23:35:49Z",
        "merged_at": null,
        "body": "If -Werror is turned on (as it is in our build system) TOT will not build. This fixes the warnings.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-12-17T04:47:04Z",
        "closed_at": "2014-07-31T17:08:04Z",
        "merged_at": null,
        "body": "- ARGS -> ARGV\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2012-12-16T02:22:42Z",
        "closed_at": "2014-07-31T17:10:02Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2012-12-15T12:49:27Z",
        "closed_at": "2013-03-19T02:00:03Z",
        "merged_at": null,
        "body": "All testes passed.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2012-12-14T17:58:24Z",
        "closed_at": "2014-11-19T16:50:42Z",
        "merged_at": null,
        "body": "reading REDISCLI_HISTFILE from shell environment.\n\nand using it.\n\nrelated issue: https://github.com/antirez/redis/issues/831\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-12-14T16:48:41Z",
        "closed_at": "2013-03-30T10:09:15Z",
        "merged_at": null,
        "body": "add \"How to Handle Redis Build Error\" Section to README\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 22,
        "changed_files": 1,
        "created_at": "2012-12-13T22:06:49Z",
        "closed_at": "2014-07-31T17:11:24Z",
        "merged_at": null,
        "body": "I've performed three trivial changes:\n\n \"ot\"       -> \"to\".\n \"verison\"  -> \"version\"\n \"distater\" -> \"disaster\"\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-12-13T11:23:45Z",
        "closed_at": "2012-12-14T16:43:48Z",
        "merged_at": null,
        "body": "Before finding suitable solution to solve redis build error.\n\nI think it is a good way that giving some information, how to handle these build error \n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2012-12-12T16:04:15Z",
        "closed_at": "2012-12-12T17:34:41Z",
        "merged_at": "2012-12-12T17:34:41Z",
        "body": "At least with NetBSD 5 and 6, _XOPEN_SOURCE needs to be defined as it is for Linux. My inclination would be to use this definition as the default and special case the systems where it doesn't work, but this change works for me. I've tested this on both NetBSD 5 and 6, on 32- and 64-bit systems.\n\nFull disclosure: to link on 32-bit systems, I also had to add a -march setting to CFLAGS. Otherwise I get an undefined error for __sync_add_and_fetch_4. I think that's just a general gcc issue and I don't have a patch for it.\n\nThanks.\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-12-12T14:04:38Z",
        "closed_at": "2013-02-18T01:57:29Z",
        "merged_at": null,
        "body": "Hi, I am looking forward to Redis Cluster, and now trying some features.\n\nI found that redis-trib.rb does not work on my environment (Ubuntu 12.10 + GCC 4.7.2) because of an error as follows.\n\n```\n% ruby redis-trib.rb create 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381\nCreating cluster\nConnecting to node 127.0.0.1:6379: OK\nredis-trib.rb:95:in `split': invalid byte sequence in US-ASCII (ArgumentError)\n        from redis-trib.rb:95:in `load_info'\n        from redis-trib.rb:450:in `block in create_cluster_cmd'\n        from redis-trib.rb:446:in `each'\n        from redis-trib.rb:446:in `create_cluster_cmd'\n        from redis-trib.rb:492:in `<main>'\n```\n\nThis is because of the uninitialized fields _ip_ and _port_ of the _clusterNode_ for _myself_. We can also check this by the CLUSTER NODES command.\n\n```\n% redis-cli\nredis 127.0.0.1:6379> CLUSTER NODES\n04c162326adf4c1266b91ae19715e3b51fdb329f ?:-259964664 myself - 0 0 connected\n                                         ^^^^^^^^^^^^\n```\n\nThis commit adds the code to initialize the _ip_ and _port_ fields in the _createClusterNode_ function. I hope that this pull request will help people try Redis Cluster.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 39,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-12-11T18:58:20Z",
        "closed_at": "2018-07-06T13:47:52Z",
        "merged_at": null,
        "body": "Hi,\n\nI asked the question on the mailing list, but I figured it would be easier to post some code.\nBasically, according to the guidelines for sentinel-aware clients (http://redis.io/topics/sentinel-clients), clients should call get-master-addr-by-name to get the master address. However, sentinels will respond to this call with the master address even if the number of sentinels that appear to be connected to this master is less than the quorum. In some cases, this means that depending on the order in which the sentinels are queried, the resulting master address could be different if the sentinels are desynchronised.\n\nThis pull request adds a new method, get-master-that-met-quorum-addr-by-name (awful name, I agree), that answers with IDONTKNOW if the number of sentinels that appear to be connected to this master is less than the quorum.\n###### Example of a real-world scenario that would trigger a desynchronization:\n\nTwo normal redis instances (called R1 and R2, with R1 being the master), and 3 sentinel instances S1, S2 and S3, quorum set to 2.\nS1 and R1 become unavailable. Failover is initiated and completed, R2 is the new master, S2 and S3 respond accordingly. \nNow S1 and R1 are available again. They don't know about the failover, so S1 becomes a master, and S1 answers to get-master-addr-by-name wtih R1.\nAt this point, if a client was given S1, S2 and S3 as starting sentinels, depending on the order in which it queries these sentinels, masters could be set to R1 (if calling S1 first) or R2 (if calling S2 or S3 first). If it were to call get-master-that-met-quorum-addr-by-name, S1 would respond with IDONTKNOW, and S2 and S3 would answer with R2, which is IMO the expected behaviour.\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2012-12-10T17:41:29Z",
        "closed_at": "2013-06-17T10:08:14Z",
        "merged_at": "2013-06-17T10:08:14Z",
        "body": "It makes no sense to listen to 0.0.0.0 for make test.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-12-05T10:17:22Z",
        "closed_at": "2021-10-04T17:13:20Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-12-05T01:44:07Z",
        "closed_at": "2014-08-25T21:19:56Z",
        "merged_at": null,
        "body": "when using SETEX/PSETEX and when their expire time is less than 0\ntheir error message are the same.\n\n``` c\nredis 127.0.0.1:6379> SETEX mykey -1 \"Hello\"\n(error) ERR invalid expire time in SETEX\n(5.60s)\nredis 127.0.0.1:6379> PSETEX mykey -1 \"Hello\"\n(error) ERR invalid expire time in SETEX\n(3.69s)\n```\n\nso I changed to use their own names when PSETEX fails\n\n``` c\nredis 127.0.0.1:6379> SETEX mykey -1 \"Hello\"\n(error) ERR invalid expire time in SETEX\nredis 127.0.0.1:6379> PSETEX mykey -1 \"Hello\"\n(error) ERR invalid expire time in PSETEX\nredis 127.0.0.1:6379> \n```\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-12-03T04:50:32Z",
        "closed_at": "2012-12-03T11:30:13Z",
        "merged_at": "2012-12-03T11:30:13Z",
        "body": "... and update-rc.d compatability.  #Issue804\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2012-12-02T01:50:32Z",
        "closed_at": "2014-05-30T23:30:40Z",
        "merged_at": null,
        "body": "Issue #374: Resolve compilation errors when using clang. Test for Clang and remove linking option(s).\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 58,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2012-12-01T12:19:42Z",
        "closed_at": "2021-06-01T06:25:32Z",
        "merged_at": null,
        "body": "It's now possible to use ~ or environmental variables like $HOME in\nthe config file (where appropriate).\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2012-12-01T11:30:29Z",
        "closed_at": "2013-03-30T10:09:39Z",
        "merged_at": null,
        "body": "remove unnecessary codes\n1. nwritten = 0; is not necessary.\n- not anti warning.\n1. variabless l, j is not used after that position.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 32,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2012-11-29T07:39:54Z",
        "closed_at": "2021-06-08T08:51:19Z",
        "merged_at": null,
        "body": "sortCompare has a bug in sort.c\n\nif server.sort_alpha is true, sort_alpha has nothing to do with sort_bypattern.\n\nso it has to compare with strcoll instead of compareStringObject.\n\nand add locale config parameter to set locale easily.\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 118,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2012-11-28T19:59:58Z",
        "closed_at": "2012-11-29T14:34:44Z",
        "merged_at": null,
        "body": "1. Why?\n   some users use KEYS command to search special keyspaces. and they uses KEYS Command in production. so it can cause many problems. because KEYS command has to visit all keys. so for example, if redis has more than 1M~10M keys, it can't process other commands well.\n   so. there is alternative. storing key name in the sorted set or other collections. but it is not useful because it doesn;t support regexp like KEYS. and I choose Sorted Set, Inserting and Deleting function has to be fast to use in above situation.\n\n2.How?\n so I suggest zregCommand like KEYS. it can use like this.\n\n``` c\nzreg keyname <regexp>\n```\n\nThank you.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-11-28T11:04:18Z",
        "closed_at": "2013-05-09T14:29:12Z",
        "merged_at": null,
        "body": "merge two patchs to one patch\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 71,
        "deletions": 35,
        "changed_files": 2,
        "created_at": "2012-11-27T19:41:56Z",
        "closed_at": "2012-11-28T10:35:39Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-11-25T23:39:04Z",
        "closed_at": "2012-11-28T10:45:34Z",
        "merged_at": null,
        "body": "it is @erbenmo 's idea.\n\nand I am sure it is unnecessary condition\n\nhere is no case d->ht[0].size > d->ht[0].used in the line which calls dictExpand. It just needs d->ht[0].used \\* 2\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 23,
        "changed_files": 5,
        "created_at": "2012-11-23T17:44:57Z",
        "closed_at": "2013-05-03T16:19:03Z",
        "merged_at": null,
        "body": "if end of str is \\r ,in function seekNewLine,s[pos+1] is accessing invalid address.usually we will get random value,but if it is '\\n',hiredis thinks it get a \\r\\n but not.when next tcp packet carries \\n in first byte,protocol error occurs.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-11-23T11:32:54Z",
        "closed_at": "2013-01-19T14:30:18Z",
        "merged_at": null,
        "body": "previous auth patch for benchmark doesn't work. and cause some problem.\n\nso I patch it. \n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-11-22T21:55:08Z",
        "closed_at": "2012-11-23T11:44:18Z",
        "merged_at": "2012-11-23T11:44:18Z",
        "body": "in debug.c, lack of including \"bio.h\" \nit can cause compile warning. and it can potentially cause compile error \n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-11-22T19:04:44Z",
        "closed_at": "2013-03-30T10:08:49Z",
        "merged_at": null,
        "body": "add 32bitsync label for some people who are suffering from __sync_add_and_fetch_4\n\nthese days,  many people experienced this build failure and adding CFLAGS=-march=i686 is not intuitive . because make 32bit overload user define CFLAGS. so 32bitsync will be useful for some people who wants to solve this problem.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-11-21T04:28:38Z",
        "closed_at": "2014-05-30T22:59:43Z",
        "merged_at": null,
        "body": "`id -u` is in the POSIX and LSB specs [1](http://pubs.opengroup.org/onlinepubs/009695399/utilities/id.html), while `whoami` is not.\nFurthermore, the username of UID 0 can be changed (and is sometimes\nrecommended in a misguided attempt to increase security).\n\nFreeBSD users logged in as `toor` would also slip through the cracks.\n\n```\n http://refspecs.linuxfoundation.org/LSB_4.1.0/LSB-Core-generic/LSB-Core-generic/command.html\n```\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-11-19T19:16:04Z",
        "closed_at": "2018-03-22T16:11:32Z",
        "merged_at": null,
        "body": "in 475 line, retval is not initialized.. because of it.\n\nif encoding is not match below if cases.\n\nit can pass redisAssert statement. it can be initialized as DICT_OK.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-11-19T18:54:05Z",
        "closed_at": "2013-07-02T10:18:18Z",
        "merged_at": "2013-07-02T10:18:18Z",
        "body": "p is not initialized. but After fixing it, I found no one use randstring\n\nbut It is obviously bug. so I sent a patch.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2012-11-15T21:13:06Z",
        "closed_at": "2021-10-14T06:32:17Z",
        "merged_at": null,
        "body": "https://github.com/antirez/redis/issues/767\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-11-15T09:41:13Z",
        "closed_at": "2014-07-31T17:19:49Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2012-11-14T22:56:13Z",
        "closed_at": "2013-03-30T09:39:08Z",
        "merged_at": null,
        "body": "Remove cluster timeout 1000 to define DEFAULT_REDIS_CLUSTER_TIMEOUT\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2012-11-13T07:42:27Z",
        "closed_at": "2013-03-30T09:40:59Z",
        "merged_at": null,
        "body": "redis doesn't need to call getExpire before check server.loading.\nso moving call getExpire after server.loading check statement.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-11-08T01:12:46Z",
        "closed_at": "2017-07-03T11:10:10Z",
        "merged_at": null,
        "body": "Hi, antirez:\n    Solaris platform doesn't support HAVE_BACKTRACE, so this will introduce compile error in Solaris platform. So I suggest remove (__sun) definition.\n    Please help to check it, thanks very much!!\nBest Regards\nNan Xiao\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 22687,
        "deletions": 14089,
        "changed_files": 185,
        "created_at": "2012-11-04T22:16:10Z",
        "closed_at": "2014-08-01T21:20:45Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-11-02T16:47:06Z",
        "closed_at": "2021-05-20T15:47:38Z",
        "merged_at": null,
        "body": "This allows the return code of redis-check-dump to be used in scripts.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2012-11-01T10:20:51Z",
        "closed_at": "2012-11-01T21:26:02Z",
        "merged_at": "2012-11-01T21:26:02Z",
        "body": "Hi, \n\nI am reading your source code, and fixed some typos in comments.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-10-31T18:20:18Z",
        "closed_at": "2012-11-02T11:10:47Z",
        "merged_at": "2012-11-02T11:10:47Z",
        "body": "fix a typo in redis.h line 595 comment\n\nthanks  ^_^\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-10-29T15:01:06Z",
        "closed_at": "2012-10-30T17:58:16Z",
        "merged_at": null,
        "body": "This updates redis-cli help.h and adds the command descriptions of BITCOUNT and BITOP to the string group. I would appreciate it if you could kindly review the commit.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 36,
        "deletions": 29,
        "changed_files": 6,
        "created_at": "2012-10-28T05:11:34Z",
        "closed_at": "2013-05-17T08:27:55Z",
        "merged_at": null,
        "body": "rebased version of https://github.com/antirez/redis/pull/591\nplus one more fix.\n",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-10-26T23:59:47Z",
        "closed_at": "2012-10-27T06:47:29Z",
        "merged_at": null,
        "body": "I think it is better to apply travis ci to redis to prevent build error before releasing.\n\ntravis-ci is a free ci server.\n\nand you will be able to check build status after commit.\n\nbut in redis, \"make test\" took too long time, so I just applied \n\n\"scirpt: make\" \n\nif you decide to use travis-ci . you have to register in travis-ci(just using github account)\n\nand turn on your project compile  \n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-10-26T03:42:03Z",
        "closed_at": "2016-05-08T16:04:41Z",
        "merged_at": "2016-05-08T16:04:41Z",
        "body": "Removed dofile function from the Lua environment. \n\ndofile() called without arguments reads from STDIN and blocks and never finishes.\n\ndofile called with file arguments can be used to determine if files exist or not, as well as it's probably not good to read scripts from disk for atomic operations.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-10-25T20:36:00Z",
        "closed_at": "2012-10-26T13:32:43Z",
        "merged_at": "2012-10-26T13:32:43Z",
        "body": "Have to change 0x020617 -> 0x020611 (it is hex code)\n__GLIBC_PREREQ is better, linux supports this function.\n",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-10-25T00:53:18Z",
        "closed_at": "2014-04-20T09:58:51Z",
        "merged_at": null,
        "body": "This pull request adds `max_memory` and `max_memory_human` to the `INFO` command's output.\n\nRedis hosting providers often disable or rename the `CONFIG` command to lock down the Redis instance to a specific configuration based on a customer's plan. This makes it difficult to programmatically get the actual max memory (and subsequently the memory utilization) of the Redis instance.\n\nAddresses #355\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-10-24T08:49:20Z",
        "closed_at": "2012-10-31T08:29:04Z",
        "merged_at": "2012-10-31T08:29:04Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-10-24T03:38:09Z",
        "closed_at": "2014-05-30T22:59:46Z",
        "merged_at": null,
        "body": "If you install redis w/ install_server.sh on RHEL(or its flavors), you'll end up with the following error:\n\n```\n$ sudo ./install_server.sh\nWelcome to the redis service installer\nThis script will help you easily set up a running redis server\n...\nInstalling service...\n./install_server.sh: line 176: update-rc.d: command not found\n exists, process is already running or crashed\nInstallation successful!\n```\n\nThis happens because negation of which command result is not handled correctly.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2012-10-23T18:37:02Z",
        "closed_at": "2012-10-25T20:10:37Z",
        "merged_at": null,
        "body": "patch to build rhel5 correctly.\n\nI think that many user uses rh5, so it is helpful to build rh5 without any other work.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-10-23T02:15:06Z",
        "closed_at": "2014-08-01T21:23:10Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 94,
        "deletions": 61,
        "changed_files": 4,
        "created_at": "2012-10-22T22:06:08Z",
        "closed_at": "2021-07-20T10:16:26Z",
        "merged_at": null,
        "body": "SipHash is a cryptographically strong MAC designed for use in hash tables. Previously\nRedis switched to murmur2 to try to prevent hash flooding DoS attacks:\n\nhttps://github.com/antirez/redis/commit/da920e75d4836897b9a7109b6d4743e201cd8a4f\n\nUnfortunately, murmur2 and murmur3 are both easy to attack, as there are algorithms which\ncan quickly generate arbitrarily many keys that all hash to the same value regardless of\nwhat the seed is:\n\nhttps://www.131002.net/siphash/murmur2collisions-20120821.tar.gz\n\nBy switching to SipHash, we get strong resistance to this kind of attack, without any\nnoticeable slowdown on either redis-benchmark or \"DEBUG POPULATE 1000000\". According to\nthe SUPERCOP benchmarks, this good hash performance holds true for both large and small\nkeys across all CPU architectures and models tested:\n\nhttp://bench.cr.yp.to/impl-auth/siphash24.html\n\nThis patch also switches to using /dev/urandom as a source of high-quality randomness for\nkey generation on server startup, unless it is unavailable, in which case time and pid\nare used instead.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1300,
        "deletions": 6,
        "changed_files": 12,
        "created_at": "2012-10-22T03:05:31Z",
        "closed_at": "2015-05-08T01:03:24Z",
        "merged_at": null,
        "body": "Hey,\n\nThese commits implement the Trie data structure into Redis and provide a new data type which makes use of it.\n\nThe main advantages over hash tables are:\n- Memory efficient. No node in the tree stores the key associated with that\n  node. Instead, its position in the tree defines the key with which it's\n  associated. This results in memory savings as common key prefixes are stored\n  only once.\n- No collisions. One of the biggest problems with hash tables are collisions.\n  They yeild a worst case performance of O(N), require constant re-hashing,\n  and can be attack vectors, as seen here:\n  https://github.com/antirez/redis/issues/663\n- Prefix search. Traversing a trie from a given prefix doesn't require walking\n  through every node, which opens up a lot of new possibilities.\n\nIn addition to the data structure, I've implemented the TRIE data type which\nmakes use of it.\n\nThe TRIE data type is 100% compatible with the HASH type and supports\nall of its commands (HGET becomes TGET, HSET becomes TGET and so on).\n\nIn fact, the trie unit tests were simply copied from the hash table and\nslightly modified.\n\nAdditionally, tries support prefix-based traversal for TKEYS, TVALS and\nTGETALL with the same performance.\n\nExample:\n\n> tset trie hello xxx\n> (integer) 1\n> tset trie hey xxx\n> (integer) 1\n> tset trie foobar xxx\n> (integer) 1\n> tkeys trie\n> 1) \"foobar\"\n> 2) \"hey\"\n> 3) \"hello\"\n> tkeys trie he\n> 1) \"hey\"\n> 2) \"hello\"\n\nThis opens up many possibilities, such as using Redis to build an efficient\nAuto Complete service.\n\nPerformance wise, this implementation delivers about the same performance as\nhash tables.\n\nI've run a couple of benchmarks on my laptop, so please don't consider those\nas a reference:\n\n./redis-benchmark -n 1000000 -r 1000000 -t HSET,HGET\nHSET: 35617.61 requests per second\nHGET: 35335.69 requests per second\nused_memory_human:75.85M\nused_memory_peak_human:80.69M\n\n./redis-benchmark -n 1000000 -r 1000000 -t TSET,TGET\nTSET: 36926.26 requests per second\nTGET 36587.15 requests per second\nused_memory_human:42.31M\nused_memory_peak_human:45.01M\n\nWhile the performance is about the same, it saves more than 50% of memory.\n\nNote that the redis-benchmark test is a little biased:\nIn real world tests the memory usage will highly depend on the key distribution\nand the values it holds.\nBecause redis-benchmark uses the same key prefix and increments the suffix\nof the keys, tries perform very well on it, which might not reflect real-world\nusage patterns.\n\nFor now, tries live as a separate data type.\n\nIf the concept is accepted, 3 things could happen:\n\n1/ Keep it as is - a new data type compatible with hashes that provide some\n  extra features.\n\n2/ Make HT-based data types (HASH, SET, ...) use the trie data structure\n\n3/ Whatever the outcome of 1 and 2 are, another possibility is to replace the\n   main object database with a Trie. Not only this could lead to improved memory\n   efficiency, but the KEYS command could be special cased to take advantage of\n   tries for prefix-based lookups (e.g. KEYS foo*).\n\nPlease keep in mind that I've just started wrapping my head around Redis\ninternals, so this should not be considered production ready.\n\nLet me know what you think.\n",
        "comments": 26
    },
    {
        "merged": false,
        "additions": 51,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2012-10-08T09:58:39Z",
        "closed_at": "2017-07-15T15:28:57Z",
        "merged_at": null,
        "body": "We want to introduce the possibility for doing extra fsyncs during the rewrite of AOF file.\n\nSee issue for more info: \nhttps://github.com/antirez/redis/issues/700\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2012-10-03T05:03:00Z",
        "closed_at": "2012-10-22T09:55:23Z",
        "merged_at": "2012-10-22T09:55:23Z",
        "body": "Came via http://news.ycombinator.com/item?id=4599740 and noticed some minor cosmetic typos in the dict.h header file.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 149,
        "deletions": 149,
        "changed_files": 39,
        "created_at": "2012-09-21T18:54:03Z",
        "closed_at": "2014-07-31T17:19:14Z",
        "merged_at": null,
        "body": "Mostly in comments, but a few printed strings.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-09-18T08:26:01Z",
        "closed_at": "2013-08-14T19:27:34Z",
        "merged_at": null,
        "body": "Bug fix for auth in redis-benchmark in order to close this issue: https://github.com/antirez/redis/issues/244\n\nI am skipping steps 1 and 2 on the contributing guidelines because 1) its a bug fix, and 2) its been discussed on google groups already a few times, and also because its a fairly trivial bit of code so nothing lost if not wanted.\n\nI was using this code to run against my servers all night for testing, and also ran with make test with no problems. If there is any other testing I can do let me know.\n\nThe downside to this change is that failure to auth will still report \"ops per second\" which is a bit misleading, but is no different than running against a server without this patch. Its not a regression, I just didn't know of a nice and clean way to ensure that the auth was successful before letting the benchmark continue. Let me know if you have a preferred approach so I can implemented it if needed.\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-09-13T12:48:09Z",
        "closed_at": "2017-07-15T15:53:25Z",
        "merged_at": null,
        "body": "redisFree() to disconnect and free the context\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2012-09-10T16:48:28Z",
        "closed_at": "2017-07-15T15:53:57Z",
        "merged_at": null,
        "body": "Hey,\n\nI was writing some tests for a project I was working on today, and realized that this command didn't exist (I thought it did, for some reason). I figured I would code it up for my own use.\n\nThis function basically re-implements the `keysCommand`, but returns the amount of matched keys instead of the keys themselves. The same functionality is possible by simply checking the number of keys in the response to a `keys` query on the client, but implementing it server-side saves resources, saves a step, and feels natural.\n\n```\nredis 127.0.0.1:6379> count *\n(integer) 0\nredis 127.0.0.1:6379> set foo bar\nOK\nredis 127.0.0.1:6379> set baz qux\nOK\nredis 127.0.0.1:6379> count *\n(integer) 2\nredis 127.0.0.1:6379> count f*\n(integer) 1\n```\n\nIf this has been previously discussed (I don't keep up-to-date with the mailing list) and found to be superfluous, or if you just don't want it in Redis, I totally understand. If you okay this command, I can provide documentation and tests in the next day or two.\n\nLooking forward to your thoughts.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2012-08-31T23:46:27Z",
        "closed_at": "2012-09-01T20:53:43Z",
        "merged_at": null,
        "body": "Match configuration template.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 40,
        "deletions": 44,
        "changed_files": 1,
        "created_at": "2012-08-29T02:27:01Z",
        "closed_at": "2012-08-30T06:22:24Z",
        "merged_at": null,
        "body": "When system time changes back, the timer will not worker properly\nhence some core functionality of redis will stop working(e.g. replication,\nbgsave, etc). See issue #633 for more details.\n\nThe patch saves the previous time and when a system clock skew is detected,\nit will force expire all timers.\n\nThis patch also makes the timer list in sorted order, so getting the nearest\ntimer is O(1) now.(Although adding/deleting timers are still O(N)).\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2012-08-28T02:50:03Z",
        "closed_at": "2014-07-31T17:12:26Z",
        "merged_at": null,
        "body": "fix typo and make comment catching up with code.\n\nthanks,\nanfernee\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2012-08-24T23:58:34Z",
        "closed_at": "2020-12-13T20:40:20Z",
        "merged_at": null,
        "body": "This fixes a few build issues on Solaris 5.10 with gcc as well as native compiler.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2012-08-15T09:05:27Z",
        "closed_at": "2012-08-15T15:35:06Z",
        "merged_at": null,
        "body": "fixed for `set' command.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 61,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2012-08-13T21:15:08Z",
        "closed_at": "2012-08-22T09:32:27Z",
        "merged_at": "2012-08-22T09:32:27Z",
        "body": "This fixes the bug described in #627.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 66,
        "deletions": 1,
        "changed_files": 6,
        "created_at": "2012-08-08T13:28:42Z",
        "closed_at": "2017-09-07T15:19:29Z",
        "merged_at": null,
        "body": "Add a configuration option slave-allow-key-expires to allow TTL-based expires on the slave. This could be very useful for non-readonly slaves where temporary data is created based on the replicated keys and it needs to be expired after a certain amount of time.\n\nThis code is used in production at LivingSocial.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 17996,
        "deletions": 13330,
        "changed_files": 169,
        "created_at": "2012-08-03T20:02:44Z",
        "closed_at": "2012-08-03T21:36:45Z",
        "merged_at": null,
        "body": "... in release notes.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 53,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2012-07-30T10:50:19Z",
        "closed_at": "2014-07-09T13:16:20Z",
        "merged_at": null,
        "body": "Now that sentinel is coming along I'd like to reintroduce this PR\nit operates in the same cluster ordering space (though it is\norders of magnitude simpler). For simple setups avoid the\ninstallation of snowflake or similar tools has a lot of merit.\n\nThis was the original PR text:\n\nThis is similar to what snowflake and the recent boundary\nsolution do, but it makes sense to use redis for that type\nof use cases for people wanting a simple way to get\nincremental ids in distributed systems without an additional\ndaemon requirement.\n\nUnique IDs are composed as follows:\n\nepoch seconds: 4 bytes\nepoch mseconds: 4 bytes\nhost name: 6 bytes\nsequence id: 2 bytes\nhost name is truncated to 6 chars, so the appropriate config\ndirective id-generation-name should be set on each machines\nwanting to yield ids if truncating hostname do 6 chars does\nnot suffice.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-07-30T10:49:11Z",
        "closed_at": "2014-07-09T13:15:32Z",
        "merged_at": null,
        "body": "This helps packager on various OSs. This was a previously submitted which had stray commits in them, it is now cleaned up.\n",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-07-26T06:53:10Z",
        "closed_at": "2012-07-31T09:55:40Z",
        "merged_at": null,
        "body": "This error wasn't being caught because the compiler evaluated the #if conditional to false.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-07-24T19:24:59Z",
        "closed_at": "2012-07-25T14:15:53Z",
        "merged_at": "2012-07-25T14:15:53Z",
        "body": "Currently if you load redis-sentinel without a config file, it tells you to use redis-server instead. This fixes the warning.\n\nTook a suggestion from @pietern to use argv[0] - this was initially to avoid checking redis.sentinel_mode, but I like this compromise because it provides both the exact string used to start the binary + the right name of the conf file.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2012-07-24T18:55:03Z",
        "closed_at": "2012-07-25T14:15:03Z",
        "merged_at": "2012-07-25T14:15:03Z",
        "body": "Add the port to the top of the sentinel.conf file and make the rest a bit more consistent.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-07-21T17:11:42Z",
        "closed_at": "2012-07-21T18:12:35Z",
        "merged_at": "2012-07-21T18:12:35Z",
        "body": "This is regarding issue #585, noticed this problem last week and fixed it. Forgot to push it though.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2012-07-19T12:57:17Z",
        "closed_at": "2012-10-28T05:12:12Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-07-18T00:38:30Z",
        "closed_at": "2012-07-27T10:56:48Z",
        "merged_at": "2012-07-27T10:56:48Z",
        "body": "If Redis only manages to write out a partial buffer, the AOF file won't load back into Redis the next time it starts up.  It is better to discard the short write than waste time running `redis-check-aof`.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2012-07-18T00:18:21Z",
        "closed_at": "2012-07-27T10:55:20Z",
        "merged_at": "2012-07-27T10:55:20Z",
        "body": "AOF rewriting is too important to go unmonitored.  Redis can easily fill its filesystem with AOF data if failing rewrites go unnoticed by operators.  This pull request introduces a new data point in `INFO` that behaves just like `rdb_last_bgsave_status` for AOF persistence.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2012-07-16T13:55:29Z",
        "closed_at": "2021-06-27T08:42:02Z",
        "merged_at": null,
        "body": "Support glob patterns in the 'include' config directive.\n\nThis allows e.g. to create a /etc/redis/conf.d directory,\nwhere distribution packages can drop their config files,\nand source everything from it with\n\n  include /etc/redis/conf.d/*.conf\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-07-15T09:49:11Z",
        "closed_at": "2012-09-05T14:20:51Z",
        "merged_at": null,
        "body": "remove unsafe and unnecessary cast.\nuntil now, this cast may lead segmentation fault when end > UINT_MAX\n# example\n\nsetbit foo 0 1\nbitcount  foo 0 4294967295\n=> ok\nbitcount  foo 0 4294967296\n=> cause segmentation fault.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 420,
        "deletions": 0,
        "changed_files": 8,
        "created_at": "2012-07-09T08:03:26Z",
        "closed_at": "2013-11-25T10:20:40Z",
        "merged_at": null,
        "body": "I'll write up the details for this technique later (wrote a nice and details commit message, but vim trashed it).\n",
        "comments": 36
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-07-06T08:03:11Z",
        "closed_at": "2014-05-30T23:24:19Z",
        "merged_at": null,
        "body": "Invalid param name, change \"$$PIDFILE\" to \"$PIDFILE\"\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 83,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2012-07-05T14:42:01Z",
        "closed_at": "2023-04-27T17:57:07Z",
        "merged_at": null,
        "body": "lsplice \"1 2 3 4\" 1 2 -> \"1 4\"\n\nIssue #550.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2012-07-05T04:50:49Z",
        "closed_at": "2012-09-05T13:59:37Z",
        "merged_at": "2012-09-05T13:59:37Z",
        "body": "REDIS_REPL_PING_SLAVE_PERIOD controls how often the master should\ntransmit a heartbeat (PING) to its slaves.  This period, which defaults\nto 10, is measured in seconds.\n\nRedis 2.4 masters used to ping their slaves every ten seconds, just like\nit says on the tin.\n\nThe Redis 2.6 masters I have been experimenting with, on the other hand,\nping their slaves _every second_.  (master_last_io_seconds_ago never\napproaches 10.)  I think the ping period was inadvertently slashed to\none-tenth of its nominal value around the time REDIS_HZ was introduced.\nThis commit reintroduces correct ping schedule behaviour.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-07-02T12:22:25Z",
        "closed_at": "2013-04-25T22:14:14Z",
        "merged_at": null,
        "body": "This is to assist the facilitation of data and performance collection\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-06-29T04:09:22Z",
        "closed_at": "2012-07-09T10:14:08Z",
        "merged_at": "2012-07-09T10:14:08Z",
        "body": "Similar with pull request #520,  to prevent high cpu load as issue #518\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2012-06-15T14:42:15Z",
        "closed_at": "2012-06-16T22:43:43Z",
        "merged_at": null,
        "body": "It is not a good advice to just invoke sudo or su to run redis-server\nin non-root, nor is it good to run it just as root.\nHowever, there are environments/circumstances, where\nstart-stop-daemon/startproc are not meant to be used,\ne.g. when writing platform independant resource agents,\nor, when you have to keep the dependencies as low as possible.\n\nThis patch adds a new configuration variable \"user\" that gets one\nstring argument, and the redis-server tries to drop its\ngroup and user privileges down to this user and this user's groups\nand exits early on failure.\n\nI would also like to get this commit cherry-picked to 2.x branches,\nespecially 2.2, since this is, what Ubuntu 12.04 is shipping with.\n",
        "comments": 11
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2012-06-13T05:41:37Z",
        "closed_at": "2012-06-13T08:25:52Z",
        "merged_at": "2012-06-13T08:25:52Z",
        "body": "A rather minor detail, but satisfies the copy editor in me. Seems to read best without periods at the end, except for circumstances (like `-i interval`) where there are two discrete comments. \n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-06-12T14:05:37Z",
        "closed_at": "2012-07-30T10:08:55Z",
        "merged_at": null,
        "body": "when preparing to create a package, it's often nice to honor the DESTDIR environment variable, this allows \"fake\" installs that go to a special dir then packaging up that dir. \n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 502,
        "changed_files": 2,
        "created_at": "2012-06-08T13:08:51Z",
        "closed_at": "2012-10-05T10:30:58Z",
        "merged_at": "2012-10-05T10:30:58Z",
        "body": "I've fixed the server install script. It now no longer rewrites a template, but rather takes the default config and uses sed to rewrite it with the script's overrides and install this.\n\nThis will avoid a situation where changes to the config need to be ported to the template.\n\nPS:\nThis patch also fixes the syntax error that caused install_server.sh not to run in unix shell but use bash, so I've changed the interpreter back to /bin/sh\n",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2012-06-04T19:35:57Z",
        "closed_at": "2015-12-07T18:10:08Z",
        "merged_at": null,
        "body": "Disable backtrace support in src/config.h if uClibc is detected.\n\nTested on the following platforms:\n- sh4-linux-uclibc (GCC 4.1.1, uClibc 0.9.29)\n- mipsel-linux-uclibc (GCC 4.4.5, uClibc 0.9.29)\n\nTo cross-compile:\n\n```\n$ make CC=mipsel-linux-gcc MALLOC=libc\n```\n\nedit: updated commit message to include `__UCLIBC_SUBLEVEL__` in version\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-06-04T13:36:28Z",
        "closed_at": "2017-07-15T15:51:07Z",
        "merged_at": null,
        "body": "When an external diff tool is defined the make process gets hung waiting for the diff tool to exit. It never shows up anywhere (probably due to running under `...`).\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-06-04T10:05:14Z",
        "closed_at": "2014-05-30T23:21:43Z",
        "merged_at": null,
        "body": "Both ops per sec samples and timer were not reset by \"CONFIG RESETSTAT\"\nwhich means:\n    1) sometimes user can see negative value for instanteneous ops per\n    sec stat\n    2) that same value can show measurements from before RESETSTAT\n\nThis commit fixes both issues\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2012-06-02T01:57:05Z",
        "closed_at": "2012-06-02T08:28:03Z",
        "merged_at": null,
        "body": "Update fixes issue #532 (MIGRATE fails for hashes) as described in this comment\nhttps://github.com/antirez/redis/issues/532#issuecomment-6073545\nand adds unit test for the case.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-05-23T01:36:16Z",
        "closed_at": "2012-05-23T09:56:11Z",
        "merged_at": null,
        "body": "Looks like it's fixed in every branch except this one.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 10,
        "changed_files": 6,
        "created_at": "2012-05-19T12:27:08Z",
        "closed_at": "2021-03-14T18:08:20Z",
        "merged_at": null,
        "body": "remove some dead  assignment code.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-05-18T12:42:51Z",
        "closed_at": "2014-08-25T08:28:22Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 305,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2012-05-14T18:21:58Z",
        "closed_at": "2012-05-15T09:16:55Z",
        "merged_at": "2012-05-15T09:16:55Z",
        "body": "This drastically improves performance high concurrency.\n\nMany thanks to @davepacheco and @joyent for contributing.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-05-07T21:11:02Z",
        "closed_at": "2012-09-18T17:43:27Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2012-05-05T00:35:18Z",
        "closed_at": "2012-05-06T08:03:16Z",
        "merged_at": "2012-05-06T08:03:16Z",
        "body": "Because of the introduction of new integer encoding types for ziplists in the 2.6 tree, the same integer value may have a different encoding in different versions of the ziplist implementation. This means that the encoding can NOT be used as a fast path in comparing integers.\n\nThis results in problems for users running 2.6 RC code, who loaded a data set produced by a 2.4 instance, which contains ziplist encoded sorted sets, where the members are integers in the following ranges:\n- `[0, 12]` (immediate encoding)\n- `[-2**8, 2**8-1]` (8 bit signed)\n- `[-2**24, -2**16-1]`, `[2**16, 2**24-1]` (24 bit signed not in 16 bit signed range)\n\nIf so, sorted set code may be unable to find members resulting in duplicate members in sorted sets. While duplicate members in sorted sets are incorrect to begin with, it inevitably leads to a crash.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 137,
        "deletions": 57,
        "changed_files": 6,
        "created_at": "2012-05-04T23:06:49Z",
        "closed_at": "2017-07-15T15:24:17Z",
        "merged_at": null,
        "body": "## Background\n\nThere are two scenarios when dealing with failover that would be greatly improved by the introduction of a `readonly` flag for redis.\n### 1. During a graceful failover\n\nTo perform an upgrade of your redis cluster, it is possible to quickly quiesce all writes and quickly do the failover with a very small interruption.\n\nDuring the period where we invoke the slave statements, it is possible that writes could be made to the old master and would cause them to be lost when the slave of statement is invoked.\n\nBeing able to set both the old and new masters to readonly during that window would allow us to guarantee no clients thought they successfully wrote to the server when they didn't.\n### 2. When a client isn't properly notified of the new master\n\nIf a client (for any number of reasons, including a network partition, a service not running properly, a code bug, etc.) does not properly update it's configuration data to reflect the new master IP and continues to write to the old one, those writes will never be seen by the master and by the rest of the systems.\n## Solution\n\nThis pull request introduces a configuration setting called `readonly` that can be set in the configuration file or runtime and blocks all commands that modify data.\n\nIt does this by introducing a flag called `REDIS_CMD_MODIFIES_DATA` that is applied to redis commands that modify data and rejects all writes that do not come from a client that is marked as `REDIS_MASTER`.\n## Using it\n\nThe setting can be set to \"yes\" or \"no\" in the configuration file:\n\n```\n# Change setting to 'yes' to prevent writes to this server\nreadonly no\n```\n\nor via the config command:\n\n```\n$ redis-cli config set readonly yes\n```\n\nIf a blocked command is sent from a client, the server responds with:\n\n```\n-ERR command not allowed when readonly is set to yes\n```\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-05-04T19:11:11Z",
        "closed_at": "2012-05-15T09:18:03Z",
        "merged_at": "2012-05-15T09:18:03Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2012-04-30T17:20:47Z",
        "closed_at": "2012-05-01T08:50:25Z",
        "merged_at": "2012-05-01T08:50:25Z",
        "body": "Every matched key in a KEYS call is checked for expiration. When the key is set to expire, the call to `getExpire` will assert that the key also exists in the main dictionary. This in turn causes a rehashing step to be executed. Rehashing a dictionary when there is an iterator active may result in the iterator emitting duplicate entries, or not emitting some entries at all. By using a safe iterator, the rehash step is omitted.\n\nThis fixes #487.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-04-30T16:03:38Z",
        "closed_at": "2012-04-30T17:21:42Z",
        "merged_at": null,
        "body": "Hi Redis,\n\nwe encountered a small problem when the 'keys' command is misbehaving. Or at least not behaving as we expect. This test has the minimal sequence we need to reproduce.\n\nPiping this to redis-cli gives similar results:\n\nflushall\nset a c\nset t c\nset e c\nset s c\nset foo b\nexpire s 5\nkeys *\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 36,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2012-04-29T09:14:12Z",
        "closed_at": "2012-05-17T05:04:33Z",
        "merged_at": null,
        "body": "It is useful to specify a full path for rdb and aof files, however the temporary files still get created in the current directory and this can later lead to very slow rename operations (that require re-copying the temporary file to a different filesystem).\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3293,
        "deletions": 3519,
        "changed_files": 82,
        "created_at": "2012-04-26T18:49:26Z",
        "closed_at": "2017-07-15T15:51:38Z",
        "merged_at": null,
        "body": "I had a use case for enabling this because I need to adjust the value to a key depending on the amount of server time that had elapsed since it was last written to. \n\nFor lack of a better name I implemented \"time decaying lazy counters\" using a LUA script and INCR.\n\nThe option is named \"lua-nondeterministic-calls\" and is disabled by default for obvious reasons.\n\nI modified default.conf in tests/assets/ and all the tests pass when it is \"no\" or not set. \n\nThe 2 tests you would expect to fail do fail when it is enabled.\n\n-- Jeff\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-04-25T19:23:00Z",
        "closed_at": "2012-04-25T20:39:14Z",
        "merged_at": "2012-04-25T20:39:14Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-04-17T23:04:45Z",
        "closed_at": "2015-01-12T14:56:36Z",
        "merged_at": null,
        "body": "If the config file has either specified \"daemonize yes\" or \"pidfile [/path/to/redis.pid]\" then it should create  pidfile.\n\nSome background for this is: I usually run redis via daemontools. That entails running redis-server on the foreground. Given that, I'd also want redis-server to create a pidfile so other processes (e.g. nagios) can run checks for that.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2012-04-17T20:24:07Z",
        "closed_at": "2012-04-23T09:07:21Z",
        "merged_at": "2012-04-23T09:07:21Z",
        "body": "Tcl's exec can send data to stdout itself, no need to call cat/echo for that usually.\n\nSee also this message at stackoverflow:  http://stackoverflow.com/questions/10188886/redis-error-when-running-tcl-helper-script-on-windows-7-32bit-edition \n\nThere are quite a few [exec cat $something] calls in the tests that could be replaced by simple open/read/close calls too.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 65,
        "deletions": 54,
        "changed_files": 2,
        "created_at": "2012-04-14T01:12:48Z",
        "closed_at": "2012-04-18T14:25:59Z",
        "merged_at": "2012-04-18T14:25:59Z",
        "body": "A common pattern for building and installing Redis is:\n\n```\n$ make 32bit\n$ make install PREFIX=/tmp/whatever\n```\n\nThis is not possible now, as changes in some flags may trigger a rebuild. The rebuild is triggered because a change in ARCH / CFLAGS / LDFLAGS / etc requires dependencies to be updated.\n\nThis change persists the flags, so they remain the same across runs. When any of the flags change, everything will be rebuilt.\n\nThe 2.6 version of this PR is #455.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 65,
        "deletions": 54,
        "changed_files": 2,
        "created_at": "2012-04-14T01:11:01Z",
        "closed_at": "2012-04-18T14:26:36Z",
        "merged_at": "2012-04-18T14:26:36Z",
        "body": "A common pattern for building and installing Redis is:\n\n```\n$ make 32bit\n$ make install PREFIX=/tmp/whatever\n```\n\nThis is not possible now, as changes in some flags may trigger a rebuild. The rebuild is triggered because a change in ARCH / CFLAGS / LDFLAGS / etc requires dependencies to be updated.\n\nThis change persists the flags, so they remain the same across runs. When any of the flags change, everything will be rebuilt.\n\nThe unstable version of this PR is #456.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 332,
        "deletions": 4,
        "changed_files": 10,
        "created_at": "2012-04-12T21:11:46Z",
        "closed_at": "2017-07-03T11:16:27Z",
        "merged_at": null,
        "body": "What do you think of this in general? I would imagine you would want to write it yourself, but I like the idea of having all the commands available in Redis loaded dynamically and the ability to easily extend Redis by 3rd parties. This would probably much reduce the feature request load you get everyday.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-04-11T15:05:21Z",
        "closed_at": "2012-04-11T16:25:44Z",
        "merged_at": "2012-04-11T16:25:44Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 63,
        "deletions": 8,
        "changed_files": 10,
        "created_at": "2012-04-11T06:15:43Z",
        "closed_at": "2021-06-25T17:19:50Z",
        "merged_at": null,
        "body": "I've posted to an existing topic on the [google group](https://groups.google.com/forum/?fromgroups#!topic/redis-db/8X9pZ2EIhlU) (pending approval) to explain our use case, but I'll go ahead and duplicate it here.\n\nWe use Redis to select the next object to be shown to a user for classification.\n\nTwo things are important in this selection:\n1. Users are shown objects only once (unique selection)\n2. Objects are scored based upon prior classifications (the score is updated in real time) and users should be shown the most highly scored object they haven't seen.\n\nCurrently we do the following:\n- Store many keys for object scores (`objects_score_<id>`)\n- Store a set of object ids seen by a user (`seen_objects_for_<user_id>`)\n- Store a set of object ids available for classification (`objects`)\n- Remove the diff set after selection is done (since the stored order from the score is invalidated shortly after creation)\n\nThen perform:\n\n```\nsdiffstore user_<id>_unseen_objects objects seen_objects_for_user_<id>\nsort user_<id>_unseen_objects DESC BY objects_score_* LIMIT 0 1\ndel user_<id>_unseen_objects\n```\n\nGiven we have over 600,000 users and well over 10,000,000 objects between our projects, staying responsive while handling rates above 100 requests/second is non-trivial.  Optimizing this specific use-case is pretty critical for us.\n\nPlease feel free to suggest changes, revisions, etc.\n\nThanks!\n\n-Michael\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 16,
        "changed_files": 4,
        "created_at": "2012-04-07T12:49:50Z",
        "closed_at": "2012-04-21T10:31:07Z",
        "merged_at": "2012-04-21T10:31:06Z",
        "body": "No actual code changed.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2012-04-07T07:45:57Z",
        "closed_at": "2012-04-07T08:58:31Z",
        "merged_at": null,
        "body": "Instead of using _exit in rdb/aof childs, just flush buffers of buffered IO before fork, then exit() is just fine. It's much cleaner aproach and it also fixe issue with not generating coverage from those slaves.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-04-05T06:41:34Z",
        "closed_at": "2012-04-05T08:42:24Z",
        "merged_at": null,
        "body": "First commit futureproofs version comparison in zmalloc.h, as the comparison will be broken in case of new major tc/je malloc version.\n\nAlso if we are forced to use pure malloc on glibc, we can use malloc_usable_size (simillar #elif could be added for BSD as well, but I currently don't have any BSD on hand to test it)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2012-04-04T17:21:16Z",
        "closed_at": "2012-04-05T10:24:04Z",
        "merged_at": null,
        "body": "This contains few changes to testsuite (regarding how the redis is ended, because for coverage/profiling there's need to exit cleanly). LCOV report can be generated by simply \"make lcov\", source is then cleaned, builded with coverage support, test suite is run in single-client mode and lcov report is generated and saved to src/lcov-html directory.\n\nThis report is standalone set of static html files, it would be nice to have them as part of redis CI :-)\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-04-03T15:43:04Z",
        "closed_at": "2012-04-05T08:52:40Z",
        "merged_at": "2012-04-05T08:52:40Z",
        "body": "Allocate and set stack for SIGSEGV/etc handler (currently NOSTACK was used, but by default alternate stack is not set)\nAnd also don't use nodefer, nostack and resethand for SIGTERM, so receiving second TERM wouldn't kill redis in unclean way.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2012-04-03T12:25:55Z",
        "closed_at": "2012-04-05T08:56:12Z",
        "merged_at": "2012-04-05T08:56:12Z",
        "body": "First commit add new option (--clients) via which user can setup number of test client to run (would be usefull for code coverage generation)\n\nThe second commit changes how the redis is killed, instead of sending SIGTERM every 10msec and beating it into submission, send SIGTERM once, then wait and when redis doesn't quit in 5 sec, send SIGKILL.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2012-04-02T09:57:20Z",
        "closed_at": "2012-04-05T08:54:09Z",
        "merged_at": "2012-04-05T08:54:09Z",
        "body": "In the code there was few mentions or now removed VM, also there was now unneeded type of object.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-04-01T06:44:09Z",
        "closed_at": "2012-04-01T08:14:53Z",
        "merged_at": "2012-04-01T08:14:53Z",
        "body": "In install_server.sh, the chkconfig command that set redis to start on runlevels 3, 4, and 5 never executes because of a missing space.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 32,
        "deletions": 15,
        "changed_files": 6,
        "created_at": "2012-04-01T02:46:40Z",
        "closed_at": "2012-04-01T08:03:57Z",
        "merged_at": null,
        "body": "Fixed INCRBYFLOAT and HINCRBYFLOAT encoding to use scientific notation when necessary.\nRemoved hardcoded 17-digit precision limit, added _incrbyfloat-precision_ config (Defaulting to 17). Different machines will perform double double arithmetic with different levels of precision; it therefore seems fitting to let the user decide with what precision redis should store double doubles.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-03-31T10:10:50Z",
        "closed_at": "2012-03-31T19:02:18Z",
        "merged_at": null,
        "body": "Added simple tab completion for key names using the `KEYS ...` command.\n\nTests still work. I couldn't find a way to test auto completion and the current auto completion of command doesn't seem to have a test either.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 22,
        "changed_files": 2,
        "created_at": "2012-03-30T19:23:38Z",
        "closed_at": "2012-04-23T09:34:19Z",
        "merged_at": "2012-04-23T09:34:19Z",
        "body": "Very simple changes. Some of the sds functions still had arguments which content wasn't changed but lacked the `const` keyword.\n\nCompiling is still the same (no new warnings).\nTests still work.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 26,
        "changed_files": 3,
        "created_at": "2012-03-29T17:37:35Z",
        "closed_at": "2012-03-30T08:28:58Z",
        "merged_at": "2012-03-30T08:28:58Z",
        "body": "It occurs when two or more dbs are replicated and at least one of them is >= db10.\nI think every redis versions since 2.0.x are affected. (2.2.x and 2.4.x confirmed)\n\nYou can reproduce the problem with the following steps:\n1. prepare for two redis instances (master-slave).\n   A redis instance listening port 6379\n   A redis instance as slave of above\n2. check and write down the master redis instance's memory usages by INFO command.\n3. executes the following commands concurrently (i.e. from its own terminal).\n   term1 $ redis-cli -r 1000000 set foo bar\n   term2 $ redis-cli -n 15 -r 1000000 set foo bar\n4. check the master redis instance's memory usage again when two SET queries finishes.\n   then you must confirm a memory leak.\n\nThanks.\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-03-29T10:24:51Z",
        "closed_at": "2012-04-21T10:27:15Z",
        "merged_at": "2012-04-21T10:27:15Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 32,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2012-03-29T03:50:20Z",
        "closed_at": "2017-07-15T15:50:44Z",
        "merged_at": null,
        "body": "I mis-read the bug, and added a CHANNELS command. This time, I added CHANNELS as a subcommand to DEBUG.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 47,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2012-03-29T01:25:50Z",
        "closed_at": "2012-06-09T06:43:13Z",
        "merged_at": null,
        "body": "@antirez I decided to dive into the code some more, and as an exercise, implement approve feature #221. I was more careful this time to understand the functions used, and even added a test.\n\nHopefully this is less work than just doing it yourself. :)\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2012-03-28T09:52:31Z",
        "closed_at": "2012-04-05T14:04:07Z",
        "merged_at": null,
        "body": "Simplify handling (p)expire(at), now the code (look at offset var) is easier to grasp. Also, now we avoid slight loss of precission in pexpireat (between the call to mstime in expireat command and the one in expiregeneric, the mstime can change). And for (p)expireat, there's no reason to call mstime anyway :-)\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 52,
        "changed_files": 8,
        "created_at": "2012-03-27T15:49:11Z",
        "closed_at": "2012-03-27T20:58:24Z",
        "merged_at": "2012-03-27T20:58:24Z",
        "body": "Content:\n- commit fixing usage of time(NULL) instead of mstime in expireifNeeded\n- use server.unixtime on more places where time(NULL) was currently used (and where unixtime is safe to use), cluster.c not checked though\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 256,
        "deletions": 210,
        "changed_files": 5,
        "created_at": "2012-03-26T17:31:09Z",
        "closed_at": "2012-04-11T10:05:11Z",
        "merged_at": null,
        "body": "Instead of overriding non-standard variables such as `ARCH` and `PROF`, use standard variables `CFLAGS` and `LDFLAGS` to override Makefile settings. Move dependencies generated by `make dep` to a separate file.\n\nVerified (built and ran tests) that this works on OSX, Linux and Solaris, for both the 64-bit and 32-bit build, with latest `make` and `make` 3.80.\n\nThis should also cleanly apply to unstable. I can submit another PR for that if you want. I have a similar branch for 2.4, but that should only be used by people who really need to override `CFLAGS` and/or `LDFLAGS` (users running Solaris).\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-03-15T06:29:43Z",
        "closed_at": "2012-03-27T21:00:08Z",
        "merged_at": "2012-03-27T21:00:08Z",
        "body": "I guess it should be \"USED\" here, not \"USER\".\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 1,
        "changed_files": 6,
        "created_at": "2012-03-14T20:21:09Z",
        "closed_at": "2017-07-15T15:27:30Z",
        "merged_at": null,
        "body": "This is described in this redis-db mailing list thread:\n\nhttp://groups.google.com/group/redis-db/browse_thread/thread/6badf6abf8f44eb0\n\nIt adds an option to the config \"repl_auto_resync\" which defaults to \"yes\" and controls whether or not a slave will automatically try to resync with the master when the connection between them fails.\n\nTest suite passes with this which is based on 2.4. And the code runs for me in a live environment where I tested as well.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-03-10T18:09:54Z",
        "closed_at": "2012-03-16T20:37:14Z",
        "merged_at": null,
        "body": "Added this for my code adventures with redis and thought maybe you want also some simple CI :)\nYou only need to activate the github service hook. http://about.travis-ci.org/docs/user/getting-started/\n\nExample: http://travis-ci.org/#!/Skomski/redis/builds/834852\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-03-10T18:06:39Z",
        "closed_at": "2014-06-15T04:44:56Z",
        "merged_at": null,
        "body": "Nothing to snazzy, but nicer format to the README :)\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-03-09T12:18:29Z",
        "closed_at": "2012-03-09T16:32:46Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2012-02-26T14:18:19Z",
        "closed_at": "2012-02-26T15:18:43Z",
        "merged_at": "2012-02-26T15:18:43Z",
        "body": "Redis does not compile for me as of https://github.com/antirez/redis/commit/80ff1fc6d0e3da53ae7bea0354216a38f7303971. The following should fix it.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 41,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2012-02-25T15:34:37Z",
        "closed_at": "2012-02-26T10:47:58Z",
        "merged_at": null,
        "body": "Implementation of issue #357. Currently allow __sync builtins only for GCC && (i386||amd64), but for other architectures and/or clang it should be OK too.\n\nIt has huge performance benefits. Tested on otherwise idle machine, 1 run for heatup, result is best of 3 test (best by SET performance) with: ./redis-benchmark -q -P 16 -n 10000000 -t set,get,incr,lpush,lpop,sadd,spop\n\nbefore:\nSET: 715870.88 requests per second\nGET: 929627.25 requests per second\nINCR: 765931.38 requests per second\nLPUSH: 773993.81 requests per second\nLPOP: 770653.50 requests per second\nSADD: 796178.31 requests per second\nSPOP: 1056747.25 requests per second\n\nwith sync builtins:\nSET: 832986.25 requests per second \nGET: 1110864.25 requests per second\nINCR: 911743.25 requests per second\nLPUSH: 941353.62 requests per second\nLPOP: 961723.44 requests per second\nSADD: 1015228.38 requests per second\nSPOP: 1317523.00 requests per second\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-02-25T13:38:41Z",
        "closed_at": "2012-02-26T10:48:07Z",
        "merged_at": null,
        "body": "in redis-benchmark there's memory leak due to not freeing reply objects. \n\nFun fact: with memory leak fixed I get even better numbers from benchmark, probably due to less work for memory allocator. Maybe time to retest those blogpost numbers ;-)\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-02-24T05:34:26Z",
        "closed_at": "2012-02-24T09:22:14Z",
        "merged_at": null,
        "body": "When creating a monitoring script, it is easy to check memory threshold that\nwe can get both used_memory and max_memory in info command rather than\nmaxmemory in redis.conf statically.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2012-02-22T23:46:27Z",
        "closed_at": "2020-06-12T14:50:20Z",
        "merged_at": null,
        "body": "Sorry for the double pull request. This contains the actual code for the status) section.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2012-02-20T17:30:41Z",
        "closed_at": "2017-07-15T15:51:19Z",
        "merged_at": null,
        "body": "  Two files changed to fix the issue #333. The detail discussion in https://github.com/antirez/redis/issues/333, could you help to review it?\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 358,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2012-02-13T21:15:26Z",
        "closed_at": "2012-02-13T23:05:59Z",
        "merged_at": "2012-02-13T23:05:59Z",
        "body": "I included `lua_struct.c` from http://www.inf.puc-rio.br/~roberto/struct/\nI have tested it with a python client for both doubles and integers and it works as expected.\nHappy to write some tests for redis, but unsure where there should be placed (I was looking for test for cjson and didn't find them).\n\nIn essence, this extension can be **very** useful if one needs to manipulate redis strings as double or integer arrays in `lua` domain.\n\nLuca\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2012-01-30T06:42:21Z",
        "closed_at": "2012-01-30T09:25:34Z",
        "merged_at": "2012-01-30T09:25:34Z",
        "body": "Hi! I love redis, thank you for developing it. I've noticed this ticket lying around, so I coded a patch. Hope that helps.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-01-29T14:01:26Z",
        "closed_at": "2012-01-30T09:40:17Z",
        "merged_at": "2012-01-30T09:40:17Z",
        "body": "A cosmetic change to how Lua functions are constructed from the EVAL argument causing errors in scripts to be reported with the correct line number.\n\nThe right thing to do would be to compile them in separate Lua contexts but I assume it's been considered and would have a negative impact on the performance?\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2012-01-27T18:36:52Z",
        "closed_at": "2012-02-21T09:06:35Z",
        "merged_at": null,
        "body": "ignored 80 columns restriction as later in the file\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2012-01-25T04:19:54Z",
        "closed_at": "2016-08-30T17:19:24Z",
        "merged_at": null,
        "body": "",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2012-01-23T02:35:02Z",
        "closed_at": "2012-01-25T19:51:55Z",
        "merged_at": null,
        "body": "Done and done. I understand why sudo isn't always a good idea. Though I personally encountered difficulty when trying to `make install`, and `make -B install` worked for me.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2012-01-20T22:58:16Z",
        "closed_at": "2012-01-23T00:31:27Z",
        "merged_at": null,
        "body": "Added `make uninstall`.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2012-01-18T15:13:38Z",
        "closed_at": "2012-02-22T16:19:54Z",
        "merged_at": "2012-02-22T16:19:54Z",
        "body": "updated utils/install_server.sh to support chkconfig boxes (redhat/centos) as well as debian/ubuntu\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 17,
        "changed_files": 2,
        "created_at": "2012-01-14T03:08:44Z",
        "closed_at": "2012-02-29T21:10:55Z",
        "merged_at": null,
        "body": "Not doing this may cause `LPUSH` to be replicated with a wrong number of arguments. The values to push are scanned from left to right, pushing them to waiting clients as needed. When one or more values cannot be pushed to waiting clients and are pushed onto a real list, the global dirty count is incremented, which causes the command to be replicated. When replicated, it should only include the values that were pushed to a real list, ignoring the values that were pushed to waiting clients.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2012-01-14T02:06:55Z",
        "closed_at": "2012-01-16T09:18:41Z",
        "merged_at": "2012-01-16T09:18:41Z",
        "body": "A user reported a slave that would show an monotonically increase input buffer length, shortly after completing a SYNC. Also, `INFO` output showed a single blocked client, which could only be the master link. Investigation showed that indeed the `BRPOP` command was fed by the master. This command can only end up in the stream of write operations when it did **NOT** block, and effectively executed `RPOP`. However, when the key involved in the `BRPOP` is expired **BEFORE** the command is executed, the client executing it will block. The client in this case, is the master link.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 59,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2012-01-13T23:07:50Z",
        "closed_at": "2012-07-30T10:50:27Z",
        "merged_at": null,
        "body": "This is similar to what snowflake and the recent boundary\nsolution do, but it makes sense to use redis for that type\nof use cases for people wanting a simple way to get\nincremental ids in distributed systems without an additional\ndaemon requirement.\n\nUnique IDs are composed as follows:\n- epoch seconds: 4 bytes\n- epoch mseconds: 4 bytes\n- host name: 6 bytes\n- sequence id: 2 bytes\n\nhost name is truncated to 6 chars, so the appropriate config\ndirective id-generation-name should be set on each machines\nwanting to yield ids if truncating hostname do 6 chars does\nnot suffice.\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2012-01-06T11:01:20Z",
        "closed_at": "2012-05-23T09:20:17Z",
        "merged_at": null,
        "body": "This fix issue #267\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 783,
        "deletions": 407,
        "changed_files": 17,
        "created_at": "2012-01-03T06:26:21Z",
        "closed_at": "2012-03-10T10:23:30Z",
        "merged_at": "2012-03-10T10:23:30Z",
        "body": "This patch switches the encoding of small hashes from zipmap to ziplist (continuation of issue #188). This is a first stab, and it may need more work: tests pass, but I haven't done performance testing (throughput, memory). Because the vanilla ziplist API is used, which doesn't provide a function to _replace_ an element, HSET is implemented as a delete followed by a push. This causes the fields in the ziplist encoded hash to be ordered by modification time (I see this more as a by-effect than a feature).\n\nI haven't tested the loading of a zipmap encoded hash from an older RDB yet. We may want to add an integration test for that (i.e. one that includes a prefabricated RDB with such a key).\n",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2011-12-28T21:14:34Z",
        "closed_at": "2012-01-16T11:44:00Z",
        "merged_at": "2012-01-16T11:44:00Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 68,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2011-12-23T18:44:48Z",
        "closed_at": "2012-02-02T03:36:22Z",
        "merged_at": null,
        "body": "As I commented on issue #165, I tried to fix the following commands:\n- SMEMBERS\n- SINTER\n- SUNION\n- SDIFF\n\nNotice that the command will be O(N*logN) complexity where N is the size of result set. \n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-12-22T11:25:40Z",
        "closed_at": "2011-12-22T15:00:42Z",
        "merged_at": "2011-12-22T15:00:42Z",
        "body": "Hey!, I love your docs. Just found a typo and decided to clone and fix. I hope some day I can be more helpful to this great project :-)\n\nKindly regards and a big thank you!,\n-dk\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 15,
        "changed_files": 4,
        "created_at": "2011-12-19T12:07:35Z",
        "closed_at": "2011-12-19T13:38:32Z",
        "merged_at": "2011-12-19T13:38:32Z",
        "body": "Some commands uses `atoi` and accepts non-numerical string as integer parameter. Using `getLongFromObjectOrReply` instead of `atoi` to fix the issue. \n\nBesides, added a test case for SORT-LIMIT statement. \n\n(Sorry, I just made some mistake when using github pull request. So I resent this pull request). \n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2011-12-15T10:55:38Z",
        "closed_at": "2012-01-16T15:58:51Z",
        "merged_at": null,
        "body": "I've just copy README to README,md and your github main page look match better!\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-12-15T01:21:07Z",
        "closed_at": "2011-12-15T09:11:16Z",
        "merged_at": null,
        "body": "Can we increase this limit. Better yet can it be made configurable?\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2011-12-08T16:52:33Z",
        "closed_at": "2014-08-01T22:12:48Z",
        "merged_at": null,
        "body": "subj. the pull request fixes the issue and closes issues like: https://github.com/antirez/redis/issues/216\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2011-12-07T16:51:33Z",
        "closed_at": "2017-07-15T15:29:24Z",
        "merged_at": null,
        "body": "Thanks to Jurij Smakov jurij@wooyd.org.\n\nSigned-off-by: Chris Lamb lamby@debian.org\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 50,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2011-12-04T02:26:49Z",
        "closed_at": "2011-12-04T11:41:19Z",
        "merged_at": null,
        "body": "Decided to hack this in for my weekend project.  Went pretty quickly! This is a proposal for an optional 3rd parameter to SLOWLOG.  This would leave the slowlog API looking like this:\n- slowlog len - returns length of slowlog list\n- slowlog reset - resets slowlog\n- slowlog get n - gets all members of slowlog from 0 - n\n- slowlog get n p -  gets all members of slowlog from n - p\n\nI added the fourth, which turns the 2nd parameter into the beginning and the 3rd parameter into the end of a range of slowlog queries to return.  This will be very helpful in assembling UIs for slowlog queries (i.e. for pagination).  \n\nSince the slowlog is a list in memory, I'm not that concerned with it's relatively naive implementation (naive in this case because it rewinds the slowlog list, iterates to the beginning of the range, and then ouputs the rest).  This is a safer, easier way to add this feature, because it means not having to change the slowlog data structure or much about the code at all.\n\nI also added some simple tests.\n\nNote: the GET right now returns an exclusive range, e.g. SLOWLOG GET 1 3 returns 1 and 2 - 2 entries.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 160,
        "deletions": 64,
        "changed_files": 3,
        "created_at": "2011-12-01T14:52:49Z",
        "closed_at": "2016-08-01T18:50:49Z",
        "merged_at": null,
        "body": "With this feature Redis will expand the applications in numerous scenarios where right now can only be performed in an SQL-like environment.\n\n**Usage**\nSORT key [BY pattern [ASC|DESC] [ALPHA] [BY pattern\u2026]] [LIMIT offset count] [GET pattern [GET pattern ...]] [STORE destination]\n\n**Documentation**\nThe BY option can be used multiple times in order to sort by multiple fields. If the list elements have the same value when ordering by the first BY pattern, then it sorts by the second BY pattern and so on.\n\nThe following example will order the list mylist by weight and by title. Sorting first the list of elements by weight, and then by title when the elements have the same weight:\n\nSORT mylist BY weight_\\* BY title_\\* ALPHA\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2011-11-29T18:58:07Z",
        "closed_at": "2013-01-23T23:55:15Z",
        "merged_at": null,
        "body": "Since the 2.4 branch still uses the \"old style\" malloc checks, we need to set `USE_JEMALLOC=no` if `USE_TCMALLOC` is defined since `USE_JEMALLOC` gets set if Linux is the host os\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-11-25T02:36:31Z",
        "closed_at": "2011-11-25T11:13:10Z",
        "merged_at": "2011-11-25T11:13:10Z",
        "body": "...I/EXEC may fail in the same RedisClient\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 535,
        "deletions": 202,
        "changed_files": 24,
        "created_at": "2011-11-23T20:41:25Z",
        "closed_at": "2011-11-25T15:29:55Z",
        "merged_at": "2011-11-25T15:29:55Z",
        "body": "This fixes a pretty serious issue that can result in memory corruption. See entire change log here: http://www.canonware.com/cgi-bin/gitweb.cgi?p=jemalloc.git;a=blob_plain;f=ChangeLog;hb=master\n\nWasn't able to skip creating an issue (someone knows how to do such pull requests?), so I'm going to close my older issue. Sorry 'bout this.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-11-21T01:39:14Z",
        "closed_at": "2011-11-22T10:43:28Z",
        "merged_at": "2011-11-22T10:43:28Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 136,
        "deletions": 55,
        "changed_files": 4,
        "created_at": "2011-11-15T21:32:58Z",
        "closed_at": "2011-11-16T09:29:42Z",
        "merged_at": "2011-11-16T09:29:42Z",
        "body": "This change set also breaks out the build rules for the dependencies to their own Makefile in deps/. It also forces recompilation of both the dependencies and Redis itself when the ARCH that is specified does not match the ARCH that was used in the last build. It forces recompilation of only the Redis source when the MALLOC that is specified does not match the MALLOC that was used in the last build. See the description in the separate commit messages for more information.\n\nI tested the updated Makefiles on OSX and Linux.\n\nThis fixes #190.\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-11-10T01:09:57Z",
        "closed_at": "2011-11-15T14:36:54Z",
        "merged_at": "2011-11-15T14:36:54Z",
        "body": "I've been playing around with Redis Cluster and got this error when I used only one source during a reshard:\n\n```\nredis-trib.rb:327:in `/': ClusterNode can't be coerced into Float (TypeError)\nfrom redis-trib.rb:327:in `block in compute_reshard_table'\nfrom redis-trib.rb:324:in `each'\nfrom redis-trib.rb:324:in `each_with_index'\nfrom redis-trib.rb:324:in `compute_reshard_table'\nfrom redis-trib.rb:434:in `reshard_cluster_cmd'\nfrom redis-trib.rb:493:in `<main>'\n```\n\nThis commit fixes this exception.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2011-11-01T19:58:57Z",
        "closed_at": "2011-11-18T13:34:57Z",
        "merged_at": "2011-11-18T13:34:57Z",
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-10-30T19:25:22Z",
        "closed_at": "2011-10-31T08:57:42Z",
        "merged_at": null,
        "body": "It is currently not possible to reset the password by using CONFIG SET. Once setup via \n\n```\nCONFIG SET requirepass secret\n```\n\nthere is no way to get rid of it again. I'm not quite sure if this is perfect for every use-case, but I think setting it to an empty string should reset it.\n\n```\nCONFIG SET requirepass \"\"\n```\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-10-30T03:27:34Z",
        "closed_at": "2011-10-31T10:21:09Z",
        "merged_at": "2011-10-31T10:21:09Z",
        "body": "lua_json.c uses isinf() and that's only available when you compile with `__C99FEATURES__=1`\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2011-10-21T02:09:12Z",
        "closed_at": "2011-10-26T11:01:01Z",
        "merged_at": "2011-10-26T11:01:01Z",
        "body": "I apologize, I screwed up in my commit by not setting errno to zero before checking it after the strtol call.  The other uses of it in config.c check to see if there was an err before checking errno, so it is safe to assume errno is set correctly.  In my case, it is not.  On my test system, it worked because there weren't any errors before I checked.  But on the two very different production Linux servers I use (Ubuntu 10.10 and CentOS 5.6), something is generating a \"File/Directory not found\" (errno == 2) before it gets to my code and thus it fails every time.  Here is the very simple fix.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 6,
        "changed_files": 5,
        "created_at": "2011-10-18T21:04:14Z",
        "closed_at": "2011-10-31T10:11:18Z",
        "merged_at": "2011-10-31T10:11:18Z",
        "body": "Hi,\n\nAbout that feature request I did, to be able to setup ping slave period/interval and replication timeout, I decided to do a small patch to help you out.\n\nbest regards,\n\nHerbert\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 194,
        "changed_files": 5,
        "created_at": "2011-10-18T18:30:05Z",
        "closed_at": "2011-10-20T07:53:23Z",
        "merged_at": "2011-10-20T07:53:23Z",
        "body": "There are a bunch of pretty old documents in the unstable branch's redis root directory. \n- I changed the Google code links to Github issues links.\n- 00-RELEASENOTES deleted (was quite old)\n- Deleted the CLUSTER file, as there is now a much better and more recent version at http://redis.io/topics/cluster-spec\n- Copied over the README from 2.4, that changes tcmalloc info and redis.io link\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2011-10-14T01:26:20Z",
        "closed_at": "2011-10-14T14:40:18Z",
        "merged_at": "2011-10-14T14:40:18Z",
        "body": "Fixed an error or two while reading the redis.conf. Not sure if this is the right branch to merge into, I can resubmit if so.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2011-10-13T14:49:08Z",
        "closed_at": "2011-10-17T10:21:58Z",
        "merged_at": null,
        "body": "I know this isn't the \"redis way\"; but distributions generally dislike the notion of upstream bundling 3rd-party packages. This at least gives them (or people wanting to use the benefits of shared libraries) the option. I don't see any harm in having this in, as long as it doesn't disrupt the ordinary build system.\n\nPossible caveat: jemalloc prefix `--with-jemalloc-prefix=je_`\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 3,
        "changed_files": 6,
        "created_at": "2011-10-09T08:00:15Z",
        "closed_at": "2011-10-10T20:05:17Z",
        "merged_at": "2011-10-10T20:05:17Z",
        "body": "This simply fixes an issue I just opened.  Pretty straightforward.\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2011-10-04T15:01:17Z",
        "closed_at": "2011-10-05T10:49:53Z",
        "merged_at": "2011-10-05T10:49:53Z",
        "body": "Hi Salvatore, \nI did a few fixes to the install script that make it actually work :)\nit set wrong pidfiles and could not stop redis before. sorry :)\n\nanyway these are not the real fixes we wanted, but at least the script is bug free now.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1561,
        "deletions": 552,
        "changed_files": 54,
        "created_at": "2011-09-22T17:00:35Z",
        "closed_at": "2011-09-22T19:30:56Z",
        "merged_at": null,
        "body": "I've been working with redis lately [http://maxpert.tumblr.com/post/10319226886/a-journey-to-center-of-redis blog post describes the whole problem and reasons], so while working with some of my problems I've added this new feature in Redis that's configurable (you can enable disable it) and makes Redis do a PUBLISH on key expiration. I would love this feature to be part of upcoming redis versions, so I am starting off with code directly.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-09-01T18:23:59Z",
        "closed_at": "2011-09-20T08:58:55Z",
        "merged_at": "2011-09-20T08:58:55Z",
        "body": "Applied patch from Issue 119 provided by \npahowes@gmail.com\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-08-26T07:40:33Z",
        "closed_at": "2011-09-20T09:00:58Z",
        "merged_at": "2011-09-20T09:00:58Z",
        "body": "",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-08-25T05:35:48Z",
        "closed_at": "2011-09-14T13:27:47Z",
        "merged_at": "2011-09-14T13:27:47Z",
        "body": "Added a small note about Tcl in the README\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2011-08-22T16:19:43Z",
        "closed_at": "2017-07-15T15:47:33Z",
        "merged_at": null,
        "body": "This should allow the user to uninstall redis via `make uninstall`. I find this really useful while installing/uninstalling redis from source.\n\nThx for your work.\n\nRegards\nPhilipp\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2011-08-12T12:59:51Z",
        "closed_at": "2011-09-20T09:09:18Z",
        "merged_at": null,
        "body": "currently the INFO command shows the values for\n- hash_max_zipmap_entries\n- hash_max_zipmap_value\n\nthis commit add printing of the following values\n- list-max-ziplist-entries:512\n- list-max-ziplist-value:64\n- set-max-intset-entries:512\n- zset-max-ziplist-entries:128\n- zset-max-ziplist-value:64\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2011-08-02T13:08:11Z",
        "closed_at": "2011-09-20T08:55:35Z",
        "merged_at": "2011-09-20T08:55:35Z",
        "body": "Hello, this is regarding issue 620 http://code.google.com/p/redis/issues/detail?id=620\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-07-27T04:11:20Z",
        "closed_at": "2011-08-02T01:04:26Z",
        "merged_at": null,
        "body": "Give an error message back to the client if getcwd which is part of configGetCommand fails.\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 17,
        "changed_files": 6,
        "created_at": "2011-07-26T03:21:49Z",
        "closed_at": "2011-09-20T09:12:14Z",
        "merged_at": "2011-09-20T09:12:14Z",
        "body": "Before I start looking at the OS X performance ...\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 569,
        "deletions": 145,
        "changed_files": 30,
        "created_at": "2011-07-15T13:46:26Z",
        "closed_at": "2011-09-20T09:10:09Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 128,
        "deletions": 122,
        "changed_files": 1,
        "created_at": "2011-07-13T15:26:27Z",
        "closed_at": "2017-07-15T15:48:17Z",
        "merged_at": null,
        "body": "Basically, exactly what it says on the tin. I went through the redis.conf file and corrected spelling errors, cleaned up some of the sentence structure, and made the use of punctuation more consistent (i.e. config directives and values always use double quotes, lists of possible options always have colons followed by complete sentences, etc etc).\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2011-07-13T01:51:30Z",
        "closed_at": "2011-07-13T10:00:28Z",
        "merged_at": "2011-07-13T10:00:28Z",
        "body": "",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2011-07-07T22:40:46Z",
        "closed_at": "2011-09-20T09:25:44Z",
        "merged_at": null,
        "body": "Hello!\n\nI have a problem in production with restarting redis.\nWe ask redis to stop, and it stops (pid file is absent), but when we start new redis instance it cannot bind to a port (port is still open). Seems like we launch new redis instance during this short time - between unlinking pid file and calling exit(0). Of course we could put sleep(1) into our restarting code but I think it's better to clean-up stuff in redis.\n\nIn this change I'm closing listening sockets in prepareForShutdown function.\nSeems like it resolves our issue.\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2011-07-06T21:35:36Z",
        "closed_at": "2011-11-16T17:18:14Z",
        "merged_at": "2011-11-16T17:18:14Z",
        "body": "Fix FTBFS on kfreebsd.\nSupport kqueue under kfreebsd.\n\nhttp://bugs.debian.org/632499\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2011-07-03T12:12:30Z",
        "closed_at": "2011-07-06T11:04:03Z",
        "merged_at": null,
        "body": "See the discussion on the mailing list (Redis crash with 'appendonly yes' and message queue using lpush/brpop). It's a small change, but fixes at least two issues related to the combination of expiration and append-only files.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2011-07-02T22:17:12Z",
        "closed_at": "2011-07-06T21:34:35Z",
        "merged_at": null,
        "body": "This might break someone using some specialist libc under linux (dietlibc?)\nbut they probably have bigger problems.\n\nAlso add support for kqueue under this architecture.\n\nSigned-off-by: Chris Lamb lamby@debian.org\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 13,
        "changed_files": 5,
        "created_at": "2011-06-28T18:30:38Z",
        "closed_at": "2014-05-28T21:18:46Z",
        "merged_at": null,
        "body": "Several comment fixes and one logged string fix.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 4,
        "changed_files": 6,
        "created_at": "2011-06-23T07:16:06Z",
        "closed_at": "2017-07-15T15:49:41Z",
        "merged_at": null,
        "body": "The first four commits are from my work on stable, the last is a bug in unstable.\n\nHaiku OS: http://haiku-os.org/\n\nPS: If I screwed up by forgetting to do a topic branch lemme know how to transpose these commits there, thanks.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 225,
        "deletions": 97,
        "changed_files": 10,
        "created_at": "2011-06-18T19:51:19Z",
        "closed_at": "2013-07-11T15:15:05Z",
        "merged_at": null,
        "body": "This branch builds on the commits in pull request #60 to add IPv6 support, unfortunately I can't work out how to have github add a pull request against another pull request so unfortunately this one includes the commits from the addrinfo branch in pull request #60. If #60 is pulled first I can close and re-submit the this pull request, provided theres interest in it of course.\n\nThere is currently one part of redis which I've not yet touched with respect to IPv6 as I wanted to know how best to proceed.\n\nThe `anetTcpServer` function is currently used to create an IPv4 listening socket. This could be changed to create an IPv6 socket which then disables `V6ONLY` socket option so that it accepts IPv4 connections as well, seen in netstat as tcp46.\n\nSome operating systems by default require separate stack sockets, such that you have to create an IPv4 socket and an IPv6 socket. These operating systems can usually be configured to allow dual-stack sockets, but it does require additional work by the administrator.\n\nI believe separate sockets would be the desired implementation in redis. Were this the case then `anetTcpServer` would continue to return IPv4 sockets and another function `anetTcp6Server` would return IPv6 sockets. The IPv6 socket would then sit along side the `ipfd` descriptor as `ip6fd` or something similar.\n\nDoes this seem acceptable?\n",
        "comments": 20
    },
    {
        "merged": false,
        "additions": 117,
        "deletions": 75,
        "changed_files": 8,
        "created_at": "2011-06-18T19:04:54Z",
        "closed_at": "2014-05-30T22:51:21Z",
        "merged_at": null,
        "body": "Using getaddrinfo(3) simplifies the socket creation process and is also required for supporting IPv6. The IP address formatting and parsing functions inet_ntop(3) and inet_pton(3) support both IPv4 and IPv6 string formats.\n\nI'm also working on an ipv6 branch which depends on this branch, I've decided to separate them as the addrinfo code should be of use even without ipv6 support.\n\nThe `make test` suite currently passes.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-06-18T05:21:46Z",
        "closed_at": "2012-04-07T13:10:38Z",
        "merged_at": null,
        "body": "",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-06-13T06:01:31Z",
        "closed_at": "2011-06-14T08:26:43Z",
        "merged_at": "2011-06-14T08:26:43Z",
        "body": "I think this was how it was intended. Just a small typo in the growth calculation. Ignore if intended or already fixed.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2408,
        "deletions": 0,
        "changed_files": 17,
        "created_at": "2011-06-06T00:40:40Z",
        "closed_at": "2011-09-20T09:41:12Z",
        "merged_at": null,
        "body": "Ported server to Ruby.  Ruby 1.9.2-p180 recommended.\n\nmake a src/redis-server to hook up TCL tests:\n\n> # !/usr/bin/env ruby\n> \n> load File.expand_path '../../bin/ruby-redis', **FILE**\n\nAlso: https://github.com/dturnbull/ruby-redis\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-06-01T11:15:53Z",
        "closed_at": "2017-07-07T15:08:38Z",
        "merged_at": null,
        "body": "just quick one, added a check in fmacros to check for NetBSD\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 34830,
        "deletions": 15,
        "changed_files": 158,
        "created_at": "2011-05-14T18:08:36Z",
        "closed_at": "2017-07-15T15:49:58Z",
        "merged_at": null,
        "body": "Hi @antirez,\nI noticed I had done my scripting branch improvements on the wrong branch, and so I took a couple minutes to re-commit them against your scripting branch built from unstable.  I will be easier for you to do some selective merging.\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-05-10T16:55:26Z",
        "closed_at": "2012-03-10T14:05:30Z",
        "merged_at": null,
        "body": "This is pretty minor and probably obvious to most people, but I wasted some time today on an install because of this.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 35486,
        "deletions": 405,
        "changed_files": 187,
        "created_at": "2011-05-10T14:28:43Z",
        "closed_at": "2011-05-14T18:19:43Z",
        "merged_at": null,
        "body": "compiles Lua with cjson module built in.\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-04-29T09:58:35Z",
        "closed_at": "2011-09-08T20:14:16Z",
        "merged_at": null,
        "body": "Wrong variable was being used for the stop function in the init script. Should be /proc/{PID} where PID is the actual PID inside the .pid file.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2011-04-20T00:04:07Z",
        "closed_at": "2017-07-15T15:29:48Z",
        "merged_at": null,
        "body": "A simple patch that adds supported for evicted key count on a per database level and with outputs to INFO\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2011-04-18T16:36:18Z",
        "closed_at": "2014-05-30T22:50:32Z",
        "merged_at": null,
        "body": "Hi,\n\nI've just tried to execute make and had an error about libtool on my MacBook Pro (10.7.0).\n\nFixed the libtool error for Mac Os X by updating the Makefile of hiredis.\nSrc: http://code.google.com/p/redis/issues/detail?id=443\n\nThank you for Redis!\n\nBest regards,\nPierre Urban\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-04-11T18:14:32Z",
        "closed_at": "2017-07-15T15:31:26Z",
        "merged_at": null,
        "body": "I had to change the redis-benchmark target slightly to successfully build against tcmalloc. I don't actually use redis-benchmark, but perhaps this is a problem for others attempting to use redis-server with tcmalloc.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 71,
        "deletions": 58,
        "changed_files": 3,
        "created_at": "2011-03-24T02:09:15Z",
        "closed_at": "2014-05-30T22:47:24Z",
        "merged_at": null,
        "body": "Use flags to identify if a command is mutable.\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 46788,
        "deletions": 10033,
        "changed_files": 325,
        "created_at": "2011-03-21T15:35:25Z",
        "closed_at": "2014-05-30T22:47:02Z",
        "merged_at": null,
        "body": "We needed to append to a set the content of anothers and, as there isn't an \"append\" command, we used the sunionstore. The problem was that as the destination set grew, the sunionstore started to take more time as it has to remove all the elements of the set and add them to the destination, which is inneficient if the destination set is one of those involved in the union.\n\nThis detects when the destination key is one of the source keys and, in this case, if the set is not empty it is used as the destination set instead of creating a new one.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 153,
        "deletions": 157,
        "changed_files": 11,
        "created_at": "2011-02-22T00:13:53Z",
        "closed_at": "2014-05-30T22:46:02Z",
        "merged_at": null,
        "body": "- Fixed some typos (spelling mistakes).\n- Added .md extension to doc files.\n- Added some Markdown formatting. (For example, using `` around code or commands puts it in a monospace font in a slightly different color).\n\nCompare the README at https://github.com/veganstraightedge/redis to https://github.com/antirez/redis\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-02-17T10:46:22Z",
        "closed_at": "2014-05-30T22:45:34Z",
        "merged_at": null,
        "body": "Nothing more, nothing less.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2011-02-17T00:17:13Z",
        "closed_at": "2012-07-25T02:23:53Z",
        "merged_at": null,
        "body": "simplest pull request ever! the googlecode.com URL just says \"we've moved!\" and sends you to redis.io, so, little README update.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 313,
        "deletions": 50,
        "changed_files": 8,
        "created_at": "2011-02-05T00:49:20Z",
        "closed_at": "2014-05-30T22:45:19Z",
        "merged_at": null,
        "body": "Based on the \"BPOP\" commands this adds a generic locking command (similar to advisory locks in mysql).\n\nBasic usage is\n\n> C1: GRAB sesskey.AE123 0\n> \n> C1: OK\n> \n> C2: GRAB sesskey.AE123 0\n> \n> C1: RELEASE sesskey.AE123\n> \n> C1: OK\n> \n> C2: OK\n\nAlso if a client disconnects all of it's locks are released.\n\nA client MAY lock multiple keys.\n\nSome things I don't like about how it's implemented right now.\nI've kludged in some \"reserving\" of the Key in (doing a dbAdd of a 0 value) and checking to make sure it's a REDIS_STRING (so as to not conflict with BPOP). However this has a few issues.\n\nthe lock does nothing to prevent working with the actual key (and really shouldn't). So another client can delete and readd the key as a REDIS_LIST and screw things up.\n\nIdeally I need to create a separate blocking system from the BPOP blocks, or adjust how they are stored so that BPOP will ignore GRAB blocks and GRAB will ignore BPOP locks.\n\nAny thoughts on this are very welcome.\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 535,
        "deletions": 299,
        "changed_files": 26,
        "created_at": "2011-02-05T00:46:56Z",
        "closed_at": "2011-02-05T08:48:00Z",
        "merged_at": null,
        "body": "Based on the \"BPOP\" commands this adds a generic locking command (similar to advisory locks in mysql).\n\nBasic usage is\n\n> C1: GRAB sesskey.AE123 0\n> \n> C1: OK\n> \n> C2: GRAB sesskey.AE123 0\n> \n> C1: RELEASE sesskey.AE123\n> \n> C1: OK\n> \n> C2: OK\n\nAlso if a client disconnects all of it's locks are released.\n\nA client MAY lock multiple keys.\n\nSome things I don't like about how it's implemented right now.\nI've kludged in some \"reserving\" of the Key in (doing a dbAdd of a 0 value) and checking to make sure it's a REDIS_STRING (so as to not conflict with BPOP). However this has a few issues.\n\nthe lock does nothing to prevent working with the actual key (and really shouldn't). So another client can delete and readd the key as a REDIS_LIST and screw things up.\n\nIdeally I need to create a separate blocking system from the BPOP blocks, or adjust how they are stored so that BPOP will ignore GRAB blocks and GRAB will ignore BPOP locks.\n\nAny thoughts on this are very welcome.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2011-01-25T20:43:57Z",
        "closed_at": "2011-01-30T20:26:09Z",
        "merged_at": null,
        "body": "I've rewritten zslRandomLevel to use two calls (for p=0.25) to random() regardless of the resulting node level.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1225,
        "deletions": 6,
        "changed_files": 25,
        "created_at": "2011-01-22T16:54:21Z",
        "closed_at": "2011-01-24T17:06:52Z",
        "merged_at": null,
        "body": "",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1207,
        "deletions": 342,
        "changed_files": 30,
        "created_at": "2011-01-05T14:00:55Z",
        "closed_at": "2011-03-07T11:59:34Z",
        "merged_at": null,
        "body": "The current behavior of redis-cli's REPL is quite dangerous when using the SELECT command:\n\n```\nredis> set \"in0\" \"foo\"\nOK\nredis> keys *\n1) \"in0\"\nredis> select 1\nOK\nredis> set \"in1\" \"bar\"\nOK\nredis> keys *\n1) \"in1\"\n# Here, wait a few minutes until you are disconnected...\nredis> keys *\n1) \"in0\" # Oops...\n```\n\nNow imagine the last `keys *` was `flushdb` instead: you just wiped DB 0 when you wanted to wipe DB 1, and you didn't even notice.\n\nThis commit fixes this by acknowledging in redis-cli itself the fact that the user works in the context of DB 1. This way, when reconnecting, redis-cli will also perform a select.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1490,
        "deletions": 274,
        "changed_files": 47,
        "created_at": "2011-01-03T19:41:34Z",
        "closed_at": "2011-11-18T14:58:21Z",
        "merged_at": null,
        "body": "NOTE: pull request claims I want this into master, but it's actually for 2.2, and only one commit\n\nfix portability of printf when size_t and long have different sizes\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 143,
        "deletions": 120,
        "changed_files": 3,
        "created_at": "2011-01-03T00:28:21Z",
        "closed_at": "2014-07-31T17:24:37Z",
        "merged_at": null,
        "body": "Sorry about the previous mess, this should be better.  And it correctly clears the counts from CONFIG RESETSTAT now, too.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 126,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2010-12-30T19:21:23Z",
        "closed_at": "2011-09-13T23:29:01Z",
        "merged_at": null,
        "body": "Hi Salvatore,\n\nAt my company we are experimenting with weighted queues, and one of our ideas was to use a Redis zset.  We looked at the ZPOP example that used WATCH, but we'd have high contention as we'll have dozens of workers querying against it.  We were quite excited to see that it would be pretty easy to implement ZPOP/ZREVPOP on the server, so I spent an afternoon last week putting it in.  We have had success with it in our testing so I figured you might be interested in the branch...\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 38,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2010-12-22T12:43:38Z",
        "closed_at": "2011-11-18T15:00:01Z",
        "merged_at": null,
        "body": "This returns the number of the matching key patterns.\n",
        "comments": 19
    },
    {
        "merged": true,
        "additions": 70,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2010-12-09T16:17:53Z",
        "closed_at": "2010-12-14T16:54:05Z",
        "merged_at": "2010-12-14T16:54:05Z",
        "body": "Per discussion, this patch adds support for logging to syslog with the option to configure the syslog identification (syslog-ident [default=redis]) and facility (syslog-facility [default=local0]).  syslog is disabled by default (syslog-enabled).  The configuration naming follows syslog-*, as it appears to be the newer style.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 210,
        "deletions": 30,
        "changed_files": 4,
        "created_at": "2010-12-08T21:12:56Z",
        "closed_at": "2010-12-09T15:35:48Z",
        "merged_at": null,
        "body": "This branch contains support for multiple simultaneous log destinations (file, file+stdout, stdout+syslog) and additionally includes added support for logging to stderr and syslog.\n\nWhile the configuration values have changed, the default behavior (logging to stdout) has been maintained.\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 66,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2010-11-27T02:16:16Z",
        "closed_at": "2017-07-03T11:22:19Z",
        "merged_at": null,
        "body": "Hello Antirez/Salvatore,\n\nI have made a modification to the rpoplpush command, in the modification i am allowing a third optional parameters which will provide the following behavior:\n\nallow as an option a third argument which would be pushed as new element\non the destination list  instead of pushing the popped element.\n\n  RPOPLPUSH srclist dstlist element2:\n\n   IF LLEN(srclist) > 0\n     element = RPOP srclist\n     LPUSH dstlist element2\n     RETURN element\n   ELSE\n     RETURN nil\n   END\n  END\n\nThis is very useful in my opinion to provide a way to atomically remove an element from a list and pass a new one to another list.\n\nLet me provide an example.\n\nScript A receive an element to be processed on list_A.\n\nScript a uses the traditional rpoplpush this way element = rpoplpush list_A list_A_processing.\n\nThe script is processing element, and modifying it.\n\nOnce the modification is done we need to atomically remove the element from list_A_processing and pass the modified element to another list so that script B can process the newly modified element.\n\nSo script will use the modified way of rpoplpush this way. rpoplpush list_A_processing list_B newelement\n\nThis is the only way in my understanding to pass the element to the other script (using redis of course) and at the same time remove it from the processing list without risking to break the atomicity of this action.\n\nImagine the script is killed in the middle of this operation (pop and then push, or push and then pop) we would have an issue of integrity.\n\nI know this will affect brpoplpush but I think it will also be beneficial to it.\n\nBy the way I am not an expert in git/github so i dont know how to remove from this pull request the other pull request i have made regarding the list command.\n\nThanks in advance,\nKader\n\nPython Example:\n\n> > > import desir\n> > > \n> > > r=desir.Redis()\n> > > \n> > > r.keys(\"*\")\n> > > \n> > > []\n> > > \n> > > r.lpush(\"list1\",\"message 1\")\n> > > \n> > > 1\n> > > \n> > > r.rpoplpush(\"list1\",\"list2\")\n> > > \n> > > 'message 1'\n> > > \n> > > r.rpoplpush(\"list2\",\"list1\")\n> > > \n> > > 'message 1'\n> > > \n> > > r.rpoplpush(\"list1\",\"list2\",\"message 2\")\n> > > \n> > > 'message 1'\n> > > \n> > > r.rpoplpush(\"list2\",\"list1\")\n> > > \n> > > 'message 2'\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3290,
        "deletions": 1341,
        "changed_files": 52,
        "created_at": "2010-11-21T14:40:58Z",
        "closed_at": "2011-04-22T17:47:02Z",
        "merged_at": null,
        "body": "Description:  \nAdd support for DESTDIR and use `install` instead of `cp` / `mkdir`.\nAlso remove useless variable definitions, define PROGRAMS as the set\nof binaries we're building and use $@ instead of the target name\ndirectly.\n\nI just pushed an updated version of the patch, rebased on top of the latest master.\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 59,
        "deletions": 42,
        "changed_files": 1,
        "created_at": "2010-11-16T16:26:42Z",
        "closed_at": "2014-05-30T22:40:48Z",
        "merged_at": null,
        "body": "Hello,\n\nI have added new cmdline switch -b (bare format) to the redis-cli. It then prints out results in \"bare format\", which is almost raw, except there are newlines after each reply/set/list entry etc. This format is easily parsable by e.g. shell scripts, where you can read things line by line. \n\nI have also changed help printout: now it prints help also on help command (& doesn't try to connect to redis first) and I have also replaced -iv with -vb in the usage(), so that it doesn't list deprecated switch anymore, but the new one instead.\n\nSee the changes yourself. I hope redis-cli will finally have parsable output...\n\nBTW., there's maybe a bug in the new redis-cli implementation (that of 2.0.4 works fine) - when I have multi line string and retrieve it via get, it doesn't print \\n instead of newlines, but only \\ (maybe this is intention)\n\nBest regards,\n\nm. tekel\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2010-11-12T01:51:10Z",
        "closed_at": "2011-05-22T12:10:25Z",
        "merged_at": null,
        "body": "",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 50,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2010-11-06T20:44:30Z",
        "closed_at": "2014-05-30T22:37:41Z",
        "merged_at": null,
        "body": "sorry \nupdated typo for the previous pull request\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 66,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2010-11-06T20:38:33Z",
        "closed_at": "2011-11-18T15:01:21Z",
        "merged_at": null,
        "body": "Hello Antirez,\n\nI have added the listcommands command which provides a list of the available commands plus some parameters (arity, flags, firstkey,lastkey,keystep).\n\nThis command would help me for the new redis python client i am currently writing ( https://github.com/aallamaa/desir ) in order to avoid copy/pasting readonlyCommandTable inside my python code to get the list of available commands.\n\nThanks in advance,\nKader\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 1,
        "changed_files": 4,
        "created_at": "2010-10-31T17:47:07Z",
        "closed_at": "2011-02-05T19:13:27Z",
        "merged_at": null,
        "body": "maybe this is a bad idea, but i was reading [this article](http://www.pauladamsmith.com/articles/redis_under_the_hood.html), and he suggests,\n\n> ... give users the opportunity, based on knowledge of their applications and needs, to increase the number of shared integers for greater memory savings.\n\nalso, i noticed in your [redis weekly update no. 4](http://antirez.com/m/p.php?i=209) concerning shared integers, that you mention,\n\n> If you have many small counters, or in general many numbers in this range, this will be a huge win memory-wise. To enlarge the rage there is just to change a define.\n\nso, for fun and experience, i thought i'd add this in. the following commit seems to do it. decreasing the number seems to free up memory. however, if i increase it to more that 10850, i get a signal 10 bus error. i'm not sure why that is. any idea? i don't know much about c programming.\n\nthanks\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 256,
        "deletions": 24,
        "changed_files": 7,
        "created_at": "2010-10-30T14:56:18Z",
        "closed_at": "2015-01-05T17:49:43Z",
        "merged_at": null,
        "body": "Sample input/output\n\n<pre>\nredis> set a\n(error) ERR wrong number of arguments for 'set' command\nredis> set a b\nOK\nredis> set b c \nOK\nredis> get a\n\"b\"\n$0 = 'b'\nredis> get $0\n\"c\"\n$1 = 'c'\nredis> get $1\n(nil)\n\nredis> sadd set:a 1\n(integer) 1\n$2 = '1'\nredis> sadd set:a 2\n(integer) 1\n$3 = '1'\nredis> sadd set:a 3\n(integer) 1\n$4 = '1'\nredis> sadd set:b 2\n(integer) 1\n$5 = '1'\nredis> sadd set:b 3\n(integer) 1\n$6 = '1'\nredis> sadd set:b 4\n(integer) 1\n$7 = '1'\nredis> sadd set set:a\n(integer) 1\n$8 = '1'\nredis> sadd set set:b\n(integer) 1\n$9 = '1'\nredis> smembers set\n1. \"set:a\"\n2. \"set:b\"\n$10 = 'set:a set:b'\nredis> sinter $10\n1. \"2\"\n2. \"3\"\n$11 = '2 3'\nredis> sunion $10\n1. \"1\"\n2. \"2\"\n3. \"3\"\n4. \"4\"\n$12 = '1 2 3 4'\nredis> \n</pre>\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 131,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2010-10-29T06:57:21Z",
        "closed_at": "2014-07-31T17:25:44Z",
        "merged_at": null,
        "body": "Acording to TODO file and http://groups.google.com/group/redis-db/browse_thread/thread/e766d84eb375cd41 implemented msadd.\n\nIt's my first contribution to the project, so let me know if there is anything I did wrong or can improve on the future!\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2010-10-11T09:49:48Z",
        "closed_at": "2010-10-13T04:55:43Z",
        "merged_at": null,
        "body": "maxmemory INFO field added.\nhope it useful.\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2010-10-03T22:50:54Z",
        "closed_at": "2011-12-11T20:03:52Z",
        "merged_at": null,
        "body": "A small one\u2026\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2010-10-03T07:07:45Z",
        "closed_at": "2014-05-30T22:35:05Z",
        "merged_at": null,
        "body": "Fixed a bunch of typos / gramatical errors while reading this.\n\nUnrelated to my change:\nI'm confused by the proxy's design. If it's just sending the writes to all 3 nodes in parallel, you'll have race conditions that cause inconsistent data. For instance, in a cluster with two hosts and M=2,  and two proxies:\nProxy A is asked to SET foo LEFT\nProxy B is asked to SET foo RIGHT\nProxy A tells Host 1 to SET foo LEFT\nProxy B tells Host 1 to SET foo RIGHT\nProxy B tells Host 2 to SET foo RIGHT\nProxy A tells Host 2 to SET foo LEFT\n\nIn the end Host 1 has foo=RIGHT, Host 2 has foo=LEFT. With random read balancing you'll get random responses for the same key.\n\nAm I misunderstanding the design doc?\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2010-09-06T17:28:01Z",
        "closed_at": "2014-07-15T09:49:56Z",
        "merged_at": null,
        "body": "The SORT command used the dictionary to build up the vector from a ZSET,\nbut it should use the skiplist instead, elsewise its not possible to create a SORT in the order of the sorted set (which is usually what you want)\n",
        "comments": 0
    }
]